<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	
	xmlns:georss="http://www.georss.org/georss"
	xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#"
	
	>
<channel>
	<title>
	Comments on: Upgrading to Barracuda &#038; getting rid of huge ibdata1 file	</title>
	<atom:link href="https://shlomi-noach.github.io/blog/mysql/upgrading-to-barracuda-getting-rid-of-huge-ibdata1-file/feed" rel="self" type="application/rss+xml" />
	<link>https://shlomi-noach.github.io/blog/mysql/upgrading-to-barracuda-getting-rid-of-huge-ibdata1-file</link>
	<description>Blog by Shlomi Noach</description>
	<lastBuildDate>Tue, 28 May 2013 05:12:15 +0000</lastBuildDate>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=5.3.3</generator>
			<item>
				<title>
				By: shlomi				</title>
				<link>https://shlomi-noach.github.io/blog/mysql/upgrading-to-barracuda-getting-rid-of-huge-ibdata1-file/comment-page-1#comment-209893</link>
		<dc:creator><![CDATA[shlomi]]></dc:creator>
		<pubDate>Tue, 28 May 2013 05:12:15 +0000</pubDate>
		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=3304#comment-209893</guid>
					<description><![CDATA[I&#039;ve never set the &quot;innodb_file_format_max&quot; so apparently not necessary. From the docs:

&quot;... If the server creates or opens a table with a ‚Äúhigher‚Äù file format, it sets the value of innodb_file_format_max to that format. &quot;]]></description>
		<content:encoded><![CDATA[<p>I&#8217;ve never set the &#8220;innodb_file_format_max&#8221; so apparently not necessary. From the docs:</p>
<p>&#8220;&#8230; If the server creates or opens a table with a ‚Äúhigher‚Äù file format, it sets the value of innodb_file_format_max to that format. &#8220;</p>
]]></content:encoded>
						</item>
						<item>
				<title>
				By: Avi Vainshtein				</title>
				<link>https://shlomi-noach.github.io/blog/mysql/upgrading-to-barracuda-getting-rid-of-huge-ibdata1-file/comment-page-1#comment-209754</link>
		<dc:creator><![CDATA[Avi Vainshtein]]></dc:creator>
		<pubDate>Mon, 27 May 2013 15:25:57 +0000</pubDate>
		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=3304#comment-209754</guid>
					<description><![CDATA[Many thanks for the post, it&#039;s very useful.

Is it necessary to set both the innodb_file_format and innodb_file_format_max to Barracuda in the my.cnf ?

best regards,
A.V.]]></description>
		<content:encoded><![CDATA[<p>Many thanks for the post, it&#8217;s very useful.</p>
<p>Is it necessary to set both the innodb_file_format and innodb_file_format_max to Barracuda in the my.cnf ?</p>
<p>best regards,<br />
A.V.</p>
]]></content:encoded>
						</item>
						<item>
				<title>
				By: Getting rid of huge ibdata file, no dump required &#124; code.openark.org				</title>
				<link>https://shlomi-noach.github.io/blog/mysql/upgrading-to-barracuda-getting-rid-of-huge-ibdata1-file/comment-page-1#comment-93080</link>
		<dc:creator><![CDATA[Getting rid of huge ibdata file, no dump required &#124; code.openark.org]]></dc:creator>
		<pubDate>Tue, 22 May 2012 05:33:22 +0000</pubDate>
		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=3304#comment-93080</guid>
					<description><![CDATA[[...] Upgrading to Barracuda &#038; getting rid of huge ibdata1 file [...]]]></description>
		<content:encoded><![CDATA[<p>[&#8230;] Upgrading to Barracuda &amp; getting rid of huge ibdata1 file [&#8230;]</p>
]]></content:encoded>
						</item>
						<item>
				<title>
				By: Ronald Bradford				</title>
				<link>https://shlomi-noach.github.io/blog/mysql/upgrading-to-barracuda-getting-rid-of-huge-ibdata1-file/comment-page-1#comment-30938</link>
		<dc:creator><![CDATA[Ronald Bradford]]></dc:creator>
		<pubDate>Wed, 16 Feb 2011 22:57:14 +0000</pubDate>
		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=3304#comment-30938</guid>
					<description><![CDATA[Thanks for the post. You reiterated my earlier post about the same topic.

http://ronaldbradford.com/blog/leveraging-the-innodb-plugin-2011-02-11/]]></description>
		<content:encoded><![CDATA[<p>Thanks for the post. You reiterated my earlier post about the same topic.</p>
<p><a href="http://ronaldbradford.com/blog/leveraging-the-innodb-plugin-2011-02-11/" rel="nofollow ugc">http://ronaldbradford.com/blog/leveraging-the-innodb-plugin-2011-02-11/</a></p>
]]></content:encoded>
						</item>
						<item>
				<title>
				By: shlomi				</title>
				<link>https://shlomi-noach.github.io/blog/mysql/upgrading-to-barracuda-getting-rid-of-huge-ibdata1-file/comment-page-1#comment-30889</link>
		<dc:creator><![CDATA[shlomi]]></dc:creator>
		<pubDate>Wed, 16 Feb 2011 14:31:45 +0000</pubDate>
		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=3304#comment-30889</guid>
					<description><![CDATA[Gabi,
I thinks this task is not hard to achieve. It&#039;s a one time job, I&#039;ve done plenty of these before. We can discuss more privately if you like; it&#039;s a bit too lengthy to explain everything here.]]></description>
		<content:encoded><![CDATA[<p>Gabi,<br />
I thinks this task is not hard to achieve. It&#8217;s a one time job, I&#8217;ve done plenty of these before. We can discuss more privately if you like; it&#8217;s a bit too lengthy to explain everything here.</p>
]]></content:encoded>
						</item>
						<item>
				<title>
				By: Gabi Davis				</title>
				<link>https://shlomi-noach.github.io/blog/mysql/upgrading-to-barracuda-getting-rid-of-huge-ibdata1-file/comment-page-1#comment-30886</link>
		<dc:creator><![CDATA[Gabi Davis]]></dc:creator>
		<pubDate>Wed, 16 Feb 2011 13:29:52 +0000</pubDate>
		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=3304#comment-30886</guid>
					<description><![CDATA[Hi shlomi,
Thanks for the timely reply :-)
Our entire dataset is almost double that, the innodb file is 350G on its own... and although two days of traffic is huge there are definitely ways to overcome the disk issue but in general, it&#039;s sad to realize that this is the only way to go. We were hoping to digg deeper and find something else we could do that wouldn&#039;t require a dump and reload of the entire data. 
If we have to go this way we&#039;ll definitely take the opportunity to make some structural changes we had been planning for long and did not run due to the size of the tables and the nature of &#039;alter table&#039;, and probably also upgrade to 5.5.
As for the fast index creation - that is definitely  great, but there&#039;s also the problem of not being able to add/drop columns (and I have to read more about the plugin to see if there&#039;s anything about that).
Thanks again,
Gabi]]></description>
		<content:encoded><![CDATA[<p>Hi shlomi,<br />
Thanks for the timely reply üôÇ<br />
Our entire dataset is almost double that, the innodb file is 350G on its own&#8230; and although two days of traffic is huge there are definitely ways to overcome the disk issue but in general, it&#8217;s sad to realize that this is the only way to go. We were hoping to digg deeper and find something else we could do that wouldn&#8217;t require a dump and reload of the entire data.<br />
If we have to go this way we&#8217;ll definitely take the opportunity to make some structural changes we had been planning for long and did not run due to the size of the tables and the nature of &#8216;alter table&#8217;, and probably also upgrade to 5.5.<br />
As for the fast index creation &#8211; that is definitely  great, but there&#8217;s also the problem of not being able to add/drop columns (and I have to read more about the plugin to see if there&#8217;s anything about that).<br />
Thanks again,<br />
Gabi</p>
]]></content:encoded>
						</item>
						<item>
				<title>
				By: shlomi				</title>
				<link>https://shlomi-noach.github.io/blog/mysql/upgrading-to-barracuda-getting-rid-of-huge-ibdata1-file/comment-page-1#comment-30876</link>
		<dc:creator><![CDATA[shlomi]]></dc:creator>
		<pubDate>Wed, 16 Feb 2011 10:08:23 +0000</pubDate>
		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=3304#comment-30876</guid>
					<description><![CDATA[@Gabi,

When working with a slave, you can get the benefit of using mk-parallel-dump + restore. I&#039;ve had these boost up export/import time by magnitude of 6x.

Assuming your dataset size is indeed 350GB, I would guess this should take a day or two at the most (pure guess, based on experience with commodity hardware?). Why don&#039;t you try offline and see how much time it takes for export/import, without actually taking down a slave? I mean, export from a (stopped) slave, then import on a local office machine; measure the time for export + import. Then you can see if you have enough room to store the binary logs.

Since I&#039;m assuming your dataset always grows, I should add: the sooner the better.

With regard to altering without rewrite of full table: InnoDB plugin&#039;s barracuda format allows for &lt;a href=&quot;https://shlomi-noach.github.io/blog/mysql/reasons-to-use-innodb-plugin&quot; rel=&quot;nofollow&quot;&gt;fast index creation&lt;/a&gt;.
Or you could use Facebook&#039;s OSC, or openark kit&#039;s oak-online-alter-table]]></description>
		<content:encoded><![CDATA[<p>@Gabi,</p>
<p>When working with a slave, you can get the benefit of using mk-parallel-dump + restore. I&#8217;ve had these boost up export/import time by magnitude of 6x.</p>
<p>Assuming your dataset size is indeed 350GB, I would guess this should take a day or two at the most (pure guess, based on experience with commodity hardware?). Why don&#8217;t you try offline and see how much time it takes for export/import, without actually taking down a slave? I mean, export from a (stopped) slave, then import on a local office machine; measure the time for export + import. Then you can see if you have enough room to store the binary logs.</p>
<p>Since I&#8217;m assuming your dataset always grows, I should add: the sooner the better.</p>
<p>With regard to altering without rewrite of full table: InnoDB plugin&#8217;s barracuda format allows for <a href="https://shlomi-noach.github.io/blog/mysql/reasons-to-use-innodb-plugin" rel="nofollow">fast index creation</a>.<br />
Or you could use Facebook&#8217;s OSC, or openark kit&#8217;s oak-online-alter-table</p>
]]></content:encoded>
						</item>
						<item>
				<title>
				By: Gabi Davis				</title>
				<link>https://shlomi-noach.github.io/blog/mysql/upgrading-to-barracuda-getting-rid-of-huge-ibdata1-file/comment-page-1#comment-30867</link>
		<dc:creator><![CDATA[Gabi Davis]]></dc:creator>
		<pubDate>Wed, 16 Feb 2011 09:37:41 +0000</pubDate>
		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=3304#comment-30867</guid>
					<description><![CDATA[Hi shlomi,
Good post.
However, when handling large and very busy databases the restore from dump is not an option. The time to dump and load as well as the storage required for keeping binary log files till this entire operation is over is a kill over.
Are there no other ways to do this? we&#039;re carrying on our back a file of 350G and the only option we currently have is to keep carrying it even though we plan to have no tables left in it in the near future.
Also, are there no plans in the mysql world to finally allow altering tables without a rewrite of the full table? that&#039;s a huge downside of mysql as I see it.]]></description>
		<content:encoded><![CDATA[<p>Hi shlomi,<br />
Good post.<br />
However, when handling large and very busy databases the restore from dump is not an option. The time to dump and load as well as the storage required for keeping binary log files till this entire operation is over is a kill over.<br />
Are there no other ways to do this? we&#8217;re carrying on our back a file of 350G and the only option we currently have is to keep carrying it even though we plan to have no tables left in it in the near future.<br />
Also, are there no plans in the mysql world to finally allow altering tables without a rewrite of the full table? that&#8217;s a huge downside of mysql as I see it.</p>
]]></content:encoded>
						</item>
						<item>
				<title>
				By: shlomi				</title>
				<link>https://shlomi-noach.github.io/blog/mysql/upgrading-to-barracuda-getting-rid-of-huge-ibdata1-file/comment-page-1#comment-30796</link>
		<dc:creator><![CDATA[shlomi]]></dc:creator>
		<pubDate>Tue, 15 Feb 2011 18:18:44 +0000</pubDate>
		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=3304#comment-30796</guid>
					<description><![CDATA[@Dani√´l,
I&#039;m wondering about your NetApp setup. Do you have your different filesystems mapped to different disks on your NetApp?]]></description>
		<content:encoded><![CDATA[<p>@Dani√´l,<br />
I&#8217;m wondering about your NetApp setup. Do you have your different filesystems mapped to different disks on your NetApp?</p>
]]></content:encoded>
						</item>
						<item>
				<title>
				By: Dani√´l van Eeden				</title>
				<link>https://shlomi-noach.github.io/blog/mysql/upgrading-to-barracuda-getting-rid-of-huge-ibdata1-file/comment-page-1#comment-30779</link>
		<dc:creator><![CDATA[Dani√´l van Eeden]]></dc:creator>
		<pubDate>Tue, 15 Feb 2011 13:16:38 +0000</pubDate>
		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=3304#comment-30779</guid>
					<description><![CDATA[@Shlomi

Even InnoDB-only setups do have MyISAM tables for the internal mysql.* tables.

Just doing a dump/restore for InnoDB tables could be nice if there are a few big MyISAM tables.

One of the files you might want to safe is the master.info file if it&#039;s a slave. And it might be nice to safe errorlogs and slow query logs to compare with the situation after the restore.

With NetApp you could use one volume with multiple Qtrees to create atomic snapshots and still use multiple mountpoints.]]></description>
		<content:encoded><![CDATA[<p>@Shlomi</p>
<p>Even InnoDB-only setups do have MyISAM tables for the internal mysql.* tables.</p>
<p>Just doing a dump/restore for InnoDB tables could be nice if there are a few big MyISAM tables.</p>
<p>One of the files you might want to safe is the master.info file if it&#8217;s a slave. And it might be nice to safe errorlogs and slow query logs to compare with the situation after the restore.</p>
<p>With NetApp you could use one volume with multiple Qtrees to create atomic snapshots and still use multiple mountpoints.</p>
]]></content:encoded>
						</item>
			</channel>
</rss>
