<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	
	xmlns:georss="http://www.georss.org/georss"
	xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#"
	
	>
<channel>
	<title>
	Comments on: SQL: selecting top N records per group	</title>
	<atom:link href="https://shlomi-noach.github.io/blog/mysql/sql-selecting-top-n-records-per-group/feed" rel="self" type="application/rss+xml" />
	<link>https://shlomi-noach.github.io/blog/mysql/sql-selecting-top-n-records-per-group</link>
	<description>Blog by Shlomi Noach</description>
	<lastBuildDate>Fri, 28 Jun 2013 21:02:37 +0000</lastBuildDate>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=5.3.3</generator>
			<item>
				<title>
				By: Limit each group in group by &#124; BlogoSfera				</title>
				<link>https://shlomi-noach.github.io/blog/mysql/sql-selecting-top-n-records-per-group/comment-page-1#comment-214142</link>
		<dc:creator><![CDATA[Limit each group in group by &#124; BlogoSfera]]></dc:creator>
		<pubDate>Fri, 28 Jun 2013 21:02:37 +0000</pubDate>
		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=3164#comment-214142</guid>
					<description><![CDATA[[...] think one of these solutions could work, but not how: https://shlomi-noach.github.io/blog/mysql/sql-selecting-top-n-records-per-group [...]]]></description>
		<content:encoded><![CDATA[<p>[&#8230;] think one of these solutions could work, but not how: <a href="https://shlomi-noach.github.io/blog/mysql/sql-selecting-top-n-records-per-group" rel="nofollow ugc">https://shlomi-noach.github.io/blog/mysql/sql-selecting-top-n-records-per-group</a> [&#8230;]</p>
]]></content:encoded>
						</item>
						<item>
				<title>
				By: shlomi				</title>
				<link>https://shlomi-noach.github.io/blog/mysql/sql-selecting-top-n-records-per-group/comment-page-1#comment-207061</link>
		<dc:creator><![CDATA[shlomi]]></dc:creator>
		<pubDate>Thu, 16 May 2013 05:14:38 +0000</pubDate>
		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=3164#comment-207061</guid>
					<description><![CDATA[@Erich,

Yes, the use of commas assume no commas in values; I also sometimes use char(0) or char(9) which nobody uses within texts]]></description>
		<content:encoded><![CDATA[<p>@Erich,</p>
<p>Yes, the use of commas assume no commas in values; I also sometimes use char(0) or char(9) which nobody uses within texts</p>
]]></content:encoded>
						</item>
						<item>
				<title>
				By: Erich				</title>
				<link>https://shlomi-noach.github.io/blog/mysql/sql-selecting-top-n-records-per-group/comment-page-1#comment-206574</link>
		<dc:creator><![CDATA[Erich]]></dc:creator>
		<pubDate>Tue, 14 May 2013 09:19:07 +0000</pubDate>
		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=3164#comment-206574</guid>
					<description><![CDATA[this is just what i needed - i suggest using char(1) rather the &#039;,&#039; tho]]></description>
		<content:encoded><![CDATA[<p>this is just what i needed &#8211; i suggest using char(1) rather the &#8216;,&#8217; tho</p>
]]></content:encoded>
						</item>
						<item>
				<title>
				By: MySQL/QueryScript use case: DELETE all but top N records per group &#124; code.openark.org				</title>
				<link>https://shlomi-noach.github.io/blog/mysql/sql-selecting-top-n-records-per-group/comment-page-1#comment-69272</link>
		<dc:creator><![CDATA[MySQL/QueryScript use case: DELETE all but top N records per group &#124; code.openark.org]]></dc:creator>
		<pubDate>Thu, 09 Feb 2012 08:33:17 +0000</pubDate>
		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=3164#comment-69272</guid>
					<description><![CDATA[[...] SQL: selecting top N records per group [...]]]></description>
		<content:encoded><![CDATA[<p>[&#8230;] SQL: selecting top N records per group [&#8230;]</p>
]]></content:encoded>
						</item>
						<item>
				<title>
				By: shlomi				</title>
				<link>https://shlomi-noach.github.io/blog/mysql/sql-selecting-top-n-records-per-group/comment-page-1#comment-63093</link>
		<dc:creator><![CDATA[shlomi]]></dc:creator>
		<pubDate>Thu, 15 Dec 2011 16:55:42 +0000</pubDate>
		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=3164#comment-63093</guid>
					<description><![CDATA[@Keith,
Yes, that&#039;s possible; there&#039;s a balance one would want to maintain. It&#039;s easy to store 1000 rows in a table, and is faster (verify) than doing 3 joins of 10 rows. Storing 100000 is not as efficient, and joins are required.]]></description>
		<content:encoded><![CDATA[<p>@Keith,<br />
Yes, that&#8217;s possible; there&#8217;s a balance one would want to maintain. It&#8217;s easy to store 1000 rows in a table, and is faster (verify) than doing 3 joins of 10 rows. Storing 100000 is not as efficient, and joins are required.</p>
]]></content:encoded>
						</item>
						<item>
				<title>
				By: Keith				</title>
				<link>https://shlomi-noach.github.io/blog/mysql/sql-selecting-top-n-records-per-group/comment-page-1#comment-63065</link>
		<dc:creator><![CDATA[Keith]]></dc:creator>
		<pubDate>Thu, 15 Dec 2011 14:40:39 +0000</pubDate>
		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=3164#comment-63065</guid>
					<description><![CDATA[Hi

Useful idea.

One minor suggestion would be to use a table with the numbers 0 to 9 which can then be repeatedly cross joined against itself to produce as big an integer as required.

Maybe a touch less efficient but more flexible and can then easily be used on several different queries.]]></description>
		<content:encoded><![CDATA[<p>Hi</p>
<p>Useful idea.</p>
<p>One minor suggestion would be to use a table with the numbers 0 to 9 which can then be repeatedly cross joined against itself to produce as big an integer as required.</p>
<p>Maybe a touch less efficient but more flexible and can then easily be used on several different queries.</p>
]]></content:encoded>
						</item>
						<item>
				<title>
				By: shlomi				</title>
				<link>https://shlomi-noach.github.io/blog/mysql/sql-selecting-top-n-records-per-group/comment-page-1#comment-26849</link>
		<dc:creator><![CDATA[shlomi]]></dc:creator>
		<pubDate>Mon, 10 Jan 2011 04:03:53 +0000</pubDate>
		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=3164#comment-26849</guid>
					<description><![CDATA[@januzi,
I&#039;m not sure what it is you&#039;re trying to avoid or what your situation is.
The query *requests* that you scan the entire table. It says: &quot;for all rows in the table, group them and sort them within the groups&quot;.
It&#039;s true that eventually we then only use top n rows per group. If your groups are small, this makes little difference. If your groups are large, it does.]]></description>
		<content:encoded><![CDATA[<p>@januzi,<br />
I&#8217;m not sure what it is you&#8217;re trying to avoid or what your situation is.<br />
The query *requests* that you scan the entire table. It says: &#8220;for all rows in the table, group them and sort them within the groups&#8221;.<br />
It&#8217;s true that eventually we then only use top n rows per group. If your groups are small, this makes little difference. If your groups are large, it does.</p>
]]></content:encoded>
						</item>
						<item>
				<title>
				By: januzi				</title>
				<link>https://shlomi-noach.github.io/blog/mysql/sql-selecting-top-n-records-per-group/comment-page-1#comment-26811</link>
		<dc:creator><![CDATA[januzi]]></dc:creator>
		<pubDate>Sun, 09 Jan 2011 16:45:14 +0000</pubDate>
		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=3164#comment-26811</guid>
					<description><![CDATA[So, the most reasonable solution is to save the query result in the cache (secondary table, memcached, file, etc). I&#039;ll check that last query. Maybe it will give me less than 80k+ Rows_examined (as mysql+percona log patch says in the slow query log).]]></description>
		<content:encoded><![CDATA[<p>So, the most reasonable solution is to save the query result in the cache (secondary table, memcached, file, etc). I&#8217;ll check that last query. Maybe it will give me less than 80k+ Rows_examined (as mysql+percona log patch says in the slow query log).</p>
]]></content:encoded>
						</item>
						<item>
				<title>
				By: shlomi				</title>
				<link>https://shlomi-noach.github.io/blog/mysql/sql-selecting-top-n-records-per-group/comment-page-1#comment-26768</link>
		<dc:creator><![CDATA[shlomi]]></dc:creator>
		<pubDate>Sun, 09 Jan 2011 06:47:00 +0000</pubDate>
		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=3164#comment-26768</guid>
					<description><![CDATA[We could do a contest: who gets as many &quot;Extra&quot; comments in a single query :)

There&#039;s no avoiding the &quot;Using join buffer&quot;m as we&#039;re essentially joining the tinyint_asc table to Country without an index.
Here&#039;s yet another possible execution plan for the above query.
&lt;pre&gt;
+----+-------------+-------------+-------+---------------+---------+---------+------+------+---------------------------------------------+
&#124; id &#124; select_type &#124; table       &#124; type  &#124; possible_keys &#124; key     &#124; key_len &#124; ref  &#124; rows &#124; Extra                                       &#124;
+----+-------------+-------------+-------+---------------+---------+---------+------+------+---------------------------------------------+
&#124;  1 &#124; SIMPLE      &#124; Country     &#124; ALL   &#124; NULL          &#124; NULL    &#124; NULL    &#124; NULL &#124;  239 &#124; Using temporary; Using filesort             &#124;
&#124;  1 &#124; SIMPLE      &#124; tinyint_asc &#124; range &#124; PRIMARY       &#124; PRIMARY &#124; 1       &#124; NULL &#124;    4 &#124; Using where; Using index; Using join buffer &#124;
+----+-------------+-------------+-------+---------------+---------+---------+------+------+---------------------------------------------+
&lt;/pre&gt;
This may soften the previous impression.
We only iterate the Country table once. For each row we join 4 rows from tinyint_asc using join buffer.
Since we&#039;re grouping by columns from two distinct tables, we can&#039;t utilize an index to work out the GROUP + ORDER BY.]]></description>
		<content:encoded><![CDATA[<p>We could do a contest: who gets as many &#8220;Extra&#8221; comments in a single query ðŸ™‚</p>
<p>There&#8217;s no avoiding the &#8220;Using join buffer&#8221;m as we&#8217;re essentially joining the tinyint_asc table to Country without an index.<br />
Here&#8217;s yet another possible execution plan for the above query.</p>
<pre>
+----+-------------+-------------+-------+---------------+---------+---------+------+------+---------------------------------------------+
| id | select_type | table       | type  | possible_keys | key     | key_len | ref  | rows | Extra                                       |
+----+-------------+-------------+-------+---------------+---------+---------+------+------+---------------------------------------------+
|  1 | SIMPLE      | Country     | ALL   | NULL          | NULL    | NULL    | NULL |  239 | Using temporary; Using filesort             |
|  1 | SIMPLE      | tinyint_asc | range | PRIMARY       | PRIMARY | 1       | NULL |    4 | Using where; Using index; Using join buffer |
+----+-------------+-------------+-------+---------------+---------+---------+------+------+---------------------------------------------+
</pre>
<p>This may soften the previous impression.<br />
We only iterate the Country table once. For each row we join 4 rows from tinyint_asc using join buffer.<br />
Since we&#8217;re grouping by columns from two distinct tables, we can&#8217;t utilize an index to work out the GROUP + ORDER BY.</p>
]]></content:encoded>
						</item>
						<item>
				<title>
				By: januzi				</title>
				<link>https://shlomi-noach.github.io/blog/mysql/sql-selecting-top-n-records-per-group/comment-page-1#comment-26745</link>
		<dc:creator><![CDATA[januzi]]></dc:creator>
		<pubDate>Sun, 09 Jan 2011 00:44:12 +0000</pubDate>
		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=3164#comment-26745</guid>
					<description><![CDATA[Nice combo in the &quot;extra&quot; column.]]></description>
		<content:encoded><![CDATA[<p>Nice combo in the &#8220;extra&#8221; column.</p>
]]></content:encoded>
						</item>
			</channel>
</rss>
