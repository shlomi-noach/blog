<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	
	xmlns:georss="http://www.georss.org/georss"
	xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#"
	
	>
<channel>
	<title>
	Comments on: Thoughts on MaxScale automated failover (and Orchestrator)	</title>
	<atom:link href="https://shlomi-noach.github.io/blog/mysql/thoughts-on-maxscale-automated-failover-and-orchestrator/feed" rel="self" type="application/rss+xml" />
	<link>https://shlomi-noach.github.io/blog/mysql/thoughts-on-maxscale-automated-failover-and-orchestrator</link>
	<description>Blog by Shlomi Noach</description>
	<lastBuildDate>Wed, 08 Mar 2017 10:21:09 +0000</lastBuildDate>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=5.3.3</generator>
			<item>
				<title>
				By: Replication Manager is Ready for Flashback and Much More! &#8211; Cloud Data Architect				</title>
				<link>https://shlomi-noach.github.io/blog/mysql/thoughts-on-maxscale-automated-failover-and-orchestrator/comment-page-1#comment-385629</link>
		<dc:creator><![CDATA[Replication Manager is Ready for Flashback and Much More! &#8211; Cloud Data Architect]]></dc:creator>
		<pubDate>Wed, 08 Mar 2017 10:21:09 +0000</pubDate>
		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=7439#comment-385629</guid>
					<description><![CDATA[[&#8230;] in the roadmap. I guess this task is addressing some of our fellow ace director musings found here.  Also, Etcd is already in the roadmap and will be worked on in the future and receive [&#8230;]]]></description>
		<content:encoded><![CDATA[<p>[&#8230;] in the roadmap. I guess this task is addressing some of our fellow ace director musings found here.  Also, Etcd is already in the roadmap and will be worked on in the future and receive [&#8230;]</p>
]]></content:encoded>
						</item>
						<item>
				<title>
				By: Prakash				</title>
				<link>https://shlomi-noach.github.io/blog/mysql/thoughts-on-maxscale-automated-failover-and-orchestrator/comment-page-1#comment-355888</link>
		<dc:creator><![CDATA[Prakash]]></dc:creator>
		<pubDate>Tue, 19 Apr 2016 03:41:39 +0000</pubDate>
		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=7439#comment-355888</guid>
					<description><![CDATA[@Dipti / @Shlomi,

Very interesting and nice presentation. Thank you for the detailed explanation.

Would like to know, when can we expect Multiple-Maxscale servers for deployment? Having single MaxScale is a single point of failure for critical applications. Similar to master-slave for MariaDB database, it&#039;s better to have the same concept for MaxScale as well. 

If MaxScale server is down, then there is no point of having master with many slaves. It will defeat the purpose.

If you recommend any work-around for this? Your suggestion is greatly appreciated. Thank you very much in advance.]]></description>
		<content:encoded><![CDATA[<p>@Dipti / @Shlomi,</p>
<p>Very interesting and nice presentation. Thank you for the detailed explanation.</p>
<p>Would like to know, when can we expect Multiple-Maxscale servers for deployment? Having single MaxScale is a single point of failure for critical applications. Similar to master-slave for MariaDB database, it&#8217;s better to have the same concept for MaxScale as well. </p>
<p>If MaxScale server is down, then there is no point of having master with many slaves. It will defeat the purpose.</p>
<p>If you recommend any work-around for this? Your suggestion is greatly appreciated. Thank you very much in advance.</p>
]]></content:encoded>
						</item>
						<item>
				<title>
				By: shlomi				</title>
				<link>https://shlomi-noach.github.io/blog/mysql/thoughts-on-maxscale-automated-failover-and-orchestrator/comment-page-1#comment-334919</link>
		<dc:creator><![CDATA[shlomi]]></dc:creator>
		<pubDate>Wed, 18 Nov 2015 20:11:42 +0000</pubDate>
		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=7439#comment-334919</guid>
					<description><![CDATA[@Dipti,
Thank you for further clarifying the internals of MaxScale]]></description>
		<content:encoded><![CDATA[<p>@Dipti,<br />
Thank you for further clarifying the internals of MaxScale</p>
]]></content:encoded>
						</item>
						<item>
				<title>
				By: Dipti Joshi				</title>
				<link>https://shlomi-noach.github.io/blog/mysql/thoughts-on-maxscale-automated-failover-and-orchestrator/comment-page-1#comment-334911</link>
		<dc:creator><![CDATA[Dipti Joshi]]></dc:creator>
		<pubDate>Wed, 18 Nov 2015 18:57:53 +0000</pubDate>
		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=7439#comment-334911</guid>
					<description><![CDATA[Shlomi:

Thanks for the write up and suggestions.

Few clarification and few comments on failover
(1) MaxScale does not trigger failover it self , but has mechanism to invoke the failover script based on configured events
(2) MaxScale differentiates between having lost communication with master (lost_master event) and having communication with master but the database being down on master (master_down event). 
(3) MaxScale can be configured to trigger the external script only on master_down event and not lost_master event. 
(4)  MaxScale does not automatically invoke MariaDB Replication Manager - MaxScale can be configured to invoke any other external script - could be MHA or could be custom developed by any other user. Some of our users do not like automatic failover, but just notification of master failure - for such users the script could be simply a curl script to send email notification of failure.
(5) Yes, MariaDB Replication Manager script is for MariaDB based database server using MariaDB GTID.  
(6) Since MaxScale has knowledge of before failure topology - I agree improvements can be made to MariaDB Replication Manager to use this information rather then rediscovering and not having the pre-failure topology information
(7) While MaxScale and scripts are decoupled, MaxScale does not automatically assume that the failover has been resolved upon triggering of script - Maxscale still continues to monitor the topology and detects when a new master has been promoted through its monitoring.
(8)  MaxScale has configurable monitor_interval as well as backend_connect_timeout that can be used to control flapping - though another period can be added specifically for triggering failover script to minimize flapping further
(9) Single MaxScale can handle multiple topologies.
(10) We will take your input on shell script and what you have done in orchestrator, as well as tighter integration between MaxScale and the failover script/tool

On High Availability of MaxScale itself:
 We are exploring, Multiple-MaxScale servers to load-balance on our roadmap - and will consider all your points on leader election, group communication, taking over from failed MaxScale 

Thanks again for detailed blog about your experience and observation on what can be done with MaxScale to address operational scenarios.]]></description>
		<content:encoded><![CDATA[<p>Shlomi:</p>
<p>Thanks for the write up and suggestions.</p>
<p>Few clarification and few comments on failover<br />
(1) MaxScale does not trigger failover it self , but has mechanism to invoke the failover script based on configured events<br />
(2) MaxScale differentiates between having lost communication with master (lost_master event) and having communication with master but the database being down on master (master_down event).<br />
(3) MaxScale can be configured to trigger the external script only on master_down event and not lost_master event.<br />
(4)  MaxScale does not automatically invoke MariaDB Replication Manager &#8211; MaxScale can be configured to invoke any other external script &#8211; could be MHA or could be custom developed by any other user. Some of our users do not like automatic failover, but just notification of master failure &#8211; for such users the script could be simply a curl script to send email notification of failure.<br />
(5) Yes, MariaDB Replication Manager script is for MariaDB based database server using MariaDB GTID.<br />
(6) Since MaxScale has knowledge of before failure topology &#8211; I agree improvements can be made to MariaDB Replication Manager to use this information rather then rediscovering and not having the pre-failure topology information<br />
(7) While MaxScale and scripts are decoupled, MaxScale does not automatically assume that the failover has been resolved upon triggering of script &#8211; Maxscale still continues to monitor the topology and detects when a new master has been promoted through its monitoring.<br />
(8)  MaxScale has configurable monitor_interval as well as backend_connect_timeout that can be used to control flapping &#8211; though another period can be added specifically for triggering failover script to minimize flapping further<br />
(9) Single MaxScale can handle multiple topologies.<br />
(10) We will take your input on shell script and what you have done in orchestrator, as well as tighter integration between MaxScale and the failover script/tool</p>
<p>On High Availability of MaxScale itself:<br />
 We are exploring, Multiple-MaxScale servers to load-balance on our roadmap &#8211; and will consider all your points on leader election, group communication, taking over from failed MaxScale </p>
<p>Thanks again for detailed blog about your experience and observation on what can be done with MaxScale to address operational scenarios.</p>
]]></content:encoded>
						</item>
						<item>
				<title>
				By: shlomi				</title>
				<link>https://shlomi-noach.github.io/blog/mysql/thoughts-on-maxscale-automated-failover-and-orchestrator/comment-page-1#comment-334839</link>
		<dc:creator><![CDATA[shlomi]]></dc:creator>
		<pubDate>Wed, 18 Nov 2015 11:11:14 +0000</pubDate>
		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=7439#comment-334839</guid>
					<description><![CDATA[@Guillaume,

Thank you. Indeed you are not the first to raise this point, and I understand it. At this time I don&#039;t just rely on MySQL at the backend, I rely on the relational model (so sometimes complex GROUP BY queries etc.). So migration to etcd will be non-trivial.

But there are thoughts in that direction; I don&#039;t expect this to come up in the near future unless someone else steps up to do it.]]></description>
		<content:encoded><![CDATA[<p>@Guillaume,</p>
<p>Thank you. Indeed you are not the first to raise this point, and I understand it. At this time I don&#8217;t just rely on MySQL at the backend, I rely on the relational model (so sometimes complex GROUP BY queries etc.). So migration to etcd will be non-trivial.</p>
<p>But there are thoughts in that direction; I don&#8217;t expect this to come up in the near future unless someone else steps up to do it.</p>
]]></content:encoded>
						</item>
						<item>
				<title>
				By: Guillaume Lefranc				</title>
				<link>https://shlomi-noach.github.io/blog/mysql/thoughts-on-maxscale-automated-failover-and-orchestrator/comment-page-1#comment-334836</link>
		<dc:creator><![CDATA[Guillaume Lefranc]]></dc:creator>
		<pubDate>Wed, 18 Nov 2015 10:56:20 +0000</pubDate>
		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=7439#comment-334836</guid>
					<description><![CDATA[Hi Shlomi,

very interesting presentation, thank you. I&#039;ll make sure to try Orchestrator in the near future!

However, something that puts me off is the reliance on a mysql datastore. I understand the temptation to use mysql for everything, but wouldn&#039;t a distributed datastore such as consul or etcd be more simple and efficient? I just don&#039;t want to setup a galera cluster for that. Plus consul/etcd also have leader election, so you might find that interesting.]]></description>
		<content:encoded><![CDATA[<p>Hi Shlomi,</p>
<p>very interesting presentation, thank you. I&#8217;ll make sure to try Orchestrator in the near future!</p>
<p>However, something that puts me off is the reliance on a mysql datastore. I understand the temptation to use mysql for everything, but wouldn&#8217;t a distributed datastore such as consul or etcd be more simple and efficient? I just don&#8217;t want to setup a galera cluster for that. Plus consul/etcd also have leader election, so you might find that interesting.</p>
]]></content:encoded>
						</item>
			</channel>
</rss>
