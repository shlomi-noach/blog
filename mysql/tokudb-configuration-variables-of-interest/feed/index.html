<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	
	xmlns:georss="http://www.georss.org/georss"
	xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#"
	
	>
<channel>
	<title>
	Comments on: TokuDB configuration variables of interest	</title>
	<atom:link href="https://shlomi-noach.github.io/blog/mysql/tokudb-configuration-variables-of-interest/feed" rel="self" type="application/rss+xml" />
	<link>https://shlomi-noach.github.io/blog/mysql/tokudb-configuration-variables-of-interest</link>
	<description>Blog by Shlomi Noach</description>
	<lastBuildDate>Tue, 12 Jan 2016 09:18:09 +0000</lastBuildDate>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=5.3.3</generator>
			<item>
				<title>
				By: Igor Shevtsov				</title>
				<link>https://shlomi-noach.github.io/blog/mysql/tokudb-configuration-variables-of-interest/comment-page-1#comment-342263</link>
		<dc:creator><![CDATA[Igor Shevtsov]]></dc:creator>
		<pubDate>Tue, 12 Jan 2016 09:18:09 +0000</pubDate>
		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=6613#comment-342263</guid>
					<description><![CDATA[Hi Shlomi, what a nice article! I keep coming back to it.
Percona included Hot TokuDB backup into 5.6 Percona Server.
It works great, but I found 1 issue I want to share here. If tokudb_commit_sync = 0 and flushing to a storage controlled by  tokudb_fsync_log_period, on a busy server you might have issues connecting to the Master. I take tokudb_backup snapshot on the slave with replication position recorded, so when the snapshot is restored and TokuDB recovered, replication fails with ERROR for update/delete row missing. I don&#039;t have issues when    tokudb_commit_sync = 1.]]></description>
		<content:encoded><![CDATA[<p>Hi Shlomi, what a nice article! I keep coming back to it.<br />
Percona included Hot TokuDB backup into 5.6 Percona Server.<br />
It works great, but I found 1 issue I want to share here. If tokudb_commit_sync = 0 and flushing to a storage controlled by  tokudb_fsync_log_period, on a busy server you might have issues connecting to the Master. I take tokudb_backup snapshot on the slave with replication position recorded, so when the snapshot is restored and TokuDB recovered, replication fails with ERROR for update/delete row missing. I don&#8217;t have issues when    tokudb_commit_sync = 1.</p>
]]></content:encoded>
						</item>
						<item>
				<title>
				By: Operationalizing TokuDB &#124; MySQL and Stuff				</title>
				<link>https://shlomi-noach.github.io/blog/mysql/tokudb-configuration-variables-of-interest/comment-page-1#comment-308691</link>
		<dc:creator><![CDATA[Operationalizing TokuDB &#124; MySQL and Stuff]]></dc:creator>
		<pubDate>Mon, 09 Feb 2015 19:44:49 +0000</pubDate>
		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=6613#comment-308691</guid>
					<description><![CDATA[[&#8230;] seeing &#8220;low availableÂ memory&#8221; alerts. A bit of digging led me to Shlomi Noach&#8217;sÂ blog post where he tried TokuDB, As he mentions, by default,Â TokuDB relies on the OS file cache for [&#8230;]]]></description>
		<content:encoded><![CDATA[<p>[&#8230;] seeing &#8220;low availableÂ memory&#8221; alerts. A bit of digging led me to Shlomi Noach&#8217;sÂ blog post where he tried TokuDB, As he mentions, by default,Â TokuDB relies on the OS file cache for [&#8230;]</p>
]]></content:encoded>
						</item>
						<item>
				<title>
				By: Shantanu Oak				</title>
				<link>https://shlomi-noach.github.io/blog/mysql/tokudb-configuration-variables-of-interest/comment-page-1#comment-221954</link>
		<dc:creator><![CDATA[Shantanu Oak]]></dc:creator>
		<pubDate>Sun, 27 Oct 2013 03:11:32 +0000</pubDate>
		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=6613#comment-221954</guid>
					<description><![CDATA[TokuDB default parameters worked surprisingly well for me. I got an error &quot;Too many open files&quot;, so I increased the file limit.

open-files-limit=40000]]></description>
		<content:encoded><![CDATA[<p>TokuDB default parameters worked surprisingly well for me. I got an error &#8220;Too many open files&#8221;, so I increased the file limit.</p>
<p>open-files-limit=40000</p>
]]></content:encoded>
						</item>
						<item>
				<title>
				By: Igor Shevtsov				</title>
				<link>https://shlomi-noach.github.io/blog/mysql/tokudb-configuration-variables-of-interest/comment-page-1#comment-221756</link>
		<dc:creator><![CDATA[Igor Shevtsov]]></dc:creator>
		<pubDate>Thu, 24 Oct 2013 18:54:19 +0000</pubDate>
		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=6613#comment-221756</guid>
					<description><![CDATA[@shlomi  I did it both ways: LOAD INTO and INSERT INTO SELECT *... both failed. I didn&#039;t want to ALTER in place as that table was still in use by our customers. I thought you had the same problem altering a big table and you had your binary log enabled, so why didn&#039;t you have this issue as I did? what is your secret? :) I will ask this on Tokudb google-list,sure.]]></description>
		<content:encoded><![CDATA[<p>@shlomi  I did it both ways: LOAD INTO and INSERT INTO SELECT *&#8230; both failed. I didn&#8217;t want to ALTER in place as that table was still in use by our customers. I thought you had the same problem altering a big table and you had your binary log enabled, so why didn&#8217;t you have this issue as I did? what is your secret? ðŸ™‚ I will ask this on Tokudb google-list,sure.</p>
]]></content:encoded>
						</item>
						<item>
				<title>
				By: shlomi				</title>
				<link>https://shlomi-noach.github.io/blog/mysql/tokudb-configuration-variables-of-interest/comment-page-1#comment-221753</link>
		<dc:creator><![CDATA[shlomi]]></dc:creator>
		<pubDate>Thu, 24 Oct 2013 18:23:50 +0000</pubDate>
		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=6613#comment-221753</guid>
					<description><![CDATA[But, I&#039;m not too focused. If this is TokuDB you&#039;re importing to, best to ask on TokuDB-user google group to get answer from Tokutek.]]></description>
		<content:encoded><![CDATA[<p>But, I&#8217;m not too focused. If this is TokuDB you&#8217;re importing to, best to ask on TokuDB-user google group to get answer from Tokutek.</p>
]]></content:encoded>
						</item>
						<item>
				<title>
				By: shlomi				</title>
				<link>https://shlomi-noach.github.io/blog/mysql/tokudb-configuration-variables-of-interest/comment-page-1#comment-221751</link>
		<dc:creator><![CDATA[shlomi]]></dc:creator>
		<pubDate>Thu, 24 Oct 2013 18:19:45 +0000</pubDate>
		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=6613#comment-221751</guid>
					<description><![CDATA[You tried to LOAD DATA INFILE a 31GB cvs file? I guess the problem is now solved. See, it loads as a single transaction. 
Try splitting the file (eg using unix split command) and load in smaller chunks. 

Here&#039;s an old yet relevant read: http://www.mysqlperformanceblog.com/2008/07/03/how-to-load-large-files-safely-into-innodb-with-load-data-infile/]]></description>
		<content:encoded><![CDATA[<p>You tried to LOAD DATA INFILE a 31GB cvs file? I guess the problem is now solved. See, it loads as a single transaction.<br />
Try splitting the file (eg using unix split command) and load in smaller chunks. </p>
<p>Here&#8217;s an old yet relevant read: <a href="http://www.mysqlperformanceblog.com/2008/07/03/how-to-load-large-files-safely-into-innodb-with-load-data-infile/" rel="nofollow ugc">http://www.mysqlperformanceblog.com/2008/07/03/how-to-load-large-files-safely-into-innodb-with-load-data-infile/</a></p>
]]></content:encoded>
						</item>
						<item>
				<title>
				By: Igor Shevtsov				</title>
				<link>https://shlomi-noach.github.io/blog/mysql/tokudb-configuration-variables-of-interest/comment-page-1#comment-221732</link>
		<dc:creator><![CDATA[Igor Shevtsov]]></dc:creator>
		<pubDate>Thu, 24 Oct 2013 11:38:37 +0000</pubDate>
		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=6613#comment-221732</guid>
					<description><![CDATA[@shiomi the max_binlog_cache_size was a default value of 4294963200 . I changed it to 62914560000 . 
SET GLOBAL max_binlog_cache_size = 60000 * 1024 * 1024;
The Transaction size.. How to measure it? I did a load into csv file 31Gig. - failed
Than I did another import of 5Gig csv file. failed as well with max_binlog_cache_size set to 60gig
binlog_format set to &#039;MIXED&#039;.]]></description>
		<content:encoded><![CDATA[<p>@shiomi the max_binlog_cache_size was a default value of 4294963200 . I changed it to 62914560000 .<br />
SET GLOBAL max_binlog_cache_size = 60000 * 1024 * 1024;<br />
The Transaction size.. How to measure it? I did a load into csv file 31Gig. &#8211; failed<br />
Than I did another import of 5Gig csv file. failed as well with max_binlog_cache_size set to 60gig<br />
binlog_format set to &#8216;MIXED&#8217;.</p>
]]></content:encoded>
						</item>
						<item>
				<title>
				By: shlomi				</title>
				<link>https://shlomi-noach.github.io/blog/mysql/tokudb-configuration-variables-of-interest/comment-page-1#comment-221730</link>
		<dc:creator><![CDATA[shlomi]]></dc:creator>
		<pubDate>Thu, 24 Oct 2013 10:43:22 +0000</pubDate>
		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=6613#comment-221730</guid>
					<description><![CDATA[@Igor, well what was your max_binlog_cache_size? What was the size of your transaction?]]></description>
		<content:encoded><![CDATA[<p>@Igor, well what was your max_binlog_cache_size? What was the size of your transaction?</p>
]]></content:encoded>
						</item>
						<item>
				<title>
				By: Igor Shevtsov				</title>
				<link>https://shlomi-noach.github.io/blog/mysql/tokudb-configuration-variables-of-interest/comment-page-1#comment-221719</link>
		<dc:creator><![CDATA[Igor Shevtsov]]></dc:creator>
		<pubDate>Thu, 24 Oct 2013 08:03:11 +0000</pubDate>
		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=6613#comment-221719</guid>
					<description><![CDATA[@Shiomi once again thanks for a great article. I&#039;m going to send it to my manager, so he appropriate the work we did with the migration process.
One more issue I had trying to load a big file was an error: &quot;ERROR 1197 (HY000): Multi-statement transaction required more than &#039;max_binlog_cache_size&#039; bytes of storage; increase this mysqld variable and try again&quot; I fixed it by disabling writing to binlog for my session. SET sql_log_bin = 0; Did you have this issue and how did you deal with it? many thanks.]]></description>
		<content:encoded><![CDATA[<p>@Shiomi once again thanks for a great article. I&#8217;m going to send it to my manager, so he appropriate the work we did with the migration process.<br />
One more issue I had trying to load a big file was an error: &#8220;ERROR 1197 (HY000): Multi-statement transaction required more than &#8216;max_binlog_cache_size&#8217; bytes of storage; increase this mysqld variable and try again&#8221; I fixed it by disabling writing to binlog for my session. SET sql_log_bin = 0; Did you have this issue and how did you deal with it? many thanks.</p>
]]></content:encoded>
						</item>
						<item>
				<title>
				By: shlomi				</title>
				<link>https://shlomi-noach.github.io/blog/mysql/tokudb-configuration-variables-of-interest/comment-page-1#comment-221711</link>
		<dc:creator><![CDATA[shlomi]]></dc:creator>
		<pubDate>Thu, 24 Oct 2013 05:29:34 +0000</pubDate>
		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=6613#comment-221711</guid>
					<description><![CDATA[@Gerry,
Thanks. Apologies on the typo, fixed.]]></description>
		<content:encoded><![CDATA[<p>@Gerry,<br />
Thanks. Apologies on the typo, fixed.</p>
]]></content:encoded>
						</item>
			</channel>
</rss>
