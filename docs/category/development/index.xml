<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Development on code.openark.org</title>
    <link>/blog/category/development/</link>
    <description>Recent content in Development on code.openark.org</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2016. All rights reserved.</copyright>
    <lastBuildDate>Mon, 23 Nov 2015 14:22:34 +0000</lastBuildDate>
    <atom:link href="/blog/category/development/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Forking Golang repositories on GitHub and managing the import path</title>
      <link>/blog/development/forking-golang-repositories-on-github-and-managing-the-import-path/</link>
      <pubDate>Mon, 23 Nov 2015 14:22:34 +0000</pubDate>
      
      <guid>/blog/development/forking-golang-repositories-on-github-and-managing-the-import-path/</guid>
      <description>&lt;p&gt;Problem: there&#39;s an awesome Golang project on GitHub which you want to fork. You want to develop &amp;amp; collaborate on that fork, but the golang import path, in your source code, still references the original path, breaking everything.&lt;/p&gt;
&lt;p&gt;A couple solutions offered below. First, though, let&#39;s get some names.&lt;/p&gt;
&lt;h3&gt;A sample case, the problem at hand&lt;/h3&gt;
&lt;p&gt;There&#39;s an awesome tool on &lt;strong&gt;&lt;em&gt;http://github.com/awsome-org/tool&lt;/em&gt;&lt;/strong&gt;. You successfully fork it onto &lt;strong&gt;&lt;em&gt;http://github.com/awesome-you/tool&lt;/em&gt;&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;You want to collaborate on &lt;strong&gt;&lt;em&gt;http://github.com/awesome-you/tool&lt;/em&gt;&lt;/strong&gt;; you wish to pull, commit &amp;amp; push. Maybe you want to send pull requests to the origin.&lt;/p&gt;
&lt;p&gt;The following is commonly found throughout &lt;strong&gt;.go&lt;/strong&gt; files in the repository:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;import (
    &#34;github.com/awesome-org/tool/config&#34;
    &#34;github.com/awesome-org/tool/driver&#34;
    &#34;github.com/awesome-org/tool/net&#34;
    &#34;github.com/awesome-org/tool/util&#34;
)&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;If you:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;go get http://github.com/awesome-you/tool&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;em&gt;golang&lt;/em&gt; creates your &lt;strong&gt;$GOPATH/src/github.com/awesome-you/tool/&lt;/strong&gt;, which is awesome. However, as you resolve dependencies via&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;cd $GOPATH/src/github.com/awesome-you/tool/ ; go get ./...&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;em&gt;golang&lt;/em&gt; digs into the source code, finds references to &lt;strong&gt;github.com/awesome-org/tool/config&lt;/strong&gt;, &lt;strong&gt;github.com/awesome-org/tool/driver&lt;/strong&gt; etc, and fetches &lt;em&gt;those&lt;/em&gt; from &lt;strong&gt;http://github.com/awsome-org/tool&lt;/strong&gt; and onto &lt;strong&gt;$GOPATH/src/github.com/awesome-org/tool/&lt;/strong&gt;, which is not awesome. You actually have two copies of the code, one from your fork, one from the origin, and your own fork will be largely ignored as it mostly points back to the origin.&lt;/p&gt;
&lt;h3&gt;A bad solution&lt;/h3&gt;
&lt;p&gt;The dirty, bad solution would be for you to go over the source code and replace &lt;strong&gt;&#34;github.com/awesome-org/tool&#34;&lt;/strong&gt; entries with &lt;strong&gt;&#34;github.com/awesome-you/tool&#34;&lt;/strong&gt;. It is bad for two reasons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You will not be able to further pull changes from upstream&lt;/li&gt;
&lt;li&gt;You will not be able to pull-request and push your own changes upstream&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;!--more--&gt;When I say &#34;You will not be able&#34; I mean &#34;in a reasonable, developer-friendly manner&#34;. The code will be incompatible with upstream and you have effectively detached your code. You will need to keep editing and re-editing those entries anytime you wish to pull/push upstream.&lt;/p&gt;
&lt;h3&gt;Solution #1: add remote&lt;/h3&gt;
&lt;p&gt;Described in &lt;a href=&#34;http://blog.campoy.cat/2014/03/github-and-go-forking-pull-requests-and.html&#34;&gt;GitHub and Go: forking, pull requests, and go-getting&lt;/a&gt;, follow these procedures:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;go get http://github.com/awesome-org/tool
git remote add &lt;strong&gt;awesome-you-fork&lt;/strong&gt; http://github.com/awesome-you/tool&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;You&#39;re adding your repository as &lt;a href=&#34;http://git-scm.com/book/en/v2/Git-Basics-Working-with-Remotes&#34;&gt;remote&lt;/a&gt;. You will from now on need to explicitly:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;git pull --rebase &lt;strong&gt;awesome-you-fork&lt;/strong&gt;
git push &lt;strong&gt;awesome-you-fork&lt;/strong&gt;&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;If you forget to add the &lt;strong&gt;&#34;awesome-you-fork&#34;&lt;/strong&gt; argument, you are pulling and pushing from upstream.&lt;/p&gt;
&lt;h3&gt;Solution #2: cheat &#34;go get&#34;, DIY&lt;/h3&gt;
&lt;p&gt;The problem began with the &lt;strong&gt;go get&lt;/strong&gt; command, which copied the URI path onto &lt;strong&gt;$GOPATH/src&lt;/strong&gt;. However &lt;strong&gt;go get&lt;/strong&gt; implicitly issues a git clone, and we can do the same ourselves. We will dirty our hands just once, and then benefit from an ambiguous-less environment.&lt;/p&gt;
&lt;p&gt;We will now create our git repository in the name of &lt;strong&gt;awesome-org&lt;/strong&gt; but with the contents of &lt;strong&gt;awesome-you&lt;/strong&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;cd $GOPATH
mkdir -p {src,bin,pkg}
mkdir -p &lt;strong&gt;src/github.com/awesome-org/&lt;/strong&gt;
cd src/github.com/awesome-org/
git clone git@github.com:&lt;strong&gt;awesome-you/tool.git&lt;/strong&gt; # OR: git clone https://github.com/&lt;strong&gt;awesome-you/tool.git&lt;/strong&gt;
cd tool/
go get ./...&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;The &lt;strong&gt;mkdir -p {src,bin,pkg}&lt;/strong&gt; is there just in case you do not have anything setup in your &lt;strong&gt;$GOPATH&lt;/strong&gt;. We then create the repository path under the name of &lt;strong&gt;awesome-org&lt;/strong&gt;, but once inside clone from &lt;strong&gt;awesome-you&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The source code&#39;s import path fits your directory layout now, but as you push/pull you are only speaking to your own &lt;strong&gt;awesome-you&lt;/strong&gt; repository.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>zookeepercli: lightweight, powerful, controlled command line client for ZooKeeper</title>
      <link>/blog/linux/zookeepercli-lightweight-powerful-controlled-command-line-client-for-zookeeper/</link>
      <pubDate>Wed, 17 Sep 2014 09:08:20 +0000</pubDate>
      
      <guid>/blog/linux/zookeepercli-lightweight-powerful-controlled-command-line-client-for-zookeeper/</guid>
      <description>&lt;p&gt;I&#39;m happy to announce the availability of &lt;a href=&#34;https://github.com/outbrain/zookeepercli&#34;&gt;&lt;strong&gt;zookeepercli&lt;/strong&gt;&lt;/a&gt;: a lightweight, simple, fast and controlled command line client for ZooKeeper.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;zookeepercli&lt;/strong&gt; allows for:&lt;/p&gt;
&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;Basic CRUD-like operations: &lt;strong&gt;&lt;code&gt;create&lt;/code&gt;&lt;/strong&gt;,  &lt;strong&gt;&lt;code&gt;set&lt;/code&gt;&lt;/strong&gt;,  &lt;strong&gt;&lt;code&gt;delete&lt;/code&gt;&lt;/strong&gt;,  &lt;strong&gt;&lt;code&gt;exists&lt;/code&gt;&lt;/strong&gt;,  &lt;strong&gt;&lt;code&gt;get&lt;/code&gt;&lt;/strong&gt;,  &lt;strong&gt;&lt;code&gt;ls&lt;/code&gt;&lt;/strong&gt; (aka &lt;strong&gt;&lt;code&gt;children&lt;/code&gt;&lt;/strong&gt;).&lt;/li&gt;
&lt;li&gt;Extended operations: &lt;strong&gt;&lt;code&gt;lsr&lt;/code&gt;&lt;/strong&gt; (ls recursive),  &lt;strong&gt;&lt;code&gt;creater&lt;/code&gt;&lt;/strong&gt; (create recursively)&lt;/li&gt;
&lt;li&gt;Well formatted and controlled output: supporting either &lt;strong&gt;&lt;code&gt;txt&lt;/code&gt;&lt;/strong&gt; or &lt;strong&gt;&lt;code&gt;json&lt;/code&gt;&lt;/strong&gt; format&lt;/li&gt;
&lt;li&gt;Single, no-dependencies binary file, based on a native Go ZooKeeper library by &lt;a href=&#34;http://github.com/samuel/go-zookeeper&#34;&gt;github.com/samuel/go-zookeeper&lt;/a&gt; (&lt;a href=&#34;https://github.com/outbrain/zookeepercli/blob/master/go-zookeeper-LICENSE&#34;&gt;LICENSE&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I was dissatisfied with existing command line access to ZooKeeper. Uncontrolled and noisy output as well as large footprint were among the reasons. &lt;strong&gt;zookeepercli&lt;/strong&gt; overcomes the above and provides with often required powers.&lt;/p&gt;
&lt;p&gt;Usage samples:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;&lt;code&gt;
$ zookeepercli --servers srv-1,srv-2,srv-3 -c create /demo_only &#34;path placeholder&#34;
$ zookeepercli --servers srv-1,srv-2,srv-3 -c create /demo_only/key1 &#34;value1&#34;
$ zookeepercli --servers srv-1,srv-2,srv-3 -c create /demo_only/key2 &#34;value2&#34;
$ zookeepercli --servers srv-1,srv-2,srv-3 -c create /demo_only/key3 &#34;value3&#34;

$ zookeepercli --servers srv-1,srv-2,srv-3 -c ls /demo_only
&lt;span style=&#34;color: #808000;&#34;&gt;key3
key2
key1
&lt;/span&gt;
&lt;span style=&#34;color: #ff6600;&#34;&gt;# Same as above, JSON format output:&lt;/span&gt;
$ zookeepercli --servers srv-1,srv-2,srv-3 --format=json -c ls /demo_only
&lt;span style=&#34;color: #808000;&#34;&gt;[&#34;key3&#34;,&#34;key2&#34;,&#34;key1&#34;]&lt;/span&gt;

$ zookeepercli --servers srv-1,srv-2,srv-3 -c delete /demo_only/key1
$ zookeepercli --servers srv-1,srv-2,srv-3 -c delete /demo_only/key2
$ zookeepercli --servers srv-1,srv-2,srv-3 -c delete /demo_only/key3
$ zookeepercli --servers srv-1,srv-2,srv-3 -c delete /demo_only

&lt;span style=&#34;color: #ff6600;&#34;&gt;# Create a path recursively (auto-generate parent directories if not exist):&lt;/span&gt;
$ zookeepercli --servers=srv-1,srv-2,srv-3 -c creater &#34;/demo_only/child/key1&#34; &#34;val1&#34;
$ zookeepercli --servers=srv-1,srv-2,srv-3 -c creater &#34;/demo_only/child/key2&#34; &#34;val2&#34;

$ zookeepercli --servers=srv-1,srv-2,srv-3 -c get &#34;/demo_only/child/key1&#34;
&lt;span style=&#34;color: #808000;&#34;&gt;val1&lt;/span&gt;

&lt;span style=&#34;color: #ff6600;&#34;&gt;# This path was auto generated due to recursive create:&lt;/span&gt;
$ zookeepercli --servers=srv-1,srv-2,srv-3 -c get &#34;/demo_only&#34;
&lt;span style=&#34;color: #808000;&#34;&gt;zookeepercli auto-generated&lt;/span&gt;

&lt;span style=&#34;color: #ff6600;&#34;&gt;# ls recursively a path and all sub children:&lt;/span&gt;
$ zookeepercli --servers=srv-1,srv-2,srv-3 -c lsr &#34;/demo_only&#34;
&lt;span style=&#34;color: #808000;&#34;&gt;child
child/key1
child/key2 &lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;zookeepercli&lt;/strong&gt; is released as open source by &lt;a href=&#34;https://github.com/outbrain&#34;&gt;Outbrain&lt;/a&gt; under the &lt;a href=&#34;https://github.com/outbrain/zookeepercli/blob/master/LICENSE&#34;&gt;Apache 2.0 license&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Quick links:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/outbrain/zookeepercli&#34;&gt;Project page&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/outbrain/zookeepercli/releases&#34;&gt;Pre-built binaries&lt;/a&gt; for download&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/outbrain/zookeepercli/blob/master/LICENSE&#34;&gt;License&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Documentation in SQL: CALL for help()</title>
      <link>/blog/mysql/documentation-in-sql-call-for-help/</link>
      <pubDate>Wed, 11 Jan 2012 09:01:54 +0000</pubDate>
      
      <guid>/blog/mysql/documentation-in-sql-call-for-help/</guid>
      <description>&lt;p&gt;Documentation is an important part of any project. On the projects I maintain I put a lot of effort on documentation, and, frankly, the majority of time spent on my projects is on documentation.&lt;/p&gt;
&lt;p&gt;The matter of keeping the documentation faithful is a topic of interest. I&#39;d like to outline a few documentation bundling possibilities, and the present the coming new documentation method for &lt;a href=&#34;http://code.google.com/p/common-schema/&#34; rel=&#34;nofollow&#34;&gt;common_schema&lt;/a&gt;. I&#39;ll talk about any bundling that is NOT &lt;em&gt;man pages&lt;/em&gt;.&lt;/p&gt;
&lt;h4&gt;High level: web docs&lt;/h4&gt;
&lt;p&gt;This is the initial method of documentation I used for &lt;a title=&#34;openark kit&#34; href=&#34;../../forge/openark-kit&#34;&gt;openark kit&lt;/a&gt; and &lt;a title=&#34;mycheckpoint&#34; href=&#34;../../forge/mycheckpoint&#34;&gt;mycheckpoint&lt;/a&gt;. It&#39;s still valid for &lt;em&gt;mycheckpoint&lt;/em&gt;. Documentation is web-based. You need Internet access to read it. It&#39;s in HTML format.&lt;/p&gt;
&lt;p&gt;Well, not exactly HTML format: I wrote it in WordPress. Yes, it&#39;s HTML, but there&#39;s a lot of noise around (theme, menus, etc.) which is not strictly part of the documentation.&lt;/p&gt;
&lt;p&gt;While this is perhaps the easiest way to go, here&#39;s a few drawbacks:&lt;!--more--&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You&#39;re bound to some framework (WordPress in this case)&lt;/li&gt;
&lt;li&gt;Docs are split between MySQL database (my underlying WordPRess storage) &amp;amp; WordPress files (themes, style, header, footer etc.)&lt;/li&gt;
&lt;li&gt;Documentation is separate from your code - they&#39;re just not in the same place&lt;/li&gt;
&lt;li&gt;There is no version control over the documentation.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The result is a single source of documentation, which applies to whatever version is latest. It&#39;s impossible to maintain docs for multiple versions. You must manually synchronize your WordPress updates with code commits (or rather - code release!).&lt;/p&gt;
&lt;h4&gt;Mid level: version controlled HTML docs&lt;/h4&gt;
&lt;p&gt;I first saw this approach on Baron&#39;s &lt;a href=&#34;http://www.xaprb.com/blog/2010/09/22/aspersa-gets-a-user-manual/&#34; rel=&#34;bookmark&#34;&gt;Aspersa gets a user manual&lt;/a&gt; post. I loved it: the documentation is HTML, but stored as part of your project&#39;s code, in same version control.&lt;/p&gt;
&lt;p&gt;This means one can &lt;a href=&#34;http://openarkkit.googlecode.com/svn/trunk/openarkkit/doc/html/introduction.html&#34;&gt;browse the documentation&lt;/a&gt; (&lt;em&gt;openark kit&lt;/em&gt; in this example) exactly as it appears in the baseline. Depending on your project hosting, one may be able to do so per version.&lt;/p&gt;
&lt;p&gt;The approach has the great benefit of having the docs tightly coupled with the code in terms of development. Before committing code, one updates documentation for that code, then commits/releases both together.&lt;/p&gt;
&lt;p&gt;You&#39;re also not bound to any development framework. You may edit with &lt;em&gt;vim, emacs, gedit, bluefish, eclipse,&lt;/em&gt; ... any tool of your choice. It&#39;s all down to plain old text files.&lt;/p&gt;
&lt;h4&gt;Mid level #2: documentation bundling&lt;/h4&gt;
&lt;p&gt;One thing I started doing with common_schema is to release a doc bundle with the code. So one can download a compressed bundle of all HTML files. That way one is absolutely certain what&#39;s the right documentation for revision &lt;strong&gt;178&lt;/strong&gt;. There&#39;s no effort about it: the docs are already tightly coupled with code versions. Just compress and distribute.&lt;/p&gt;
&lt;h4&gt;Low level: documentation coupled with your code&lt;/h4&gt;
&lt;p&gt;Perl scripts can be written as Perl modules, in which case they are eligible for using the &lt;em&gt;perldoc&lt;/em&gt; convention. You code your documentation within your script itself, as comment. &lt;em&gt;Perldoc&lt;/em&gt; can extract the documentation and present in man-like format. Same happens with Python&#39;s &lt;em&gt;pydoc&lt;/em&gt;. Baron&#39;s &lt;a href=&#34;http://www.xaprb.com/blog/2011/11/07/when-documentation-is-code/&#34; rel=&#34;bookmark&#34;&gt;When documentation is code&lt;/a&gt; illustrates that approach. &lt;a href=&#34;http://www.maatkit.org/&#34;&gt;Maatkit&lt;/a&gt; (now &lt;em&gt;Percona Toolkit&lt;/em&gt;) has been using it for years.&lt;/p&gt;
&lt;p&gt;This method has the advantage of having the documentation ready right within your shell. You don&#39;t need a browser, nor firewall access. The docs are just there for you in the same environment where you&#39;re executing the code.&lt;/p&gt;
&lt;h4&gt;SQL Low level: CALL for help()&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;common_schema&lt;/em&gt; is a different type of project. It is merely a schema. There&#39;s no Perl nor Python. One imports the schema into one&#39;s MySQL server.&lt;/p&gt;
&lt;p&gt;What&#39;s the low-level approach for this type of code?&lt;/p&gt;
&lt;p&gt;For &lt;em&gt;common_schema&lt;/em&gt; I use three levels of documentation: the mid-level, where one can &lt;a href=&#34;http://common-schema.googlecode.com/svn/trunk/common_schema/doc/html/introduction.html&#34;&gt;browse through the versioned docs&lt;/a&gt;, the 2nd mid-level, where one can &lt;a href=&#34;http://code.google.com/p/common-schema/downloads/list&#34;&gt;download bundled documentation&lt;/a&gt;, and then a low-level approach: documentation embedded within the code.&lt;/p&gt;
&lt;p&gt;MySQL&#39;s documentation is also built into the server: see the &lt;strong&gt;help_*&lt;/strong&gt; tables within the &lt;strong&gt;mysql&lt;/strong&gt; schema. The &lt;em&gt;mysql&lt;/em&gt; command line client allows one to access help by supporting the help command, e.g.&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;mysql&amp;gt; help create table;&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;The client intercepts this command (this is not server side command) and searches through the &lt;strong&gt;mysql.help_*&lt;/strong&gt; docs.&lt;/p&gt;
&lt;p&gt;With &lt;em&gt;common_schema&lt;/em&gt;, I don&#39;t have control over the client; it&#39;s all on server side. But the code being a schema, what with stored routines and tables, it&#39;s easy enough to set up documentation.&lt;/p&gt;
&lt;p&gt;As of the next version of &lt;em&gt;common_schema&lt;/em&gt;, and following MySQL&#39;s method, &lt;em&gt;common_schema&lt;/em&gt; provides a &lt;strong&gt;help&lt;/strong&gt; table:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;DESC help;
+--------------+-------------+------+-----+---------+-------+
| Field        | Type        | Null | Key | Default | Extra |
+--------------+-------------+------+-----+---------+-------+
| topic        | varchar(32) | NO   | PRI | NULL    |       |
| help_message | text        | NO   |     | NULL    |       |
+--------------+-------------+------+-----+---------+-------+&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;And a &lt;strong&gt;help()&lt;/strong&gt; procedure, so that you can call for &lt;em&gt;help()&lt;/em&gt;. The procedure will look for the best matching document based on your search expression:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;root@mysql-5.1.51&amp;gt; &lt;strong&gt;CALL help(&#39;match&#39;);&lt;/strong&gt;
&lt;strong&gt;+---------------------------------------&lt;/strong&gt;----------------------------------------+
| help                                                                          |
+-------------------------------------------------------------------------------+
|                                                                               |
| NAME                                                                          |
|                                                                               |
| match_grantee(): Match an existing account based on user+host.                |
|                                                                               |
| TYPE                                                                          |
|                                                                               |
| Function                                                                      |
|                                                                               |
| DESCRIPTION                                                                   |
|                                                                               |
| MySQL does not provide with identification of logged in accounts. It only     |
| provides with user + host:port combination within processlist. Alas, these do |
| not directly map to accounts, as MySQL lists the host:port from which the     |
| connection is made, but not the (possibly wildcard) user or host.             |
| This function matches a user+host combination against the known accounts,     |
| using the same matching method as the MySQL server, to detect the account     |
| which MySQL identifies as the one matching. It is similar in essence to       |
| CURRENT_USER(), only it works for all sessions, not just for the current      |
| session.                                                                      |
|                                                                               |
| SYNOPSIS                                                                      |
|                                                                               |
|                                                                               |
|                                                                               |
|        match_grantee(connection_user char(16) CHARSET utf8,                   |
|        connection_host char(70) CHARSET utf8)                                 |
|          RETURNS VARCHAR(100) CHARSET utf8                                    |
|                                                                               |
|                                                                               |
| Input:                                                                        |
|                                                                               |
| * connection_user: user login (e.g. as specified by PROCESSLIST)              |
| * connection_host: login host. May optionally specify port number (e.g.       |
|   webhost:12345), which is discarded by the function. This is to support      |
|   immediate input from as specified by PROCESSLIST.                           |
|                                                                               |
|                                                                               |
| EXAMPLES                                                                      |
|                                                                               |
| Find an account matching the given use+host combination:                      |
|                                                                               |
|                                                                               |
|        mysql&amp;gt; SELECT match_grantee(&#39;apps&#39;, &#39;192.128.0.1:12345&#39;) AS            |
|        grantee;                                                               |
|        +------------+                                                         |
|        | grantee    |                                                         |
|        +------------+                                                         |
|        | &#39;apps&#39;@&#39;%&#39; |                                                         |
|        +------------+                                                         |
|                                                                               |
|                                                                               |
|                                                                               |
| ENVIRONMENT                                                                   |
|                                                                               |
| MySQL 5.1 or newer                                                            |
|                                                                               |
| SEE ALSO                                                                      |
|                                                                               |
| processlist_grantees                                                          |
|                                                                               |
| AUTHOR                                                                        |
|                                                                               |
| Shlomi Noach                                                                  |
|                                                                               |
+-------------------------------------------------------------------------------+&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;I like HTML for documentation. I think it&#39;s a good format, provided you don&#39;t start doing funny things. Perhaps &lt;em&gt;TROFF&lt;/em&gt; is more suitable; certainly more popular on Unix machines. But I already have everything in HTML. So, what do I do?&lt;/p&gt;
&lt;p&gt;My decision was to keep documentation in HTML, and use the handy &lt;em&gt;html2text&lt;/em&gt; tool to do the job. And it does it pretty well! The sample you see above is an automated translation of HTML to plain text.&lt;/p&gt;
&lt;p&gt;I add a few touches of my own: SELECTing long texts is ugly, whether you do it via &#34;&lt;strong&gt;;&lt;/strong&gt;&#34; or &#34;&lt;strong&gt;\G&lt;/strong&gt;&#34;. The &lt;strong&gt;help()&lt;/strong&gt; routine breaks the text by &#39;&lt;strong&gt;\n&lt;/strong&gt;&#39;, returning a multi row result set. The above sample makes for some &lt;strong&gt;60+&lt;/strong&gt; rows, nicely formatted, broken from the original single text appearing in the &lt;strong&gt;help&lt;/strong&gt; table.&lt;/p&gt;
&lt;p&gt;So now you have an internal help method for &lt;em&gt;common_schema&lt;/em&gt;, right where the code is. You don&#39;t have to leave the command line client in order to get help.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://datacharmer.blogspot.com/&#34;&gt;Giuseppe&lt;/a&gt; offered me the idea for this, even while my own thinking about it was in early stages.&lt;/p&gt;
&lt;p&gt;The next version of &lt;em&gt;common_schema&lt;/em&gt; will be available in a few weeks. The code is pretty much ready. I just need to work on, ahem..., the documentation.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>More MySQL foreach()</title>
      <link>/blog/mysql/more-mysql-foreach/</link>
      <pubDate>Fri, 02 Dec 2011 15:55:32 +0000</pubDate>
      
      <guid>/blog/mysql/more-mysql-foreach/</guid>
      <description>&lt;p&gt;In my &lt;a href=&#34;http://code.openark.org/blog/mysql/mysql-foreach&#34;&gt;previous post&lt;/a&gt; I&#39;ve shown several generic use cases for &lt;a href=&#34;http://common-schema.googlecode.com/svn/trunk/common_schema/doc/html/foreach.html&#34;&gt;&lt;em&gt;foreach()&lt;/em&gt;&lt;/a&gt;, a new scripting functionality introduced in &lt;a href=&#34;http://code.google.com/p/common-schema/&#34; rel=&#34;nofollow&#34;&gt;common_schema&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In this part I present DBA&#39;s handy syntax for schema and table operations and maintenance.&lt;/p&gt;
&lt;p&gt;Confession: while I love &lt;strong&gt;INFORMATION_SCHEMA&lt;/strong&gt;&#39;s power, I just &lt;em&gt;hate&lt;/em&gt; writing queries against it. It&#39;s just so much typing! Just getting the list of tables in a schema makes for this heavy duty query:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;SELECT TABLE_NAME FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA=&#39;sakila&#39; AND TABLE_TYPE=&#39;BASE TABLE&#39;;&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;When a join is involved this really becomes a nightmare. I think it&#39;s cumbersome, and as result, many do not remember the names and meaning of columns, making for &lt;em&gt;&#34;oh, I need to read the manual all over again just to get that query right&#34;&lt;/em&gt;. Anyway, that&#39;s my opinion.&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;SHOW TABLES&lt;/strong&gt; statement is easier to type, but cannot be integrated into a &lt;strong&gt;SELECT&lt;/strong&gt; query (though &lt;a href=&#34;http://code.openark.org/blog/mysql/reading-results-of-show-statements-on-server-side&#34;&gt;we have a partial solution&lt;/a&gt; for that, too), and besides, when filtering out the views, the &lt;strong&gt;SHOW&lt;/strong&gt; statement becomes almost as cumbersome as the one on &lt;strong&gt;INFORMATION_SCHEMA&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Which is why &lt;em&gt;foreach()&lt;/em&gt; offers handy shortcuts to common iterations on schemata and tables, as follows:&lt;/p&gt;
&lt;h4&gt;Use case: iterate all databases&lt;/h4&gt;
&lt;blockquote&gt;
&lt;pre&gt;call foreach(&lt;span style=&#34;color: #808000;&#34;&gt;&#39;schema&#39;&lt;/span&gt;, &lt;span style=&#34;color: #003366;&#34;&gt;&#39;CREATE TABLE ${schema}.event(event_id INT, msg VARCHAR(128))&#39;&lt;/span&gt;);&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;In the above we execute a query on each database. Hmmm, maybe not such a good idea to perform this operation on all databases? Let&#39;s filter them:&lt;/p&gt;
&lt;h4&gt;Use case: iterate databases by name match&lt;/h4&gt;
&lt;blockquote&gt;
&lt;pre&gt;call foreach(&lt;span style=&#34;color: #808000;&#34;&gt;&#39;schema like wordpress_%&#39;&lt;/span&gt;, &lt;span style=&#34;color: #003366;&#34;&gt;&#39;ALTER TABLE ${schema}.wp_posts MODIFY COLUMN comment_author VARCHAR(96) NOT NULL&#39;&lt;/span&gt;);&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;The above will only iterate my WordPress databases (I have several of these), performing an &lt;strong&gt;ALTER&lt;/strong&gt; on &lt;strong&gt;wp_posts&lt;/strong&gt; for each of those databases.&lt;!--more--&gt;&lt;/p&gt;
&lt;p&gt;I don&#39;t have to quote the &lt;em&gt;like&lt;/em&gt; expression, but I can, if I wish to.&lt;/p&gt;
&lt;p&gt;I can also use a regular expression match:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;call foreach(&lt;span style=&#34;color: #808000;&#34;&gt;&#39;schema ~ /^wordpress_[0-9]+$/&#39;&lt;/span&gt;, &lt;span style=&#34;color: #003366;&#34;&gt;&#39;ALTER TABLE ${schema}.wp_posts MODIFY COLUMN comment_author VARCHAR(96) NOT NULL&#39;&lt;/span&gt;);&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;h4&gt;Use case: iterate tables in a specific schema&lt;/h4&gt;
&lt;p&gt;Time to upgrade our &lt;strong&gt;sakila&lt;/strong&gt; tables to InnoDB&#39;s compressed format. We use &lt;strong&gt;$()&lt;/strong&gt;, a synonym for &lt;em&gt;foreach()&lt;/em&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;call $(&lt;span style=&#34;color: #808000;&#34;&gt;&#39;table in sakila&#39;&lt;/span&gt;, &lt;span style=&#34;color: #003366;&#34;&gt;&#39;ALTER TABLE ${schema}.${table} ENGINE=InnoDB ROW_FORMAT=COMPRESSED&#39;&lt;/span&gt;);&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;The above will iterate on tables in &lt;strong&gt;sakila&lt;/strong&gt;. I say &lt;em&gt;tables&lt;/em&gt;, since it will avoid iterating views (there is still no specific syntax for views iteration). This is done on purpose, as my experience shows there is very little in common between tables and views when it comes to maintenance and operations.&lt;/p&gt;
&lt;h4&gt;Use case: iterate tables by name match&lt;/h4&gt;
&lt;p&gt;Here&#39;s a interesting scenario: you wish to work on all tables matching some name. The naive approach would be to:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;SELECT TABLE_SCHEMA, TABLE_NAME FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME = &#39;wp_posts&#39; AND TABLE_TYPE = &#39;BASE TABLE&#39;&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Wait!&lt;/strong&gt;&lt;/em&gt; Are you aware this may bring your server down? This query will open all databases at once, opening all &lt;strong&gt;.frm&lt;/strong&gt; files (though thankfully not data files, since we only check for name and type).&lt;/p&gt;
&lt;p&gt;Here&#39;s a better approach:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;call foreach(&lt;span style=&#34;color: #808000;&#34;&gt;&#39;table like wp_posts&#39;&lt;/span&gt;, &lt;span style=&#34;color: #003366;&#34;&gt;&#39;ALTER TABLE ${schema}.${table} ENGINE=InnoDB&#39;&lt;/span&gt;);&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;(There&#39;s now FULLTEXT to InnoDB, so the above can make sense in the near future!)&lt;/p&gt;
&lt;p&gt;The good part is that &lt;em&gt;foreach()&lt;/em&gt; will look for matching tables &lt;em&gt;one database at a time&lt;/em&gt;. It will iterate the list of database, then look for matching tables per database, thereby optimizing the query on &lt;strong&gt;INFORMATION_SCHEMA&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Here, too, I can use regular expressions:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;call $(&lt;span style=&#34;color: #808000;&#34;&gt;&#39;table ~ /^wp_.*$/&#39;&lt;/span&gt;, &lt;span style=&#34;color: #003366;&#34;&gt;&#39;ALTER TABLE ${schema}.${table} ENGINE=InnoDB&#39;&lt;/span&gt;);&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;h4&gt;Conclusion&lt;/h4&gt;
&lt;p&gt;This is work in the making, but, as someone who maintains a few productions servers, I&#39;ve already put it to work.&lt;/p&gt;
&lt;p&gt;I&#39;m hoping the syntax is easy to comprehend. I know that since I developed it it must be far more intuitive to myself than to others. I&#39;ve tried to keep close on common syntax and concepts from various programming languages.&lt;/p&gt;
&lt;p&gt;I would like to get as much feedback as possible. I have further ideas and thoughts on the direction &lt;a href=&#34;http://code.google.com/p/common-schema/&#34;&gt;common_schema&lt;/a&gt; is taking, but wish take it in small steps. Your feedback is appreciated!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Test-driven SQL development</title>
      <link>/blog/mysql/test-driven-sql-development/</link>
      <pubDate>Thu, 20 Oct 2011 19:55:04 +0000</pubDate>
      
      <guid>/blog/mysql/test-driven-sql-development/</guid>
      <description>&lt;p&gt;I&#39;m having a lot of fun writing &lt;a href=&#34;http://code.google.com/p/common-schema/&#34; rel=&#34;nofollow&#34;&gt;common_schema&lt;/a&gt;, an SQL project which includes views, tables and stored routines.&lt;/p&gt;
&lt;p&gt;As the project grows (and it&#39;s taking some interesting directions, in my opinion) more dependencies are being introduced, and a change to one routine or view may affect many others. This is why I&#39;ve turned the development on &lt;em&gt;common_schema&lt;/em&gt; to be &lt;em&gt;test driven&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Now, just how do you test drive an SQL project?&lt;/p&gt;
&lt;p&gt;Well, much like the way you test any other project in your favorite programming language. If its functions you&#39;re testing, that&#39;s all too familiar: functions get some input and provide some output. Hmmm, they might be changing SQL data during that time. With procedures it&#39;s slightly more complex, since they do not directly return output but result sets.&lt;/p&gt;
&lt;p&gt;Here&#39;s the testing scheme I use:&lt;!--more--&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Tests are divided to families. For example, there is a family of tests for the &lt;em&gt;eval()&lt;/em&gt; function.&lt;/li&gt;
&lt;li&gt;Each test in a family is responsible for checking the simplest, most &#34;atomic&#34; issue. This means many small tests.&lt;/li&gt;
&lt;li&gt;Each test can have a &lt;em&gt;&#34;pre-test&#34;&lt;/em&gt; step, which prepares the ground (for example, create a table and populate it)&lt;/li&gt;
&lt;li&gt;Likewise, a test can have a &lt;em&gt;&#34;post-test&#34;&lt;/em&gt; step, which is typically just cleanup code (since the test is already complete by the time the post step is invoked).&lt;/li&gt;
&lt;li&gt;Each test is an SQL file: a set of commands to be executed.&lt;/li&gt;
&lt;li&gt;A test may have an &lt;em&gt;&#34;expected output&#34;&lt;/em&gt; file.&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;If no explicit &lt;em&gt;expected&lt;/em&gt; exists, the test is expected to return &lt;strong&gt;&#34;1&#34;&lt;/strong&gt; (just as the most basic &lt;em&gt;JUnit&lt;/em&gt; test assumes an &#34;assert true&#34;)&lt;/li&gt;
&lt;li&gt;A test family may also have &lt;em&gt;pre-&lt;/em&gt; and &lt;em&gt;post-&lt;/em&gt; steps.&lt;/li&gt;
&lt;li&gt;Any failure in any step fails the entire process. Failures may include:
&lt;ul&gt;
&lt;li&gt;Failure to prepare the grounds for a test or family of tests&lt;/li&gt;
&lt;li&gt;Failure in executing the test&lt;/li&gt;
&lt;li&gt;Mismatch between test&#39;s output and expected result.&lt;/li&gt;
&lt;li&gt;Failure in executing the &lt;em&gt;post-&lt;/em&gt; step (may indicate yet invalid test result not intercepted by the test)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;An example&lt;/h4&gt;
&lt;p&gt;The following image presents a single test family: the &lt;em&gt;eval&lt;/em&gt; family, testing the &lt;em&gt;eval()&lt;/em&gt; routine.&lt;/p&gt;
&lt;blockquote&gt;&lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2011/10/test-driven-sql-development-01.png&#34;&gt;&lt;img class=&#34;size-full wp-image-4205 alignnone&#34; title=&#34;test-driven-sql-development-01&#34; src=&#34;/blog/blog/assets/test-driven-sql-development-01.png&#34; alt=&#34;Test driven SQL development - sample&#34; width=&#34;198&#34; height=&#34;258&#34; /&gt;&lt;/a&gt;&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;In this family, there are two tests.&lt;/li&gt;
&lt;li&gt;In both tests, we have a &lt;em&gt;pre-test&lt;/em&gt; step, and a test.&lt;/li&gt;
&lt;li&gt;We have no &lt;em&gt;post-test&lt;/em&gt; here.&lt;/li&gt;
&lt;li&gt;Nor do we have an &lt;em&gt;expected-output&lt;/em&gt; sample, which means the tests expect to return with &lt;strong&gt;&#34;1&#34;&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Implementation&lt;/h4&gt;
&lt;p&gt;But how are tests conducted? Via &lt;em&gt;mysql&lt;/em&gt;, of course. While tests are plain SQL text file, they are being executed against a running MySQL server using the &lt;em&gt;mysql&lt;/em&gt; client. It is given the test files as input, and its output is directed to file as well.&lt;/p&gt;
&lt;p&gt;This makes it very easy to code the test using a simple shell script. It takes a small measure of file iteration, process invocation, exit code check, and &lt;em&gt;diff&lt;/em&gt; execution.&lt;/p&gt;
&lt;p&gt;For example, to test &lt;em&gt;eval()&lt;/em&gt;&#39;s &lt;strong&gt;01&lt;/strong&gt; test, we first execute &lt;em&gt;mysql&lt;/em&gt; with &lt;strong&gt;01/pre.sql&lt;/strong&gt; as input. Assuming success, we execute &lt;em&gt;mysql&lt;/em&gt; again, this time with &lt;strong&gt;01/test.sql&lt;/strong&gt;. We capture the output of this execution, and compare it with &lt;em&gt;expected-output&lt;/em&gt;, or with &lt;strong&gt;&#34;1&#34;&lt;/strong&gt; when no &lt;em&gt;expected-output&lt;/em&gt; specified.&lt;/p&gt;
&lt;h4&gt;Tests pass, or no code!&lt;/h4&gt;
&lt;p&gt;Some &lt;strong&gt;12&lt;/strong&gt; years ago, I worked with a less-known version system called &lt;a href=&#34;http://aegis.sourceforge.net/documents.html&#34;&gt;aegis&lt;/a&gt;. The thing I remember most from &lt;em&gt;aegis&lt;/em&gt; was that it had a good tests infrastructure. Long before &#34;test-driven development&#34; was coined, or was even commonly practiced, &lt;em&gt;aegis&lt;/em&gt; supported tests right into your version control. &#34;Right into&#34;, in the sense that &lt;em&gt;you could not merge your code back to the baseline&lt;/em&gt; if it didn&#39;t pass all of the tests.&lt;/p&gt;
&lt;p&gt;I work with SVN for &lt;em&gt;common_schema&lt;/em&gt;, and I do not know of such an option in SVN. But I also use &lt;em&gt;ant&lt;/em&gt; to build this project, and the dependency is clear there: &lt;strong&gt;ant dist&lt;/strong&gt;, my target which creates the distribution files, is dependent on &lt;strong&gt;ant test&lt;/strong&gt;, the target which works out the tests.&lt;/p&gt;
&lt;p&gt;That is, you cannot generate the distribution files when tests fail.&lt;/p&gt;
&lt;h4&gt;Further notes&lt;/h4&gt;
&lt;p&gt;Since I&#39;m now retroactively patching tests for already existing functionality, calling it &lt;em&gt;test-driven&lt;/em&gt; development is an overstatement; nevertheless new tests are already proving invaluable when I keep changing and improving existing code. Suddenly dependent functionality no longer works as expected. What fun!&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://code.google.com/p/common-schema/source/browse/trunk/common_schema/tests/test_all.sh&#34;&gt;The code&lt;/a&gt; for the testing suite is actually much shorter than this blog post.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>oak-hook-general-log: your poor man&#39;s Query Analyzer</title>
      <link>/blog/mysql/oak-hook-general-log-your-poor-mans-query-analyzer/</link>
      <pubDate>Wed, 15 Dec 2010 19:46:06 +0000</pubDate>
      
      <guid>/blog/mysql/oak-hook-general-log-your-poor-mans-query-analyzer/</guid>
      <description>&lt;p&gt;The latest release of &lt;a href=&#34;http://code.openark.org/forge/openark-kit&#34;&gt;openark kit&lt;/a&gt; introduces &lt;a href=&#34;http://openarkkit.googlecode.com/svn/trunk/openarkkit/doc/html/oak-hook-general-log.html&#34;&gt;oak-hook-general-log&lt;/a&gt;, a handy tool which allows for some analysis of executing queries.&lt;/p&gt;
&lt;p&gt;Initially I just intended for the tool to be able to dump the general log to standard output, from any machine capable to connect to MySQL. Quick enough, I realized the power it brings.&lt;/p&gt;
&lt;p&gt;With this tool, one can dump to standard output all queries using temporary tables; or using a specific index; or doing a full index scan; or just follow up on connections; or... For example, the following execution will only log queries which make for filesort:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;oak-hook-general-log --user=root --host=localhost --password=123456 --filter-explain-filesort&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;h4&gt;The problem with using the standard logs&lt;/h4&gt;
&lt;p&gt;So you have the &lt;em&gt;general log&lt;/em&gt;, which you don&#39;t often enable, since it tends to grow huge within moments. You then have the &lt;em&gt;slow log&lt;/em&gt;. Slow log is great, and is among the top tools for MySQL diagnosis.&lt;/p&gt;
&lt;p&gt;The slow log allows for &lt;strong&gt;log-queries-not-using-indexes&lt;/strong&gt;, which is yet another nice feature. Not only should you log any query running for over &lt;strong&gt;X&lt;/strong&gt; seconds, but also log any query which does not use an index.&lt;/p&gt;
&lt;p&gt;Wait. This logs all single-row tables (no single row table will use an index), as well as very small tables (a common &lt;strong&gt;20&lt;/strong&gt; rows lookup table will most often be scanned). These are OK scans. This makes for some noise in the slow log.&lt;/p&gt;
&lt;p&gt;And how about queries which do use an index, but do so poorly? They use an index, but retrieve some &lt;strong&gt;12,500,000&lt;/strong&gt; rows, &lt;em&gt;using temporary&lt;/em&gt; table &amp;amp; &lt;em&gt;filesort&lt;/em&gt;?&lt;/p&gt;
&lt;h4&gt;What oak-hook-general-log does for you&lt;/h4&gt;
&lt;p&gt;This tool streams out the general log, and filters out queries based on their &lt;em&gt;role&lt;/em&gt; or on their &lt;em&gt;execution plan&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;To work at all, it must enable the general log. Moreover, it directs the general log to log table. Mind that this makes for a performance impact, which is why the tool auto-terminates and restores original log settings (default is &lt;strong&gt;1&lt;/strong&gt; minute, configurable). It&#39;s really not a tool you should keep running for days. But during the few moments it runs, it will:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Routinely rotate the &lt;strong&gt;mysql.general_log&lt;/strong&gt; table so that it doesn&#39;t fill up&lt;/li&gt;
&lt;li&gt;Examine entries found in the general log&lt;/li&gt;
&lt;li&gt;Cross reference entries to the PROCESSLIST so as to deduce database context (&lt;a href=&#34;http://bugs.mysql.com/bug.php?id=52554&#34;&gt;bug #52554&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;If required and appropriate, evaluate a query&#39;s execution plan&lt;/li&gt;
&lt;li&gt;Decide whether to dump each entry based on filtering rules&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Filtering rules&lt;/h4&gt;
&lt;p&gt;Filtering rules are passed as command line options. At current, only one filtering rule applies (if more than one specified only one is used, so no point in passing more than one). Some of the rules are:&lt;!--more--&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;filter-connection&lt;/strong&gt;: only log connect/quit entries&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;filter-explain-fullscan&lt;/strong&gt;: only log full table scans&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;filter-explain-temporary&lt;/strong&gt;: only log queries which create implicit temporary tables&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;filter-explain-rows-exceed&lt;/strong&gt;: only log queries where more than &lt;strong&gt;X&lt;/strong&gt; number of rows are being accessed on some table (estimated)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;filter-explain-total-rows-exceed&lt;/strong&gt;: only log queries where more than &lt;strong&gt;X&lt;/strong&gt; number of rows are accessed on all tables combined (estimated, with possibly incorrect numbers on some queries)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;filter-explain-key&lt;/strong&gt;: only log queries using a specific index. This feature somewhat overlaps with Maatkit&#39;s &lt;em&gt;mk-index-usage&lt;/em&gt; (read &lt;a href=&#34;http://www.mysqlperformanceblog.com/2010/11/11/advanced-index-analysis-with-mk-index-usage/&#34;&gt;announcement&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;filter-explain-contains&lt;/strong&gt;: a general purpose &lt;em&gt;grep&lt;/em&gt; on the execution plan. Log queries where the execution plan contains &lt;em&gt;some text&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are other filters, and I will possibly add more in due time.&lt;/p&gt;
&lt;p&gt;Here are a couple cases I used &lt;em&gt;oak-hook-general-log&lt;/em&gt; for:&lt;/p&gt;
&lt;h4&gt;Use case: temporary tables&lt;/h4&gt;
&lt;p&gt;I have a server with this alarming chart (courtesy &lt;a href=&#34;http://code.openark.org/forge/mycheckpoint&#34;&gt;mycheckpoint&lt;/a&gt;) of temporary tables:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre /&gt;&lt;img class=&#34;alignnone&#34; title=&#34;Created tmp tables per second&#34; src=&#34;/blog/blog/assets/chart?cht=lc&amp;amp;chs=370x180&amp;amp;chts=303030,12&amp;amp;chtt=Latest+24+hours:+Dec+9,+06:30++-++Dec+10,+06:30&amp;amp;chf=c,s,ffffff&amp;amp;chdl=created_tmp_tables_psec|created_tmp_disk_tables_psec&amp;amp;chdlp=b&amp;amp;chco=ff8c00,4682b4&amp;amp;chd=s:yzzy02zzz100zzz0rv9zz0zyzyz0yy2xz1t11xzztz0xr1xt2tz07vwzz100100z31z111yz1vzzzzz1zs80r902s1111010y20z03z11487zz011z11011002w0q5rxxz0y00z0s02xy1yy0,gggfghggfgggghhgYekhhghhhhhghfjghhdihfhgdghgZhgcicihpcehhhhhhhifkigjihghjehgiigjgYqiYqgiaihiifkhekhfijgiihhggggggggggfhgghffZoYgggggggggdihfggghg&amp;amp;chxt=x,y&amp;amp;chxr=1,0,35.060000&amp;amp;chxl=0:||08:00||+||12:00||+||16:00||+||20:00||+||00:00||+||04:00||+|&amp;amp;chxs=0,505050,10,0,lt&amp;amp;chg=4.17,25,1,2,2.08,0&amp;amp;chxp=0,2.08,6.25,10.42,14.59,18.76,22.93,27.10,31.27,35.44,39.61,43.78,47.95,52.12,56.29,60.46,64.63,68.80,72.97,77.14,81.31,85.48,89.65,93.82,97.99&amp;amp;tsstart=2010-12-09+06:30:00&amp;amp;tsstep=600&#34; alt=&#34;&#34; width=&#34;370&#34; height=&#34;180&#34; /&gt;

&lt;/blockquote&gt;
What could possibly create &lt;strong&gt;30&lt;/strong&gt; temporary tables per second on average?

The slow log produced nothing helpful, even with &lt;strong&gt;log-queries-not-using-indexes&lt;/strong&gt; enabled. There were a lot of queries not using indexes there, but nothing at these numbers. With:
&lt;blockquote&gt;
&lt;pre&gt;oak-hook-general-log --filter-explain-temporary&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;enabled for &lt;strong&gt;1&lt;/strong&gt; minute, nothing came out. Weird. Enabled for &lt;strong&gt;5&lt;/strong&gt; minutes, I got one entry. Turned out a scheduled script, acting once per &lt;strong&gt;5&lt;/strong&gt; minutes, was making a single complicated query involving many nested views, which accounted for some &lt;em&gt;hundreds&lt;/em&gt; of temporary tables created. All of them very small, query time was very fast. There is no temporary tables problem with this server, case closed.&lt;/p&gt;
&lt;h4&gt;Use case: connections&lt;/h4&gt;
&lt;p&gt;A server had issues with some exceptions being thrown on the client side. There was a large number of new connections created per second although the client was using a connection pool. Suspecting the pool didn&#39;t work well, I issued:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;oak-hook-general-log --filter-connect&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;The pool was working well, all right. No entries for that client were recorder in &lt;strong&gt;1&lt;/strong&gt; minute of testing. However, it turned out some old script was flooding the MySQL server with requests, every second. The log showed root@somehost, and sure enough, the script was disabled. Exceptions were due to another reason; it was good to eliminate a suspect.&lt;/p&gt;
&lt;p&gt;Some of the tool&#39;s use case is relatively easy to solve with tail, grep &amp;amp; awk; others are not. I am using it more and more often, and find it to make significant shortcuts in tracking down queries.&lt;/p&gt;
&lt;h4&gt;Get it&lt;/h4&gt;
&lt;p&gt;Download the tool as part of &lt;em&gt;openark kit&lt;/em&gt;: access the &lt;a href=&#34;http://code.google.com/p/openarkkit/&#34;&gt;openark kit project page&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Or get the &lt;a href=&#34;http://openarkkit.googlecode.com/svn/trunk/openarkkit/src/oak/oak-hook-general-log.py&#34;&gt;source code&lt;/a&gt; directly.&lt;/p&gt;
&lt;p&gt;Feedback is most welcome.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>openark-kit (rev. 170): new tools, new functionality</title>
      <link>/blog/mysql/openark-kit-rev-170-new-tools-new-functionality/</link>
      <pubDate>Wed, 15 Dec 2010 08:31:24 +0000</pubDate>
      
      <guid>/blog/mysql/openark-kit-rev-170-new-tools-new-functionality/</guid>
      <description>&lt;p&gt;I&#39;m pleased to announce a new release of the &lt;a href=&#34;http://code.openark.org/forge/openark-kit&#34;&gt;openark kit&lt;/a&gt;. There&#39;s a lot of new functionality inside; following is a brief overview.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;openark kit&lt;/em&gt; is a set of utilities for MySQL. They  solve everyday maintenance tasks, which may be complicated or time  consuming to work by hand.&lt;/p&gt;
&lt;p&gt;It&#39;s been a while since the last announced release. Most of my attention was on &lt;a href=&#34;http://code.openark.org/forge/mycheckpoint&#34;&gt;mycheckpoint&lt;/a&gt;, building new features, writing documentation etc. However my own use of &lt;em&gt;openark kit&lt;/em&gt; has only increased in the past few months, and there&#39;s new useful solutions to common problems that have been developed.&lt;/p&gt;
&lt;p&gt;I&#39;ve used and improved many tools over this time, but doing the final cut, along with proper documentation, took some time. Anyway, here are the highlights:&lt;/p&gt;
&lt;h4&gt;New tool: oak-hook-general-log&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;oak-hook-general-log&lt;/em&gt; hooks up a MySQL server and dumps the general log based on filtering rules, applying to query role or execution plan. It is possible to only dump connect/disconnect entries, queries which make a full table scan, or use temporary tables, or scan more than X number of rows, or...&lt;/p&gt;
&lt;p&gt;I&#39;ll write more on this tool shortly.&lt;/p&gt;
&lt;h4&gt;New tool: oak-prepare-shutdown&lt;/h4&gt;
&lt;p&gt;This tool makes for an orderly and faster shutdown by safely stopping replication, and flushing InnoDB pages to disk prior to shutting down (keeping server available for connections even while attempting to flush dirty pages to disk). A typical use case would be:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;oak-prepare-shutdown --user=root --ask-pass --socket=/tmp/mysql.sock &amp;amp;&amp;amp; /etc/init.d/mysql stop&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;h4&gt;New tool: oak-repeat query&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;oak-repeat-query&lt;/em&gt; repeats executing a given query until some condition holds. The condition can be:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Number of given iterations has been reached&lt;/li&gt;
&lt;li&gt;Given time has elapsed&lt;/li&gt;
&lt;li&gt;No rows have been affected by query&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The tool comes in handy for cleanup jobs, warming up caches, etc.&lt;!--more--&gt;&lt;/p&gt;
&lt;h4&gt;New tool: oak-get-slave-lag&lt;/h4&gt;
&lt;p&gt;This simple tool just returns the number of seconds a slave is behind master. But it also returns with an appropriate exit code, based on a given threshold: &lt;strong&gt;0&lt;/strong&gt; when lag is good, &lt;strong&gt;1&lt;/strong&gt; (error exit code) when lag is too great or slave fails to replicate.&lt;/p&gt;
&lt;p&gt;This tool has been used by 3rd party applications, such as a load balancer, to determine whether a slave should be accessed.&lt;/p&gt;
&lt;h4&gt;Updated tool: oak-chunk-update&lt;/h4&gt;
&lt;p&gt;This extremely useful utility breaks down very long queries into smaller chunks. These could be queries which should affect a huge amount of rows, or queries which cannot utilize an index.&lt;/p&gt;
&lt;p&gt;Updates to the tool include limiting the range of rows the tool scans, by specifying start and stop position (either by providing constant values or by SELECT query). Also added is auto-termination when no rows are found to be affected. Last, it is possible to override INFORMATION_SCHEMA lookup by explicitly specifying chunking key.&lt;/p&gt;
&lt;p&gt;This tool works great for your daily/weekly/monthly batch jobs; in creating DWH tables; populating new columns; purging old entries; clearing data based on non-indexed values; generating summary tables; and more.&lt;/p&gt;
&lt;h4&gt;Frozen tool: oak-apply-ri&lt;/h4&gt;
&lt;p&gt;I haven&#39;t been using this tool for a while. The main work down by this tool can be done with &lt;em&gt;oak-chunk-update&lt;/em&gt;. There are some additional safety checks &lt;em&gt;oak-apply-ri&lt;/em&gt; provides; I&#39;m thinking over if they justify the tool&#39;s existence.&lt;/p&gt;
&lt;h4&gt;Frozen tool: oak-online-alter-table&lt;/h4&gt;
&lt;p&gt;With the appearance of Facebook’s &lt;a href=&#34;http://www.facebook.com/note.php?note_id=430801045932&#34;&gt;Online Schema Change&lt;/a&gt; (OSC) tool, which derives from &lt;em&gt;oak-online-alter-table&lt;/em&gt;, I&#39;m not sure I will continue developing the tool. I intend to wait for general feedback on OSC before making a decision.&lt;/p&gt;
&lt;h4&gt;Documentation&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;http://openarkkit.googlecode.com/svn/trunk/openarkkit/doc/html/introduction.html&#34;&gt;Documentation&lt;/a&gt; is now part of &lt;em&gt;openark kit&lt;/em&gt;&#39;s SVN repository.&lt;/p&gt;
&lt;h4&gt;Download&lt;/h4&gt;
&lt;p&gt;The &lt;em&gt;openark kit&lt;/em&gt; project is currently hosted by Google Code.  Downloads are available at the Google Code &lt;a href=&#34;http://code.google.com/p/openarkkit/&#34;&gt;openark kit project page&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Downloads are available in the following packaging formats:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;.deb&lt;/strong&gt; package, to be installed on &lt;em&gt;debian&lt;/em&gt;, &lt;em&gt;ubuntu&lt;/em&gt; and otherwise debian based distributions.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;.rpm&lt;/strong&gt; package, architecture free (&lt;em&gt;noarch&lt;/em&gt;), for RPM supporting Linux distributions such as &lt;em&gt;RedHat&lt;/em&gt;, &lt;em&gt;Fedora&lt;/em&gt;, &lt;em&gt;CentOS&lt;/em&gt; etc.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;.tar.gz&lt;/strong&gt; using python&#39;s distutils installer.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;source&lt;/strong&gt;, directly retrieved from SVN or from above python package.&lt;/li&gt;
&lt;li&gt;Some distribution specific &lt;a href=&#34;http://software.opensuse.org/search?baseproject=ALL&amp;amp;p=1&amp;amp;q=openark-kit&#34;&gt;RPM packages&lt;/a&gt;, courtesy Lenz Grimmer.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Feedback&lt;/h4&gt;
&lt;p&gt;Your feedback is welcome! I may not always respond promptly; and I confess that some bugs were left open for more than I would have liked them to. I hope to make for good quality of code, and bug reporting is one major factor you can control.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Thoughts and ideas for Online Schema Change</title>
      <link>/blog/mysql/thoughts-and-ideas-for-online-schema-change/</link>
      <pubDate>Thu, 07 Oct 2010 10:29:10 +0000</pubDate>
      
      <guid>/blog/mysql/thoughts-and-ideas-for-online-schema-change/</guid>
      <description>&lt;p&gt;Here&#39;s a few thoughts on current status and further possibilities for Facebook&#39;s &lt;a href=&#34;http://www.facebook.com/note.php?note_id=430801045932&#34;&gt;Online Schema Change&lt;/a&gt; (OSC) tool. I&#39;ve had these thoughts for months now, pondering over improving &lt;a href=&#34;../../forge/openark-kit/oak-online-alter-table&#34;&gt;oak-online-alter-table&lt;/a&gt; but haven&#39;t got around to implement them nor even write them down. Better late than never.&lt;/p&gt;
&lt;p&gt;The tool has some limitations. Some cannot be lifted, some could. Quoting from the &lt;a href=&#34;http://www.facebook.com/notes/mysql-at-facebook/online-schema-change-for-mysql/430801045932&#34;&gt;announcement&lt;/a&gt; and looking at the code, I add a few comments. I conclude with a general opinion on the tool&#39;s abilities.&lt;/p&gt;
&lt;h4&gt;&#34;The original table must have PK. Otherwise an error is returned.&#34;&lt;/h4&gt;
&lt;p&gt;This restriction could be lifted: it&#39;s enough that the table has a UNIQUE KEY. My original &lt;em&gt;oak-online-alter-table&lt;/em&gt; handled that particular case. As far as I see from their code, the Facebook code would work just as well with any unique key.&lt;/p&gt;
&lt;p&gt;However, this restriction is of no real interest. As we&#39;re mostly interested in InnoDB tables, and since any InnoDB table &lt;em&gt;should have&lt;/em&gt; a PRIMARY KEY, we shouldn&#39;t care too much.&lt;/p&gt;
&lt;h4&gt;&#34;No foreign keys should exist. Otherwise an error is returned.&#34;&lt;/h4&gt;
&lt;p&gt;Tricky stuff. With &lt;em&gt;oak-online-alter-table&lt;/em&gt;, changes to the original table were immediately reflected in the &lt;em&gt;ghost&lt;/em&gt; table. With InnoDB tables, that meant same transaction. And although I never got to update the text and code, there shouldn&#39;t be a reason for not using child-side foreign keys (the child-side is the table on which the FK constraint is defined).&lt;/p&gt;
&lt;p&gt;The Facebook patch works differently: it captures changes and writes them to a &lt;strong&gt;delta&lt;/strong&gt; table,  to be later (asynchronously) analyzed and make for a &lt;em&gt;replay&lt;/em&gt; of actions on the &lt;em&gt;ghost&lt;/em&gt; table.&lt;!--more--&gt;&lt;/p&gt;
&lt;p&gt;So in the Facebook code, some cases will lead to undesired behavior. Consider two tables, &lt;strong&gt;country&lt;/strong&gt; and &lt;strong&gt;city&lt;/strong&gt;, with city holding a RESTRICT/NO ACTION foreign key on &lt;strong&gt;country&lt;/strong&gt;&#39;s id. Now consider the scenario:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Rows from &lt;strong&gt;city&lt;/strong&gt; are DELETEd, where the country Id is Spain&#39;s.
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;city&lt;/strong&gt;&#39;s ghost table is still unaffected, Spain&#39;s cities are still there.&lt;/li&gt;
&lt;li&gt;A change is written to the delta table to mark these rows for deletion.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;A DELETE is issued on &lt;strong&gt;country&lt;/strong&gt;&#39;s Spain record.
&lt;ul&gt;
&lt;li&gt;The DELETE should work, from the user&#39;s perspective&lt;/li&gt;
&lt;li&gt;But it will fail: city&#39;s ghost table has not received the changes yet. There&#39;s still matching rows. The NO ACTION constraint will fail the DELETE statement.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Now, this does not lead to corruption, just to seemingly unreasonable behavior on the database part. This behavior is probably undesired. NO ACTION constraint won&#39;t do.&lt;/p&gt;
&lt;p&gt;However, with CASCADE or SET NULL options, there is less of an issue: operations on the parent table (e.g. &lt;strong&gt;country&lt;/strong&gt;) cannot fail. We must make sure operations on the ghost table make it consistent with the original table (e.g. &lt;strong&gt;city&lt;/strong&gt;).&lt;/p&gt;
&lt;p&gt;Consider the following scenario:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;A new country is created, called &#34;Sleepyland&#34;. An INSERT is made to &lt;strong&gt;country&lt;/strong&gt;.
&lt;ul&gt;
&lt;li&gt;Both &lt;strong&gt;city&lt;/strong&gt; and &lt;strong&gt;city&lt;/strong&gt;&#39;s ghost are immediately aware of it.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;A new town is created and INSERTed to &lt;strong&gt;city&lt;/strong&gt;. The town is called &#34;Naphaven&#34;.
&lt;ul&gt;
&lt;li&gt;The change takes time to propagate to &lt;strong&gt;city&lt;/strong&gt;&#39;s ghost table.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Meanwhile, we realized we made a mistake. We&#39;ve been had. There&#39;s no such city nor country.
&lt;ol&gt;
&lt;li&gt;We DELETE &#34;Naphaven&#34; from &lt;strong&gt;city&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;We DELETE &#34;Sleepyland&#34; from &lt;strong&gt;country&lt;/strong&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Note that &lt;strong&gt;city&lt;/strong&gt;&#39;s ghost table still hasn&#39;t caught up with the changes.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Eventually, the INSERT statement for &#34;Naphaven&#34; reaches &lt;strong&gt;city&lt;/strong&gt;&#39;s ghost table.
&lt;ul&gt;
&lt;li&gt;What should happen now? The INSERT cannot succeed.&lt;/li&gt;
&lt;li&gt;Will this fail the entire process?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Looking at the PHP code, I see that changes written on the &lt;strong&gt;delta&lt;/strong&gt; table are blindly replayed on the ghost table.&lt;/p&gt;
&lt;p&gt;Since the process is asynchronous, this should not be the case. We can solve the above if we use INSERT IGNORE instead of INSERT. The statement will fail without failing anything else. The row cannot exist, and that&#39;s because the original row does not exist anymore.&lt;/p&gt;
&lt;p&gt;Unlike a replication corruption, this does not lead to accumulation mistakes. The &lt;strong&gt;replay&lt;/strong&gt; is static, somewhat like in &lt;em&gt;binary log format&lt;/em&gt;. Changes are &lt;em&gt;just written&lt;/em&gt;, regardless of existing data.&lt;/p&gt;
&lt;p&gt;I have given this considerable thought, and I can&#39;t say I&#39;ve covered all the possible scenario. However I believe that with proper use of INSERT IGNORE and REPLACE INTO (two statements I heavily relied on with &lt;em&gt;oak-online-alter-table&lt;/em&gt;), correctness can be achieved.&lt;/p&gt;
&lt;p&gt;There&#39;s the small pain of re-generating the foreign key definition on the &#34;ghost&#34; table (&lt;strong&gt;CREATE TABLE LIKE ...&lt;/strong&gt; does not copy FK definitions). And since foreign key names are unique, a new name must be picked up. Not pretty, but perfectly doable.&lt;/p&gt;
&lt;h4&gt;&#34;No AFTER_{INSERT/UPDATE/DELETE} triggers must exist.&#34;&lt;/h4&gt;
&lt;p&gt;It would be nicer if MySQL had an ALTER TRIGGER statement. There isn&#39;t such statement. If there were such an atomic statement, then we would be able to rewrite the trigger, so as to add our own code to the &lt;em&gt;end of the trigger&#39;s code&lt;/em&gt;. Yuck. Would be even nicer if we were &lt;a href=&#34;http://code.openark.org/blog/mysql/triggers-use-case-compilation-part-ii&#34;&gt;allowed to have multiple triggers&lt;/a&gt; of same event.&lt;/p&gt;
&lt;p&gt;So, we are left with DROP and CREATE triggers. Alas, this makes for a short period where the trigger does not exist. Bad. The easy solution would be to LOCK WRITE the table, but apparently you can&#39;t DROP the trigger (*) when the table is locked. Sigh.&lt;/p&gt;
&lt;p&gt;(*) Happened to me, apparently to Facebook too; With latest 5.1 (5.1.51) version this actually works. With 5.0 it didn&#39;t use to; this needs more checking.&lt;/p&gt;
&lt;h4&gt;Use of INFORMATION_SCHEMA&lt;/h4&gt;
&lt;p&gt;As with oak-online-alter-table, the OSC checks for triggers, indexes, column by searching on the INFORMATION_SCHEMA tables. This makes for nice SQL for getting the exact listing and types of PRIMARY KEY columns, whether or not AFTER triggers exist, and so on.&lt;/p&gt;
&lt;p&gt;I&#39;ve always considered this to be the weak part of &lt;a href=&#34;http://code.openark.org/forge/openark-kit&#34;&gt;openark-kit&lt;/a&gt;, that it relies on INFORMATION_SCHEMA so much. It&#39;s easier, it&#39;s cleaner, it&#39;s even &lt;em&gt;more correct&lt;/em&gt; to work that way -- but it just puts too much locks. I think Baron Schwartz (and now Daniel Nichter) did amazing work on analyzing table schemata by parsing the SHOW CREATE TABLE and other SHOW commands regex-wise with &lt;a href=&#34;http://www.maatkit.org/&#34;&gt;Maatkit&lt;/a&gt;. It&#39;s a crazy work! Had I written &lt;em&gt;openark-kit&lt;/em&gt; in Perl, I would have just import their code. But I&#39;m too &lt;span style=&#34;text-decoration: line-through;&#34;&gt;lazy&lt;/span&gt; busy to do the conversion from Perl to Python, and rewrite that code, what with all the debugging.&lt;/p&gt;
&lt;p&gt;OSC is written in PHP. Again, much conversion work. I think performance-wise this is an important step to make.&lt;/p&gt;
&lt;h4&gt;A word for the critics&lt;/h4&gt;
&lt;p&gt;Finally, a word for the critics. I&#39;ve read some Facebook/MySQL bashing comments and wish to relate.&lt;/p&gt;
&lt;p&gt;In his &lt;a href=&#34;http://www.theregister.co.uk/2010/09/21/facebook_online_schema_change_for_mysql/&#34;&gt;interview to The Register&lt;/a&gt;, Mark Callaghan gave the example that &#34;Open Schema Change lets the company update indexes without user downtime, according to Callaghan&#34;.&lt;/p&gt;
&lt;p&gt;PostgreSQL was mentioned for being able to add index with only read locks taken, or being able to do the work with no locks using CREATE INDEX CONCURRENTLY. I wish MySQL had that feature! Yes, MySQL has a lot to improve upon, and the latest PostgreSQL 9.0 brings valuable new features. (Did I make it clear I have no intention of bashing PostgreSQL? If not, please re-read this paragraph until convinced).&lt;/p&gt;
&lt;p&gt;Bashing related to the notion of MySQL being so poor that Facebook used an even poorer mechanism to work out the ALTER TABLE.&lt;/p&gt;
&lt;p&gt;Well, allow me to add a few words: the CREATE INDEX is by far not the only thing you can achieve with OSC (although it may be Facebook&#39;s major concern). You should be able to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Add columns&lt;/li&gt;
&lt;li&gt;Drop columns&lt;/li&gt;
&lt;li&gt;Convert character sets&lt;/li&gt;
&lt;li&gt;Modify column types&lt;/li&gt;
&lt;li&gt;Add partitioning&lt;/li&gt;
&lt;li&gt;Reorganize partitioning&lt;/li&gt;
&lt;li&gt;Compress the table&lt;/li&gt;
&lt;li&gt;Otherwise changing table format&lt;/li&gt;
&lt;li&gt;Heck, you could even modify the storage engine! (To other transactional engine)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These are giant steps. How easy would it be to write these down into the database? It only takes a few weeks time to work out a working solution with reasonable limitations, just using the resources the MySQL server provides you with. The &lt;a href=&#34;http://www.facebook.com/MySQLatFacebook&#34;&gt;MySQL@Facebook team&lt;/a&gt; should be given credit for that.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Table refactoring &amp; application version upgrades, Part II</title>
      <link>/blog/mysql/table-refactoring-application-version-upgrades-part-ii/</link>
      <pubDate>Thu, 12 Aug 2010 05:24:06 +0000</pubDate>
      
      <guid>/blog/mysql/table-refactoring-application-version-upgrades-part-ii/</guid>
      <description>&lt;p&gt;Continuing &lt;a href=&#34;http://code.openark.org/blog/mysql/table-refactoring-application-version-upgrades-part-i&#34;&gt;Table refactoring &amp;amp; application version upgrades, Part I&lt;/a&gt;, we now discuss code &amp;amp; database upgrades which require &lt;strong&gt;DROP&lt;/strong&gt; operations. As before, we break apart the upgrade process into sequential steps, each involving either the application or the database, but not both.&lt;/p&gt;
&lt;p&gt;As I&#39;ll show, DROP operations are significantly simpler than creation operations. Interestingly, it&#39;s the same as in life.&lt;/p&gt;
&lt;h4&gt;DROP COLUMN&lt;/h4&gt;
&lt;p&gt;A column turns to be redundant, unused. Before it is dropped from the database, we must ensure no one is using it anymore. The steps are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;App: &lt;strong&gt;V1&lt;/strong&gt; -&amp;gt; &lt;strong&gt;V2&lt;/strong&gt;. Remove all references to column; make sure no queries use said column.&lt;/li&gt;
&lt;li&gt;DB: &lt;strong&gt;V1&lt;/strong&gt; -&amp;gt; &lt;strong&gt;V2&lt;/strong&gt; (possibly failover from &lt;strong&gt;M1&lt;/strong&gt; to &lt;strong&gt;M2&lt;/strong&gt;), change is &lt;strong&gt;DROP COLUMN&lt;/strong&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;DROP INDEX&lt;/h4&gt;
&lt;p&gt;A possibly simpler case here. Why would you drop an index? Is it because you found out you never use it anymore? Then all you have to do is just drop it.&lt;/p&gt;
&lt;p&gt;Or perhaps you don&#39;t need the functionality the index supports anymore? Then first drop the functionality:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;(optional) App: &lt;strong&gt;V1&lt;/strong&gt; -&amp;gt; &lt;strong&gt;V2&lt;/strong&gt;. Discard using functionality which relies on index.&lt;/li&gt;
&lt;li&gt;DB: &lt;strong&gt;V1&lt;/strong&gt; -&amp;gt; &lt;strong&gt;V2&lt;/strong&gt; (possibly failover from &lt;strong&gt;M1&lt;/strong&gt; to &lt;strong&gt;M2&lt;/strong&gt;), change is &lt;strong&gt;DROP INDEX&lt;/strong&gt;. Check out InnoDB Plugin here.&lt;!--more--&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;DROP UNIQUE INDEX&lt;/h4&gt;
&lt;p&gt;When using Master-Slave failover for table refactoring, we&#39;re now removing a constraint from the slave. Since the master is more constrained than the slave, there is no problem here. It&#39;s mostly the same as with a normal DROP INDEX, with a minor addition:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;(optional) App: &lt;strong&gt;V1&lt;/strong&gt; -&amp;gt; &lt;strong&gt;V2&lt;/strong&gt;. Discard using functionality which relies on index.&lt;/li&gt;
&lt;li&gt;DB: &lt;strong&gt;V1&lt;/strong&gt; -&amp;gt; &lt;strong&gt;V2&lt;/strong&gt; (possibly failover from &lt;strong&gt;M1&lt;/strong&gt; to &lt;strong&gt;M2&lt;/strong&gt;), change is &lt;strong&gt;DROP INDEX&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;(optional) App: &lt;strong&gt;V2&lt;/strong&gt; -&amp;gt; &lt;strong&gt;V3&lt;/strong&gt;. Enable functionality that inserts duplicates.&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;DROP FOREIGN KEY&lt;/h4&gt;
&lt;p&gt;Again, we are removing a constraint.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;DB: &lt;strong&gt;V1&lt;/strong&gt; -&amp;gt; &lt;strong&gt;V2&lt;/strong&gt; (possibly failover from &lt;strong&gt;M1&lt;/strong&gt; to &lt;strong&gt;M2&lt;/strong&gt;), change is &lt;strong&gt;DROP INDEX&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;(optional) App: &lt;strong&gt;V2&lt;/strong&gt; -&amp;gt; &lt;strong&gt;V3&lt;/strong&gt;. Enable functionality that conflicts with removed constraint. I mean, if you really know what you are doing.&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;DROP TABLE&lt;/h4&gt;
&lt;p&gt;The very simple steps are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;App: &lt;strong&gt;V1&lt;/strong&gt; -&amp;gt; &lt;strong&gt;V2&lt;/strong&gt;. Make sure no reference to table is made.&lt;/li&gt;
&lt;li&gt;DB: &lt;strong&gt;V1&lt;/strong&gt; -&amp;gt; &lt;strong&gt;V2&lt;/strong&gt;. Issue a &lt;strong&gt;DROP TABLE&lt;/strong&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;With &lt;strong&gt;ext3&lt;/strong&gt; dropping a large table is no less than a nightmare. Not only does the action take long time, it also locks down the table cache, which very quickly leads to having dozens of queries hang. &lt;strong&gt;xfs&lt;/strong&gt; is a good alternative.&lt;/p&gt;
&lt;h4&gt;Conclusion&lt;/h4&gt;
&lt;p&gt;We looked at single table operations, coupled with application upgrades. By carefully looking at the process breakdown, multiple changes can be addressed with ease and safety. Not all operations are completely safe when used with replication failover. But they are mostly safe if you have some trust in your code.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Table refactoring &amp; application version upgrades, Part I</title>
      <link>/blog/mysql/table-refactoring-application-version-upgrades-part-i/</link>
      <pubDate>Tue, 10 Aug 2010 14:36:28 +0000</pubDate>
      
      <guid>/blog/mysql/table-refactoring-application-version-upgrades-part-i/</guid>
      <description>&lt;p&gt;A developer&#39;s major concern is: &lt;em&gt;How do I do application &amp;amp; database upgrades with minimal downtime? How do I synchronize between a DB&#39;s version upgrade and an application&#39;s version upgrade?&lt;br /&gt;
&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;I will break down the discussion into types of database refactoring operations, and I will limit to single table refactoring. The discussion will try to understand the need for refactoring and will dictate the steps towards a successful upgrade.&lt;/p&gt;
&lt;h4&gt;Reader prerequisites&lt;/h4&gt;
&lt;p&gt;I will assume MySQL to be the underlying database. To take a major component out of the equation: we may need to deal with very large tables, for which an &lt;strong&gt;ALTER&lt;/strong&gt; command may take long hours. I will assume familiarity with Master-Master (Active-Passive) replication, with possible use of &lt;a href=&#34;http://mysql-mmm.org/&#34;&gt;MMM for MySQL&lt;/a&gt;. When I describe &#34;Failover from &lt;strong&gt;M1&lt;/strong&gt; to &lt;strong&gt;M2&lt;/strong&gt;&#34;, I mean &#34;Make the &lt;strong&gt;ALTER&lt;/strong&gt; changes on &lt;strong&gt;M2&lt;/strong&gt; (passive), then switch your application from &lt;strong&gt;M1&lt;/strong&gt; to &lt;strong&gt;M2&lt;/strong&gt; (change of IPs, VIP, etc.), promoting &lt;strong&gt;M2&lt;/strong&gt; to active position, then apply same changes on &lt;strong&gt;M1&lt;/strong&gt; (now passive) or completely rebuild it&#34;.&lt;/p&gt;
&lt;p&gt;Phew, a one sentence description of M-M usage...&lt;/p&gt;
&lt;p&gt;I also assume the reader&#39;s understanding that a table&#39;s schema can be different on master &amp;amp; slave, which is the basis for the &#34;use replication for refactoring&#34; trick. But it cannot be too different, or, to be precise, the two schemata must both support the ongoing queries for the table.&lt;/p&gt;
&lt;p&gt;A full discussion of the above is beyond the scope of this post.&lt;/p&gt;
&lt;h4&gt;Types of refactoring needs&lt;/h4&gt;
&lt;p&gt;As I limit this discussion to single table refactoring,we can look at major refactoring operations and their impact on application &amp;amp; upgrades. We will discuss ADD/DROP COLUMN, ADD/DROP INDEX, ADD/DROP UNIQUE INDEX, ADD/DROP FOREIGN KEY, ADD/DROP TABLE.&lt;/p&gt;
&lt;p&gt;We will assume the database and application are both in Version #1 (&lt;strong&gt;V1&lt;/strong&gt;), and need to be upgraded to &lt;strong&gt;V2&lt;/strong&gt; or greater.&lt;!--more--&gt;&lt;/p&gt;
&lt;h4&gt;ADD INDEX&lt;/h4&gt;
&lt;p&gt;Starting with the easier actions. Why would you add an index? Either:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;There is some existing query which can be optimized by the new query&lt;/li&gt;
&lt;li&gt;Or there is some new functionality which issues a query for which the new index is required.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Adding an index is an easy action in that the table&#39;s data does not really change.&lt;/p&gt;
&lt;p&gt;In case &lt;strong&gt;#1&lt;/strong&gt;, all you need to do is to add the new index (if the table is large, fail over from &lt;strong&gt;M1&lt;/strong&gt; to &lt;strong&gt;M2&lt;/strong&gt;). There is no application upgrade, so all that happens is that the database upgrades &lt;strong&gt;V1 &lt;/strong&gt;-&amp;gt;&lt;strong&gt; V2&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;In case &lt;strong&gt;#2&lt;/strong&gt;, the database must be prepared with new schema before the new functionality/query is introduced (since it depends on the existence of the index). The steps, therefore, are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;DB: &lt;strong&gt;V1&lt;/strong&gt; -&amp;gt; &lt;strong&gt;V2&lt;/strong&gt; (possibly failover from &lt;strong&gt;M1&lt;/strong&gt; to &lt;strong&gt;M2&lt;/strong&gt;)&lt;/li&gt;
&lt;li&gt;(Sometime later) App: &lt;strong&gt;V1&lt;/strong&gt; -&amp;gt; &lt;strong&gt;V2&lt;/strong&gt;. Application will issue queries which utilize the new index.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The application does not have to be upgraded at the same instant the DB gets upgraded. In fact, we&#39;ll see that this is a typical scenario: we can separate upgrades into smaller steps, which allow for time lapse. One &lt;em&gt;could&lt;/em&gt; work out steps &lt;strong&gt;1&lt;/strong&gt; &amp;amp; &lt;strong&gt;2&lt;/strong&gt; together, but that would take an extra effort.&lt;/p&gt;
&lt;h4&gt;ADD COLUMN&lt;/h4&gt;
&lt;p&gt;This must be one of the most common table schema upgrades: a new property is needed on the application side. It must be supported by the database. Perhaps a new field in some Java Object, with Hibernate mapping that field onto a new column. Or maybe the new column is there for purpose of de-normalization.&lt;/p&gt;
&lt;p&gt;This is also a more complicated task. Let&#39;s look at the required steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;DB: &lt;strong&gt;V1&lt;/strong&gt; -&amp;gt; &lt;strong&gt;V2&lt;/strong&gt; (possibly failover from &lt;strong&gt;M1&lt;/strong&gt; to &lt;strong&gt;M2&lt;/strong&gt;), change is &lt;strong&gt;ADD COLUMN&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;App: &lt;strong&gt;V1&lt;/strong&gt; -&amp;gt; &lt;strong&gt;V2&lt;/strong&gt;. Change is: provide column value for newly &lt;strong&gt;INSERT&lt;/strong&gt;ed rows.&lt;/li&gt;
&lt;li&gt;If needed, retroactively update column values for all pre-existing rows.&lt;/li&gt;
&lt;li&gt;App: &lt;strong&gt;V2&lt;/strong&gt; -&amp;gt; &lt;strong&gt;V3&lt;/strong&gt;. Application begins to use (read, &lt;strong&gt;SELECT&lt;/strong&gt;) new column.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The above procedure assumes that the new column must have some calculated value. A 10-million rows table must now be updated, to have the correct values filled in. So we ask of the application to start filling in data for new rows, which makes the invalid row set static. We can just take a &#34;from row&#34; and a &#34;to row&#34; and fill in the missing column&#39;s value for those rows. Only when all rows contain valid values can we let the application start using that row. This makes for &lt;em&gt;two&lt;/em&gt; application upgrades.&lt;/p&gt;
&lt;p&gt;If you&#39;re content with just a static &lt;strong&gt;DEFAULT&lt;/strong&gt; value, then step &lt;strong&gt;3&lt;/strong&gt; can be skipped, and step &lt;strong&gt;4&lt;/strong&gt; can be merged with step &lt;strong&gt;2&lt;/strong&gt;.&lt;/p&gt;
&lt;h4&gt;ADD UNIQUE INDEX&lt;/h4&gt;
&lt;p&gt;This is an altogether different case than the normal &lt;strong&gt;ADD INDEX&lt;/strong&gt;, even though they may seem similar. And the case is particularly different when using Master-Slave failover for rebuilding the table.&lt;/p&gt;
&lt;p&gt;Consider the case where we add a &lt;strong&gt;UNIQUE INDEX&lt;/strong&gt; on a slave. Some &lt;strong&gt;INSERT&lt;/strong&gt; query executes on the master, successfully, and is logged to the binary log. The slave picks it up, tries to execute it, to find that it fails on a DUPLICATE KEY error.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;UNIQUE INDEX&lt;/strong&gt; is a constraint, and it makes the slave more constrained than the master. This is a delicate situation. Here how to (mostly) work it out:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;App: &lt;strong&gt;V1&lt;/strong&gt; -&amp;gt; &lt;strong&gt;V2&lt;/strong&gt;. Change &lt;strong&gt;INSERT&lt;/strong&gt; queries on relevant table to &lt;strong&gt;INSERT IGNORE&lt;/strong&gt; or &lt;strong&gt;REPLACE&lt;/strong&gt; queries, whichever is more appropriate.&lt;/li&gt;
&lt;li&gt;DB: &lt;strong&gt;V1&lt;/strong&gt; -&amp;gt; &lt;strong&gt;V2&lt;/strong&gt; (possibly failover from &lt;strong&gt;M1&lt;/strong&gt; to &lt;strong&gt;M2&lt;/strong&gt;), change is &lt;strong&gt;ADD UNIQUE KEY&lt;/strong&gt; (and while at it, a tip: are you aware of &lt;a href=&#34;http://dev.mysql.com/doc/refman/5.1/en/alter-table.html&#34;&gt;ALTER IGNORE TABLE&lt;/a&gt;?)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The change of query ensures that the query will succeed on the slave (either by silently doing nothing or by actually replacing content). It also means that the slave can now have different data than the master. Of course, it you trust your application to never &lt;strong&gt;INSERT&lt;/strong&gt; duplicates, you can sleep better.&lt;/p&gt;
&lt;p&gt;We do not handle &lt;strong&gt;UPDATE&lt;/strong&gt; statements here.&lt;/p&gt;
&lt;h4&gt;ADD CONSTRAINT FOREIGN KEY&lt;/h4&gt;
&lt;p&gt;As with &lt;strong&gt;ADD UNIQUE INDEX&lt;/strong&gt;, there is a new constraint here. A slave becomes more constrained than the master. But we now have to make sure &lt;strong&gt;INSERT&lt;/strong&gt;, &lt;strong&gt;UPDATE&lt;/strong&gt; and &lt;strong&gt;DELETE&lt;/strong&gt; statements all go peacefully (well, it also depends on the type of &lt;strong&gt;ON DELETE&lt;/strong&gt; and &lt;strong&gt;ON UPDATE&lt;/strong&gt; property of the FK).&lt;/p&gt;
&lt;p&gt;The steps would be:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;DB: &lt;strong&gt;V1&lt;/strong&gt; -&amp;gt; &lt;strong&gt;V2&lt;/strong&gt; (possibly failover from &lt;strong&gt;M1&lt;/strong&gt; to &lt;strong&gt;M2&lt;/strong&gt;), change is &lt;strong&gt;ADD CONSTRAINT FOREIGN KEY&lt;/strong&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;And then cross your fingers or have trust in your application. If the table is small enough, one does not have to use replication to do the refactoring, and life is simpler. Just execute the &lt;strong&gt;ALTER&lt;/strong&gt; on the active master, and continue with your life.&lt;/p&gt;
&lt;h4&gt;CREATE TABLE&lt;/h4&gt;
&lt;p&gt;This is a simple case, since the table is new. The steps are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;DB: &lt;strong&gt;V1&lt;/strong&gt; -&amp;gt; &lt;strong&gt;V2&lt;/strong&gt; (no need to use slaves here)&lt;/li&gt;
&lt;li&gt;App: &lt;strong&gt;V1&lt;/strong&gt; -&amp;gt; &lt;strong&gt;V2&lt;/strong&gt;. Application will start using new table.&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;Conslusion&lt;/h4&gt;
&lt;p&gt;Having such steps formalized help with development management and database management. It makes clear what is expected of the application, and what is expected of the database. The breaking down of these operations into sequential steps allows us to work more slowly; make preparation work; work within our own working hours; get a chance to see the family.&lt;/p&gt;
&lt;p&gt;In this post we took a look at &#34;creation&#34; refactoring changes. New columns, new keys, new constraints. In the next part of this article, we&#39;ll discuss &lt;strong&gt;DROP&lt;/strong&gt; operations.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>SQL: good comments conventions</title>
      <link>/blog/mysql/sql-good-comments-conventions/</link>
      <pubDate>Thu, 01 Jul 2010 09:36:32 +0000</pubDate>
      
      <guid>/blog/mysql/sql-good-comments-conventions/</guid>
      <description>&lt;p&gt;I happened upon a customer who left me in awe and admiration. The reason: excellent comments for their SQL code.&lt;/p&gt;
&lt;p&gt;I list four major places where SQL comments are helpful. I&#39;ll use the &lt;a href=&#34;http://dev.mysql.com/doc/sakila/en/sakila.html&#34;&gt;sakila&lt;/a&gt; database. It is originally scarcely commented; I&#39;ll present it now enhanced with comments, to illustrate.&lt;/p&gt;
&lt;h4&gt;Table definitions&lt;/h4&gt;
&lt;p&gt;The &lt;strong&gt;CREATE TABLE&lt;/strong&gt; statement allows for a comment, intended to describe the nature of the table:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;CREATE TABLE `film_text` (
 `film_id` smallint(6) NOT NULL,
 `title` varchar(255) NOT NULL,
 `description` text,
 PRIMARY KEY (`film_id`),
 FULLTEXT KEY `idx_title_description` (`title`,`description`)
) ENGINE=MyISAM DEFAULT CHARSET=utf8 &lt;strong&gt;COMMENT=&#39;Reflection of `film`, used for FULLTEXT search.&#39;&lt;/strong&gt;
&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;It&#39;s too bad the comment&#39;s max length is 60 characters, though. However, it&#39;s a very powerful field.&lt;/p&gt;
&lt;h4&gt;Column definitions&lt;/h4&gt;
&lt;p&gt;One may comment particular columns:&lt;!--more--&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;CREATE TABLE `film` (
 `film_id` smallint(5) unsigned NOT NULL AUTO_INCREMENT,
 `title` varchar(255) NOT NULL,
 `description` text,
 `release_year` year(4) DEFAULT NULL,
 `language_id` tinyint(3) unsigned NOT NULL &lt;strong&gt;COMMENT &#39;Soundtrack spoken language&#39;&lt;/strong&gt;,
 `original_language_id` tinyint(3) unsigned DEFAULT NULL &lt;strong&gt;COMMENT &#39;Filmed spoken language&#39;&lt;/strong&gt;,
 `rental_duration` tinyint(3) unsigned NOT NULL DEFAULT &#39;3&#39;,
 `rental_rate` decimal(4,2) NOT NULL DEFAULT &#39;4.99&#39;,
 `length` smallint(5) unsigned DEFAULT NULL,
 `replacement_cost` decimal(5,2) NOT NULL DEFAULT &#39;19.99&#39;,
  ...
) ENGINE=InnoDB AUTO_INCREMENT=1001 DEFAULT CHARSET=utf8
&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;h4&gt;Stored routines definitions&lt;/h4&gt;
&lt;p&gt;Here&#39;s an original &lt;strong&gt;sakila&lt;/strong&gt; procedure, untouched. It is already commented:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;CREATE DEFINER=`root`@`localhost` PROCEDURE `rewards_report`(
 IN min_monthly_purchases TINYINT UNSIGNED
 , IN min_dollar_amount_purchased DECIMAL(10,2) UNSIGNED
 , OUT count_rewardees INT
)
 READS SQL DATA
 &lt;strong&gt;COMMENT &#39;Provides a customizable report on best customers&#39;&lt;/strong&gt;
BEGIN

 DECLARE last_month_start DATE;
 DECLARE last_month_end DATE;
 ...
&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;h4&gt;SQL queries&lt;/h4&gt;
&lt;p&gt;Last but not least, while not part of the schema, SQL queries define the use of the schema. That is, the schema exists for the sole reason of being able to query it.&lt;/p&gt;
&lt;p&gt;Where did &lt;em&gt;that&lt;/em&gt; query come from? Which piece of code issued it? Why? What&#39;s its purpose?&lt;/p&gt;
&lt;p&gt;Looking at the &lt;strong&gt;PROCESSLIST&lt;/strong&gt;, the slow log, etc., it is easier when the queries are commented:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;SELECT
 &lt;strong&gt;/* List film details along with participating actors */&lt;/strong&gt;
 &lt;strong&gt;/* Issued by analytics module */&lt;/strong&gt;
 film.*,
 COUNT(*) AS count_actors,
 GROUP_CONCAT(CONCAT(actor.first_name, &#39; &#39;, actor.last_name))
FROM
 film
 JOIN film_actor USING(film_id)
 JOIN actor USING(actor_id)
GROUP BY film.film_id;
&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;h4&gt;Conclusion&lt;/h4&gt;
&lt;p&gt;Source code commenting is an important practice, and usually watched out for. SQL &amp;amp; table definitions commenting are often scarce or non-existent. I urge DBAs to adopt a comments coding convention for SQL, and apply it whenever they can.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Database schema under version control</title>
      <link>/blog/mysql/database-schema-under-version-control/</link>
      <pubDate>Thu, 22 Apr 2010 09:04:04 +0000</pubDate>
      
      <guid>/blog/mysql/database-schema-under-version-control/</guid>
      <description>&lt;p&gt;How many organization use version control for development? Probably almost every single one.&lt;/p&gt;
&lt;p&gt;How many store the database schema under version control? Alas, not as many.&lt;/p&gt;
&lt;p&gt;Coupling one&#39;s application with table schema is essential. Organization who actively support multiple versions of the product understand that well. Smaller organizations not always have this realization.&lt;/p&gt;
&lt;p&gt;How is it done?&lt;/p&gt;
&lt;h4&gt;Ideally&lt;/h4&gt;
&lt;p&gt;Ideally one would have:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The schema, generated by hand&lt;/li&gt;
&lt;li&gt;Essential data (&lt;strong&gt;INSERT INTO&lt;/strong&gt; statements for those lookup tables without which you cannot have an application)&lt;/li&gt;
&lt;li&gt;Sample data: sufficient real-life data on which to act. This would include customers data, logs, etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you can work this way, then creating a staging environment consists of re-creating the entire schema from out schema code, and filling in the essential &amp;amp; sample data.&lt;/p&gt;
&lt;p&gt;The thing with this method is that one does not (usually?) apply it on one&#39;s live system. Say we were to add a column. On our live servers we would issue an &lt;strong&gt;ALTER TABLE t ADD COLUMN&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;But this means we&#39;re using different methods on our staging server and on our production server.&lt;/p&gt;
&lt;h4&gt;Incremental&lt;/h4&gt;
&lt;p&gt;Another kind of solution would be to hold:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The static schema, as before&lt;/li&gt;
&lt;li&gt;Essential data, as before&lt;/li&gt;
&lt;li&gt;Sample data, as before&lt;/li&gt;
&lt;li&gt;A migration script, which is the concatenation of all ALTERs, CREATEs etc. as of the static schema.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Once in a while one can do a &#34;reset&#34;, and update the static schema with the existing design.&lt;/p&gt;
&lt;h4&gt;As you go along&lt;/h4&gt;
&lt;p&gt;This solution simply means &#34;we apply the changes on staging; test + version them; then apply on production&#34;.&lt;/p&gt;
&lt;p&gt;A side effect of this solution is that the database generates the schema, but the schema does not generate the database as in previous cases. This makes for an uglier solution, where you first apply the changes to the database, and then, based on what the database report, enter data into the version control.&lt;/p&gt;
&lt;p&gt;How to do that? Easiest would be to use &lt;strong&gt;mysqldump --routines --no-data&lt;/strong&gt;. Some further parsing should be done to strip out the &lt;strong&gt;AUTO_INCREMENT&lt;/strong&gt; values, which tend to change, as well as the surrounding variables settings (strip out the character set settings etc.).&lt;/p&gt;
&lt;h4&gt;Summary&lt;/h4&gt;
&lt;p&gt;However you do it, make sure you have some kind of version control on your schema. It pays off just as with doing version control for your code. You get to compare, understand the incremental changes, understand the change in design, etc.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>