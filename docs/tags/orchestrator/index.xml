<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Orchestrator on code.openark.org</title>
    <link>/blog/tags/orchestrator/</link>
    <description>Recent content in Orchestrator on code.openark.org</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2016. All rights reserved.</copyright>
    <lastBuildDate>Wed, 23 Dec 2015 18:01:59 +0000</lastBuildDate>
    <atom:link href="/blog/tags/orchestrator/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Orchestrator progress</title>
      <link>/blog/2015/12/23/orchestrator-progress/</link>
      <pubDate>Wed, 23 Dec 2015 18:01:59 +0000</pubDate>
      
      <guid>/blog/2015/12/23/orchestrator-progress/</guid>
      <description>&lt;p&gt;This comes mostly to reassure, having moved into GitHub: &lt;a href=&#34;https://github.com/outbrain/orchestrator&#34;&gt;orchestrator&lt;/a&gt; development continues.&lt;/p&gt;
&lt;p&gt;I will have the privilege of working on this open source solution in GitHub. There are a few directions we can take orchestrator to, and we will be looking into the possibilities. We will continue to strengthen the crash recovery process, and in fact I&#39;ve got a couple ideas on drastically shortening Pseudo-GTID recovery time as well as other debts. We will look into yet other directions, which we will share. My new and distinguished team will co-work on/with orchestrator and will no doubt provide useful and actionable input.&lt;/p&gt;
&lt;p&gt;Orchestrator continues to be open for pull requests, with a temporal latency in response time (it&#39;s the Holidays, mostly).&lt;/p&gt;
&lt;p&gt;Some Go(lang) limitations (namely the import path, I&#39;ll blog more about it) will most probably imply some changes to the code, which will be well communicated to existing collaborators.&lt;/p&gt;
&lt;p&gt;Most of all, we will keep orchestrator a generic solution, while keeping focus on what we think is most important - and there&#39;s some interesting vision here. Time will reveal as we make progress.&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>State of automated recovery via Pseudo-GTID &amp; Orchestrator @ Booking.com</title>
      <link>/blog/2015/11/20/state-of-automated-recovery-via-pseudo-gtid-orchestrator-booking-com/</link>
      <pubDate>Fri, 20 Nov 2015 11:41:13 +0000</pubDate>
      
      <guid>/blog/2015/11/20/state-of-automated-recovery-via-pseudo-gtid-orchestrator-booking-com/</guid>
      <description>&lt;p&gt;This post sums up some of my work on MySQL resilience and high availability at &lt;a href=&#34;http://www.booking.com&#34;&gt;Booking.com&lt;/a&gt; by presenting the current state of automated master and intermediate master recoveries via &lt;a href=&#34;http://code.openark.org/blog/mysql/refactoring-replication-topology-with-pseudo-gtid&#34;&gt;Pseudo-GTID&lt;/a&gt; &amp;amp; &lt;strong&gt;&lt;a href=&#34;https://github.com/outbrain/orchestrator&#34;&gt;Orchestrator&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Booking.com uses many different MySQL topologies, of varying vendors, configurations and workloads: Oracle MySQL, MariaDB, statement based replication, row based replication, hybrid, OLTP, OLAP, GTID (few), no GTID (most), Binlog Servers, filters, hybrid of all the above.&lt;/p&gt;
&lt;p&gt;Topologies size varies from a single server to many-many-many. Our typical topology has a master in one datacenter, a bunch of slaves in same DC, a slave in another DC acting as an intermediate master to further bunch of slaves in the other DC. Something like this, give or take:&lt;/p&gt;
&lt;blockquote&gt;&lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2015/11/booking-topology-sample.png&#34;&gt;&lt;img class=&#34;alignnone wp-image-7480 size-medium&#34; src=&#34;/blog/blog/assets/booking-topology-sample-300x169.png&#34; alt=&#34;booking-topology-sample&#34; width=&#34;300&#34; height=&#34;169&#34; /&gt;&lt;/a&gt;&lt;/blockquote&gt;
&lt;p&gt;However as we are building our third data center (with MySQL deployments mostly completed) the graph turns more complex.&lt;/p&gt;
&lt;p&gt;Two high availability questions are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What happens when an intermediate master dies? What of all its slaves?&lt;/li&gt;
&lt;li&gt;What happens when the master dies? What of the entire topology?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is not a technical drill down into the solution, but rather on overview of the state. For more, please refer to recent presentations in &lt;a href=&#34;https://speakerdeck.com/shlominoach/managing-and-visualizing-your-replication-topologies-with-orchestrator&#34;&gt;September&lt;/a&gt; and &lt;a href=&#34;https://speakerdeck.com/shlominoach/pseudo-gtid-and-easy-mysql-replication-topology-management&#34;&gt;April&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;At this time we have:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pseudo-GTID deployed on all chains
&lt;ul&gt;
&lt;li&gt;Injected every 5 seconds&lt;/li&gt;
&lt;li&gt;Using the &lt;a href=&#34;http://code.openark.org/blog/mysql/pseudo-gtid-ascending&#34;&gt;monotonically ascending&lt;/a&gt; variation&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Pseudo-GTID based automated failover for intermediate masters on all chains&lt;/li&gt;
&lt;li&gt;Pseudo-GTID based automated failover for masters on roughly 30% of the chains.
&lt;ul&gt;
&lt;li&gt;The rest of 70% of chains are set for manual failover using Pseudo-GTID.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Pseudo-GTID is in particular used for:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Salvaging slaves of a dead intermediate master&lt;/li&gt;
&lt;li&gt;Correctly grouping and connecting slaves of a dead master&lt;/li&gt;
&lt;li&gt;Routine refactoring of topologies. This includes:
&lt;ul&gt;
&lt;li&gt;Manual repointing of slaves for various operations (e.g. offloading slaves from a busy box)&lt;/li&gt;
&lt;li&gt;Automated refactoring (for example, used by our automated upgrading script, which consults with &lt;em&gt;orchestrator&lt;/em&gt;, upgrades, shuffles slaves around, updates intermediate master, suffles back...)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;(In the works), failing over binlog reader apps that audit our binary logs.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;!--more--&gt;Furthermore, Booking.com is also &lt;a href=&#34;https://www.percona.com/live/europe-amsterdam-2015/sessions/binlog-servers-bookingcom&#34;&gt;working on Binlog Servers&lt;/a&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;These take production traffic and offload masters and intermediate masters&lt;/li&gt;
&lt;li&gt;Often co-serve slaves using round-robin VIP, such that failure of one Binlog Server makes for simple slave replication self-recovery.&lt;/li&gt;
&lt;li&gt;Are interleaved alongside standard replication
&lt;ul&gt;
&lt;li&gt;At this time we have no &#34;pure&#34; Binlog Server topology in production; we always have normal intermediate masters and slaves&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;This hybrid state makes for greater complexity:
&lt;ul&gt;
&lt;li&gt;Binlog Servers are not designed to participate in a game of changing masters/intermediate master, unless &lt;a href=&#34;http://jfg-mysql.blogspot.nl/2015/09/abstracting-binlog-servers-and-mysql-master-promotion-wo-reconfiguring-slaves.html&#34;&gt;successors come from their own sub-topology&lt;/a&gt;, which is not the case today.
&lt;ul&gt;
&lt;li&gt;For example, a Binlog Server that replicates directly from the master, cannot be repointed to just any new master.&lt;/li&gt;
&lt;li&gt;But can still hold valuable binary log entries that other slaves may not.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Are not actual MySQL servers, therefore of course cannot be promoted as masters&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Orchestrator&lt;/em&gt; &amp;amp; Pseudo-GTID makes this hybrid topology still resilient:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Orchestrator&lt;/em&gt; understands the limitations on the hybrid topology and can salvage slaves of 1st tier Binlog Servers via Pseudo-GTID&lt;/li&gt;
&lt;li&gt;In the case where the Binlog Servers were the most up to date slaves of a failed master, &lt;em&gt;orchestrator&lt;/em&gt; knows to first move potential candidates under the Binlog Server and then extract them out again.&lt;/li&gt;
&lt;li&gt;At this time Binlog Servers are still unstable. Pseudo-GTID allows us to comfortably test them on a large setup with reduced fear of losing slaves.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Otherwise &lt;em&gt;orchestrator&lt;/em&gt; already understands pure Binlog Server topologies and can do master promotion. When pure binlog servers topologies will be in production &lt;em&gt;orchestrator&lt;/em&gt; will be there to watch over.&lt;/p&gt;
&lt;h3&gt;Summary&lt;/h3&gt;
&lt;p&gt;To date, Pseudo-GTID has high scores in automated failovers of our topologies; &lt;em&gt;orchestrator&#39;s&lt;/em&gt; &lt;a href=&#34;http://code.openark.org/blog/mysql/what-makes-a-mysql-server-failurerecovery-case&#34;&gt;holistic approach&lt;/a&gt; makes for reliable diagnostics; together they reduce our dependency on specific servers &amp;amp; hardware, physical location, latency implied by SAN devices.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Orchestrator &amp; Pseudo-GTID for binlog reader failover</title>
      <link>/blog/2015/11/19/orchestrator-pseudo-gtid-for-binlog-reader-failover/</link>
      <pubDate>Thu, 19 Nov 2015 10:52:16 +0000</pubDate>
      
      <guid>/blog/2015/11/19/orchestrator-pseudo-gtid-for-binlog-reader-failover/</guid>
      <description>&lt;p&gt;One of our internal apps at &lt;strong&gt;Booking.com&lt;/strong&gt; audits changes to our tables on various clusters. We used to use &lt;em&gt;tungsten replicator&lt;/em&gt;, but have since migrated onto our own solution.&lt;/p&gt;
&lt;p&gt;We have a binlog reader (uses &lt;a href=&#34;https://github.com/zendesk/open-replicator&#34;&gt;open-replicator&lt;/a&gt;) running on a slave. It expects Row Based Replication, hence our slave runs with &lt;strong&gt;log-slave-updates&lt;/strong&gt;, &lt;strong&gt;binlog-format=&#39;ROW&#39;&lt;/strong&gt;, to translate from the master&#39;s Statement Based Replication. The binlog reader reads what it needs to read, audits what it needs to audit, and we&#39;re happy.&lt;/p&gt;
&lt;h3&gt;However what happens if that slave dies?&lt;/h3&gt;
&lt;p&gt;In such case we need to be able to point our binlog reader to another slave, and it needs to be able to pick up auditing from the same point.&lt;/p&gt;
&lt;p&gt;This sounds an awful lot like slave repointing in case of master/intermediate master failure, and indeed the solutions are similar. However our binlog reader is not a real MySQL server and does not understands replication. It does not really replicate, it just parses binary logs.&lt;/p&gt;
&lt;p&gt;We&#39;re also not using GTID. But we &lt;em&gt;are&lt;/em&gt; using Pseudo-GTID. As it turns out, the failover solution is already built in by &lt;a href=&#34;https://github.com/outbrain/orchestrator&#34;&gt;orchestrator&lt;/a&gt;, and this is how it goes:&lt;/p&gt;
&lt;h3&gt;Normal execution&lt;/h3&gt;
&lt;p&gt;Our binlog app reads entries from the binary log. Some are of interest for auditing purposes, some are not. An occasional Pseudo-GTID entry is found, and is being stored to ZooKeeper tagged as  &#34;last seen and processed Pseudo-GTID&#34;.&lt;/p&gt;
&lt;h3&gt;Upon slave failure&lt;/h3&gt;
&lt;p&gt;We recognize the death of a slave; we have other slaves in the pool; we pick another. Now we need to find the coordinates from which to carry on.&lt;/p&gt;
&lt;p&gt;We read our &#34;last seen and processed Pseudo-GTID&#34;. Say it reads:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;drop view if exists `meta`.`_pseudo_gtid_hint__asc:56373F17:00000000012B1C8B:50EC77A1`&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;. We now issue:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;$ orchestrator &lt;strong&gt;-c find-binlog-entry&lt;/strong&gt; &lt;strong&gt;-i new.slave.fqdn.com&lt;/strong&gt; --pattern=&#39;drop view if exists `meta`.`_pseudo_gtid_hint__asc:56373F17:00000000012B1C8B:50EC77A1`&#39;&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;The output of such command are the binlog coordinates of that same entry as found in the new slave&#39;s binlogs:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;binlog.000148:43664433&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;Pseudo-GTID entries are only injected once every few seconds (&lt;strong&gt;5&lt;/strong&gt; in our case). Either:&lt;!--more--&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We are OK to reprocess up to &lt;strong&gt;5&lt;/strong&gt; seconds worth of data (and indeed we are, our mechanism is such that this merely overwrites our previous audit, no corruption happens)&lt;/li&gt;
&lt;li&gt;Or our binlog reader also keeps track of the number of events since the last processed Pseudo-GTID entry, skipping the same amount of events after failing over.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Planned failover&lt;/h3&gt;
&lt;p&gt;In case we plan to repoint our binlog reader to another slave, we can further use orchestrator&#39;s power in making an exact correlation between the binlog positions of two slaves. This has always been within its power, but only recently exposed as it own command. We can, at any stage:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;$ sudo orchestrator &lt;strong&gt;-c correlate-binlog-pos&lt;/strong&gt; -i current.instance.fqdn.com --binlog=binlog.002011:72656109 -d some.other.instance.fqdn.com&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;The output is the binlog coordinates in &lt;strong&gt;some.other.instance.fqdn.com&lt;/strong&gt; that exactly correlate with &lt;strong&gt;binlog.002011:72656109&lt;/strong&gt; in &lt;strong&gt;current.instance.fqdn.com&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The case of failure of the binlog reader itself is also handled, but is not the subject of this blog post.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Thoughts on MaxScale automated failover (and Orchestrator)</title>
      <link>/blog/2015/11/18/thoughts-on-maxscale-automated-failover-and-orchestrator/</link>
      <pubDate>Wed, 18 Nov 2015 11:17:48 +0000</pubDate>
      
      <guid>/blog/2015/11/18/thoughts-on-maxscale-automated-failover-and-orchestrator/</guid>
      <description>&lt;p&gt;Having attended a talk (as part of the &lt;a href=&#34;https://blog.mariadb.org/2015-developers-meeting-amsterdam/&#34;&gt;MariaDB Developer Meeting in Amsterdam&lt;/a&gt;) about recent developments of &lt;a href=&#34;https://mariadb.com/products/mariadb-maxscale&#34;&gt;MaxScale&lt;/a&gt; in executing automated failovers, here are some (late) observations of mine.&lt;/p&gt;
&lt;p&gt;I will begin by noting that the project is stated to be pre-production, and so of course none of the below are complaints, but rather food for thought, points for action and otherwise recommendations.&lt;/p&gt;
&lt;p&gt;Some functionality of the MaxScale failover is also implemented by &lt;strong&gt;&lt;a href=&#34;https://github.com/outbrain/orchestrator&#34;&gt;orchestrator&lt;/a&gt;&lt;/strong&gt;, which I author. &lt;em&gt;Orchestrator&lt;/em&gt; was built in production environments by and for operational people. In this respect it has gained many insights and had to cope with many real-world cases, special cases &amp;amp; Murphy&#39;s law cases. This post compares logic, feature set and capabilities of the two where relevant. To some extent the below will read as &#34;hey, I&#39;ve already implemented this; shame to re-implement the same&#34;, and indeed I think that way; but it wouldn&#39;t be the first time a code of mine would just be re-implemented by someone else and I&#39;ve done the same, myself.&lt;/p&gt;
&lt;p&gt;I&#39;m describing the solution the way I understood it from the talk. If I&#39;m wrong on any account I&#39;m happy to be corrected via comments below. &lt;strong&gt;Edit:&lt;/strong&gt; &lt;em&gt;please see comment by&lt;/em&gt; &lt;a class=&#34;url&#34; href=&#34;http://www.mariadb.com/&#34; rel=&#34;external nofollow&#34;&gt;Dipti Joshi&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;General overview&lt;/h3&gt;
&lt;p&gt;The idea is that MaxScale operates as a proxy to your topology. You do not connect to your master directly, but rather through MaxScale. Thus, MaxScale acts as a proxy to your master.&lt;/p&gt;
&lt;p&gt;The next phase is that MaxScale would also auto-detect master failure, fix the topology for you, promote a new master, and will have your application unaware of all the complexity and without the app having to change setup/DNS/whatever. Of course some write downtime is implied.&lt;/p&gt;
&lt;p&gt;Now for some breakdown.&lt;/p&gt;
&lt;h3&gt;Detection&lt;/h3&gt;
&lt;p&gt;The detection of a dead master, the check by which a failover is initiated, is based on MaxScale not being able to query the master. This calls for some points for consideration:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Typically, I would see &#34;I can&#39;t connect to the master therefore failover&#34; as too hysterical, and the basis for a lot of false positives.&lt;/li&gt;
&lt;li&gt;However, since in the discussed configuration MaxScale is &lt;em&gt;the only access point&lt;/em&gt; to the master, the fact MaxScale cannot connect to the master means the master is inaccessible &lt;em&gt;de-facto&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;In light of the above, the decision makes sense - but I still hold that it would make false positives.&lt;/li&gt;
&lt;li&gt;I&#39;m unsure (I &lt;em&gt;think&lt;/em&gt; not; can anyone comment?) if MaxScale would make multiple attempts over time and only reach the conclusion after X successive failures. This would reduce the false positives.&lt;/li&gt;
&lt;li&gt;I&#39;m having a growing dislike to a &#34;check for 4 successive times then alert/failover&#34; Nagios-style behavior. &lt;em&gt;Orchestrator&lt;/em&gt; takes a different approach where it recognizes a master&#39;s death by not being able to connect to the master &lt;em&gt;as well as&lt;/em&gt; being able to connect to 1st tier slaves, check their status and observe that &lt;em&gt;they&#39;re unable to connect to the master as well&lt;/em&gt;. See &lt;a title=&#34;Permanent Link to What makes a MySQL server failure/recovery case?&#34; href=&#34;http://code.openark.org/blog/mysql/what-makes-a-mysql-server-failurerecovery-case&#34; rel=&#34;bookmark&#34;&gt;What makes a MySQL server failure/recovery case?&lt;/a&gt;. This approach still calls for further refinement (what if the master is temporarily deadlocked? Is this a failover or not?).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;!--more--&gt;&lt;/p&gt;
&lt;h3&gt;Assumed topology&lt;/h3&gt;
&lt;p&gt;MaxScale assumes the topology is all MariaDB, and all slaves are using (MariaDB) GTID replication. Well, MaxScale does not actually assumes that. It is assumed so by the &lt;a href=&#34;https://github.com/mariadb-corporation/replication-manager&#34;&gt;MariaDB Replication Manager&lt;/a&gt; which MaxScale invokes. But I&#39;m getting ahead of myself here.&lt;/p&gt;
&lt;h3&gt;Topology detection&lt;/h3&gt;
&lt;p&gt;MaxScale does not recognize the master by configuration but rather by state. It observes the servers it should observe, and concludes which is the master.&lt;/p&gt;
&lt;p&gt;I&#39;m using similar approach in &lt;em&gt;orchestrator&lt;/em&gt;. I maintain that this approach works well and opens the Chakras for complex recovery options.&lt;/p&gt;
&lt;h3&gt;Upon failure detection&lt;/h3&gt;
&lt;p&gt;When MaxScale detects failure, it invokes external scripts to fix the problem. There are some similar and different particulars here as compared to &lt;em&gt;orchestrator&lt;/em&gt;, and I will explain what&#39;s wrong with the MaxScale approach:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Although MaxScale observes the topology and understands who is the master and who isn&#39;t, the executed scripts do not. They need to re-discover everything by themselves.&lt;/li&gt;
&lt;li&gt;This implies the scripts start without memory of &#34;what was last observed&#34;. This is one of the greatest strengths of &lt;em&gt;orchestrator&lt;/em&gt;: it knows what the state was just before the failure, and, having the bigger picture, can make informed decisions.
&lt;ul&gt;
&lt;li&gt;As a nasty example, what do you do when some the first tier slaves also happen to be inaccessible at that time? What if one of those happens to further have slaves of its own?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;The MariaDB Replication Manager script (to be referenced as &lt;em&gt;repmgr&lt;/em&gt;) assumes all instances to be MariaDB with GTID.
&lt;ul&gt;
&lt;li&gt;It is also implied that all my slaves are configured with binary logs &amp;amp; log-slave-updates&lt;/li&gt;
&lt;li&gt;That&#39;s &lt;strong&gt;way too restrictive&lt;/strong&gt;.
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Orchestrator&lt;/em&gt; handles all following topologies: Oracle MySQL with/out GTID, MariaDB with/out GTID, MariaDB hybrid GTID &amp;amp; non-GTID replication, Pseudo-GTID (MySQL and/or MariaDB), hybrid normal &amp;amp; binlog servers topologies, slaves with/out log-slave-updates, hybrid Oracle &amp;amp; MariaDB &amp;amp; Binlog Servers &amp;amp; Pseudo-GTID.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;em&gt;repmgr&lt;/em&gt; is unaware of data centers &amp;amp; physical environments. You want failover to be as local to your datacenters as possible. Avoid too many cross-DC replication streams.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Failover invocation&lt;/h3&gt;
&lt;p&gt;MaxScale invokes the failover scripts &lt;em&gt;asynchronously&lt;/em&gt;. This is a major flaw imho, as the decoupling between the monitoring and acting processes leads to further problems, see further.&lt;/p&gt;
&lt;h3&gt;After failover&lt;/h3&gt;
&lt;p&gt;MaxScale continuously scans the topology and observes that some other server has been promoted. This behavior is similar to &lt;em&gt;orchestrator&#39;s&lt;/em&gt;. But the following differences are noteworthy:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Because of both the decoupling as well as the asynchronous invocation by MaxScale, it doesn&#39;t really have any idea if and how the promotion resolved.&lt;/li&gt;
&lt;li&gt;I don&#39;t know that there&#39;s any anti-flapping mechanism, nor that there could be. If MaxScale doesn&#39;t care what happened to the failover script, it shouldn&#39;t be able to keep up with flapping scenarios.&lt;/li&gt;
&lt;li&gt;Nor is there a minimal suspend period between any two failure recoveries, that I know of. MaxScale can actually have easier life than &lt;em&gt;orchestrator&lt;/em&gt; in this regard as it is (I suspect) strictly associated with &lt;em&gt;a topology&lt;/em&gt;. Not like there&#39;s a single MaxScale handling multiple topologies. So it should be very easy to keep track of failures.&lt;/li&gt;
&lt;li&gt;Or, if there is a minimal period and I&#39;m just uninformed -- what makes sure it is not smaller than the time it takes for the failover?&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Further on failover&lt;/h3&gt;
&lt;p&gt;I wish to point out that one component of the system analyses a failure scenario, and another one fixes it. I suggest this is an undesired design. The &#34;fixer&#34; must have its own ability to diagnose problems as it makes progress (or else it is naive and would fail in many production cases). And the &#34;analyzer&#34; part must have some wisdom of its own so as to suggest course of action; or understand the consequences of the recovery done by the &#34;fixer&#34;.&lt;/p&gt;
&lt;h3&gt;Use of shell scripts&lt;/h3&gt;
&lt;p&gt;Generally speaking, the use of shell scripts as external hooks is evil:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Shell scripts tend to be poorly audited&lt;/li&gt;
&lt;li&gt;With poor clarity as for what went wrong&lt;/li&gt;
&lt;li&gt;Killing them has operational difficulty (detect the shell script, find possible children, detached children)&lt;/li&gt;
&lt;li&gt;The approach of &#34;if you want something else, just write a shell script for it&#34; is nice for some things, but as the problem turns complex, you turn out to just write big parts of the solution in shell. This decouples your code to unwanted degree.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;At this time, &lt;em&gt;orchestrator&lt;/em&gt; also uses external hooks. However:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Fixing the topology happens within &lt;em&gt;orchestrator&lt;/em&gt;, not by external scripts. There is an elaborate, auditable, visible decision making.
&lt;ul&gt;
&lt;li&gt;Decision making includes data center considerations, different configuration of servers involved, servers hinted as candidates, servers configured to be ignored, servers known to be downtimed.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Leaving the external scripts with the task of managing DNS changes or what have you.
&lt;ul&gt;
&lt;li&gt;Today, at Booking.com, we have a special operational tool (called the dba tool) which does that, manages rosters, issues puppet etc. This tool is itself well audited. Granted, there is still decoupling, but information does not just get lost.&lt;/li&gt;
&lt;li&gt;Sometime in the future I suspect I will extend &lt;strong&gt;&lt;a href=&#34;https://github.com/outbrain/orchestrator-agent&#34;&gt;orchestrator-agent&lt;/a&gt;&lt;/strong&gt; to participate in failovers, which means the entire flow is within &lt;em&gt;orchestrator&#39;s&lt;/em&gt; scope.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;High availability&lt;/h3&gt;
&lt;p&gt;All the above is only available via a single MaxScale server. What happens if it dies?&lt;/p&gt;
&lt;p&gt;There is a MaxScale/pacemaker setup I&#39;m aware of. If one MaxScale dies, pacemaker takes charge and starts another on another box.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;But this means real downtime&lt;/li&gt;
&lt;li&gt;There are no multiple-MaxScale servers to load-balance on&lt;/li&gt;
&lt;li&gt;The MaxScale started by pacemaker is newly born, and does not have the big picture of the topology. It needs to go through a &#34;peaceful time&#34; to understand what&#39;s going on.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;More High Availability&lt;/h3&gt;
&lt;p&gt;At a time where MaxScale will be able to load-balance and run on multiple nodes, MariaDB will have to further tackle:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Leader election&lt;/li&gt;
&lt;li&gt;Avoiding concurrent initiation of failovers
&lt;ul&gt;
&lt;li&gt;Either via group communication&lt;/li&gt;
&lt;li&gt;Or via shared datastore&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Taking off from a failed/crashed MaxScale server&#39;s work
&lt;ul&gt;
&lt;li&gt;Or rolling it back&lt;/li&gt;
&lt;li&gt;Or just cleaning it up&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;And generally share all those little pieces of information, such as &#34;Hey, now this server is the master&#34; (are all MaxScales in complete agreement on the topology?) or &#34;I have failed over this topology, we should avoid failing it over again for the next 10 minutes&#34; and more.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The above are supported by &lt;em&gt;orchestrator&lt;/em&gt;. It provides leader election, automated leader promotion, fair recognition of various failure scenarios, picking up a failed recovery from a failed &lt;em&gt;orchestrator&lt;/em&gt;. Data is shared by a backend MySQL datastore, and before you shout &lt;em&gt;SPOF&lt;/em&gt;, make it Galera/NDB.&lt;/p&gt;
&lt;h3&gt;Further little things that can ruin your day&lt;/h3&gt;
&lt;h4&gt;How about having a delayed replica?&lt;/h4&gt;
&lt;p&gt;Here&#39;s an operational use case we had to tackle.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You have a slave configured to lag by &lt;strong&gt;24&lt;/strong&gt; hours. You know the drill: hackers / accidental &lt;strong&gt;DROP TABLE&lt;/strong&gt;...&lt;/li&gt;
&lt;li&gt;How much time will an automated tool spend on reconnecting this slave to the topology?
&lt;ul&gt;
&lt;li&gt;This could take long minutes&lt;/li&gt;
&lt;li&gt;Will your recovery hang till this is resolved?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Since &lt;em&gt;orchestrator&lt;/em&gt; heals the topology in-house, it knows how to push certain operations till after specific other operations took place. For example, &lt;em&gt;orchestrator&lt;/em&gt; wants to heal the entire topology, but pushes the delayed replicas aside, under the assumption that it will be able to fix them later (fair assumption, because they are known to be behind our promoted master); it will proceed to fix everything else, execute external hooks (change DNS etc.) and only then come back to the delayed replica. All the while, the process is audited.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Flapping ruins your day&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Not only do you want some stall period between two failovers, you also want your team to respond to a failover and acknowledge it. Or clear up the stall period having verified the source of the problem. Or force the next failover even if it comes sooner than the stall period termination.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Binlog formats&lt;/h4&gt;
&lt;p&gt;It is still not uncommon to have Statement Based Replication running. And then it is also not uncommon to have one or two slaves translating to Row Based Replication because of:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Some app that has to read ROW based format&lt;/li&gt;
&lt;li&gt;Experimenting with RBR for purposes of upgrade&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You just can&#39;t promote such a RBR slave on top of SBR slaves; it wouldn&#39;t work. &lt;em&gt;Orchestrator&lt;/em&gt; is aware of such rules. I still need to integrate this particular consideration into the promotion algorithm.&lt;/p&gt;
&lt;h4&gt;Versions&lt;/h4&gt;
&lt;p&gt;Likewise, not all your slaves are of same version. You should not promote a newer version slave on top of an older version slave. Again, &lt;em&gt;orchestrator&lt;/em&gt; will not allow putting such a topology, and again, I still need to integrate this consideration into the promotion algorithm.&lt;/p&gt;
&lt;h3&gt;In summary&lt;/h3&gt;
&lt;p&gt;There is a long way for MaxScale failover to go. When you consider the simplest, all-MariaDB-GTID-equal-slaves small topology case, things are kept simple and probably sustainable. But issues like complex topologies, flapping, special slaves, different configuration, high availability, leadership, acknowledgements, and more, call for a more advanced solution.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Speaking at Percona Live Amsterdam: Orchestrator</title>
      <link>/blog/2015/09/15/speaking-at-percona-live-amsterdam-orchestrator/</link>
      <pubDate>Tue, 15 Sep 2015 09:27:38 +0000</pubDate>
      
      <guid>/blog/2015/09/15/speaking-at-percona-live-amsterdam-orchestrator/</guid>
      <description>&lt;p&gt;In a week&#39;s time I&#39;ll be speaking at &lt;a href=&#34;https://www.percona.com/live/europe-amsterdam-2015/&#34;&gt;Percona Live Amsterdam&lt;/a&gt;. I will be presenting:&lt;/p&gt;
&lt;blockquote&gt;&lt;a href=&#34;https://www.percona.com/live/europe-amsterdam-2015/sessions/managing-and-visualizing-your-replication-topologies-orchestrator&#34;&gt;Managing and Visualizing your replication topologies with Orchestrator&lt;br /&gt;
&lt;/a&gt;23 September 4:20PM&lt;a href=&#34;https://www.percona.com/live/europe-amsterdam-2015/sessions/managing-and-visualizing-your-replication-topologies-orchestrator&#34;&gt;&lt;br /&gt;
&lt;/a&gt;&lt;/blockquote&gt;
&lt;p&gt;This talk will present &lt;strong&gt;&lt;a href=&#34;https://github.com/outbrain/orchestrator&#34;&gt;orchestrator&lt;/a&gt;&lt;/strong&gt;, on which I&#39;ve been working for the last year and a half, originally at &lt;a href=&#34;http://www.outbrain.com&#34;&gt;Outbrain&lt;/a&gt; and now at &lt;a href=&#34;https://workingatbooking.com/&#34;&gt;Booking.com&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I will show off what &lt;em&gt;orchestrator&lt;/em&gt; can do to manage your replication topologies. From visualization, through topology refactoring to automated crash recoveries, &lt;em&gt;orchestrator&lt;/em&gt; today plays a key role at Booking.com infrastructure, at scale (oh I love using these words).&lt;/p&gt;
&lt;p&gt;You can expect an outrageous demo, a visual walkthrough, some command line examples, and a lot on the logic and mechanisms behind &lt;em&gt;orchestrator&lt;/em&gt;. I will present the difficult problems &lt;em&gt;orchestrator&lt;/em&gt; covers.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;orchestrator&lt;/em&gt; is free and open source, and is built to be as generic as possible; it is known to be used by multiple well known companies these days, so please join the party.&lt;/p&gt;
&lt;p&gt;With that, I conclude with the almighty motto:&lt;!--more--&gt;&lt;/p&gt;
&lt;blockquote&gt;&lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2015/09/keep-calm-and-let-orchestrator-handle-it-transp-m.png&#34;&gt;&lt;img class=&#34;size-full wp-image-7399 alignnone&#34; src=&#34;/blog/blog/assets/keep-calm-and-let-orchestrator-handle-it-transp-m.png&#34; alt=&#34;keep-calm-and-let-orchestrator-handle-it-transp-m&#34; width=&#34;500&#34; height=&#34;571&#34; /&gt;&lt;/a&gt;&lt;/blockquote&gt;
&lt;p&gt;https://speakerdeck.com/shlominoach/managing-and-visualizing-your-replication-topologies-with-orchestrator&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Orchestrator visual cheatsheet, TL;DR the &#34;smart&#34; way</title>
      <link>/blog/2015/09/02/orchestrator-visual-cheatsheet-tldr-the-smart-way/</link>
      <pubDate>Wed, 02 Sep 2015 09:14:05 +0000</pubDate>
      
      <guid>/blog/2015/09/02/orchestrator-visual-cheatsheet-tldr-the-smart-way/</guid>
      <description>&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/outbrain/orchestrator&#34;&gt;Orchestrator&lt;/a&gt;&lt;/strong&gt; is &lt;em&gt;really&lt;/em&gt; growing. And the amount of users (DBAs, sys admins) using it is growing. Which gives me a lot of immediate feedback in the form of &lt;em&gt;&#34;Look, there&#39;s just too many options to move slaves around! Which ones should we use?&#34;&lt;/em&gt;&lt;/p&gt;
&lt;h3&gt;TL;DR look at the two visualized commands below&lt;/h3&gt;
&lt;p&gt;They are enough&lt;/p&gt;
&lt;h3&gt;The &#34;smart&#34; commands to end all commands&lt;/h3&gt;
&lt;p&gt;So all relocation commands are important, and give you fine-grained, pin-pointed control of the method of topology refactoring. However, most of the time you &lt;em&gt;just want to move those servers around&lt;/em&gt;. Which is why there&#39;s a new &#34;smart&#34; mode which support these two commands, which you should be happy using:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;relocate&lt;/strong&gt;: move a single slave to another position&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;relocate-slaves&lt;/strong&gt;: move all/some slaves of some server to another position.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;What makes these commands Smart? You can move slaves around from &lt;em&gt;anywhere&lt;/em&gt; to &lt;em&gt;anywhere&lt;/em&gt;. And &lt;em&gt;orchestrator&lt;/em&gt; figures out the bast execution path. If possible, it uses GTID. Not possible? Is Pseudo-GTID available? Great, using Pseudo-GTID. Oh, are there binlog servers involved? Really simple, use them. None of the above? &lt;em&gt;Orchestrator&lt;/em&gt; will use &#34;standard&#34; binlog file:pos math (with limitations). &lt;em&gt;Orchestrator&lt;/em&gt; will even figure out if multiple steps are necessary and will combine any of the above.&lt;/p&gt;
&lt;p&gt;So you don&#39;t have to remember all the possible ways and options. The visual cheatsheet now boils down to these two:&lt;/p&gt;
&lt;blockquote&gt;&lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2015/08/orchestrator-cheatsheet-visualized-relocate.png&#34;&gt;&lt;img class=&#34;alignnone size-full wp-image-7357&#34; src=&#34;/blog/blog/assets/orchestrator-cheatsheet-visualized-relocate.png&#34; alt=&#34;orchestrator-cheatsheet-visualized-relocate&#34; width=&#34;720&#34; height=&#34;405&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2015/08/orchestrator-cheatsheet-visualized-relocate-slaves.png&#34;&gt;&lt;img class=&#34;alignnone size-full wp-image-7350&#34; src=&#34;/blog/blog/assets/orchestrator-cheatsheet-visualized-relocate-slaves.png&#34; alt=&#34;orchestrator-cheatsheet-visualized-relocate-slaves&#34; width=&#34;720&#34; height=&#34;405&#34; /&gt;&lt;/a&gt;&lt;/blockquote&gt;
&lt;p&gt;Let&#39;s take a slightly deeper look&lt;!--more--&gt;&lt;/p&gt;
&lt;h3&gt;relocate&lt;/h3&gt;
&lt;p&gt;Moves a single slave &lt;strong&gt;X&lt;/strong&gt; from any point to replicate another some server &lt;strong&gt;Z&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;As usual, &lt;em&gt;orchestrator&lt;/em&gt; first confirms that &lt;strong&gt;X&lt;/strong&gt; &lt;em&gt;can&lt;/em&gt; replicate from &lt;strong&gt;Z&lt;/strong&gt; (&lt;strong&gt;Z&lt;/strong&gt; has &lt;strong&gt;log-slave-updates&lt;/strong&gt; or is a binlog server; binlog format compatible, etc.)&lt;/li&gt;
&lt;li&gt;With GTID/Pseudo-GTID, move from any point to any point&lt;/li&gt;
&lt;li&gt;With binlog servers, move around the binlog server environment (at this point you are not using binlog servers, so ignore)&lt;/li&gt;
&lt;li&gt;With normal replication, requires an &#34;atomic&#34; operation: either move the slave one level up, or make it replicate from a sibling.&lt;/li&gt;
&lt;li&gt;You can &lt;em&gt;relocate&lt;/em&gt; the same master, effectively repointing the slave back to its existing position. This serves to re-resolve master hostname; to reset relay logs; to verify slave is aligned with master.&lt;/li&gt;
&lt;li&gt;Or combination of the above&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;relocate-slaves&lt;/h3&gt;
&lt;p&gt;Moves multiple slaves of server &lt;strong&gt;X&lt;/strong&gt; to replicate from some other server &lt;strong&gt;W&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;By default moves all slaves of &lt;strong&gt;X&lt;/strong&gt;, where possible&lt;/li&gt;
&lt;li&gt;Each slave verified to be able to replicate from &lt;strong&gt;W&lt;/strong&gt;. Those that can&#39;t are left behind.&lt;/li&gt;
&lt;li&gt;Can filter using regular expression via &lt;strong&gt;--pattern=some?[reg]ex&lt;/strong&gt; on slave hostnames&lt;/li&gt;
&lt;li&gt;Can relocate under same master, effectively repointing all slaves (see above explanation)&lt;/li&gt;
&lt;li&gt;Can relocate below one of the very slaves of X. If Y is in itself a slave of X and you&#39;re executing:&lt;br /&gt;
&lt;strong&gt;orchestrator -c relocate-slaves -i X -d Y&lt;/strong&gt;&lt;br /&gt;
then &lt;strong&gt;Y&lt;/strong&gt; is excluded from the list of relocated slaves. This effectively means &#34;make &lt;strong&gt;Y&lt;/strong&gt; local master of its current siblings&#34;. Very cool stuff.&lt;/li&gt;
&lt;li&gt;When binlog servers involved, simple math-less repointing takes place&lt;/li&gt;
&lt;li&gt;When GTID involved, let MySQL/MariaDB (both supported) do the math on a per-server basis&lt;/li&gt;
&lt;li&gt;When Pseudo-GTID involved, &lt;em&gt;greatly&lt;/em&gt; optimize by dividing into &lt;a href=&#34;https://en.wikipedia.org/wiki/Equivalence_class&#34;&gt;equivalence classes&lt;/a&gt; and only doing the math on a representative of each class.&lt;/li&gt;
&lt;li&gt;Or combination of the above&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;What about the other commands?&lt;/h3&gt;
&lt;p&gt;The above covers such commands as &lt;strong&gt;move-up&lt;/strong&gt;, &lt;strong&gt;move-below&lt;/strong&gt;, &lt;strong&gt;repoint&lt;/strong&gt;, &lt;strong&gt;repoint-slaves&lt;/strong&gt;, &lt;strong&gt;match-below&lt;/strong&gt;, &lt;strong&gt;multi-match-slaves&lt;/strong&gt;, &lt;strong&gt;regroup-slaves&lt;/strong&gt; and more. It does not cover &lt;strong&gt;enslave-master&lt;/strong&gt; and &lt;strong&gt;make-co-master&lt;/strong&gt; which are a bit different.&lt;/p&gt;
&lt;p&gt;My guess is you can pass &lt;strong&gt;98%&lt;/strong&gt; of your operations with &lt;strong&gt;relocate &lt;/strong&gt;and &lt;strong&gt;relocate-slaves&lt;/strong&gt;. Otherwise just run &lt;em&gt;orchestrator&lt;/em&gt; with no arguments nor options to get a full-blown breakdown of available commands.&lt;/p&gt;
&lt;h3&gt;GUI drag-n-drop&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;relocate-slaves&lt;/strong&gt; is achieved by dragging the slaves of an instance on top of a new master, as follows:&lt;/p&gt;
&lt;blockquote&gt;&lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2015/08/orchestrator-relocate-slaves-before.png&#34;&gt;&lt;img class=&#34;alignnone size-full wp-image-7372&#34; src=&#34;/blog/blog/assets/orchestrator-relocate-slaves-before.png&#34; alt=&#34;orchestrator-relocate-slaves-before&#34; width=&#34;994&#34; height=&#34;369&#34; /&gt;&lt;/a&gt;&lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2015/08/orchestrator-relocate-slaves-hover.png&#34;&gt;&lt;img class=&#34;alignnone size-full wp-image-7373&#34; src=&#34;/blog/blog/assets/orchestrator-relocate-slaves-hover.png&#34; alt=&#34;orchestrator-relocate-slaves-hover&#34; width=&#34;996&#34; height=&#34;339&#34; /&gt;&lt;/a&gt;&lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2015/08/orchestrator-relocate-slaves-drag1.png&#34;&gt;&lt;img class=&#34;alignnone size-large wp-image-7377&#34; src=&#34;/blog/blog/assets/orchestrator-relocate-slaves-drag1.png&#34; alt=&#34;orchestrator-relocate-slaves-drag&#34; width=&#34;980&#34; height=&#34;363&#34; /&gt;&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2015/08/orchestrator-relocate-slaves-after.png&#34;&gt;&lt;img class=&#34;alignnone size-full wp-image-7375&#34; src=&#34;/blog/blog/assets/orchestrator-relocate-slaves-after.png&#34; alt=&#34;orchestrator-relocate-slaves-after&#34; width=&#34;993&#34; height=&#34;345&#34; /&gt;&lt;/a&gt;&lt;/blockquote&gt;
&lt;p&gt;Find latest &lt;em&gt;orchestrator&lt;/em&gt; release at &lt;a href=&#34;https://github.com/outbrain/orchestrator/releases&#34;&gt;https://github.com/outbrain/orchestrator/releases&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Orchestrator 1.4.340: GTID, binlog servers, Smart Mode, failovers and lots of goodies</title>
      <link>/blog/2015/09/01/orchestrator-1-4-340-gtid-binlog-servers-smart-mode-failovers-and-lots-of-goodies/</link>
      <pubDate>Tue, 01 Sep 2015 12:10:15 +0000</pubDate>
      
      <guid>/blog/2015/09/01/orchestrator-1-4-340-gtid-binlog-servers-smart-mode-failovers-and-lots-of-goodies/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://github.com/outbrain/orchestrator/releases&#34;&gt;&lt;strong&gt;Orchestrator&lt;/strong&gt; 1.4.340 is released&lt;/a&gt;. Not quite competing with the MySQL latest changelog, and as I haven&#39;t blogged about &lt;em&gt;orchestrator&lt;/em&gt; featureset in a while, this is a quick listing of &lt;em&gt;orchestrator&lt;/em&gt; features available since my last publication:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Supports &lt;strong&gt;GTID&lt;/strong&gt; (Oracle &amp;amp; MariaDB)
&lt;ul&gt;
&lt;li&gt;GTID still not being used in automated recovery -- in progress.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;enable-gtid&lt;/strong&gt;, &lt;strong&gt;disable-gtid&lt;/strong&gt;, &lt;strong&gt;skip-query&lt;/strong&gt; for GTID commands&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Supports &lt;strong&gt;binlog servers&lt;/strong&gt; (MaxScale)
&lt;ul&gt;
&lt;li&gt;Discovery &amp;amp; operations on binlog servers&lt;/li&gt;
&lt;li&gt;Understanding slave repositioning in a binlog-server architecture&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Smart mode&lt;/strong&gt;: relocate &amp;amp; relocate-below commands (or Web/GUI drag-n-drop) let &lt;em&gt;orchestrator&lt;/em&gt; figure out the best way of slave repositioning. &lt;em&gt;Orchestrator&lt;/em&gt; picks from GTID, Pseudo GTID, binlog servers, binlog file:pos math (and more) options, or combinations of the above. Fine grained commands still there, but mostly you won&#39;t need them.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Crash recoveries&lt;/strong&gt; (did you know &lt;em&gt;orchestrator&lt;/em&gt; does that?):
&lt;ul&gt;
&lt;li&gt;For intermediate master recovery: improved logic in picking the best recovery plan (prefer in-DC, prefer promoting local slave, supporting binlog server topologies, ...)&lt;/li&gt;
&lt;li&gt;For master recovery: even better slave promotion; supports &lt;em&gt;candidate slaves &lt;/em&gt;(prefer promoting such slaves); supports binlog server shared topologies&lt;/li&gt;
&lt;li&gt;Better auditing and logging of recovery cases&lt;/li&gt;
&lt;li&gt;Better analysis of crash scenarios, also in the event of lost VIPs, hanging connections; emergent checks in crash suspected scenarios&lt;/li&gt;
&lt;li&gt;recover-lite: do all topology-only recovery steps, without invoking external processes&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Better browser support&lt;/strong&gt;: used to only work on Firefox and Chrome (and the latter has had issues), the Web UI should now work well on all browsers, at the cost of reduced &lt;strong&gt;d3&lt;/strong&gt; animation. More work still in progress.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Faster&lt;/strong&gt;, more parallel, less blocking operations on all counts; removed a lots of serialized code; less locks.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Web enhancements&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;More verbose drag-n-drop (operation hint; color hints)&lt;/li&gt;
&lt;li&gt;Drag-n-drop for &lt;em&gt;slaves-of-a-server&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Replication/crash analysis dashboard&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pools&lt;/strong&gt;: &lt;em&gt;orchestrator&lt;/em&gt; can be &lt;em&gt;told&lt;/em&gt; about instance-to-pool association (&lt;strong&gt;submit-pool-instances&lt;/strong&gt; command)
&lt;ul&gt;
&lt;li&gt;And can then present pool status (web)&lt;/li&gt;
&lt;li&gt;Or pool hints within topologies (web)&lt;/li&gt;
&lt;li&gt;Or queried for all pools (&lt;strong&gt;cluster-pool-instances&lt;/strong&gt; command)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Other:
&lt;ul&gt;
&lt;li&gt;Supports MySQL &lt;strong&gt;5.7&lt;/strong&gt; (tested with &lt;strong&gt;5.7.8&lt;/strong&gt;)&lt;/li&gt;
&lt;li&gt;Configurable graphite path for metrics&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;--noop&lt;/strong&gt; flag; does all the work except for actually changing master on slaves. Shows intentions.&lt;/li&gt;
&lt;li&gt;Web (or cli &lt;strong&gt;which-cluster-osc-slaves&lt;/strong&gt; command) provide list of control slaves to use in &lt;strong&gt;pt-osc&lt;/strong&gt; operation&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;hostname-unresolve&lt;/strong&gt;: force &lt;em&gt;orchestrator&lt;/em&gt; to unresolve a fqdn into VIP/CNAME/... when issuing a CHANGE MASTER TO&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;3rd party contributions (hey, thanks!) include:
&lt;ul&gt;
&lt;li&gt;More &amp;amp; better SSL support&lt;/li&gt;
&lt;li&gt;Vagrant templates&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;For developers:
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Orchestrator&lt;/em&gt; now go-gettable. Just &lt;strong&gt;go get github.com/outbrain/orchestrator&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Improved build script; supports more architectures&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;!--more--&gt;&lt;/p&gt;
&lt;p&gt;Also consider these manuals:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/outbrain/orchestrator/wiki/Orchestrator-Manual&#34;&gt;The Orchestrator Manual&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/outbrain/orchestrator/wiki/Orchestrator-deployment&#34;&gt;Orchestrator deployment&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/outbrain/orchestrator/wiki/Orchestrator-first-steps&#34;&gt;Orchestrator first steps&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/outbrain/orchestrator/wiki/Orchestrator-for-developers&#34;&gt;Orchestrator for developers&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Orchestrator&lt;/em&gt; is free and open source (Apache 2.0 License).&lt;/p&gt;
&lt;p&gt;I&#39;ll be &lt;a href=&#34;https://www.percona.com/live/europe-amsterdam-2015/sessions/managing-and-visualizing-your-replication-topologies-orchestrator&#34;&gt;speaking about orchestrator in PerconaLive Amsterdam&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Baffling 5.7 global/status variables issues, unclean migration path</title>
      <link>/blog/2015/08/07/baffling-5-7-globalstatus-variables-issues-unclean-migration-path/</link>
      <pubDate>Fri, 07 Aug 2015 14:39:59 +0000</pubDate>
      
      <guid>/blog/2015/08/07/baffling-5-7-globalstatus-variables-issues-unclean-migration-path/</guid>
      <description>&lt;p&gt;MySQL &lt;strong&gt;5.7&lt;/strong&gt; introduces a change in the way we query for global variables and status variables: the &lt;strong&gt;INFORMATION_SCHEMA.(GLOBAL|SESSION)_(VARIABLES|STATUS)&lt;/strong&gt; tables are now deprecated and empty. Instead, we are to use the respective &lt;strong&gt;performance_schema.(global|session)_(variables|status)&lt;/strong&gt; tables.&lt;/p&gt;
&lt;p&gt;But the change goes farther than that; there is also a security change. Oracle created a pitfall of &lt;strong&gt;2&lt;/strong&gt; changes at the same time:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Variables/status moved to a different table&lt;/li&gt;
&lt;li&gt;Privileges required on said table&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;As an example, my non-root user gets:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;mysql&amp;gt; show session variables like &#39;tx_isolation&#39;;
ERROR 1142 (42000): SELECT command denied to user &#39;normal_user&#39;@&#39;my_host&#39; for table &#39;session_variables&#39;&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;Who gets affected by this? Nearly &lt;em&gt;everyone and everything&lt;/em&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Your Nagios will not be able to read status variables&lt;/li&gt;
&lt;li&gt;Your ORM will not be able to determine session variables&lt;/li&gt;
&lt;li&gt;Your replication user will fail connecting (see &lt;a href=&#34;http://datacharmer.blogspot.nl/2015/08/mysql-578-features-bugs-and-rumors.html&#34;&gt;this post by Giuseppe&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;And most everyone else.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The problem with the above is that involves two unrelated changes to your setup, which are not entirely simple to coordinate:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Change your app code to choose the correct schema (information_schema vs. performance_schema)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GRANT&lt;/strong&gt; the permissions on your database&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Perhaps at this point you still do not consider this to be a problem. You may be thinking: &lt;em&gt;well, let&#39;s first prepare by creating the GRANTs, and once that is in place, we can, at our leisure, modify the code&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Not so fast. Can you really that simply create those GRANTs?&lt;!--more--&gt;&lt;/p&gt;
&lt;h3&gt;Migration woes&lt;/h3&gt;
&lt;p&gt;How do you migrate to a new MySQL version? You do not reinstall all your servers. You want an easy migration path, and that path is: introduce one or two slaves of a newer version, see that everything works to your satisfaction, slowly upgrade all your other slaves, eventually switchover/upgrade your master.&lt;/p&gt;
&lt;p&gt;This should not be any different for &lt;strong&gt;5.7&lt;/strong&gt;. We would like to provision a &lt;strong&gt;5.7&lt;/strong&gt; slave in our topologies and just see that everything works. Well, we have, and things don&#39;t just work. Our Nagios stops working for that &lt;strong&gt;5.7&lt;/strong&gt; slave. &lt;em&gt;Orchestrator&lt;/em&gt; started complaining (by this time I&#39;ve &lt;a href=&#34;https://github.com/outbrain/orchestrator/releases/tag/v1.4.291&#34;&gt;already fixed it&lt;/a&gt; to be more tolerant for the &lt;strong&gt;5.7&lt;/strong&gt; problems so no crashes here).&lt;/p&gt;
&lt;p&gt;I hope you see the problem by now.&lt;/p&gt;
&lt;blockquote&gt;You cannot issue a &lt;strong&gt;GRANT SELECT ON performance_schema.global_variables TO &#39;...&#39;&lt;/strong&gt; on your &lt;strong&gt;5.6&lt;/strong&gt; master.&lt;/blockquote&gt;
&lt;p&gt;The table simply does not exist there, which means the statement will not go to binary logs, which means it will not replicate on your &lt;strong&gt;5.7&lt;/strong&gt; slave, which means you will not be able to &lt;strong&gt;SHOW GLOBAL VARIABLES&lt;/strong&gt; on your slave, which means everything remains broken.&lt;/p&gt;
&lt;p&gt;Yes, you can issue this directly on your &lt;strong&gt;5.7&lt;/strong&gt; slaves. It&#39;s &lt;em&gt;doable&lt;/em&gt;, but &lt;em&gt;undesired&lt;/em&gt;. It&#39;s ugly in terms of automation (and will quite possibly break some assumptions and sanity checks your automation uses); in terms of validity testing. It&#39;s unfriendly to GTID (make sure to &lt;strong&gt;SET SQL_LOG_BIN=0&lt;/strong&gt; before that).&lt;/p&gt;
&lt;h3&gt;WHY in the first place?&lt;/h3&gt;
&lt;p&gt;It seems like a security thing. I&#39;m not sure whether this was intended. So you prevent a &lt;strong&gt;SHOW GLOBAL VARIABLES&lt;/strong&gt; for a normal user. Makes sense. And yet:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;mysql&amp;gt; show global variables like &#39;hostname&#39;;
ERROR 1142 (42000): SELECT command denied to user &#39;normal_user&#39;@&#39;my_host&#39; for table &#39;global_variables&#39;

mysql&amp;gt; select @@global.hostname;
+---------------------+
| @@global.hostname   |
+---------------------+
| myhost.mydomain.com |
+---------------------+

mysql&amp;gt; select @@version;
+--------------+
| @@version    |
+--------------+
| 5.7.8-rc-log |
+--------------+

&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;Seems like I&#39;m allowed access to that info after all. So it&#39;s not strictly a security design decision. For status variable, I admit, I don&#39;t have a similar workaround.&lt;/p&gt;
&lt;h3&gt;Solutions?&lt;/h3&gt;
&lt;p&gt;The following are meant to be solutions, but do not really solve the problem:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;SHOW&lt;/strong&gt; commands. &lt;strong&gt;SHOW GLOBAL|SESSION VARIABLES|STATUS&lt;/strong&gt; will work properly, and will implicitly know whether to provide the results via &lt;strong&gt;information_schema&lt;/strong&gt; or &lt;strong&gt;performance_schema&lt;/strong&gt; tables.
&lt;ul&gt;
&lt;li&gt;But, aren&#39;t we meant to be happier with &lt;strong&gt;SELECT&lt;/strong&gt; queries? So that I can really do stuff that is smarter than &lt;strong&gt;LIKE &#39;variable_name%&#39;&lt;/strong&gt;?&lt;/li&gt;
&lt;li&gt;And of course you cannot use &lt;strong&gt;SHOW&lt;/strong&gt; in server side cursors. Your stored routines are in a mess now.&lt;/li&gt;
&lt;li&gt;This does not solve the GRANTs problem.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;http://dev.mysql.com/doc/refman/5.7/en/server-system-variables.html#sysvar_show_compatibility_56&#34;&gt;show_compatibility_56&lt;/a&gt;&lt;/strong&gt;: an introduced variable in &lt;strong&gt;5.7&lt;/strong&gt;, boolean. It truly is a time-travel-paradox novel in disguise, in multiple respects.
&lt;ul&gt;
&lt;li&gt;Documentation introduces it, and says it is deprecated.
&lt;ul&gt;
&lt;li&gt;time-travel-paradox :O&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;But it actually works in &lt;strong&gt;5.7.8&lt;/strong&gt; (latest)
&lt;ul&gt;
&lt;li&gt;time-travel-paradox plot thickens&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Your automation scripts do not know in advance whether your MySQL has this variable
&lt;ul&gt;
&lt;li&gt;Hence &lt;strong&gt;SELECT @@global.show_compatibility_56&lt;/strong&gt; will produce an error on &lt;strong&gt;5.6&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;But the &#34;safe&#34; way of &lt;strong&gt;SHOW GLOBAL VARIABLES LIKE &#39;show_compatibility_56&#39;&lt;/strong&gt; will fail on a privilege error on &lt;strong&gt;5.7&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;time-travel-paradox :O&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Actually advised by my colleague Simon J. Mudd, &lt;strong&gt;show_compatibility_56&lt;/strong&gt; defaults to &lt;strong&gt;OFF&lt;/strong&gt;. I &lt;em&gt;support&lt;/em&gt; this line of thought. Or else it&#39;s &lt;strong&gt;old_passwords=1&lt;/strong&gt; all over again.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;show_compatibility_56&lt;/strong&gt; doesn&#39;t solve the GRANTs problem.&lt;/li&gt;
&lt;li&gt;This does not solve any migration path. It just postpones the moment when I will hit the same problem. When I flip the variable from &lt;strong&gt;&#34;1&#34;&lt;/strong&gt; to &lt;strong&gt;&#34;0&#34;&lt;/strong&gt;, I&#39;m back at square one.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Suggestion&lt;/h3&gt;
&lt;p&gt;I claim security is not the issue, as presented above. I claim Oracle will yet again fall into the trap of no-easy-way-to-migrate-to-GTID in &lt;strong&gt;5.6&lt;/strong&gt; if the current solution is unchanged. I claim that there have been too many changes at once. Therefore, I suggest one of the alternative two flows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Flow 1&lt;/strong&gt;: keep &lt;strong&gt;information_schema&lt;/strong&gt;, later migration into &lt;strong&gt;performance_schema&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;In &lt;strong&gt;5.7&lt;/strong&gt;, &lt;strong&gt;information_schema&lt;/strong&gt; tables should still produce the data.&lt;/li&gt;
&lt;li&gt;No security constraints on &lt;strong&gt;information_schema&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Generate WARNINGs on reading from &lt;strong&gt;information_schema&lt;/strong&gt; (&#34;...this will be deprecated...&#34;)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;performance_schema &lt;/strong&gt;&lt;em&gt;also available&lt;/em&gt;. With security constraints, whatever.&lt;/li&gt;
&lt;li&gt;In &lt;strong&gt;5.8&lt;/strong&gt; remove &lt;strong&gt;information_schema&lt;/strong&gt; tables; we are left with &lt;strong&gt;performance_schema&lt;/strong&gt; only.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Flow 2&lt;/strong&gt;: easy migration into &lt;strong&gt;performance_schema&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;In &lt;strong&gt;5.7&lt;/strong&gt;, &lt;strong&gt;performance_schema&lt;/strong&gt; tables should not require any special privileges. Any user can read from them.&lt;/li&gt;
&lt;li&gt;Keep &lt;strong&gt;show_compatibility_56 &lt;/strong&gt;as it is.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SHOW&lt;/strong&gt; commands choose between &lt;strong&gt;information_schema&lt;/strong&gt; or &lt;strong&gt;performance_schema&lt;/strong&gt; on their own -- just as things are done now.&lt;/li&gt;
&lt;li&gt;In &lt;strong&gt;5.8&lt;/strong&gt;, &lt;strong&gt;performance_schema&lt;/strong&gt; tables will require &lt;strong&gt;SELECT&lt;/strong&gt; privileges.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;As always, I love the work done by the engineers; and I love how they listen to the community.&lt;/p&gt;
&lt;p&gt;Comments are most welcome. Have I missed the simple solution here? Are there even more complications to these features? Thoughts on my suggested two flows?&lt;/p&gt;
&lt;h3&gt;[UPDATE 2015-08-19]&lt;/h3&gt;
&lt;p&gt;Please &lt;a href=&#34;http://www.tocker.ca/2015/08/18/a-followup-on-show_compatibility_56.html&#34;&gt;see this followup&lt;/a&gt; by Morgan Tocker of Oracle.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Pseudo GTID, ASCENDING</title>
      <link>/blog/2015/07/29/pseudo-gtid-ascending/</link>
      <pubDate>Wed, 29 Jul 2015 12:59:50 +0000</pubDate>
      
      <guid>/blog/2015/07/29/pseudo-gtid-ascending/</guid>
      <description>&lt;p&gt;Pseudo GTID is a technique where we inject Globally Unique entries into MySQL, gaining GTID abilities without using GTID. It is supported by &lt;strong&gt;&lt;a href=&#34;https://github.com/outbrain/orchestrator&#34;&gt;orchestrator&lt;/a&gt;&lt;/strong&gt; and described in more detail &lt;a href=&#34;https://speakerdeck.com/shlominoach/pseudo-gtid-and-easy-mysql-replication-topology-management&#34;&gt;here&lt;/a&gt;, &lt;a href=&#34;http://code.openark.org/blog/tag/pseudo-gtid&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;https://github.com/outbrain/orchestrator/wiki/Orchestrator-Manual#pseudo-gtid&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Quick recap: we can join two slaves to replicate from one another even if they never were in parent-child relationship, based on our uniquely identifiable entries which can be found in the slaves&#39; binary logs or relay logs. Having Pseudo-GTID injected and controlled by us allows us to optimize failovers into quick operations, especially where a large number of server is involved.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ascending Pseudo-GTID&lt;/strong&gt; further speeds up this process for delayed/lagging slaves.&lt;/p&gt;
&lt;h3&gt;Recap, visualized&lt;/h3&gt;
&lt;p&gt;(but do look at the &lt;a href=&#34;https://speakerdeck.com/shlominoach/pseudo-gtid-and-easy-mysql-replication-topology-management&#34;&gt;presentation&lt;/a&gt;):&lt;/p&gt;
&lt;blockquote&gt;&lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2015/07/pseudo-gtid-quick1.png&#34;&gt;&lt;img class=&#34;alignnone size-full wp-image-7303&#34; src=&#34;/blog/blog/assets/pseudo-gtid-quick1.png&#34; alt=&#34;pseudo-gtid-quick&#34; width=&#34;636&#34; height=&#34;366&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Find last pseudo GTID in slave’s binary log (or last applied one in relay log)&lt;/li&gt;
&lt;li&gt;Search for exact match on new master’s binary logs&lt;/li&gt;
&lt;li&gt;Fast forward both through successive identical statements until end of slave’s applied entries is reached&lt;/li&gt;
&lt;li&gt;Point slave into cursor position on master&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;What happens if the slave we wish to reconnect is lagging? Or perhaps it is a delayed replica, set to run &lt;strong&gt;24&lt;/strong&gt; hours behind its master?&lt;/p&gt;
&lt;p&gt;The naive approach would expand bullet &lt;strong&gt;#2&lt;/strong&gt; into:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Search for exact match on master’s last binary logs&lt;/li&gt;
&lt;li&gt;Unfound? Move on to previous (older) binary log on master&lt;/li&gt;
&lt;li&gt;Repeat&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The last Pseudo-GTID executed by the slave was issued by the master over &lt;strong&gt;24&lt;/strong&gt; hours ago. Suppose the master generates one binary log per hour. This means we would need to full-scan &lt;strong&gt;24&lt;/strong&gt; binary logs of the master where the entry will not be found; to only be matched in the &lt;strong&gt;25th&lt;/strong&gt; binary log (it&#39;s an off-by-one problem, don&#39;t hold the exact number against me).&lt;/p&gt;
&lt;h3&gt;Ascending Pseudo GTID&lt;/h3&gt;
&lt;p&gt;Since we control the generation of Pseudo-GTID, and since we control the search for Pseudo-GTID, we are free to choose the form of Pseudo-GTID entries. We recently switched into using Ascending Pseudo-GTID entries, and this works like a charm. Consider these Pseudo-GTID entries:&lt;!--more--&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;drop view if exists `meta`.`&lt;strong&gt;_pseudo_gtid_hint__asc:55B364E3:0000000000056EE2:6DD57B85&lt;/strong&gt;`
drop view if exists `meta`.`&lt;strong&gt;_pseudo_gtid_hint__asc:55B364E8:0000000000056EEC:ACF03802&lt;/strong&gt;`
drop view if exists `meta`.`&lt;strong&gt;_pseudo_gtid_hint__asc:55B364ED:0000000000056EF8:06279C24&lt;/strong&gt;`
drop view if exists `meta`.`&lt;strong&gt;_pseudo_gtid_hint__asc:55B364F2:0000000000056F02:19D785E4&lt;/strong&gt;`&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;The above entries are ascending in lexical order. The above is generated using a UTC timestamp, along with other watchdog/random values. For a moment let&#39;s trust that our generation is indeed always ascending. How does that help us?&lt;/p&gt;
&lt;p&gt;Suppose the last entry found in the slave is&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;drop view if exists `meta`.`&lt;strong&gt;_pseudo_gtid_hint__asc:55B364E3:0000000000056EE2:6DD57B85&lt;/strong&gt;`&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;And this is what we&#39;re to search on the master&#39;s binary logs. Starting with the optimistic hope that the entry is in the master&#39;s last binary log, we start reading. By nature of binary logs we have to scan them sequentially from start to end. As we read the binary log entries, we soon meet the first Pseudo-GTID injection, and it reads:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;drop view if exists `meta`.`&lt;strong&gt;_pseudo_gtid_hint__asc:55B730E6:0000000000058F02:19D785E4&lt;/strong&gt;`&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;p&gt;At this stage we know we can completely skip scanning the rest of the binary log. Our entry will not be there: this entry is larger than the one we&#39;re looking for, and they&#39;ll only get larger as we get along in the binary log. It is therefore safe to ignore the rest of this file and move on to the next-older binary log on the master, to repeat our search there.&lt;/p&gt;
&lt;p&gt;Binary logs where the entry cannot be in are only briefly examined: &lt;em&gt;orchestrator&lt;/em&gt; will probably read no more than first &lt;strong&gt;1,000&lt;/strong&gt; entries or so (can&#39;t give you a number, it&#39;s your workload) before giving up on the binary log.&lt;/p&gt;
&lt;p&gt;On every topology chain we have &lt;strong&gt;2&lt;/strong&gt; delayed replica slaves, to help us out in the case we make a grave mistake of DELETing the wrong data. These slaves would take, on some chains, &lt;strong&gt;5-6&lt;/strong&gt; minutes to reconnect to a new master using Pseudo-GTID, since it required scanning many many GBs of binary logs. This is no longer the case; we&#39;ve reduced scan time for such servers to about &lt;strong&gt;25s&lt;/strong&gt; at worst, and much quicker on average. There can still be dozens of binary logs to open, but all but one are given up very quickly. I should stress that those &lt;strong&gt;25s&lt;/strong&gt; are nonblocking for other slaves which are mote up to date than the delayed replicas.&lt;/p&gt;
&lt;h3&gt;Can there be a mistake?&lt;/h3&gt;
&lt;p&gt;Notice that the above algorithm does not require each and every entry to be ascending; it just compares the first entry in each binlog to determine whether our target entry is there or not. This means if we&#39;ve messed up our Ascending order and injected some out-of-order entries, we can still get away with it -- as long as those entries are not the first ones in the binary log, nor are they the last entries executed by the slave.&lt;/p&gt;
&lt;p&gt;But why be so negative? We&#39;re using UTC timestamp as the major sorting order, and inject Pseudo-GTID every &lt;strong&gt;5&lt;/strong&gt; seconds; even with leap second we&#39;re comfortable.&lt;/p&gt;
&lt;p&gt;On my TODO is to also include a &#34;Plan B&#34; full-scan search: if the Ascending algorithm fails, we can still opt for the full scan option. So there would be no risk at all.&lt;/p&gt;
&lt;h3&gt;Example&lt;/h3&gt;
&lt;p&gt;We inject Pseudo-GTID via event-scheduler. These are the good parts of the event definition:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;create event if not exists
  create_pseudo_gtid_event
  on schedule every 5 second starts current_timestamp
  on completion preserve
  enable
  do
    begin
      set @connection_id := connection_id();
      set @now := now();
      set @rand := floor(rand()*(1 &amp;lt;&amp;lt; 32));
      &lt;strong&gt;set @pseudo_gtid_hint := concat_ws(&#39;:&#39;, lpad(hex(unix_timestamp(@now)), 8, &#39;0&#39;), lpad(hex(@connection_id), 16, &#39;0&#39;), lpad(hex(@rand), 8, &#39;0&#39;));&lt;/strong&gt;
&lt;strong&gt;
      set @_create_statement := concat(&#39;drop &#39;, &#39;view if exists `meta`.`_pseudo_gtid_&#39;, &#39;hint__asc:&#39;, @pseudo_gtid_hint, &#39;`&#39;);&lt;/strong&gt;
      PREPARE st FROM @_create_statement;
      EXECUTE st;
      DEALLOCATE PREPARE st;
&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;We accompany this by the following &lt;em&gt;orchestrator&lt;/em&gt; configuration:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt; &#34;PseudoGTIDPattern&#34;: &#34;drop view if exists .*?`_pseudo_gtid_hint__&#34;,
 &#34;PseudoGTIDMonotonicHint&#34;: &#34;asc:&#34;,&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;&#34;PseudoGTIDMonotonicHint&#34;&lt;/strong&gt; notes a string; if that string (&lt;strong&gt;&#34;asc:&#34;&lt;/strong&gt;) is found in the slave&#39;s Pseudo-GTID entry, then the entry is assumed to have been injected as part of ascending entries, and the optimization kicks in.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/outbrain/orchestrator/wiki/Orchestrator-Manual#pseudo-gtid&#34;&gt;The Manual&lt;/a&gt; has more on this.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>What makes a MySQL server failure/recovery case?</title>
      <link>/blog/2015/07/25/what-makes-a-mysql-server-failurerecovery-case/</link>
      <pubDate>Sat, 25 Jul 2015 09:00:03 +0000</pubDate>
      
      <guid>/blog/2015/07/25/what-makes-a-mysql-server-failurerecovery-case/</guid>
      <description>&lt;p&gt;Or: How do you reach the conclusion your MySQL master/intermediate-master is dead and must be recovered?&lt;/p&gt;
&lt;p&gt;This is an attempt at making a holistic diagnosis of our replication topologies. The aim is to cover obvious and not-so-obvious crash scenarios, and to be able to act accordingly and heal the topology.&lt;/p&gt;
&lt;p&gt;At &lt;strong&gt;Booking.com&lt;/strong&gt; we are dealing with very large amounts of MySQL servers. We have many topologies, and many servers in each topology. &lt;a href=&#34;https://speakerdeck.com/shlominoach/pseudo-gtid-and-easy-mysql-replication-topology-management&#34;&gt;See past numbers&lt;/a&gt; to get a feel for it. At these numbers failures happen frequently. Typically we would see normal slaves failing, but occasionally -- and far more frequently than we would like to be paged for -- an intermediate master or a master would crash. But our current (and ever in transition) setup also include SANs, DNS records, VIPs, any of which can fail and bring down our topologies.&lt;/p&gt;
&lt;p&gt;Tackling issues of monitoring, disaster analysis and recovery processes, I feel safe to claim the following statements:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The fact your monitoring tool cannot access your database does not mean your database has failed.&lt;/li&gt;
&lt;li&gt;The fact your monitoring tool can access your database does not mean your database is available.&lt;/li&gt;
&lt;li&gt;The fact your database master is unwell does not mean you should fail over.&lt;/li&gt;
&lt;li&gt;The fact your database master is alive and well does not mean you should not fail over.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Bummer. Let&#39;s review a simplified topology with a few failure scenarios. Some of these scenarios you will find familiar. Some others may be caused by setups you&#39;re not using. I would love to say &lt;em&gt;I&#39;ve seen it all&lt;/em&gt; but the more I see the more I know how strange things can become.&lt;!--more--&gt;&lt;/p&gt;
&lt;p&gt;We will consider the simplified case of a master with three replicas: we have &lt;strong&gt;M&lt;/strong&gt; as master, &lt;strong&gt;A&lt;/strong&gt;, &lt;strong&gt;B&lt;/strong&gt;, &lt;strong&gt;C&lt;/strong&gt; as slaves.&lt;/p&gt;
&lt;blockquote&gt;&lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2015/07/mysql-topologies-failures.png&#34;&gt;&lt;img class=&#34;alignnone size-full wp-image-7280&#34; src=&#34;/blog/blog/assets/mysql-topologies-failures.png&#34; alt=&#34;mysql-topologies-failures&#34; width=&#34;192&#34; height=&#34;108&#34; /&gt;&lt;/a&gt;&lt;/blockquote&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;p&gt;A common monitoring scheme is to monitor each machine&#39;s IP, availability of MySQL port (&lt;strong&gt;3306&lt;/strong&gt;) and responsiveness to some simple query (e.g. &lt;strong&gt;&#34;SELECT 1&#34;&lt;/strong&gt;). Some of these checks may run local to the machine, others remote.&lt;/p&gt;
&lt;p&gt;Now consider your monitoring tool fails to connect to your master.&lt;/p&gt;
&lt;blockquote&gt;&lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2015/07/mysql-topologies-failures-1.png&#34;&gt;&lt;img class=&#34;alignnone size-full wp-image-7281&#34; src=&#34;/blog/blog/assets/mysql-topologies-failures-1.png&#34; alt=&#34;mysql-topologies-failures (1)&#34; width=&#34;192&#34; height=&#34;108&#34; /&gt;&lt;/a&gt;&lt;/blockquote&gt;
&lt;p&gt;I&#39;ve marked the slaves with question marks as the common monitoring schema does not associate the master&#39;s monitoring result to the slaves&#39;.  Can you safely conclude your master is dead? Are your feeling comfortable with initiating a failover process? How about:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Temporary network partitioning; it just so happens that your monitoring tool cannot access the master, though everyone else can.&lt;/li&gt;
&lt;li&gt;DNS/VIP/name cache/name resolving issue. Sometimes similar to the above; does you monitoring tool host think the master&#39;s IP is what it really is? Has something just changed? Some cache expired? Some cache is stale?&lt;/li&gt;
&lt;li&gt;MySQL connection rejection. This could be due to a serious &#34;Too many connections&#34; problem on the master, or due to accidental network noise.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now consider the following case: a first tier slave is failing to connect to the master:&lt;/p&gt;
&lt;blockquote&gt;&lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2015/07/mysql-topologies-failures-2.png&#34;&gt;&lt;img class=&#34;alignnone size-full wp-image-7282&#34; src=&#34;/blog/blog/assets/mysql-topologies-failures-2.png&#34; alt=&#34;mysql-topologies-failures (2)&#34; width=&#34;192&#34; height=&#34;108&#34; /&gt;&lt;/a&gt;&lt;/blockquote&gt;
&lt;p&gt;The slave&#39;s IO thread is broken; do we have a problem here? Is the slave failing to connect because the master is dead, or because the slave itself suffers from a network partitioning glitch?&lt;/p&gt;
&lt;h3&gt;A holistic diagnosis&lt;/h3&gt;
&lt;p&gt;In the holistic approach we couple the master&#39;s monitoring with that of its direct slaves. Before I continue to describe some logic, the previous statement is something we must reflect upon.&lt;/p&gt;
&lt;p&gt;We should associate the master&#39;s state with that of its direct slaves. Hence we must know which are its direct slaves. We might have slaves D, E, F, G replicating from B, C. They are not in our story. But slaves come and go. Get provisioned and de-provisioned. They get repointed elsewhere. Our monitoring needs to be aware of the &lt;em&gt;state&lt;/em&gt; of our replication topology.&lt;/p&gt;
&lt;p&gt;My preferred tool for the job is &lt;a href=&#34;https://github.com/outbrain/orchestrator/&#34;&gt;orchestrator&lt;/a&gt;, since I author it. It is not a standard monitoring tool and does not serve metrics; but it observes your topologies and records them. And notes changes. And acts as a higher level failure detection mechanism which incorporates the logic described below.&lt;/p&gt;
&lt;p&gt;We continue our discussion under the assumption we are able to reliably claim we know our replication topology. Let&#39;s revisit our scenarios from above and then add some.&lt;/p&gt;
&lt;p&gt;We will further only require MySQL client protocol connection to our database servers.&lt;/p&gt;
&lt;h3&gt;Dead master&lt;/h3&gt;
&lt;p&gt;A &#34;real&#34; dead master is perhaps the clearest failure. MySQL has crashed (signal 11); or the kernel panicked; or the disks failed; or power went off. The server is &lt;em&gt;really not serving&lt;/em&gt;. This is observed as:&lt;/p&gt;
&lt;blockquote&gt;&lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2015/07/mysql-topologies-failures-3.png&#34;&gt;&lt;img class=&#34;alignnone size-full wp-image-7284&#34; src=&#34;/blog/blog/assets/mysql-topologies-failures-3.png&#34; alt=&#34;mysql-topologies-failures (3)&#34; width=&#34;192&#34; height=&#34;108&#34; /&gt;&lt;/a&gt;&lt;/blockquote&gt;
&lt;p&gt;In the holistic approach, we observe that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We cannot reach the master (our MySQL client connection fails).&lt;/li&gt;
&lt;li&gt;But we are able to connect to the slaves A, B, C&lt;/li&gt;
&lt;li&gt;And A, B, C &lt;em&gt;are all telling us&lt;/em&gt; they cannot connect to the master&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We have now cross referenced the death of the master with its three slaves. Funny thing is the MySQL server on the master may still be up and running. Perhaps the master is suffering from some weird network partitioning problem (when I say &#34;weird&#34;, I mean we have it; discussed further below). And &lt;em&gt;perhaps&lt;/em&gt; some application is actually still able to talk to the master!&lt;/p&gt;
&lt;p&gt;And yet our entire replication topology is broken. Replication is not there for beauty; it serves our application code. And it&#39;s turning stale. Even if by some chance things are still operating on the master, this still makes for a valid failover scenario.&lt;/p&gt;
&lt;h3&gt;Unreachable master&lt;/h3&gt;
&lt;p&gt;Compare the above with:&lt;/p&gt;
&lt;blockquote&gt;&lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2015/07/mysql-topologies-failures-4.png&#34;&gt;&lt;img class=&#34;alignnone size-full wp-image-7285&#34; src=&#34;/blog/blog/assets/mysql-topologies-failures-4.png&#34; alt=&#34;mysql-topologies-failures (4)&#34; width=&#34;192&#34; height=&#34;108&#34; /&gt;&lt;/a&gt;&lt;/blockquote&gt;
&lt;p&gt;Our monitoring scheme cannot reach our master. But it can reach the slaves, an they&#39;re all saying: &lt;em&gt;&#34;I&#39;m happy!&#34;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This gives us suspicion enough to avoid failing over. We may not actually have a problem: it&#39;s just &lt;em&gt;us&lt;/em&gt; that are unable to connect to the master.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Right?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;There are still interesting use cases. Consider the problem of &lt;strong&gt;&#34;Too many connections&#34;&lt;/strong&gt; on the master. You are unable to connect; the application starts throwing errors; but the slaves are happy. They were there first. They started replicating at the dawn of time, long before there was an issue. Their persistent connections are good to go.&lt;/p&gt;
&lt;p&gt;Or the master may suffer a deadlock. A long, blocking &lt;strong&gt;ALTER TABLE&lt;/strong&gt;. An accidental &lt;strong&gt;FLUSH TABLES WITH READ LOCK&lt;/strong&gt;. Or whatever occasional bug we hit. Slaves are still connected; but new connections are hanging; and your monitoring query is unable to process.&lt;/p&gt;
&lt;p&gt;And still our holistic approach can find that out: as we are able to connect to our slaves, we are also able to ask them: well what have your relay logs have to say about this? Are we progressing in replication position? Do we actually find application content in the slaves&#39; relay logs? We can do all this via MySQL protocol (&lt;strong&gt;&#34;SHOW SLAVE STATUS&#34;&lt;/strong&gt;, &lt;strong&gt;&#34;SHOW RELAYLOG EVENTS&#34;&lt;/strong&gt;).&lt;/p&gt;
&lt;p&gt;Understanding the topology gives you greater insight into your failure case; you have increasing leevels of confidentiality in your analysis. Strike that: in your &lt;em&gt;automated&lt;/em&gt; analysis.&lt;/p&gt;
&lt;h3&gt;Dead master and slaves&lt;/h3&gt;
&lt;p&gt;They&#39;re all &lt;em&gt;gone&lt;/em&gt;!&lt;/p&gt;
&lt;blockquote&gt;&lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2015/07/mysql-topologies-failures-5.png&#34;&gt;&lt;img class=&#34;alignnone size-full wp-image-7287&#34; src=&#34;/blog/blog/assets/mysql-topologies-failures-5.png&#34; alt=&#34;mysql-topologies-failures (5)&#34; width=&#34;192&#34; height=&#34;108&#34; /&gt;&lt;/a&gt;&lt;/blockquote&gt;
&lt;p&gt;You cannot reach the master &lt;em&gt;and&lt;/em&gt; you cannot reach any of its slaves. Once you are able to associate your master and slaves you can conclude you either have a complete DC power failure problem (or is this cross DC?) or you are having a network partitioning problem. Your application may or may not be affected -- but at least you know where to start. Compare with:&lt;/p&gt;
&lt;h3&gt;Failed DC&lt;/h3&gt;
&lt;blockquote&gt;&lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2015/07/mysql-topologies-failures-6.png&#34;&gt;&lt;img class=&#34;alignnone size-full wp-image-7289&#34; src=&#34;/blog/blog/assets/mysql-topologies-failures-6.png&#34; alt=&#34;mysql-topologies-failures (6)&#34; width=&#34;192&#34; height=&#34;108&#34; /&gt;&lt;/a&gt;&lt;/blockquote&gt;
&lt;p&gt;I&#39;m stretching it now, because when a DC fails all the red lights start flashing. Nonetheless, if M, A, B are all in one DC and C is on another, you have yet another diagnosis.&lt;/p&gt;
&lt;h3&gt;Dead master and some slaves&lt;/h3&gt;
&lt;blockquote&gt;&lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2015/07/mysql-topologies-failures-7.png&#34;&gt;&lt;img class=&#34;alignnone size-full wp-image-7290&#34; src=&#34;/blog/blog/assets/mysql-topologies-failures-7.png&#34; alt=&#34;mysql-topologies-failures (7)&#34; width=&#34;192&#34; height=&#34;108&#34; /&gt;&lt;/a&gt;&lt;/blockquote&gt;
&lt;p&gt;Things start getting complicated when you&#39;re unable to get an authorized answer from everyone. What happens if the master is dead as well as one of its slaves? We previously expected all slaves to say &#34;we cannot replicate&#34;. For us, master being unreachable, some slaves being dead and all other complaining on IO thread is good enough indication that the master is dead.&lt;/p&gt;
&lt;h3&gt;All first tier slaves not replicating&lt;/h3&gt;
&lt;blockquote&gt;&lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2015/07/mysql-topologies-failures-9.png&#34;&gt;&lt;img class=&#34;alignnone size-full wp-image-7293&#34; src=&#34;/blog/blog/assets/mysql-topologies-failures-9.png&#34; alt=&#34;mysql-topologies-failures (9)&#34; width=&#34;192&#34; height=&#34;108&#34; /&gt;&lt;/a&gt;&lt;/blockquote&gt;
&lt;p&gt;Not a failover case, but certainly needs to ring the bells. All master&#39;s direct slaves are failing replication on some SQL error or are just stopped. Our topology is turning stale.&lt;/p&gt;
&lt;h3&gt;Intermediate masters&lt;/h3&gt;
&lt;p&gt;With intermediate master the situation is not all that different. In the below:&lt;/p&gt;
&lt;blockquote&gt;&lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2015/07/Untitled-presentation.png&#34;&gt;&lt;img class=&#34;alignnone size-full wp-image-7294&#34; src=&#34;/blog/blog/assets/Untitled-presentation.png&#34; alt=&#34;Untitled presentation&#34; width=&#34;480&#34; height=&#34;270&#34; /&gt;&lt;/a&gt;&lt;/blockquote&gt;
&lt;p&gt;The servers &lt;strong&gt;E&lt;/strong&gt;, &lt;strong&gt;F&lt;/strong&gt;, &lt;strong&gt;G&lt;/strong&gt; replicating from &lt;strong&gt;C&lt;/strong&gt; provide us with the holistic view on &lt;strong&gt;C&lt;/strong&gt;. &lt;strong&gt;D&lt;/strong&gt; provides the holistic view on &lt;strong&gt;A&lt;/strong&gt;.&lt;/p&gt;
&lt;h3&gt;Reducing noise&lt;/h3&gt;
&lt;p&gt;Intermediate master failover is a much simpler operation than master failover. Changing masters require name resolve changes (of some sort), whereas moving slaves around the topology affects no one.&lt;/p&gt;
&lt;p&gt;This implies:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We don&#39;t mind over-reacting on failing over intermediate masters&lt;/li&gt;
&lt;li&gt;We pay with more noise&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Sure, we don&#39;t mind failing over &lt;strong&gt;D&lt;/strong&gt; elsewhere, but as &lt;strong&gt;D&lt;/strong&gt; is the only slave of &lt;strong&gt;A&lt;/strong&gt;, it&#39;s enough that &lt;strong&gt;D&lt;/strong&gt; hiccups that we might get an alert (&#34;all&#34; intermediate master&#39;s slaves are not replicating). To that effect &lt;em&gt;orchestrator&lt;/em&gt; treats single slave scenarios differently than multiple slaves scenarios.&lt;/p&gt;
&lt;h3&gt;Not so fun setups and failures&lt;/h3&gt;
&lt;p&gt;At Booking.com we are in transition between setups. We have some legacy configuration, we have a roadmap, two ongoing solutions, some experimental setups, and/or all of the above combined. Sorry.&lt;/p&gt;
&lt;p&gt;Some of our masters are on SAN. We are moving away from this; for those masters on SANs we have cold standbys in an active-passive mode; so master failure -&amp;gt; unmount SAN -&amp;gt; mount SAN on cold standby -&amp;gt; start MySQL on cold standby -&amp;gt; start recovery -&amp;gt; watch some TV -&amp;gt; go shopping -&amp;gt; end recovery.&lt;/p&gt;
&lt;p&gt;Only SANs fail, too. When the master fails, switching over to the cold standby is pointless if the origin of the problem is the SAN. And given that some &lt;em&gt;other&lt;/em&gt; masters share the same SAN... whoa. As I said we&#39;re moving away from this setup for Pseudo GTID and then for Binlog Servers.&lt;/p&gt;
&lt;p&gt;The SAN setup also implied using VIPs for some servers. The slaves reference the SAN master via VIP, and when the cold standby start up it assumes the VIP, and the slaves know nothing about this. Same setup goes for DC masters. What happens when the VIP goes down? MySQL is running happily, but slaves are unable to connect. Does that make for a failover scenario? For intermediate masters we&#39;re pushing it to be so, failing over to a normal local-disk based server; this improves out confidence in non-SAN setups (which we have plenty of, anyhow).&lt;/p&gt;
&lt;h3&gt;Double checking&lt;/h3&gt;
&lt;p&gt;You sample your server once every X seconds. But in a failover scenario you want to make sure your data is up to date. When &lt;em&gt;orchestrator&lt;/em&gt; suspects a dead master (i.e. cannot reach the master) it immediately contacts its direct slaves and checks their status.&lt;/p&gt;
&lt;p&gt;Likewise, when &lt;em&gt;orchestrator&lt;/em&gt; sees a first tier slave with broken IO thread, it immediately contacts the master to check if everything is fine.&lt;/p&gt;
&lt;p&gt;For intermediate masters &lt;em&gt;orchestrator&lt;/em&gt; is not so concerned and does not issue emergency checks.&lt;/p&gt;
&lt;h3&gt;How to fail over&lt;/h3&gt;
&lt;p&gt;Different story. Some other time. But failing over makes for complex decisions, based on who the replicating slaves are; with/out log-slave-updates; with-out GTID; with/out Pseudo-GTID; are binlog servers available; which slaves are available in which data centers. Or you may be using Galera (we&#39;re not) which answers most of the above.&lt;/p&gt;
&lt;p&gt;Anyway we use &lt;em&gt;orchestrator&lt;/em&gt; for that; it knows our topologies, knows how they should look like, understands how to heal them, knows MySQL replication rules, and invokes external processes to do the stuff it doesn&#39;t understand.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Orchestrator visual cheatsheet</title>
      <link>/blog/2015/06/05/orchestrator-visual-cheatsheet/</link>
      <pubDate>Fri, 05 Jun 2015 14:19:34 +0000</pubDate>
      
      <guid>/blog/2015/06/05/orchestrator-visual-cheatsheet/</guid>
      <description>&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/outbrain/orchestrator&#34;&gt;Orchestrator&lt;/a&gt;&lt;/strong&gt; is growing. Supporting automatic detection of topologies, simple refactoring of topology trees, complex refactoring via Pseudo-GTID, failure detection and automated discovery, it is becoming larger and larger by the day.&lt;/p&gt;
&lt;p&gt;One of the problems with growign projects is hwo to properly document them. Orchestrator enjoys &lt;a href=&#34;https://github.com/outbrain/orchestrator/wiki/Orchestrator-Manual&#34;&gt;a comprehensive manual&lt;/a&gt;, but as these get more and more detailed, it is becoming difficult to get oriented and pointed in the right direction. I&#39;ve done my best to advise the simple use cases throughout the manual.&lt;/p&gt;
&lt;p&gt;One thing that is difficult to put into words is topologies. Explaining &#34;failover of an intermediate master S1 that has S2,...,Sn slaves onto a sibling of S1 provided that...&#34; is too verbose. So here&#39;s a quick visual cheatsheet for (current) topology refactoring commands. Refactoring commands are a mere subset of overall orchestrator commands, but they&#39;re great to play with and perfect for visualization.&lt;/p&gt;
&lt;p&gt;The &#34;move&#34; and related commands use normal replication commands (STOP SLAVE; CHANGE MASTER TO; START SLAVE UNTIL;&#34;...).&lt;/p&gt;
&lt;p&gt;The &#34;match&#34; and related commands utilize Pseudo-GTID and use more elaborate MySQL commands (SHOW BINLOG EVENTS, SHOW RELAYLOG EVENTS).&lt;/p&gt;
&lt;p&gt;So without further ado, here&#39;s what each command does (and do run &#34;orchestrator&#34; from the command line to get a man-like explanation of everything, or just &lt;a href=&#34;https://github.com/outbrain/orchestrator/wiki/Orchestrator-Manual#executing-via-command-line&#34;&gt;go to the manual&lt;/a&gt;).&lt;!--more--&gt;&lt;/p&gt;
&lt;blockquote&gt;&lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2015/06/orchestrator-cheatsheet-visualized-7.png&#34;&gt;&lt;img class=&#34;alignnone size-full wp-image-7251&#34; src=&#34;/blog/blog/assets/orchestrator-cheatsheet-visualized-7.png&#34; alt=&#34;orchestrator-cheatsheet-visualized-7&#34; width=&#34;720&#34; height=&#34;405&#34; /&gt;&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2015/06/orchestrator-cheatsheet-visualized-8.png&#34;&gt;&lt;img class=&#34;alignnone size-full wp-image-7252&#34; src=&#34;/blog/blog/assets/orchestrator-cheatsheet-visualized-8.png&#34; alt=&#34;orchestrator-cheatsheet-visualized-8&#34; width=&#34;720&#34; height=&#34;405&#34; /&gt;&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2015/06/orchestrator-cheatsheet-visualized-9.png&#34;&gt;&lt;img class=&#34;alignnone size-full wp-image-7253&#34; src=&#34;/blog/blog/assets/orchestrator-cheatsheet-visualized-9.png&#34; alt=&#34;orchestrator-cheatsheet-visualized-9&#34; width=&#34;720&#34; height=&#34;405&#34; /&gt;&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2015/06/orchestrator-cheatsheet-visualized-10.png&#34;&gt;&lt;img class=&#34;alignnone size-full wp-image-7254&#34; src=&#34;/blog/blog/assets/orchestrator-cheatsheet-visualized-10.png&#34; alt=&#34;orchestrator-cheatsheet-visualized-10&#34; width=&#34;720&#34; height=&#34;405&#34; /&gt;&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2015/06/orchestrator-cheatsheet-visualized-11.png&#34;&gt;&lt;img class=&#34;alignnone size-full wp-image-7255&#34; src=&#34;/blog/blog/assets/orchestrator-cheatsheet-visualized-11.png&#34; alt=&#34;orchestrator-cheatsheet-visualized-11&#34; width=&#34;720&#34; height=&#34;405&#34; /&gt;&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2015/06/orchestrator-cheatsheet-visualized-12.png&#34;&gt;&lt;img class=&#34;alignnone size-full wp-image-7256&#34; src=&#34;/blog/blog/assets/orchestrator-cheatsheet-visualized-12.png&#34; alt=&#34;orchestrator-cheatsheet-visualized-12&#34; width=&#34;720&#34; height=&#34;405&#34; /&gt;&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2015/06/orchestrator-cheatsheet-visualized-13.png&#34;&gt;&lt;img class=&#34;alignnone size-full wp-image-7257&#34; src=&#34;/blog/blog/assets/orchestrator-cheatsheet-visualized-13.png&#34; alt=&#34;orchestrator-cheatsheet-visualized-13&#34; width=&#34;720&#34; height=&#34;405&#34; /&gt;&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2015/06/orchestrator-cheatsheet-visualized-1.png&#34;&gt;&lt;img class=&#34;alignnone size-full wp-image-7245&#34; src=&#34;/blog/blog/assets/orchestrator-cheatsheet-visualized-1.png&#34; alt=&#34;orchestrator-cheatsheet-visualized-1&#34; width=&#34;720&#34; height=&#34;405&#34; /&gt;&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2015/06/orchestrator-cheatsheet-visualized-2.png&#34;&gt;&lt;img class=&#34;alignnone size-full wp-image-7246&#34; src=&#34;/blog/blog/assets/orchestrator-cheatsheet-visualized-2.png&#34; alt=&#34;orchestrator-cheatsheet-visualized-2&#34; width=&#34;720&#34; height=&#34;405&#34; /&gt;&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2015/06/orchestrator-cheatsheet-visualized-3.png&#34;&gt;&lt;img class=&#34;alignnone size-full wp-image-7247&#34; src=&#34;/blog/blog/assets/orchestrator-cheatsheet-visualized-3.png&#34; alt=&#34;orchestrator-cheatsheet-visualized-3&#34; width=&#34;720&#34; height=&#34;405&#34; /&gt;&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2015/06/orchestrator-cheatsheet-visualized-4.png&#34;&gt;&lt;img class=&#34;alignnone size-full wp-image-7248&#34; src=&#34;/blog/blog/assets/orchestrator-cheatsheet-visualized-4.png&#34; alt=&#34;orchestrator-cheatsheet-visualized-4&#34; width=&#34;720&#34; height=&#34;405&#34; /&gt;&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2015/06/orchestrator-cheatsheet-visualized-5.png&#34;&gt;&lt;img class=&#34;alignnone size-full wp-image-7249&#34; src=&#34;/blog/blog/assets/orchestrator-cheatsheet-visualized-5.png&#34; alt=&#34;orchestrator-cheatsheet-visualized-5&#34; width=&#34;720&#34; height=&#34;405&#34; /&gt;&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2015/06/orchestrator-cheatsheet-visualized-6.png&#34;&gt;&lt;img class=&#34;alignnone size-full wp-image-7250&#34; src=&#34;/blog/blog/assets/orchestrator-cheatsheet-visualized-6.png&#34; alt=&#34;orchestrator-cheatsheet-visualized-6&#34; width=&#34;720&#34; height=&#34;405&#34; /&gt;&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2015/06/orchestrator-cheatsheet-visualized-14.png&#34;&gt;&lt;img class=&#34;alignnone size-full wp-image-7258&#34; src=&#34;/blog/blog/assets/orchestrator-cheatsheet-visualized-14.png&#34; alt=&#34;orchestrator-cheatsheet-visualized-14&#34; width=&#34;720&#34; height=&#34;405&#34; /&gt;&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2015/06/orchestrator-cheatsheet-visualized-15.png&#34;&gt;&lt;img class=&#34;alignnone size-full wp-image-7259&#34; src=&#34;/blog/blog/assets/orchestrator-cheatsheet-visualized-15.png&#34; alt=&#34;orchestrator-cheatsheet-visualized-15&#34; width=&#34;720&#34; height=&#34;405&#34; /&gt;&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2015/06/orchestrator-cheatsheet-visualized-16.png&#34;&gt;&lt;img class=&#34;alignnone size-full wp-image-7260&#34; src=&#34;/blog/blog/assets/orchestrator-cheatsheet-visualized-16.png&#34; alt=&#34;orchestrator-cheatsheet-visualized-16&#34; width=&#34;720&#34; height=&#34;405&#34; /&gt;&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2015/06/orchestrator-cheatsheet-visualized-17.png&#34;&gt;&lt;img class=&#34;alignnone size-full wp-image-7261&#34; src=&#34;/blog/blog/assets/orchestrator-cheatsheet-visualized-17.png&#34; alt=&#34;orchestrator-cheatsheet-visualized-17&#34; width=&#34;720&#34; height=&#34;405&#34; /&gt;&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2015/06/orchestrator-cheatsheet-visualized-18.png&#34;&gt;&lt;img class=&#34;alignnone size-full wp-image-7262&#34; src=&#34;/blog/blog/assets/orchestrator-cheatsheet-visualized-18.png&#34; alt=&#34;orchestrator-cheatsheet-visualized-18&#34; width=&#34;720&#34; height=&#34;405&#34; /&gt;&lt;/a&gt;&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Speaking at Percona Live: Pseudo GTID and Easy Replication Topology Management</title>
      <link>/blog/2015/03/31/speaking-at-percona-live-pseudo-gtid-and-easy-replication-topology-management/</link>
      <pubDate>Tue, 31 Mar 2015 17:42:21 +0000</pubDate>
      
      <guid>/blog/2015/03/31/speaking-at-percona-live-pseudo-gtid-and-easy-replication-topology-management/</guid>
      <description>&lt;p&gt;In two weeks time I will be presenting &lt;a href=&#34;https://www.percona.com/live/mysql-conference-2015/sessions/pseudo-gtid-and-easy-replication-management&#34;&gt;Pseudo GTID and Easy Replication Topology Management&lt;/a&gt; at Percona Live. From the time I submitted the proposal a LOT has been developed, experimented, deployed and used with both &lt;a href=&#34;http://code.openark.org/blog/mysql/refactoring-replication-topology-with-pseudo-gtid&#34;&gt;Pseudo GTID&lt;/a&gt; and with &lt;a href=&#34;https://github.com/outbrain/orchestrator&#34;&gt;orchestrator&lt;/a&gt;. In my talk I will:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Suggest that you skip the &#34;to GTID or not to GTID&#34; question and go for the lightweight Pseudo GTID&lt;/li&gt;
&lt;li&gt;Show how Pseudo GTID is used in production to recover from various replication failures and server crashes&lt;/li&gt;
&lt;li&gt;Do an outrageous demonstration&lt;/li&gt;
&lt;li&gt;Tell you about 50,000 successful experiments and tests done in production&lt;/li&gt;
&lt;li&gt;Show off orchestrator and its support for Pseudo GTID, including automated crash analysis and recovery mechanism.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I will further show how the orchestrator tooling makes for a less restrictive, more performant, less locking, non-intrusive, trusted and lightweight replication topology management solution.&lt;!--more--&gt;&lt;/p&gt;
&lt;blockquote&gt;&lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2015/03/orchestrator-topology-simple.png&#34;&gt;&lt;img class=&#34;wp-image-7206 size-full&#34; src=&#34;/blog/blog/assets/orchestrator-topology-simple.png&#34; alt=&#34;orchestrator-topology-simple&#34; width=&#34;1010&#34; height=&#34;286&#34; /&gt;&lt;/a&gt; An anonymized topology&lt;/blockquote&gt;
&lt;p&gt;Please come by my talk! &lt;/p&gt;
&lt;p&gt;Slides: &lt;/p&gt;
&lt;p&gt;[speakerdeck https://speakerdeck.com/shlominoach/pseudo-gtid-and-easy-mysql-replication-topology-management]&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Orchestrator 1.2.9 GA released</title>
      <link>/blog/2014/12/18/orchestrator-1-2-9-ga-released/</link>
      <pubDate>Thu, 18 Dec 2014 18:24:59 +0000</pubDate>
      
      <guid>/blog/2014/12/18/orchestrator-1-2-9-ga-released/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://github.com/outbrain/orchestrator&#34;&gt;Orchestrator&lt;/a&gt; &lt;strong&gt;1.2.9 GA&lt;/strong&gt; &lt;a href=&#34;https://github.com/outbrain/orchestrator/releases/tag/v1.2.9&#34;&gt;has been released&lt;/a&gt;. Noteworthy:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Added &#34;&lt;strong&gt;ReadOnly&lt;/strong&gt;&#34; (true/false) configuration param. You can have orchestrator completely read-only&lt;/li&gt;
&lt;li&gt;Added &lt;strong&gt;&#34;AuthenticationMethod&#34;: &#34;multi&#34;&lt;/strong&gt;: works like BasicAuth (your normal HTTP user+password) only it also accepts the special user called &lt;strong&gt;&#34;readonly&#34;&lt;/strong&gt;, which, surprise, can only view and not modify&lt;/li&gt;
&lt;li&gt;Centralized/serialized most backend database writes (with hundreds/thousands monitored servers it was possible or probable that high concurrency led to too-many-connections openned on the backend database).&lt;/li&gt;
&lt;li&gt;Fixed evil evil bug that would skip some checks if binary logs were not enabled&lt;/li&gt;
&lt;li&gt;Better hostname resolve (now also asking MySQL server to resolve hostname; resolving is cached)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pseudo-GTID&lt;/strong&gt; (read &lt;a href=&#34;http://code.openark.org/blog/mysql/refactoring-replication-topology-with-pseudo-gtid&#34;&gt;here&lt;/a&gt;, &lt;a href=&#34;http://code.openark.org/blog/mysql/orchestrator-1-2-1-beta-pseudo-gtid-support-reconnect-slaves-even-after-master-failure&#34;&gt;here&lt;/a&gt;, &lt;a href=&#34;http://code.openark.org/blog/mysql/refactoring-replication-topologies-with-pseudo-gtid-a-visual-tour&#34;&gt;here&lt;/a&gt;) support now considered stable (apart from being tested it has already been put to practice multiple times in production at &lt;strong&gt;Outbrain&lt;/strong&gt;, in different planned and unplanned crash scenarios)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I continue developing &lt;em&gt;orchestrator&lt;/em&gt; as free and open source at my new employer, &lt;a href=&#34;http://www.booking.com&#34;&gt;Booking.com&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Semi-automatic slave/master promotion via Pseudo GTID</title>
      <link>/blog/2014/11/10/semi-automatic-slavemaster-promotion-via-pseudo-gtid/</link>
      <pubDate>Mon, 10 Nov 2014 07:56:58 +0000</pubDate>
      
      <guid>/blog/2014/11/10/semi-automatic-slavemaster-promotion-via-pseudo-gtid/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://github.com/outbrain/orchestrator&#34;&gt;Orchestrator&lt;/a&gt; release &lt;a href=&#34;https://github.com/outbrain/orchestrator/releases&#34;&gt;1.2.7-beta&lt;/a&gt; now supports semi-automatic slave promotion to master upon master death, via &lt;a href=&#34;code.openark.org/blog/mysql/refactoring-replication-topology-with-pseudo-gtid&#34;&gt;Pseudo GTID&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote&gt;&lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2014/10/orchestrator-make-master-highlighted.png&#34;&gt;&lt;img class=&#34;alignnone size-full wp-image-7150&#34; src=&#34;/blog/blog/assets/orchestrator-make-master-highlighted.png&#34; alt=&#34;orchestrator-make-master-highlighted&#34; width=&#34;809&#34; height=&#34;363&#34; /&gt;&lt;/a&gt;&lt;/blockquote&gt;
&lt;p&gt;When the master is dead, &lt;em&gt;orchestrator&lt;/em&gt; automatically picks the most up-to-date slaves and marks them as &lt;strong&gt;&#34;Master candidates&#34;&lt;/strong&gt;. It allows a &lt;strong&gt;/api/make-master&lt;/strong&gt; call on such a slave (S), in which case it uses &lt;a href=&#34;http://code.openark.org/blog/mysql/refactoring-replication-topology-with-pseudo-gtid&#34;&gt;Pseudo GTID&lt;/a&gt; to enslave its siblings, and set S as &lt;strong&gt;read-only = 0&lt;/strong&gt;. All we need to do is click the &lt;strong&gt;&#34;Make master&#34;&lt;/strong&gt; button.&lt;!--more--&gt;&lt;/p&gt;
&lt;blockquote&gt;&lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2014/10/orchestrator-make-master-confirm1.png&#34;&gt;&lt;img class=&#34;alignnone size-full wp-image-7145&#34; src=&#34;/blog/blog/assets/orchestrator-make-master-confirm1.png&#34; alt=&#34;orchestrator-make-master-confirm&#34; width=&#34;946&#34; height=&#34;367&#34; /&gt;&lt;/a&gt;&lt;/blockquote&gt;
&lt;p&gt;Yes, &lt;strong&gt;&#34;OK&#34;&lt;/strong&gt;. A moment later:&lt;/p&gt;
&lt;blockquote&gt;&lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2014/10/orchestrator-make-master-refactored1.png&#34;&gt;&lt;img class=&#34;alignnone size-large wp-image-7146&#34; src=&#34;/blog/blog/assets/orchestrator-make-master-refactored1-1024x256.png&#34; alt=&#34;orchestrator-make-master-refactored&#34; width=&#34;1024&#34; height=&#34;256&#34; /&gt;&lt;/a&gt;&lt;/blockquote&gt;
&lt;p&gt;See how the two slaves &lt;strong&gt;22988&lt;/strong&gt;, &lt;strong&gt;22989&lt;/strong&gt; are happily replicating from &lt;strong&gt;22990&lt;/strong&gt;. Turning them into &lt;strong&gt;22990&lt;/strong&gt;&#39;s slave was possible even though their shared master was dead and were stopped at different replication positions.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;22990&lt;/strong&gt; is now writeable, and its on you to disconnect from its old master and to direct your application into this newly promoted instance.&lt;/p&gt;
&lt;h4&gt;Local master promotion&lt;/h4&gt;
&lt;p&gt;Likewise, a semi-automated solution for the corruption of a local master is in place. Consider the following, where &lt;strong&gt;22989&lt;/strong&gt; is dead/inaccessible. &lt;strong&gt;22988&lt;/strong&gt; and &lt;strong&gt;22990&lt;/strong&gt; are stuck in replication:&lt;/p&gt;
&lt;blockquote&gt;&lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2014/11/orchestrator-make-local-master.png&#34;&gt;&lt;img class=&#34;alignnone size-large wp-image-7164&#34; src=&#34;/blog/blog/assets/orchestrator-make-local-master-1024x263.png&#34; alt=&#34;orchestrator-make-local-master&#34; width=&#34;1024&#34; height=&#34;263&#34; /&gt;&lt;/a&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;em&gt;Orchestrator&lt;/em&gt; detects this situation and picks the most up-to-date slave, marking it as candidate for promotion. Click &lt;strong&gt;&#34;Make lock master&#34;&lt;/strong&gt; to get:&lt;/p&gt;
&lt;blockquote&gt;&lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2014/11/orchestrator-make-local-master-confirm.png&#34;&gt;&lt;img class=&#34;alignnone size-large wp-image-7165&#34; src=&#34;/blog/blog/assets/orchestrator-make-local-master-confirm-1024x257.png&#34; alt=&#34;orchestrator-make-local-master-confirm&#34; width=&#34;1024&#34; height=&#34;257&#34; /&gt;&lt;/a&gt;&lt;/blockquote&gt;
&lt;p&gt;Sure, OK:&lt;/p&gt;
&lt;blockquote&gt;&lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2014/11/orchestrator-make-local-master-refactored.png&#34;&gt;&lt;img class=&#34;alignnone size-large wp-image-7166&#34; src=&#34;/blog/blog/assets/orchestrator-make-local-master-refactored-1024x276.png&#34; alt=&#34;orchestrator-make-local-master-refactored&#34; width=&#34;1024&#34; height=&#34;276&#34; /&gt;&lt;/a&gt;&lt;/blockquote&gt;
&lt;p&gt;And now &lt;strong&gt;22990&lt;/strong&gt; takes over instead of &lt;strong&gt;22989&lt;/strong&gt;, enslaving &lt;strong&gt;22988&lt;/strong&gt;, both running happily ever after.&lt;/p&gt;
&lt;h4&gt; Automation&lt;/h4&gt;
&lt;p&gt;The above buttons are just convenience methods. You don&#39;t strictly need visualization/GUI for that. Everything is supported by the API, and can be used by a fully automated monitoring system. More to come in &lt;em&gt;orchestrator&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Refactoring replication topologies with Pseudo GTID: a visual tour</title>
      <link>/blog/2014/10/27/refactoring-replication-topologies-with-pseudo-gtid-a-visual-tour/</link>
      <pubDate>Mon, 27 Oct 2014 11:55:39 +0000</pubDate>
      
      <guid>/blog/2014/10/27/refactoring-replication-topologies-with-pseudo-gtid-a-visual-tour/</guid>
      <description>&lt;p&gt;&lt;em&gt;Orchestrator&lt;/em&gt; &lt;a href=&#34;https://github.com/outbrain/orchestrator/releases/tag/v1.2.1-beta&#34;&gt;1.2.1-beta&lt;/a&gt; supports Pseudo GTID (&lt;a href=&#34;http://code.openark.org/blog/mysql/orchestrator-1-2-1-beta-pseudo-gtid-support-reconnect-slaves-even-after-master-failure&#34;&gt;read announcement&lt;/a&gt;): a means to refactor the replication topology and connect slaves even without direct relationship; even across failed servers. This post illustrates two such scenarios and shows the visual way of mathcing/re-synching slaves.&lt;/p&gt;
&lt;p&gt;Of course, orchestrator is not just a GUI tool; anything done with drag-and-drop is also done via web API (in fact, the drag-and-drop invoke the web API) as well as via command line. I&#39;m mentioning this as this is the grounds for failover automation planned for the future.&lt;/p&gt;
&lt;h4&gt;Scenario 1: the master unexpectedly dies&lt;/h4&gt;
&lt;p&gt;The master crashes and cannot be contacted. All slaves are stopped as effect, but each in a different position. Some managed to salvage relay logs just before the master dies, some didn&#39;t. In our scenario, all three slaves are at least caught up with the relay log (that is, whatever they managed to pull through the network, they already managed to execute). So they&#39;re otherwise sitting idle waiting for something to happen. Well, something&#39;s about to happen.&lt;/p&gt;
&lt;blockquote&gt;&lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2014/10/orchestrator-pseudo-gtid-dead-master.png&#34;&gt;&lt;img class=&#34;alignnone size-large wp-image-7090&#34; src=&#34;/blog/blog/assets/orchestrator-pseudo-gtid-dead-master-1024x323.png&#34; alt=&#34;orchestrator-pseudo-gtid-dead-master&#34; width=&#34;1024&#34; height=&#34;323&#34; /&gt;&lt;/a&gt;&lt;/blockquote&gt;
&lt;p&gt;Note the green &#34;Safe mode&#34; button to the right. This means operation is through calculation of binary log files &amp;amp; positions with relation to one&#39;s master. But the master is now dead, so let&#39;s switch to adventurous mode; in this mode we can drag and drop slaves onto instances normally forbidden. At this stage the web interface allows us to drop a slave onto its sibling or any of its ancestors (including its very own parent, which is a means of reconnecting a slave with its parent). Anyhow:&lt;/p&gt;
&lt;blockquote&gt;&lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2014/10/orchestrator-pseudo-gtid-dead-master-pseudo-gtid-mode.png&#34;&gt;&lt;img class=&#34;alignnone size-large wp-image-7091&#34; src=&#34;/blog/blog/assets/orchestrator-pseudo-gtid-dead-master-pseudo-gtid-mode-1024x296.png&#34; alt=&#34;orchestrator-pseudo-gtid-dead-master-pseudo-gtid-mode&#34; width=&#34;1024&#34; height=&#34;296&#34; /&gt;&lt;/a&gt;&lt;/blockquote&gt;
&lt;p&gt;We notice that orchestrator is already kind enough to say which slave is best candidate to be the new master (&lt;strong&gt;127.0.0.1:22990&lt;/strong&gt;): this is the slave (or one of the slaves) with most up-to-date data. So we choose to take another server and make it a slave of &lt;strong&gt;127.0.0.1:22990&lt;/strong&gt;:&lt;!--more--&gt;&lt;/p&gt;
&lt;blockquote&gt;&lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2014/10/orchestrator-pseudo-gtid-dead-master-begin-drag.png&#34;&gt;&lt;img class=&#34;alignnone size-full wp-image-7093&#34; src=&#34;/blog/blog/assets/orchestrator-pseudo-gtid-dead-master-begin-drag.png&#34; alt=&#34;orchestrator-pseudo-gtid-dead-master-begin-drag&#34; width=&#34;770&#34; height=&#34;349&#34; /&gt;&lt;/a&gt;&lt;/blockquote&gt;
&lt;p&gt;And drop:&lt;/p&gt;
&lt;blockquote&gt;&lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2014/10/orchestrator-pseudo-gtid-dead-master-drop.png&#34;&gt;&lt;img class=&#34;alignnone size-full wp-image-7094&#34; src=&#34;/blog/blog/assets/orchestrator-pseudo-gtid-dead-master-drop.png&#34; alt=&#34;orchestrator-pseudo-gtid-dead-master-drop&#34; width=&#34;799&#34; height=&#34;366&#34; /&gt;&lt;/a&gt;&lt;/blockquote&gt;
&lt;p&gt;There we have it: although their shared master is inaccessible, and the two slave&#39;s binary log file names &amp;amp; position mean nothing to each other, we are able to correctly match the two and make one child of the other:&lt;/p&gt;
&lt;blockquote&gt;&lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2014/10/orchestrator-pseudo-gtid-dead-master-refactored-1.png&#34;&gt;&lt;img class=&#34;alignnone size-large wp-image-7095&#34; src=&#34;/blog/blog/assets/orchestrator-pseudo-gtid-dead-master-refactored-1-1024x269.png&#34; alt=&#34;orchestrator-pseudo-gtid-dead-master-refactored-1&#34; width=&#34;1024&#34; height=&#34;269&#34; /&gt;&lt;/a&gt;&lt;/blockquote&gt;
&lt;p&gt;Likewise, we do the same with &lt;strong&gt;127.0.0.1:22988&lt;/strong&gt;:&lt;/p&gt;
&lt;blockquote&gt;&lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2014/10/orchestrator-pseudo-gtid-dead-master-begin-drag-2.png&#34;&gt;&lt;img class=&#34;alignnone size-large wp-image-7096&#34; src=&#34;/blog/blog/assets/orchestrator-pseudo-gtid-dead-master-begin-drag-2-1024x279.png&#34; alt=&#34;orchestrator-pseudo-gtid-dead-master-begin-drag-2&#34; width=&#34;1024&#34; height=&#34;279&#34; /&gt;&lt;/a&gt;&lt;/blockquote&gt;
&lt;p&gt;And end up with our (almost) final topology:&lt;/p&gt;
&lt;blockquote&gt;&lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2014/10/orchestrator-pseudo-gtid-dead-master-refactored-2.png&#34;&gt;&lt;img class=&#34;alignnone size-large wp-image-7097&#34; src=&#34;/blog/blog/assets/orchestrator-pseudo-gtid-dead-master-refactored-2-1024x255.png&#34; alt=&#34;orchestrator-pseudo-gtid-dead-master-refactored-2&#34; width=&#34;1024&#34; height=&#34;255&#34; /&gt;&lt;/a&gt;&lt;/blockquote&gt;
&lt;p&gt;Notice how the two slaves &lt;strong&gt;22988&lt;/strong&gt;, &lt;strong&gt;22989&lt;/strong&gt; are happily replicating from &lt;strong&gt;22990&lt;/strong&gt;. As far as they&#39;re concerned, there is no problem in the topology any more. Now it&#39;s your decision: do you decommission the old master? You will need to &lt;strong&gt;RESET SLAVE&lt;/strong&gt; on &lt;strong&gt;22990&lt;/strong&gt; (can do via &lt;em&gt;orchestrator&lt;/em&gt;), turn off &lt;strong&gt;22990&lt;/strong&gt;&#39;s &lt;strong&gt;read_only&lt;/strong&gt; (can do via &lt;em&gt;orchestrator&lt;/em&gt;) and change DNS entries (or what have you).&lt;/p&gt;
&lt;h4&gt;Scenario 2: a local master (&#34;relay-master&#34;) unexpectedly dies&lt;/h4&gt;
&lt;p&gt;In this scenario we have a &lt;a href=&#34;http://code.openark.org/blog/mysql/using-deep-nested-replication-topologies&#34;&gt;deep nested topology&lt;/a&gt;, and a local master died. What of its slaves?&lt;/p&gt;
&lt;blockquote&gt; &lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2014/10/orchestrator-pseudo-gtid-dead-relay-master.png&#34;&gt;&lt;img class=&#34;alignnone size-large wp-image-7100&#34; src=&#34;/blog/blog/assets/orchestrator-pseudo-gtid-dead-relay-master-1024x237.png&#34; alt=&#34;orchestrator-pseudo-gtid-dead-relay-master&#34; width=&#34;1024&#34; height=&#34;237&#34; /&gt;&lt;/a&gt;&lt;/blockquote&gt;
&lt;p&gt;We choose one of the children and drag it over onto the master, which is up and healthy:&lt;/p&gt;
&lt;blockquote&gt;&lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2014/10/orchestrator-pseudo-gtid-dead-relay-master-begin-drag.png&#34;&gt;&lt;img class=&#34;alignnone size-large wp-image-7101&#34; src=&#34;/blog/blog/assets/orchestrator-pseudo-gtid-dead-relay-master-begin-drag-1024x292.png&#34; alt=&#34;orchestrator-pseudo-gtid-dead-relay-master-begin-drag&#34; width=&#34;1024&#34; height=&#34;292&#34; /&gt;&lt;/a&gt;&lt;/blockquote&gt;
&lt;p&gt;As you can see we are allowed (green instances are allowed drop places) to drop &lt;strong&gt;22989&lt;/strong&gt; on its sibling and on its grandparent, the latter bypassing a broken connection. There &lt;em&gt;is no&lt;/em&gt; connection between the two!&lt;/p&gt;
&lt;blockquote&gt;&lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2014/10/orchestrator-pseudo-gtid-dead-relay-master-drop.png&#34;&gt;&lt;img class=&#34;alignnone size-large wp-image-7102&#34; src=&#34;/blog/blog/assets/orchestrator-pseudo-gtid-dead-relay-master-drop-1024x282.png&#34; alt=&#34;orchestrator-pseudo-gtid-dead-relay-master-drop&#34; width=&#34;1024&#34; height=&#34;282&#34; /&gt;&lt;/a&gt;&lt;/blockquote&gt;
&lt;p&gt;And we get a new topology:&lt;/p&gt;
&lt;blockquote&gt;&lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2014/10/orchestrator-pseudo-gtid-dead-relay-master-refactored-1.png&#34;&gt;&lt;img class=&#34;alignnone size-large wp-image-7103&#34; src=&#34;/blog/blog/assets/orchestrator-pseudo-gtid-dead-relay-master-refactored-1-1024x264.png&#34; alt=&#34;orchestrator-pseudo-gtid-dead-relay-master-refactored-1&#34; width=&#34;1024&#34; height=&#34;264&#34; /&gt;&lt;/a&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;22989&lt;/strong&gt; is now lagging, but on the right path! Let&#39;s do the same for &lt;strong&gt;22988&lt;/strong&gt;:&lt;/p&gt;
&lt;blockquote&gt;&lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2014/10/orchestrator-pseudo-gtid-dead-relay-master-begin-drag-2.png&#34;&gt;&lt;img class=&#34;alignnone size-large wp-image-7104&#34; src=&#34;/blog/blog/assets/orchestrator-pseudo-gtid-dead-relay-master-begin-drag-2-1024x276.png&#34; alt=&#34;orchestrator-pseudo-gtid-dead-relay-master-begin-drag-2&#34; width=&#34;1024&#34; height=&#34;276&#34; /&gt;&lt;/a&gt;&lt;/blockquote&gt;
&lt;p&gt;And finally:&lt;/p&gt;
&lt;blockquote&gt;&lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2014/10/orchestrator-pseudo-gtid-dead-relay-master-refactored-2.png&#34;&gt;&lt;img class=&#34;alignnone size-full wp-image-7105&#34; src=&#34;/blog/blog/assets/orchestrator-pseudo-gtid-dead-relay-master-refactored-2.png&#34; alt=&#34;orchestrator-pseudo-gtid-dead-relay-master-refactored-2&#34; width=&#34;809&#34; height=&#34;357&#34; /&gt;&lt;/a&gt;&lt;/blockquote&gt;
&lt;p&gt;Great success! &lt;strong&gt;22989&lt;/strong&gt; already caught up, &lt;strong&gt;22988&lt;/strong&gt; on the way, victory is ours!&lt;/p&gt;
&lt;p&gt;The real fun, of course, is to execute with &lt;strong&gt;--debug&lt;/strong&gt; and review the DEBUG messages as orchestrator seeks, finds, matches and follows up on Pseudo GTID entries in the binary logs. We each have our pleasures.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>