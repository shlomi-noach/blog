<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>High Availability on code.openark.org</title>
    <link>/blog/tags/high-availability/</link>
    <description>Recent content in High Availability on code.openark.org</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2016. All rights reserved.</copyright>
    <lastBuildDate>Fri, 20 Nov 2015 11:41:13 +0000</lastBuildDate>
    <atom:link href="/blog/tags/high-availability/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>State of automated recovery via Pseudo-GTID &amp; Orchestrator @ Booking.com</title>
      <link>/blog/mysql/state-of-automated-recovery-via-pseudo-gtid-orchestrator-booking-com/</link>
      <pubDate>Fri, 20 Nov 2015 11:41:13 +0000</pubDate>
      
      <guid>/blog/mysql/state-of-automated-recovery-via-pseudo-gtid-orchestrator-booking-com/</guid>
      <description>&lt;p&gt;This post sums up some of my work on MySQL resilience and high availability at &lt;a href=&#34;http://www.booking.com&#34;&gt;Booking.com&lt;/a&gt; by presenting the current state of automated master and intermediate master recoveries via &lt;a href=&#34;http://code.openark.org/blog/mysql/refactoring-replication-topology-with-pseudo-gtid&#34;&gt;Pseudo-GTID&lt;/a&gt; &amp;amp; &lt;strong&gt;&lt;a href=&#34;https://github.com/outbrain/orchestrator&#34;&gt;Orchestrator&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Booking.com uses many different MySQL topologies, of varying vendors, configurations and workloads: Oracle MySQL, MariaDB, statement based replication, row based replication, hybrid, OLTP, OLAP, GTID (few), no GTID (most), Binlog Servers, filters, hybrid of all the above.&lt;/p&gt;
&lt;p&gt;Topologies size varies from a single server to many-many-many. Our typical topology has a master in one datacenter, a bunch of slaves in same DC, a slave in another DC acting as an intermediate master to further bunch of slaves in the other DC. Something like this, give or take:&lt;/p&gt;
&lt;blockquote&gt;&lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2015/11/booking-topology-sample.png&#34;&gt;&lt;img class=&#34;alignnone wp-image-7480 size-medium&#34; src=&#34;/blog/blog/assets/booking-topology-sample-300x169.png&#34; alt=&#34;booking-topology-sample&#34; width=&#34;300&#34; height=&#34;169&#34; /&gt;&lt;/a&gt;&lt;/blockquote&gt;
&lt;p&gt;However as we are building our third data center (with MySQL deployments mostly completed) the graph turns more complex.&lt;/p&gt;
&lt;p&gt;Two high availability questions are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What happens when an intermediate master dies? What of all its slaves?&lt;/li&gt;
&lt;li&gt;What happens when the master dies? What of the entire topology?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is not a technical drill down into the solution, but rather on overview of the state. For more, please refer to recent presentations in &lt;a href=&#34;https://speakerdeck.com/shlominoach/managing-and-visualizing-your-replication-topologies-with-orchestrator&#34;&gt;September&lt;/a&gt; and &lt;a href=&#34;https://speakerdeck.com/shlominoach/pseudo-gtid-and-easy-mysql-replication-topology-management&#34;&gt;April&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;At this time we have:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pseudo-GTID deployed on all chains
&lt;ul&gt;
&lt;li&gt;Injected every 5 seconds&lt;/li&gt;
&lt;li&gt;Using the &lt;a href=&#34;http://code.openark.org/blog/mysql/pseudo-gtid-ascending&#34;&gt;monotonically ascending&lt;/a&gt; variation&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Pseudo-GTID based automated failover for intermediate masters on all chains&lt;/li&gt;
&lt;li&gt;Pseudo-GTID based automated failover for masters on roughly 30% of the chains.
&lt;ul&gt;
&lt;li&gt;The rest of 70% of chains are set for manual failover using Pseudo-GTID.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Pseudo-GTID is in particular used for:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Salvaging slaves of a dead intermediate master&lt;/li&gt;
&lt;li&gt;Correctly grouping and connecting slaves of a dead master&lt;/li&gt;
&lt;li&gt;Routine refactoring of topologies. This includes:
&lt;ul&gt;
&lt;li&gt;Manual repointing of slaves for various operations (e.g. offloading slaves from a busy box)&lt;/li&gt;
&lt;li&gt;Automated refactoring (for example, used by our automated upgrading script, which consults with &lt;em&gt;orchestrator&lt;/em&gt;, upgrades, shuffles slaves around, updates intermediate master, suffles back...)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;(In the works), failing over binlog reader apps that audit our binary logs.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;!--more--&gt;Furthermore, Booking.com is also &lt;a href=&#34;https://www.percona.com/live/europe-amsterdam-2015/sessions/binlog-servers-bookingcom&#34;&gt;working on Binlog Servers&lt;/a&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;These take production traffic and offload masters and intermediate masters&lt;/li&gt;
&lt;li&gt;Often co-serve slaves using round-robin VIP, such that failure of one Binlog Server makes for simple slave replication self-recovery.&lt;/li&gt;
&lt;li&gt;Are interleaved alongside standard replication
&lt;ul&gt;
&lt;li&gt;At this time we have no &#34;pure&#34; Binlog Server topology in production; we always have normal intermediate masters and slaves&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;This hybrid state makes for greater complexity:
&lt;ul&gt;
&lt;li&gt;Binlog Servers are not designed to participate in a game of changing masters/intermediate master, unless &lt;a href=&#34;http://jfg-mysql.blogspot.nl/2015/09/abstracting-binlog-servers-and-mysql-master-promotion-wo-reconfiguring-slaves.html&#34;&gt;successors come from their own sub-topology&lt;/a&gt;, which is not the case today.
&lt;ul&gt;
&lt;li&gt;For example, a Binlog Server that replicates directly from the master, cannot be repointed to just any new master.&lt;/li&gt;
&lt;li&gt;But can still hold valuable binary log entries that other slaves may not.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Are not actual MySQL servers, therefore of course cannot be promoted as masters&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Orchestrator&lt;/em&gt; &amp;amp; Pseudo-GTID makes this hybrid topology still resilient:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Orchestrator&lt;/em&gt; understands the limitations on the hybrid topology and can salvage slaves of 1st tier Binlog Servers via Pseudo-GTID&lt;/li&gt;
&lt;li&gt;In the case where the Binlog Servers were the most up to date slaves of a failed master, &lt;em&gt;orchestrator&lt;/em&gt; knows to first move potential candidates under the Binlog Server and then extract them out again.&lt;/li&gt;
&lt;li&gt;At this time Binlog Servers are still unstable. Pseudo-GTID allows us to comfortably test them on a large setup with reduced fear of losing slaves.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Otherwise &lt;em&gt;orchestrator&lt;/em&gt; already understands pure Binlog Server topologies and can do master promotion. When pure binlog servers topologies will be in production &lt;em&gt;orchestrator&lt;/em&gt; will be there to watch over.&lt;/p&gt;
&lt;h3&gt;Summary&lt;/h3&gt;
&lt;p&gt;To date, Pseudo-GTID has high scores in automated failovers of our topologies; &lt;em&gt;orchestrator&#39;s&lt;/em&gt; &lt;a href=&#34;http://code.openark.org/blog/mysql/what-makes-a-mysql-server-failurerecovery-case&#34;&gt;holistic approach&lt;/a&gt; makes for reliable diagnostics; together they reduce our dependency on specific servers &amp;amp; hardware, physical location, latency implied by SAN devices.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Thoughts on MaxScale automated failover (and Orchestrator)</title>
      <link>/blog/mysql/thoughts-on-maxscale-automated-failover-and-orchestrator/</link>
      <pubDate>Wed, 18 Nov 2015 11:17:48 +0000</pubDate>
      
      <guid>/blog/mysql/thoughts-on-maxscale-automated-failover-and-orchestrator/</guid>
      <description>&lt;p&gt;Having attended a talk (as part of the &lt;a href=&#34;https://blog.mariadb.org/2015-developers-meeting-amsterdam/&#34;&gt;MariaDB Developer Meeting in Amsterdam&lt;/a&gt;) about recent developments of &lt;a href=&#34;https://mariadb.com/products/mariadb-maxscale&#34;&gt;MaxScale&lt;/a&gt; in executing automated failovers, here are some (late) observations of mine.&lt;/p&gt;
&lt;p&gt;I will begin by noting that the project is stated to be pre-production, and so of course none of the below are complaints, but rather food for thought, points for action and otherwise recommendations.&lt;/p&gt;
&lt;p&gt;Some functionality of the MaxScale failover is also implemented by &lt;strong&gt;&lt;a href=&#34;https://github.com/outbrain/orchestrator&#34;&gt;orchestrator&lt;/a&gt;&lt;/strong&gt;, which I author. &lt;em&gt;Orchestrator&lt;/em&gt; was built in production environments by and for operational people. In this respect it has gained many insights and had to cope with many real-world cases, special cases &amp;amp; Murphy&#39;s law cases. This post compares logic, feature set and capabilities of the two where relevant. To some extent the below will read as &#34;hey, I&#39;ve already implemented this; shame to re-implement the same&#34;, and indeed I think that way; but it wouldn&#39;t be the first time a code of mine would just be re-implemented by someone else and I&#39;ve done the same, myself.&lt;/p&gt;
&lt;p&gt;I&#39;m describing the solution the way I understood it from the talk. If I&#39;m wrong on any account I&#39;m happy to be corrected via comments below. &lt;strong&gt;Edit:&lt;/strong&gt; &lt;em&gt;please see comment by&lt;/em&gt; &lt;a class=&#34;url&#34; href=&#34;http://www.mariadb.com/&#34; rel=&#34;external nofollow&#34;&gt;Dipti Joshi&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;General overview&lt;/h3&gt;
&lt;p&gt;The idea is that MaxScale operates as a proxy to your topology. You do not connect to your master directly, but rather through MaxScale. Thus, MaxScale acts as a proxy to your master.&lt;/p&gt;
&lt;p&gt;The next phase is that MaxScale would also auto-detect master failure, fix the topology for you, promote a new master, and will have your application unaware of all the complexity and without the app having to change setup/DNS/whatever. Of course some write downtime is implied.&lt;/p&gt;
&lt;p&gt;Now for some breakdown.&lt;/p&gt;
&lt;h3&gt;Detection&lt;/h3&gt;
&lt;p&gt;The detection of a dead master, the check by which a failover is initiated, is based on MaxScale not being able to query the master. This calls for some points for consideration:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Typically, I would see &#34;I can&#39;t connect to the master therefore failover&#34; as too hysterical, and the basis for a lot of false positives.&lt;/li&gt;
&lt;li&gt;However, since in the discussed configuration MaxScale is &lt;em&gt;the only access point&lt;/em&gt; to the master, the fact MaxScale cannot connect to the master means the master is inaccessible &lt;em&gt;de-facto&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;In light of the above, the decision makes sense - but I still hold that it would make false positives.&lt;/li&gt;
&lt;li&gt;I&#39;m unsure (I &lt;em&gt;think&lt;/em&gt; not; can anyone comment?) if MaxScale would make multiple attempts over time and only reach the conclusion after X successive failures. This would reduce the false positives.&lt;/li&gt;
&lt;li&gt;I&#39;m having a growing dislike to a &#34;check for 4 successive times then alert/failover&#34; Nagios-style behavior. &lt;em&gt;Orchestrator&lt;/em&gt; takes a different approach where it recognizes a master&#39;s death by not being able to connect to the master &lt;em&gt;as well as&lt;/em&gt; being able to connect to 1st tier slaves, check their status and observe that &lt;em&gt;they&#39;re unable to connect to the master as well&lt;/em&gt;. See &lt;a title=&#34;Permanent Link to What makes a MySQL server failure/recovery case?&#34; href=&#34;http://code.openark.org/blog/mysql/what-makes-a-mysql-server-failurerecovery-case&#34; rel=&#34;bookmark&#34;&gt;What makes a MySQL server failure/recovery case?&lt;/a&gt;. This approach still calls for further refinement (what if the master is temporarily deadlocked? Is this a failover or not?).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;!--more--&gt;&lt;/p&gt;
&lt;h3&gt;Assumed topology&lt;/h3&gt;
&lt;p&gt;MaxScale assumes the topology is all MariaDB, and all slaves are using (MariaDB) GTID replication. Well, MaxScale does not actually assumes that. It is assumed so by the &lt;a href=&#34;https://github.com/mariadb-corporation/replication-manager&#34;&gt;MariaDB Replication Manager&lt;/a&gt; which MaxScale invokes. But I&#39;m getting ahead of myself here.&lt;/p&gt;
&lt;h3&gt;Topology detection&lt;/h3&gt;
&lt;p&gt;MaxScale does not recognize the master by configuration but rather by state. It observes the servers it should observe, and concludes which is the master.&lt;/p&gt;
&lt;p&gt;I&#39;m using similar approach in &lt;em&gt;orchestrator&lt;/em&gt;. I maintain that this approach works well and opens the Chakras for complex recovery options.&lt;/p&gt;
&lt;h3&gt;Upon failure detection&lt;/h3&gt;
&lt;p&gt;When MaxScale detects failure, it invokes external scripts to fix the problem. There are some similar and different particulars here as compared to &lt;em&gt;orchestrator&lt;/em&gt;, and I will explain what&#39;s wrong with the MaxScale approach:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Although MaxScale observes the topology and understands who is the master and who isn&#39;t, the executed scripts do not. They need to re-discover everything by themselves.&lt;/li&gt;
&lt;li&gt;This implies the scripts start without memory of &#34;what was last observed&#34;. This is one of the greatest strengths of &lt;em&gt;orchestrator&lt;/em&gt;: it knows what the state was just before the failure, and, having the bigger picture, can make informed decisions.
&lt;ul&gt;
&lt;li&gt;As a nasty example, what do you do when some the first tier slaves also happen to be inaccessible at that time? What if one of those happens to further have slaves of its own?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;The MariaDB Replication Manager script (to be referenced as &lt;em&gt;repmgr&lt;/em&gt;) assumes all instances to be MariaDB with GTID.
&lt;ul&gt;
&lt;li&gt;It is also implied that all my slaves are configured with binary logs &amp;amp; log-slave-updates&lt;/li&gt;
&lt;li&gt;That&#39;s &lt;strong&gt;way too restrictive&lt;/strong&gt;.
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Orchestrator&lt;/em&gt; handles all following topologies: Oracle MySQL with/out GTID, MariaDB with/out GTID, MariaDB hybrid GTID &amp;amp; non-GTID replication, Pseudo-GTID (MySQL and/or MariaDB), hybrid normal &amp;amp; binlog servers topologies, slaves with/out log-slave-updates, hybrid Oracle &amp;amp; MariaDB &amp;amp; Binlog Servers &amp;amp; Pseudo-GTID.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;em&gt;repmgr&lt;/em&gt; is unaware of data centers &amp;amp; physical environments. You want failover to be as local to your datacenters as possible. Avoid too many cross-DC replication streams.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Failover invocation&lt;/h3&gt;
&lt;p&gt;MaxScale invokes the failover scripts &lt;em&gt;asynchronously&lt;/em&gt;. This is a major flaw imho, as the decoupling between the monitoring and acting processes leads to further problems, see further.&lt;/p&gt;
&lt;h3&gt;After failover&lt;/h3&gt;
&lt;p&gt;MaxScale continuously scans the topology and observes that some other server has been promoted. This behavior is similar to &lt;em&gt;orchestrator&#39;s&lt;/em&gt;. But the following differences are noteworthy:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Because of both the decoupling as well as the asynchronous invocation by MaxScale, it doesn&#39;t really have any idea if and how the promotion resolved.&lt;/li&gt;
&lt;li&gt;I don&#39;t know that there&#39;s any anti-flapping mechanism, nor that there could be. If MaxScale doesn&#39;t care what happened to the failover script, it shouldn&#39;t be able to keep up with flapping scenarios.&lt;/li&gt;
&lt;li&gt;Nor is there a minimal suspend period between any two failure recoveries, that I know of. MaxScale can actually have easier life than &lt;em&gt;orchestrator&lt;/em&gt; in this regard as it is (I suspect) strictly associated with &lt;em&gt;a topology&lt;/em&gt;. Not like there&#39;s a single MaxScale handling multiple topologies. So it should be very easy to keep track of failures.&lt;/li&gt;
&lt;li&gt;Or, if there is a minimal period and I&#39;m just uninformed -- what makes sure it is not smaller than the time it takes for the failover?&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Further on failover&lt;/h3&gt;
&lt;p&gt;I wish to point out that one component of the system analyses a failure scenario, and another one fixes it. I suggest this is an undesired design. The &#34;fixer&#34; must have its own ability to diagnose problems as it makes progress (or else it is naive and would fail in many production cases). And the &#34;analyzer&#34; part must have some wisdom of its own so as to suggest course of action; or understand the consequences of the recovery done by the &#34;fixer&#34;.&lt;/p&gt;
&lt;h3&gt;Use of shell scripts&lt;/h3&gt;
&lt;p&gt;Generally speaking, the use of shell scripts as external hooks is evil:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Shell scripts tend to be poorly audited&lt;/li&gt;
&lt;li&gt;With poor clarity as for what went wrong&lt;/li&gt;
&lt;li&gt;Killing them has operational difficulty (detect the shell script, find possible children, detached children)&lt;/li&gt;
&lt;li&gt;The approach of &#34;if you want something else, just write a shell script for it&#34; is nice for some things, but as the problem turns complex, you turn out to just write big parts of the solution in shell. This decouples your code to unwanted degree.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;At this time, &lt;em&gt;orchestrator&lt;/em&gt; also uses external hooks. However:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Fixing the topology happens within &lt;em&gt;orchestrator&lt;/em&gt;, not by external scripts. There is an elaborate, auditable, visible decision making.
&lt;ul&gt;
&lt;li&gt;Decision making includes data center considerations, different configuration of servers involved, servers hinted as candidates, servers configured to be ignored, servers known to be downtimed.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Leaving the external scripts with the task of managing DNS changes or what have you.
&lt;ul&gt;
&lt;li&gt;Today, at Booking.com, we have a special operational tool (called the dba tool) which does that, manages rosters, issues puppet etc. This tool is itself well audited. Granted, there is still decoupling, but information does not just get lost.&lt;/li&gt;
&lt;li&gt;Sometime in the future I suspect I will extend &lt;strong&gt;&lt;a href=&#34;https://github.com/outbrain/orchestrator-agent&#34;&gt;orchestrator-agent&lt;/a&gt;&lt;/strong&gt; to participate in failovers, which means the entire flow is within &lt;em&gt;orchestrator&#39;s&lt;/em&gt; scope.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;High availability&lt;/h3&gt;
&lt;p&gt;All the above is only available via a single MaxScale server. What happens if it dies?&lt;/p&gt;
&lt;p&gt;There is a MaxScale/pacemaker setup I&#39;m aware of. If one MaxScale dies, pacemaker takes charge and starts another on another box.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;But this means real downtime&lt;/li&gt;
&lt;li&gt;There are no multiple-MaxScale servers to load-balance on&lt;/li&gt;
&lt;li&gt;The MaxScale started by pacemaker is newly born, and does not have the big picture of the topology. It needs to go through a &#34;peaceful time&#34; to understand what&#39;s going on.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;More High Availability&lt;/h3&gt;
&lt;p&gt;At a time where MaxScale will be able to load-balance and run on multiple nodes, MariaDB will have to further tackle:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Leader election&lt;/li&gt;
&lt;li&gt;Avoiding concurrent initiation of failovers
&lt;ul&gt;
&lt;li&gt;Either via group communication&lt;/li&gt;
&lt;li&gt;Or via shared datastore&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Taking off from a failed/crashed MaxScale server&#39;s work
&lt;ul&gt;
&lt;li&gt;Or rolling it back&lt;/li&gt;
&lt;li&gt;Or just cleaning it up&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;And generally share all those little pieces of information, such as &#34;Hey, now this server is the master&#34; (are all MaxScales in complete agreement on the topology?) or &#34;I have failed over this topology, we should avoid failing it over again for the next 10 minutes&#34; and more.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The above are supported by &lt;em&gt;orchestrator&lt;/em&gt;. It provides leader election, automated leader promotion, fair recognition of various failure scenarios, picking up a failed recovery from a failed &lt;em&gt;orchestrator&lt;/em&gt;. Data is shared by a backend MySQL datastore, and before you shout &lt;em&gt;SPOF&lt;/em&gt;, make it Galera/NDB.&lt;/p&gt;
&lt;h3&gt;Further little things that can ruin your day&lt;/h3&gt;
&lt;h4&gt;How about having a delayed replica?&lt;/h4&gt;
&lt;p&gt;Here&#39;s an operational use case we had to tackle.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You have a slave configured to lag by &lt;strong&gt;24&lt;/strong&gt; hours. You know the drill: hackers / accidental &lt;strong&gt;DROP TABLE&lt;/strong&gt;...&lt;/li&gt;
&lt;li&gt;How much time will an automated tool spend on reconnecting this slave to the topology?
&lt;ul&gt;
&lt;li&gt;This could take long minutes&lt;/li&gt;
&lt;li&gt;Will your recovery hang till this is resolved?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Since &lt;em&gt;orchestrator&lt;/em&gt; heals the topology in-house, it knows how to push certain operations till after specific other operations took place. For example, &lt;em&gt;orchestrator&lt;/em&gt; wants to heal the entire topology, but pushes the delayed replicas aside, under the assumption that it will be able to fix them later (fair assumption, because they are known to be behind our promoted master); it will proceed to fix everything else, execute external hooks (change DNS etc.) and only then come back to the delayed replica. All the while, the process is audited.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Flapping ruins your day&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Not only do you want some stall period between two failovers, you also want your team to respond to a failover and acknowledge it. Or clear up the stall period having verified the source of the problem. Or force the next failover even if it comes sooner than the stall period termination.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Binlog formats&lt;/h4&gt;
&lt;p&gt;It is still not uncommon to have Statement Based Replication running. And then it is also not uncommon to have one or two slaves translating to Row Based Replication because of:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Some app that has to read ROW based format&lt;/li&gt;
&lt;li&gt;Experimenting with RBR for purposes of upgrade&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You just can&#39;t promote such a RBR slave on top of SBR slaves; it wouldn&#39;t work. &lt;em&gt;Orchestrator&lt;/em&gt; is aware of such rules. I still need to integrate this particular consideration into the promotion algorithm.&lt;/p&gt;
&lt;h4&gt;Versions&lt;/h4&gt;
&lt;p&gt;Likewise, not all your slaves are of same version. You should not promote a newer version slave on top of an older version slave. Again, &lt;em&gt;orchestrator&lt;/em&gt; will not allow putting such a topology, and again, I still need to integrate this consideration into the promotion algorithm.&lt;/p&gt;
&lt;h3&gt;In summary&lt;/h3&gt;
&lt;p&gt;There is a long way for MaxScale failover to go. When you consider the simplest, all-MariaDB-GTID-equal-slaves small topology case, things are kept simple and probably sustainable. But issues like complex topologies, flapping, special slaves, different configuration, high availability, leadership, acknowledgements, and more, call for a more advanced solution.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Problems with MMM for MySQL</title>
      <link>/blog/mysql/problems-with-mmm-for-mysql/</link>
      <pubDate>Mon, 16 May 2011 07:31:32 +0000</pubDate>
      
      <guid>/blog/mysql/problems-with-mmm-for-mysql/</guid>
      <description>&lt;p&gt;I recently encountered troubling issues with MMM for MySQL deployments, leading me to the decision to cease using it on production.&lt;/p&gt;
&lt;p&gt;At the very same day I started writing about it, Baron published &lt;a href=&#34;http://www.xaprb.com/blog/2011/05/04/whats-wrong-with-mmm/&#34;&gt;What&#39;s wrong with MMM?&lt;/a&gt;. I wish to present the problems I encountered and the reasons I find MMM is flawed. In a period of two weeks, two different deployments presented me with &lt;strong&gt;4&lt;/strong&gt; crashes, in &lt;strong&gt;3&lt;/strong&gt; different scenarios.&lt;/p&gt;
&lt;p&gt;In all the following scenarios, there is an Active/Passive Master-Master deployment, with one VIP (virtual IP) set for &lt;em&gt;writer&lt;/em&gt; role, one VIP set for &lt;em&gt;reader&lt;/em&gt; role.&lt;/p&gt;
&lt;h4&gt;Problem #1: unjustified failover, broken replication&lt;/h4&gt;
&lt;p&gt;Unjustified failover must be the common scenario. It&#39;s also a scenario I can live with. A few seconds of downtime are OK with me once in a couple of months.&lt;/p&gt;
&lt;p&gt;But on two different installations, a few days apart, I had two seemingly unjustified failovers followed by a troubling issue: replication got broken.&lt;!--more--&gt;&lt;/p&gt;
&lt;p&gt;How broken? The previously active master, now turned inactive, suddenly changed master position to a roughly &lt;strong&gt;10&lt;/strong&gt; days old position. I don&#39;t keep master logs for &lt;strong&gt;10&lt;/strong&gt; days, so this led to an immediate replication fail.&lt;/p&gt;
&lt;p&gt;Now, I cannot directly point my finger at MMM, but:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;There was no power failure&lt;/li&gt;
&lt;li&gt;MySQL daemon did not go down&lt;/li&gt;
&lt;li&gt;Replication was just fine both 	ways up to the failover moment&lt;/li&gt;
&lt;li&gt;There was no human intervention on 	this (myself and once more person had access at that time).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I know the above since I&#39;ve got it all monitored. So I can&#39;t blame it on “replication not synching &lt;strong&gt;master.info&lt;/strong&gt; file to disk when power went down”. I confess, this is very suspicious; but, &lt;em&gt;twice&lt;/em&gt;, on two different deployments...&lt;/p&gt;
&lt;p&gt;So much for suspicions. Now for the smoking guns.&lt;/p&gt;
&lt;h4&gt;Problem #2: hanging master, no failover&lt;/h4&gt;
&lt;p&gt;The active master went down. Either hardware or software problem caused it to freeze. It was not executing its chores; it became inaccessible by TCP/IP.&lt;/p&gt;
&lt;p&gt;But not just inaccessible: freezing inaccessible. If you were to attempt an SSH connection, the connection would just hang; not refused. The SSH client would not terminate in any reasonable time.&lt;/p&gt;
&lt;p&gt;Ahem, time to do failover?&lt;/p&gt;
&lt;p&gt;Apparently not. Phones start ringing. Emails sent. Time for manual intervention. But, what does the MMM monitor have to say? Nothing. It&#39;s frozen. Now, I didn&#39;t read the source code; I&#39;m not even competent with PERL; but is &lt;em&gt;seems&lt;/em&gt; to me like the monitor daemon works single threaded: it attempts to connect all hosts on the same thread. But, connecting to active master makes for hanging connection, so the entire monitor goes down. Impossible to stop it gracefully, I had to kill it. I had the choice of reconfiguring it to ignore active master, but decided to try starting it up again. I had to do it twice before it started acting sanely again, and realized it was time for failover.&lt;/p&gt;
&lt;p&gt;Why is this bad? Assuming my analysis is correct, this is a major design flaw. You must never do a single threaded monitoring on a multiple-machine deployment.&lt;/p&gt;
&lt;h4&gt;Problem #3: no servers&lt;/h4&gt;
&lt;p&gt;System is down! No access to the database. What does MMM monitor have to say?&lt;/p&gt;
&lt;p&gt;Both machines are HARD_OFFLINE.&lt;/p&gt;
&lt;p&gt;Ahem. Both machines are up and running; they are both replicating each other. They are both accessible. MySQL is accessible.&lt;/p&gt;
&lt;p&gt;But neither machine has any VIP associated with.&lt;/p&gt;
&lt;p&gt;Does it matter whether both agents fail to realize their associated MySQL servers were actually up and running, or whether the monitor fail to receive that information? It does not. MMM should not have removed all VIPs. OK, suppose it believes the two machines are down. So what? Just throw all VIPs on one of the machines. If it&#39;s inaccessible, then what&#39;s wrong with that? (assuming the previous problem never existed, that is).&lt;/p&gt;
&lt;p&gt;You should always have &lt;em&gt;some&lt;/em&gt; machine associated with the VIPs.&lt;/p&gt;
&lt;h4&gt;Temper down&lt;/h4&gt;
&lt;p&gt;None of the above is meant as offense to the creators of MMM, whom I greatly respect. This isn&#39;t an easy problem to solve, and it should be obvious there&#39;s no 100% guaranteed solution. But, for myself, I will not be using MMM as it stands right now anymore.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>MMM for MySQL single reader role</title>
      <link>/blog/mysql/mmm-for-mysql-single-reader-role/</link>
      <pubDate>Thu, 12 Aug 2010 14:12:16 +0000</pubDate>
      
      <guid>/blog/mysql/mmm-for-mysql-single-reader-role/</guid>
      <description>&lt;p&gt;The standard documentation and tutorials on &lt;a href=&#34;http://mysql-mmm.org/&#34;&gt;MMM for MySQL&lt;/a&gt;, for master-master replication setup, suggest one Virtual IP for the &lt;em&gt;writer&lt;/em&gt; role, and two Virtual IPs for the &lt;em&gt;reader&lt;/em&gt; role. It can be desired to only have a single virtual IP for the reader role, as explained below.&lt;/p&gt;
&lt;h4&gt;The two IPs for the reader role&lt;/h4&gt;
&lt;p&gt;A simplified excerpt from the &lt;strong&gt;mmm_common.conf&lt;/strong&gt; sample configuration file, as can be found on the project&#39;s site and which is most quoted:&lt;!--more--&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;...
&amp;lt;host db1&amp;gt;
  ip                      192.168.0.11
  mode                    master
  peer                    db2
&amp;lt;/host&amp;gt;

&amp;lt;host db2&amp;gt;
  ip                      192.168.0.12
  mode                    master
  peer                    db1
&amp;lt;/host&amp;gt;
...
&amp;lt;role writer&amp;gt;
  hosts                   db1, db2
  ips                     192.168.0.100
  mode                    exclusive
&amp;lt;/role&amp;gt;

&amp;lt;role reader&amp;gt;
  hosts                   db1, db2
  ips                     192.168.0.101, 192.168.0.102
  mode                    balanced
&amp;lt;/role&amp;gt;
&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;In the above setup &lt;strong&gt;db1&lt;/strong&gt; &amp;amp; &lt;strong&gt;db2&lt;/strong&gt; participate in master-master active-passive replication. Whenever you need to write something, you use &lt;strong&gt;192.18.0.100&lt;/strong&gt;, which is the virtual IP for the writer role. Whenever you need to read something, you use either &lt;strong&gt;192.168.0.101&lt;/strong&gt; or &lt;strong&gt;192.168.0.102&lt;/strong&gt;, which are the virtual IPs of the two machines, this time in read role. Logic says one wishes to distribute reads between the two machines.&lt;/p&gt;
&lt;h4&gt;One IP for reader role&lt;/h4&gt;
&lt;p&gt;I have a few cases where the above setup is not satisfactory: there is a requirement to know the IP of the passive (read-only) master. Reason? There are queries which we only want to execute on the slave (reporting, long analysis), and only execute on the active master when this isn&#39;t possible. Sometimes we might even prefer waiting for a slave to come back up rather than execute a query on the master.&lt;/p&gt;
&lt;p&gt;This may involve an application level solution, or a connection-pool level solution (&#34;get me a slave&#39;s connection, or, if that&#39;s not possible, get me the master&#39;s&#34;).&lt;/p&gt;
&lt;p&gt;Anyway, neither &lt;strong&gt;192.168.0.101&lt;/strong&gt; nor &lt;strong&gt;192.168.0.102&lt;/strong&gt; relate to a particular machine&#39;s role status. That is, the fact that one of the machines is in &lt;em&gt;writer&lt;/em&gt; mode or not does not affect these virtual IPs.&lt;/p&gt;
&lt;p&gt;The solution is a minor change to the configuration file. Real minor:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;&amp;lt;role reader&amp;gt;
  hosts                   db1, db2
  ips                     192.168.0.101
  mode                    balanced
&amp;lt;/role&amp;gt;
&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;In this new setup the two nodes compete for a single &lt;em&gt;reader&lt;/em&gt; role virtual IP. There is no &lt;strong&gt;192.168.0.102&lt;/strong&gt; anymore. Although it does not reflect from the configuration file, it turns out MMM acts in a smart way; the way you would expect it to run.&lt;/p&gt;
&lt;p&gt;There is nothing to suggest in the above that the IPs &lt;strong&gt;192.168.0.100&lt;/strong&gt; &amp;amp; &lt;strong&gt;192.168.0.101&lt;/strong&gt; will be distributed between the two machines. But you would &lt;em&gt;like&lt;/em&gt; them to. And MMM does that. It makes sure that, if possible, one of the machines (say &lt;strong&gt;db1&lt;/strong&gt;) gets the &lt;em&gt;writer&lt;/em&gt; role, hence &lt;strong&gt;192.168.0.100&lt;/strong&gt;, and the other (&lt;strong&gt;db2&lt;/strong&gt;) the &lt;em&gt;reader&lt;/em&gt; role, hence &lt;strong&gt;192.168.0.101&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Moreover, it prefers that situation over a current known situation: say &lt;strong&gt;db1&lt;/strong&gt; went down. The &lt;em&gt;writer&lt;/em&gt; role moves to &lt;strong&gt;db2&lt;/strong&gt;. When &lt;strong&gt;db1&lt;/strong&gt; is up again, MMM acts smartly: it does &lt;em&gt;not&lt;/em&gt; give it back the &lt;em&gt;writer&lt;/em&gt; role (since moving the active master around is costly, after all), but &lt;em&gt;does&lt;/em&gt; give it the &lt;em&gt;reader&lt;/em&gt; role, along with the &lt;strong&gt;192.168.2.101&lt;/strong&gt; IP. So it takes care not to leave a server without a role, while preferring to move the &lt;em&gt;writer&lt;/em&gt; role as little as possible.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>