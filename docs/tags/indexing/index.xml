<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Indexing on code.openark.org</title>
    <link>/blog/tags/indexing/</link>
    <description>Recent content in Indexing on code.openark.org</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2016. All rights reserved.</copyright>
    <lastBuildDate>Wed, 14 Nov 2012 11:15:35 +0000</lastBuildDate>
    <atom:link href="/blog/tags/indexing/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Purging old rows with QueryScript: three use cases</title>
      <link>/blog/2012/11/14/purging-old-rows-with-queryscript-three-use-cases/</link>
      <pubDate>Wed, 14 Nov 2012 11:15:35 +0000</pubDate>
      
      <guid>/blog/2012/11/14/purging-old-rows-with-queryscript-three-use-cases/</guid>
      <description>&lt;p&gt;Problem: you need to purge old rows from a table. This may be your weekly/monthly cleanup task. The table is large, the amount of rows to be deleted is large, and doing so in one big &lt;strong&gt;DELETE&lt;/strong&gt; is too heavy.&lt;/p&gt;
&lt;p&gt;You can use &lt;a href=&#34;http://openarkkit.googlecode.com/svn/trunk/openarkkit/doc/html/oak-chunk-update.html&#34;&gt;oak-chunk-update&lt;/a&gt; or &lt;a href=&#34;http://www.percona.com/doc/percona-toolkit/2.1/pt-archiver.html&#34;&gt;pt-archiver&lt;/a&gt; to accomplish the task. You can also use server side scripting with &lt;a href=&#34;http://common-schema.googlecode.com/svn/trunk/common_schema/doc/html/query_script.html&#34;&gt;QueryScript&lt;/a&gt;, offering a very simple syntax with no external scripting, dependencies and command line options.&lt;/p&gt;
&lt;p&gt;I wish to present three cases of row deletion, with three different solutions. In all cases we assume some &lt;strong&gt;TIMESTAMP&lt;/strong&gt; column exists in table, by which we choose to purge the row. In all cases we assume we wish to purge rows older than &lt;strong&gt;1&lt;/strong&gt; month.&lt;/p&gt;
&lt;p&gt;We assume the naive query is this:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;DELETE FROM my_schema.my_table WHERE row_timestamp &amp;lt; CURDATE() - INTERVAL 1 MONTH&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;h4&gt;Case 1: TIMESTAMP column is indexed&lt;/h4&gt;
&lt;p&gt;I almost always index a timestamp column, if only for being able to quickly purge data (but usually also to slice data by date). In this case where the column is indexed, it&#39;s very easy to figure out which rows are older than &lt;strong&gt;1&lt;/strong&gt; month.&lt;/p&gt;
&lt;p&gt;We break the naive query into smaller parts, and execute these in sequence:&lt;!--more--&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;while (&lt;span style=&#34;color: #000080;&#34;&gt;&lt;strong&gt;DELETE FROM&lt;/strong&gt; my_schema.my_table &lt;strong&gt;WHERE&lt;/strong&gt; row_timestamp &amp;lt; CURDATE() - INTERVAL 1 MONTH &lt;strong&gt;ORDER BY&lt;/strong&gt; row_timestamp &lt;strong&gt;LIMIT&lt;/strong&gt; 1000&lt;/span&gt;)
  throttle 1;&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;How does the above work?&lt;/p&gt;
&lt;p&gt;QueryScript accepts a &lt;strong&gt;DELETE&lt;/strong&gt; statement as a conditional expression in a while loop. The expression evaluates to &lt;strong&gt;TRUE&lt;/strong&gt; when the &lt;strong&gt;DELETE&lt;/strong&gt; affects rows. Once the &lt;strong&gt;DELETE&lt;/strong&gt; ceases to affect rows (when no more rows match the &lt;strong&gt;WHERE&lt;/strong&gt; condition), the &lt;a href=&#34;http://common-schema.googlecode.com/svn/trunk/common_schema/doc/html/query_script_while.html&#34;&gt;&lt;strong&gt;while&lt;/strong&gt;&lt;/a&gt; loop terminates.&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;http://common-schema.googlecode.com/svn/trunk/common_schema/doc/html/query_script_throttle.html&#34;&gt;&lt;strong&gt;throttle&lt;/strong&gt;&lt;/a&gt; command allows us to play &lt;em&gt;nice&lt;/em&gt;: by throttling we increase the total runtime through sleeping in between loop iterations.&lt;/p&gt;
&lt;h4&gt;Case 2: TIMESTAMP column is not indexed, and there is no heuristic for matching rows&lt;/h4&gt;
&lt;p&gt;This case is hardest to tackle by means of optimization: there is no index, and we cannot assume or predict anything about the distribution of old rows. We must therefore scan the entire table so as to be able to purge old rows.&lt;/p&gt;
&lt;p&gt;This &lt;em&gt;does not&lt;/em&gt; mean we have to do one huge full table scan. As long as we have some way to split the table, we are still good. We can utilize the &lt;strong&gt;PRIMARY KEY&lt;/strong&gt; or another &lt;strong&gt;UNIQUE KEY&lt;/strong&gt; so as to break the table into smaller, distinct parts, and work our way on these smaller chunks:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;&lt;strong&gt;split&lt;/strong&gt; (&lt;span style=&#34;color: #000080;&#34;&gt;DELETE FROM my_schema.my_table WHERE row_timestamp &amp;lt; CURDATE() - INTERVAL 1 MONTH&lt;/span&gt;)
  throttle 1;&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;The &lt;a href=&#34;http://common-schema.googlecode.com/svn/trunk/common_schema/doc/html/query_script_split.html&#34;&gt;&lt;strong&gt;split&lt;/strong&gt;&lt;/a&gt; statement will automagically calculate the chunks and inject filtering conditions onto the query, such that each execution of the query relates to a distinct set of rows.&lt;/p&gt;
&lt;h4&gt;Case 3: TIMESTAMP column not indexed, but known to be monotonic&lt;/h4&gt;
&lt;p&gt;This is true for many tables. Rows with &lt;strong&gt;AUTO_INCREMENT&lt;/strong&gt; columns and &lt;strong&gt;TIMESTAMP&lt;/strong&gt; columns are created with &lt;strong&gt;CURRENT_TIMESTAMP&lt;/strong&gt; values. This makes for a monotonic function: as the &lt;strong&gt;AUTO_INCREMENT&lt;/strong&gt; grows, so does the &lt;strong&gt;TIMESTAMP&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;This makes for the following observation: it we iterate the table row by row, and reach a point where the current row is not old, then we can stop looking. Timestamps will only increase by value, which means further rows only turn to be &lt;em&gt;newer&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;With this special case at hand, we can:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;&lt;strong&gt;split&lt;/strong&gt; (&lt;span style=&#34;color: #000080;&#34;&gt;&lt;strong&gt;&lt;/strong&gt;DELETE FROM my_schema.my_table WHERE row_timestamp &amp;lt; CURDATE() - INTERVAL 1 MONTH&lt;/span&gt;) {
  if (&lt;strong&gt;$split_rowcount&lt;/strong&gt; = 0)
    break;
  throttle 1;
}&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;em&gt;split&lt;/em&gt; is a looping device, and a &lt;a href=&#34;http://common-schema.googlecode.com/svn/trunk/common_schema/doc/html/query_script_break.html&#34;&gt;&lt;strong&gt;break&lt;/strong&gt;&lt;/a&gt; statement works on &lt;em&gt;split&lt;/em&gt; just as on a &lt;strong&gt;while&lt;/strong&gt; statement.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;split&lt;/em&gt; provides with magic variables which describe current chunk status. &lt;strong&gt;$split_rowcount&lt;/strong&gt; relates to the number of rows affected by last chunk query. No more rows affected? This means we&#39;ve hit recent rows, and we do not expect to find old rows any further. We can stop looking.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>How common_schema split()s tables - internals</title>
      <link>/blog/2012/09/06/how-common_schema-splits-tables-internals/</link>
      <pubDate>Thu, 06 Sep 2012 07:25:07 +0000</pubDate>
      
      <guid>/blog/2012/09/06/how-common_schema-splits-tables-internals/</guid>
      <description>&lt;p&gt;This post exposes some of the internals, and the SQL behind QueryScript&#39;s &lt;em&gt;split&lt;/em&gt;. &lt;a href=&#34;http://common-schema.googlecode.com/svn/trunk/common_schema/doc/html/query_script.html&#34;&gt;common_schema/QueryScript&lt;/a&gt; &lt;strong&gt;1.1&lt;/strong&gt; introduces the &lt;a href=&#34;http://common-schema.googlecode.com/svn/trunk/common_schema/doc/html/query_script_split.html&#34;&gt;&lt;strong&gt;split&lt;/strong&gt;&lt;/a&gt; statement, which auto-breaks a &#34;large&#34; query (one which operates on large tables as a whole or without keys) into smaller queries, and executes them in sequence.&lt;/p&gt;
&lt;p&gt;This makes for easier transactions, less locks held, potentially (depending on the user) more idle time released back to the database. &lt;em&gt;split&lt;strong&gt;&lt;/strong&gt;&lt;/em&gt; has similar concepts to &lt;a href=&#34;http://openarkkit.googlecode.com/svn/trunk/openarkkit/doc/html/oak-chunk-update.html&#34;&gt;oak-chunk-update&lt;/a&gt; and &lt;a href=&#34;http://www.percona.com/doc/percona-toolkit/2.1/pt-archiver.html&#34;&gt;pt-archiver&lt;/a&gt;, but works differently, and implemented entirely in SQL on server side.&lt;/p&gt;
&lt;p&gt;Take the following statement as example:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;&lt;strong&gt;split&lt;/strong&gt; (&lt;strong&gt;UPDATE sakila.inventory SET last_update = last_update + INTERVAL 6 HOUR&lt;/strong&gt;)
  pass;&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;It yields with (roughly) the following statements:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;UPDATE sakila.inventory SET last_update = last_update + INTERVAL 6 HOUR &lt;strong&gt;WHERE&lt;/strong&gt; ((((`inventory`.`inventory_id` &amp;gt; &#39;1&#39;)) OR ((`inventory`.`inventory_id` = &#39;1&#39;))) AND (((`inventory`.`inventory_id` &amp;lt; &#39;1000&#39;)) OR ((`inventory`.`inventory_id` = &#39;1000&#39;))));
UPDATE sakila.inventory SET last_update = last_update + INTERVAL 6 HOUR &lt;strong&gt;WHERE&lt;/strong&gt; ((((`inventory`.`inventory_id` &amp;gt; &#39;1000&#39;))) AND (((`inventory`.`inventory_id` &amp;lt; &#39;2000&#39;)) OR ((`inventory`.`inventory_id` = &#39;2000&#39;))));
UPDATE sakila.inventory SET last_update = last_update + INTERVAL 6 HOUR &lt;strong&gt;WHERE&lt;/strong&gt; ((((`inventory`.`inventory_id` &amp;gt; &#39;2000&#39;))) AND (((`inventory`.`inventory_id` &amp;lt; &#39;3000&#39;)) OR ((`inventory`.`inventory_id` = &#39;3000&#39;))));
UPDATE sakila.inventory SET last_update = last_update + INTERVAL 6 HOUR &lt;strong&gt;WHERE&lt;/strong&gt; ((((`inventory`.`inventory_id` &amp;gt; &#39;3000&#39;))) AND (((`inventory`.`inventory_id` &amp;lt; &#39;4000&#39;)) OR ((`inventory`.`inventory_id` = &#39;4000&#39;))));
UPDATE sakila.inventory SET last_update = last_update + INTERVAL 6 HOUR &lt;strong&gt;WHERE&lt;/strong&gt; ((((`inventory`.`inventory_id` &amp;gt; &#39;4000&#39;))) AND (((`inventory`.`inventory_id` &amp;lt; &#39;4581&#39;)) OR ((`inventory`.`inventory_id` = &#39;4581&#39;))));&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;(I say &#34;roughly&#34; because internally there are user defined variables at play, but for convenience, I verbose the actual values as constants.)&lt;/p&gt;
&lt;h4&gt;How does that work?&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;common_schema&lt;/em&gt; works on server side. There is no Perl script or anything. It must therefore use server-side operations to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Identify table to be split&lt;/li&gt;
&lt;li&gt;Analyze the table in the first place, deciding how to split it&lt;/li&gt;
&lt;li&gt;Analyze the query, deciding on how to rewrite it&lt;/li&gt;
&lt;li&gt;Split the table (logically) into unique and distinct chunks&lt;/li&gt;
&lt;li&gt;Work out the query on each such chunk&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Following is an internal look at how &lt;em&gt;common_schema&lt;/em&gt; does all the above.&lt;!--more--&gt;&lt;/p&gt;
&lt;h4&gt;Identifying the table&lt;/h4&gt;
&lt;p&gt;When query operates on a single table, &lt;em&gt;split&lt;/em&gt; is able to parse the query&#39;s SQL and find out that table. When multiple tables are involved, &lt;em&gt;split&lt;/em&gt; requires user instruction: which table is it that the query should be split by?&lt;/p&gt;
&lt;h4&gt;Analyzing the table&lt;/h4&gt;
&lt;p&gt;Table analysis is done via a &lt;em&gt;similar&lt;/em&gt; method to &lt;a href=&#34;http://common-schema.googlecode.com/svn/trunk/common_schema/doc/html/candidate_keys_recommended.html&#34;&gt;candidate_keys_recommended&lt;/a&gt;. It is almost identical, only it uses &lt;a href=&#34;http://dev.mysql.com/doc/refman/5.1/en/information-schema-optimization.html&#34;&gt;INFORMATION_SCHEMA optimizations&lt;/a&gt; to make the query short and lightweight. Simulating the analysis using &lt;strong&gt;candidate_keys_recommended&lt;/strong&gt;, we get:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;mysql&amp;gt; select * from candidate_keys_recommended where table_name=&#39;inventory&#39; \G
*************************** 1. row ***************************
          table_schema: sakila
            table_name: inventory
recommended_index_name: PRIMARY
          has_nullable: 0
            is_primary: 1
 count_column_in_index: 1
          column_names: inventory_id&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is cool, simple and very easy to work with: we choose to split the table via the &lt;strong&gt;inventory_id&lt;/strong&gt; column, which is conveniently an integer. We&#39;ll soon see &lt;em&gt;split&lt;/em&gt; can handle complex cases as well.&lt;/p&gt;
&lt;h4&gt;Analyzing the query&lt;/h4&gt;
&lt;p&gt;This is done in part via Roland&#39;s &lt;a href=&#34;http://common-schema.googlecode.com/svn/trunk/common_schema/doc/html/query_analysis_routines.html&#34;&gt;query_analysis_routines&lt;/a&gt;, and in part just parsing the query, looking for &lt;strong&gt;WHERE&lt;/strong&gt;,&lt;strong&gt; GROUP BY&lt;/strong&gt;, &lt;strong&gt;LIMIT&lt;/strong&gt; etc. clauses.&lt;/p&gt;
&lt;p&gt;The nice part is injecting a &lt;strong&gt;WHERE&lt;/strong&gt; condition, which didn&#39;t appear in the original query. That &lt;strong&gt;WHERE&lt;/strong&gt; condition is what limits the query to a distinct chunk of rows.&lt;/p&gt;
&lt;h4&gt;Splitting the table&lt;/h4&gt;
&lt;p&gt;With a single &lt;strong&gt;INTEGER PRIMARY KEY&lt;/strong&gt; this sounds simple, right? Take rows &lt;strong&gt;1..1,000&lt;/strong&gt;, then &lt;strong&gt;1,001..2,000&lt;/strong&gt;, then &lt;strong&gt;2,001..3,000&lt;/strong&gt; etc.&lt;/p&gt;
&lt;p&gt;Wrong: even with this simple scenario, things are much more complex. Are the numbers successive? What if there are holes? What if there is a &lt;strong&gt;1,000,000&lt;/strong&gt; gap between every two numbers? What if there are multiple holes of differing size and frequency?&lt;/p&gt;
&lt;p&gt;And if we have two columns in our &lt;strong&gt;UNIQUE KEY&lt;/strong&gt;? What if one of them is textual, not an &lt;strong&gt;INTEGER&lt;/strong&gt;, the other a &lt;strong&gt;TIMESTAMP&lt;/strong&gt;, not an &lt;strong&gt;INTEGER&lt;/strong&gt; either?&lt;/p&gt;
&lt;p&gt;&lt;em&gt;split&lt;/em&gt; doesn&#39;t work in that naive way. It makes no assumptions on the density of values. It only requires:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;some &lt;strong&gt;UNIQUE KEY&lt;/strong&gt; to work with,&lt;/li&gt;
&lt;li&gt;which has no &lt;strong&gt;NULL&lt;/strong&gt; values.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Given the above, it uses &lt;em&gt;User Defined Variables&lt;/em&gt; to setup the chunks. With our single &lt;strong&gt;INTEGER&lt;/strong&gt; column, the minimum value is set like this:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;select 
  inventory_id 
from 
  `sakila`.`inventory` 
order by 
  inventory_id ASC 
limit 1  
into @_split_column_variable_min_1
;&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;This sets the first value of the first chunk. What value terminates this chunk? It is calculated like this:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;select 
  inventory_id 
from (
  select 
    inventory_id 
  from 
    `sakila`.`inventory` 
  where 
    (((`inventory`.`inventory_id` &amp;gt; @_split_column_variable_range_start_1)) OR ((`inventory`.`inventory_id` = @_split_column_variable_range_start_1))) and (((`inventory`.`inventory_id` &amp;lt; @_split_column_variable_max_1)) OR ((`inventory`.`inventory_id` = @_split_column_variable_max_1))) 
  order by 
    inventory_id ASC limit 1000 
  ) sel_split_range  
order by 
  inventory_id DESC 
limit 1  
into @_split_column_variable_range_end_1
;&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;Now there&#39;s a query you wouldn&#39;t want to work by hand, now would you?&lt;/p&gt;
&lt;p&gt;The cool part here is that the above works well for any type of column; this doesn&#39;t have to be an &lt;strong&gt;INTEGER&lt;/strong&gt;. Dates, strings etc. are all just fine.&lt;/p&gt;
&lt;p&gt;The above also works well for multiple columns, where the query gets more complicated (see following).&lt;/p&gt;
&lt;h4&gt;Working out the query per chunk&lt;/h4&gt;
&lt;p&gt;This part is the easy one, now that all the hard work is done. We know ho to manipulate the query, we know the lower and upper boundaries of the chunk, so we just fill in the values and execute.&lt;/p&gt;
&lt;h4&gt;Multi-columns keys&lt;/h4&gt;
&lt;p&gt;Consider a similar query on &lt;strong&gt;sakila.film_actor&lt;/strong&gt;, where the &lt;strong&gt;PRIMARY KEY&lt;/strong&gt; is a compound of two columns:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;split (UPDATE sakila.film_actor SET last_update = last_update + INTERVAL 6 HOUR)
  throttle 2;&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;The chunked queries will look like this:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;UPDATE sakila.film_actor SET last_update = last_update + INTERVAL 6 HOUR &lt;strong&gt;WHERE&lt;/strong&gt; ((((`film_actor`.`actor_id` &amp;gt; &#39;1&#39;)) OR ((`film_actor`.`actor_id` = &#39;1&#39;) AND (`film_actor`.`film_id` &amp;gt; &#39;1&#39;)) OR ((`film_actor`.`actor_id` = &#39;1&#39;) AND (`film_actor`.`film_id` = &#39;1&#39;))) AND (((`film_actor`.`actor_id` &amp;lt; &#39;39&#39;)) OR ((`film_actor`.`actor_id` = &#39;39&#39;) AND (`film_actor`.`film_id` &amp;lt; &#39;293&#39;)) OR ((`film_actor`.`actor_id` = &#39;39&#39;) AND (`film_actor`.`film_id` = &#39;293&#39;))));
UPDATE sakila.film_actor SET last_update = last_update + INTERVAL 6 HOUR &lt;strong&gt;WHERE&lt;/strong&gt; ((((`film_actor`.`actor_id` &amp;gt; &#39;39&#39;)) OR ((`film_actor`.`actor_id` = &#39;39&#39;) AND (`film_actor`.`film_id` &amp;gt; &#39;293&#39;))) AND (((`film_actor`.`actor_id` &amp;lt; &#39;76&#39;)) OR ((`film_actor`.`actor_id` = &#39;76&#39;) AND (`film_actor`.`film_id` &amp;lt; &#39;234&#39;)) OR ((`film_actor`.`actor_id` = &#39;76&#39;) AND (`film_actor`.`film_id` = &#39;234&#39;))));
UPDATE sakila.film_actor SET last_update = last_update + INTERVAL 6 HOUR &lt;strong&gt;WHERE&lt;/strong&gt; ((((`film_actor`.`actor_id` &amp;gt; &#39;76&#39;)) OR ((`film_actor`.`actor_id` = &#39;76&#39;) AND (`film_actor`.`film_id` &amp;gt; &#39;234&#39;))) AND (((`film_actor`.`actor_id` &amp;lt; &#39;110&#39;)) OR ((`film_actor`.`actor_id` = &#39;110&#39;) AND (`film_actor`.`film_id` &amp;lt; &#39;513&#39;)) OR ((`film_actor`.`actor_id` = &#39;110&#39;) AND (`film_actor`.`film_id` = &#39;513&#39;))));
UPDATE sakila.film_actor SET last_update = last_update + INTERVAL 6 HOUR &lt;strong&gt;WHERE&lt;/strong&gt; ((((`film_actor`.`actor_id` &amp;gt; &#39;110&#39;)) OR ((`film_actor`.`actor_id` = &#39;110&#39;) AND (`film_actor`.`film_id` &amp;gt; &#39;513&#39;))) AND (((`film_actor`.`actor_id` &amp;lt; &#39;146&#39;)) OR ((`film_actor`.`actor_id` = &#39;146&#39;) AND (`film_actor`.`film_id` &amp;lt; &#39;278&#39;)) OR ((`film_actor`.`actor_id` = &#39;146&#39;) AND (`film_actor`.`film_id` = &#39;278&#39;))));
UPDATE sakila.film_actor SET last_update = last_update + INTERVAL 6 HOUR &lt;strong&gt;WHERE&lt;/strong&gt; ((((`film_actor`.`actor_id` &amp;gt; &#39;146&#39;)) OR ((`film_actor`.`actor_id` = &#39;146&#39;) AND (`film_actor`.`film_id` &amp;gt; &#39;278&#39;))) AND (((`film_actor`.`actor_id` &amp;lt; &#39;183&#39;)) OR ((`film_actor`.`actor_id` = &#39;183&#39;) AND (`film_actor`.`film_id` &amp;lt; &#39;862&#39;)) OR ((`film_actor`.`actor_id` = &#39;183&#39;) AND (`film_actor`.`film_id` = &#39;862&#39;))));
UPDATE sakila.film_actor SET last_update = last_update + INTERVAL 6 HOUR &lt;strong&gt;WHERE&lt;/strong&gt; ((((`film_actor`.`actor_id` &amp;gt; &#39;183&#39;)) OR ((`film_actor`.`actor_id` = &#39;183&#39;) AND (`film_actor`.`film_id` &amp;gt; &#39;862&#39;))) AND (((`film_actor`.`actor_id` &amp;lt; &#39;200&#39;)) OR ((`film_actor`.`actor_id` = &#39;200&#39;) AND (`film_actor`.`film_id` &amp;lt; &#39;993&#39;)) OR ((`film_actor`.`actor_id` = &#39;200&#39;) AND (`film_actor`.`film_id` = &#39;993&#39;))));&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;View the complete command to realize just how much more complex each query is, and how much more complex the chunking becomes. Here&#39;s how I evaluate the chunk&#39;s &#34;next range end&#34; variables:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;select 
  actor_id, film_id 
from (
  select 
    actor_id, film_id 
  from 
    `sakila`.`film_actor` 
  where 
    (((`film_actor`.`actor_id` &amp;gt; @_split_column_variable_range_start_1)) OR ((`film_actor`.
`actor_id` = @_split_column_variable_range_start_1) AND (`film_actor`.`film_id` &amp;gt; @_split_column_variable_range_start_2))) and (((`film_actor`.`actor_id` &amp;lt; @_split_column_variable_max_1)) OR ((`film_actor`.`actor_id` = @_split_column_variable_max_1) AND (`film_actor`.`film_id` &amp;lt; @_split_column_variable_max_2)) OR ((`film_actor`.`actor_id` = @_split_column_variable_max_1) AND (`film_actor`.`film_id` = @_split_column_variable_max_2))) 
  order by 
    actor_id ASC, film_id ASC 
  limit 1000 
  ) sel_split_range  
order by 
  actor_id DESC, film_id DESC 
limit 1  
into @_split_column_variable_range_end_1, @_split_column_variable_range_end_2
;&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;By the way, you may recall that everything is done server side. The &lt;strong&gt;WHERE&lt;/strong&gt; condition for the chunked queries is in itself generated via SQL statement, and not too much by programmatic logic. Here&#39;s &lt;em&gt;part&lt;/em&gt; of the query which computes the limiting condition:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;  select
    group_concat(&#39;(&#39;, partial_comparison, &#39;)&#39; order by n separator &#39; OR &#39;) as comparison
  from (
    select 
      n,
      group_concat(&#39;(&#39;, column_name, &#39; &#39;, if(is_last, comparison_operator, &#39;=&#39;), &#39; &#39;, variable_name, &#39;)&#39; order by column_order separator &#39; AND &#39;) as partial_comparison
    from (
      select 
        n, CONCAT(mysql_qualify(split_table_name), &#39;.&#39;, mysql_qualify(column_name)) AS column_name,
        case split_variable_type
          when &#39;range_start&#39; then range_start_variable_name
          when &#39;range_end&#39; then range_end_variable_name
          when &#39;max&#39; then max_variable_name
        end as variable_name,
        _split_column_names_table.column_order, _split_column_names_table.column_order = n as is_last 
      from 
        numbers, _split_column_names_table 
      where 
        n between _split_column_names_table.column_order and num_split_columns 
      order by n, _split_column_names_table.column_order
    ) s1
    group by n
  ) s2
  into return_value
  ;&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;There is a lot of complexity to &lt;em&gt;split&lt;/em&gt; to make it able to provide with as clean a syntax for the user as possible.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Table split(...) for the masses</title>
      <link>/blog/2012/09/05/table-split-for-the-masses/</link>
      <pubDate>Wed, 05 Sep 2012 07:04:05 +0000</pubDate>
      
      <guid>/blog/2012/09/05/table-split-for-the-masses/</guid>
      <description>&lt;p&gt;(pun intended)&lt;/p&gt;
&lt;p&gt;&lt;em&gt;common_schema&lt;/em&gt;&#39;s new &lt;a href=&#34;http://common-schema.googlecode.com/svn/trunk/common_schema/doc/html/query_script_split.html&#34;&gt;&lt;strong&gt;split&lt;/strong&gt;&lt;/a&gt; statement (see &lt;a href=&#34;http://code.openark.org/blog/mysql/common_schema-1-1-released-split-try-catch-killall-profiling&#34;&gt;release announcement&lt;/a&gt;) auto-splits complex queries over large tables into smaller ones: instead of issuing one huge query, &lt;em&gt;split&lt;/em&gt; breaks one&#39;s query into smaller queries, each working on a different set of rows (a chunk).&lt;/p&gt;
&lt;p&gt;Thus, it is possible to avoid holding locks for long times, allowing for smaller transactions. It also makes for breathing space for the RDBMS, at times boosting operation speed, and at times prolonging operation speed at will.&lt;/p&gt;
&lt;p&gt;In this post I show how &lt;em&gt;split&lt;/em&gt; exposes itself to the user, should the user wish so.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;split&lt;/em&gt; can manage queries of the following forms:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;DELETE FROM table_name [WHERE]...&lt;/li&gt;
&lt;li&gt;DELETE FROM table_name USING &amp;lt;multi table syntax&amp;gt; [WHERE]...&lt;/li&gt;
&lt;li&gt;UPDATE table_name SET ... [WHERE]...&lt;/li&gt;
&lt;li&gt;UPDATE &amp;lt;multiple tables&amp;gt; SET ... [WHERE]...&lt;/li&gt;
&lt;li&gt;INSERT INTO some_table SELECT ... FROM &amp;lt;single or multiple tables&amp;gt; [WHERE]...&lt;/li&gt;
&lt;li&gt;REPLACE INTO some_table SELECT ... FROM &amp;lt;single or multiple tables&amp;gt; [WHERE]...&lt;/li&gt;
&lt;li&gt;SELECT ... FROM &amp;lt;multiple tables&amp;gt; [WHERE]...&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The latter being a non-obvious one at first sight.&lt;/p&gt;
&lt;h4&gt;Basically, it&#39; automatic&lt;/h4&gt;
&lt;p&gt;You just say:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;&lt;strong&gt;split&lt;/strong&gt; (UPDATE sakila.inventory SET last_update = last_update + INTERVAL 6 HOUR)
  throttle 2;&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;And &lt;em&gt;split&lt;/em&gt; identifies &lt;strong&gt;sakila.inventory&lt;/strong&gt; as the table which needs to be split, and injects appropriate conditions so as to work on a subset of the rows, in multiple steps.&lt;/p&gt;
&lt;p&gt;By the way, here&#39;s &lt;a href=&#34;http://common-schema.googlecode.com/svn/trunk/common_schema/doc/html/query_script_execution.html&#34;&gt;how to execute a QueryScript code&lt;/a&gt; like the above.&lt;!--more--&gt;&lt;/p&gt;
&lt;h4&gt;But you can drive in manual mode&lt;/h4&gt;
&lt;p&gt;You can use the following syntax:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;&lt;strong&gt;split&lt;/strong&gt; (sakila.inventory)
{
  -- No action taken, but this block of code
  -- is executed per chunk of the table.
  -- I wonder what can be done here?
}&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;em&gt;split&lt;/em&gt; provides with &lt;em&gt;magic variables&lt;/em&gt;, which you can use in the action block. These are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;$split_step&lt;/strong&gt;: &lt;strong&gt;1&lt;/strong&gt;-based loop counter&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;$split_rowcount&lt;/strong&gt;: number of rows affected in current chunk operation&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;$split_total_rowcount&lt;/strong&gt;: total number of rows affected during this &lt;em&gt;split&lt;/em&gt; statement&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;$split_total_elapsed_time&lt;/strong&gt;: number of seconds elapsed since beginning of this &lt;em&gt;split&lt;/em&gt; operation.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;$split_clause&lt;/strong&gt;: &lt;em&gt;the&lt;/em&gt; magic variable: the filtering condition limiting rows to current chunk.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;$split_table_schema&lt;/strong&gt;: the explicit or inferred schema of split table&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;$split_table_name&lt;/strong&gt;: the explicit or inferred table being split&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To illustrate, consider the following script:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;&lt;strong&gt;split&lt;/strong&gt; (sakila.inventory)
{
  select &lt;strong&gt;$split_step&lt;/strong&gt; as step, &lt;strong&gt;$split_clause&lt;/strong&gt; as clause;
}&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;The output is this:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;+------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| step | clause                                                                                                                                                                    |
+------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|    1 | ((((`inventory`.`inventory_id` &amp;gt; &#39;1&#39;)) OR ((`inventory`.`inventory_id` = &#39;1&#39;))) AND (((`inventory`.`inventory_id` &amp;lt; &#39;1000&#39;)) OR ((`inventory`.`inventory_id` = &#39;1000&#39;)))) |
+------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

+------+--------------------------------------------------------------------------------------------------------------------------------------+
| step | clause                                                                                                                               |
+------+--------------------------------------------------------------------------------------------------------------------------------------+
|    2 | ((((`inventory`.`inventory_id` &amp;gt; &#39;1000&#39;))) AND (((`inventory`.`inventory_id` &amp;lt; &#39;2000&#39;)) OR ((`inventory`.`inventory_id` = &#39;2000&#39;)))) |
+------+--------------------------------------------------------------------------------------------------------------------------------------+

+------+--------------------------------------------------------------------------------------------------------------------------------------+
| step | clause                                                                                                                               |
+------+--------------------------------------------------------------------------------------------------------------------------------------+
|    3 | ((((`inventory`.`inventory_id` &amp;gt; &#39;2000&#39;))) AND (((`inventory`.`inventory_id` &amp;lt; &#39;3000&#39;)) OR ((`inventory`.`inventory_id` = &#39;3000&#39;)))) |
+------+--------------------------------------------------------------------------------------------------------------------------------------+

+------+--------------------------------------------------------------------------------------------------------------------------------------+
| step | clause                                                                                                                               |
+------+--------------------------------------------------------------------------------------------------------------------------------------+
|    4 | ((((`inventory`.`inventory_id` &amp;gt; &#39;3000&#39;))) AND (((`inventory`.`inventory_id` &amp;lt; &#39;4000&#39;)) OR ((`inventory`.`inventory_id` = &#39;4000&#39;)))) |
+------+--------------------------------------------------------------------------------------------------------------------------------------+

+------+--------------------------------------------------------------------------------------------------------------------------------------+
| step | clause                                                                                                                               |
+------+--------------------------------------------------------------------------------------------------------------------------------------+
|    5 | ((((`inventory`.`inventory_id` &amp;gt; &#39;4000&#39;))) AND (((`inventory`.`inventory_id` &amp;lt; &#39;4581&#39;)) OR ((`inventory`.`inventory_id` = &#39;4581&#39;)))) |
+------+--------------------------------------------------------------------------------------------------------------------------------------+&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;So you can get yourself a nice present: the SQL clause which filters the distinct chunks.&lt;/p&gt;
&lt;h4&gt;A simple demo: what can the user do with &#34;manual mode&#34;?&lt;/h4&gt;
&lt;p&gt;Normally, I would expect the user to use the automated version of &lt;em&gt;split&lt;/em&gt;. Let it do the hard work! But sometimes, you may wish to take control into your hands.&lt;/p&gt;
&lt;p&gt;Consider an example: I wish to export a table into CSV file, but in chunks. &lt;a href=&#34;http://www.percona.com/doc/percona-toolkit/2.1/pt-archiver.html&#34;&gt;pt-archiver&lt;/a&gt; does that. But it is also easily achievable with &lt;em&gt;split&lt;/em&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;&lt;strong&gt;split&lt;/strong&gt; (sakila.inventory) {
  var &lt;strong&gt;$file_name&lt;/strong&gt; := QUOTE(CONCAT(&#39;/tmp/inventory_chunk_&#39;, &lt;strong&gt;$split_step&lt;/strong&gt;, &#39;.csv&#39;));
  select * from sakila.inventory WHERE &lt;strong&gt;:${split_clause}&lt;/strong&gt; INTO OUTFILE &lt;strong&gt;:${file_name}&lt;/strong&gt;;
}&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;This script uses the powerful &lt;a href=&#34;http://common-schema.googlecode.com/svn/trunk/common_schema/doc/html/query_script_variables.html&#34;&gt;variable expansion&lt;/a&gt; feature of QueryScript: it extracts the text behind &lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;:${split_clause}&lt;/strong&gt; and plants it as part of the query. It does the same for &lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;:${file_name}&lt;/strong&gt;, making a variable possible where MySQL would normally disallow one (the &lt;strong&gt;INTO OUTFILE&lt;/strong&gt; clause only accepts a constant string).&lt;/p&gt;
&lt;p&gt;What do we get as result?&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;&lt;strong&gt;bash:/tmp$ ls -s1 inventory_chunk_*&lt;/strong&gt;
32 inventory_chunk_1.csv
32 inventory_chunk_2.csv
32 inventory_chunk_3.csv
32 inventory_chunk_4.csv
20 inventory_chunk_5.csv&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;h4&gt;Conclusion&lt;/h4&gt;
&lt;p&gt;During the past months, and even as I developed &lt;em&gt;split&lt;/em&gt; for &lt;a href=&#34;http://common-schema.googlecode.com/svn/trunk/common_schema/doc/html/query_script.html&#34;&gt;QueryScript&lt;/a&gt;, I found myself using it more and more for my own purposes. As it evolved I realized how much more simple it makes these complex operations. Heck, it beats &lt;a href=&#34;http://openarkkit.googlecode.com/svn/trunk/openarkkit/doc/html/oak-chunk-update.html&#34;&gt;oak-chunk-update&lt;/a&gt; in its ease of use. They both have their place, but &lt;em&gt;split&lt;/em&gt; is so much more intuitive and easy to write. And, no external scripts, no package dependencies.&lt;/p&gt;
&lt;p&gt;I suggest that &lt;em&gt;split&lt;/em&gt; is a major tool for server side scripting, server maintenance, developer operations. &lt;a href=&#34;http://code.google.com/p/common-schema/&#34;&gt;Check it out&lt;/a&gt;!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>common_schema rev. 68: eval(), processlist_grantees, candidate_keys, easter_day()</title>
      <link>/blog/2011/09/06/common_schema-rev-68-eval-processlist_grantees-candidate_keys-easter_day/</link>
      <pubDate>Tue, 06 Sep 2011 09:05:34 +0000</pubDate>
      
      <guid>/blog/2011/09/06/common_schema-rev-68-eval-processlist_grantees-candidate_keys-easter_day/</guid>
      <description>&lt;p&gt;Revision &lt;strong&gt;68&lt;/strong&gt; of &lt;a rel=&#34;nofollow&#34; href=&#34;http://code.google.com/p/common-schema/&#34;&gt;common_schema&lt;/a&gt; is out, and includes some interesting features:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;eval()&lt;/strong&gt;: Evaluates the queries generated by a given query&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;match_grantee()&lt;/strong&gt;: Match an existing account based on user+host&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;processlist_grantees&lt;/strong&gt;: Assigning of GRANTEEs for connected processes&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;candidate_keys&lt;/strong&gt;: Listing of prioritized candidate keys: keys which are UNIQUE, by order of best-use.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;easter_day()&lt;/strong&gt;: Returns DATE of easter day in given DATETIME&#39;s year.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let&#39;s take a slightly closer look at these:&lt;/p&gt;
&lt;h4&gt;eval()&lt;/h4&gt;
&lt;p&gt;I&#39;ve dedicated this blog post on &lt;a href=&#34;http://code.openark.org/blog/mysql/mysql-eval&#34;&gt;MySQL eval()&lt;/a&gt; to describe it. In simple summary: &lt;strong&gt;eval()&lt;/strong&gt; takes a query which generates queries (most common use queries on &lt;strong&gt;INFORMATION_SCHEMA&lt;/strong&gt;) and auto-evaluates (executes) those queries. &lt;a href=&#34;http://common-schema.googlecode.com/svn/trunk/common_schema/doc/html/general_procedures.html#eval&#34;&gt;Read more&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;match_grantee()&lt;/h4&gt;
&lt;p&gt;As presented in &lt;a title=&#34;Link to Finding CURRENT_USER for any user&#34; rel=&#34;bookmark&#34; href=&#34;http://code.openark.org/blog/mysql/finding-current_user-for-any-user&#34;&gt;Finding CURRENT_USER for any user&lt;/a&gt;, I&#39;ve developed the algorithm to match a connected user+host details (as presented with &lt;strong&gt;PROCESSLIST&lt;/strong&gt;) with the grantee tables (i.e. the &lt;strong&gt;mysql.user&lt;/strong&gt; table), in a manner which simulates the MySQL server account matching algorithm.&lt;/p&gt;
&lt;p&gt;This is now available as a stored function: given a user+host, the function returns with the best matched grantee. &lt;a href=&#34;http://common-schema.googlecode.com/svn/trunk/common_schema/doc/html/privileges_functions.html#match_grantee&#34;&gt;Read more&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;processlist_grantees&lt;/h4&gt;
&lt;p&gt;This view relies on the above, and maps the entire &lt;strong&gt;PROCESSLIST&lt;/strong&gt; onto GRANTEEs. The view maps each process onto the GRANTEE (MySQL account) which is the owner of that process. Surprisingly, MySQL does not provide one with such information.&lt;!--more--&gt;&lt;/p&gt;
&lt;p&gt;The view also provides with the following useful metadata:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Is said process executes under a SUPER privilege?&lt;/li&gt;
&lt;li&gt;Is this a replication thread, or serving a replicating client?&lt;/li&gt;
&lt;li&gt;Is this process the current connection (myself)?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the spirit of &lt;strong&gt;common_schema&lt;/strong&gt;, it provides with the SQL commands necessary to &lt;strong&gt;KILL&lt;/strong&gt; and &lt;strong&gt;KILL QUERY&lt;/strong&gt; for each process. A sample output:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;mysql&amp;gt; SELECT * FROM common_schema.processlist_grantees;
+--------+------------+---------------------+------------------------+--------------+--------------+----------+---------+-------------------+---------------------+
| ID     | USER       | HOST                | GRANTEE                | grantee_user | grantee_host | is_super | is_repl | sql_kill_query    | sql_kill_connection |
+--------+------------+---------------------+------------------------+--------------+--------------+----------+---------+-------------------+---------------------+
| 650472 | replica    | jboss00.myweb:34266 | &#39;replica&#39;@&#39;%.myweb&#39;    | replica      | %.myweb      |        0 |       1 | KILL QUERY 650472 | KILL 650472         |
| 692346 | openarkkit | jboss02.myweb:43740 | &#39;openarkkit&#39;@&#39;%.myweb&#39; | openarkkit   | %.myweb      |        0 |       0 | KILL QUERY 692346 | KILL 692346         |
| 842853 | root       | localhost           | &#39;root&#39;@&#39;localhost&#39;     | root         | localhost    |        1 |       0 | KILL QUERY 842853 | KILL 842853         |
| 843443 | jboss      | jboss03.myweb:40007 | &#39;jboss&#39;@&#39;%.myweb&#39;      | jboss        | %.myweb      |        0 |       0 | KILL QUERY 843443 | KILL 843443         |
| 843444 | jboss      | jboss03.myweb:40012 | &#39;jboss&#39;@&#39;%.myweb&#39;      | jboss        | %.myweb      |        0 |       0 | KILL QUERY 843444 | KILL 843444         |
| 843510 | jboss      | jboss00.myweb:49850 | &#39;jboss&#39;@&#39;%.myweb&#39;      | jboss        | %.myweb      |        0 |       0 | KILL QUERY 843510 | KILL 843510         |
| 844559 | jboss      | jboss01.myweb:37031 | &#39;jboss&#39;@&#39;%.myweb&#39;      | jboss        | %.myweb      |        0 |       0 | KILL QUERY 844559 | KILL 844559         |
+--------+------------+---------------------+------------------------+--------------+--------------+----------+---------+-------------------+---------------------+&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;Finally, it is now possible to execute the following:  “Kill all slow queries which are not executed by users with the SUPER privilege or are replication threads”. To just generate the commands, execute:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;mysql&amp;gt; SELECT &lt;strong&gt;sql_kill_connection&lt;/strong&gt; FROM &lt;strong&gt;common_schema.processlist_grantees&lt;/strong&gt; WHERE is_super = 0 AND is_repl = 0;&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;Sorry, did you only want to kill the queries? Those which are very slow? Do as follows:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;mysql&amp;gt; SELECT sql_kill_connection FROM common_schema.processlist_grantees JOIN INFORMATION_SCHEMA.PROCESSLIST &lt;strong&gt;USING(ID)&lt;/strong&gt; WHERE &lt;strong&gt;TIME &amp;gt; 10&lt;/strong&gt; AND is_super = 0 AND is_repl = 0;&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;But, really, we don&#39;t just want &lt;em&gt;commands&lt;/em&gt;. We really want to execute this!&lt;/p&gt;
&lt;p&gt;Good! Step in &lt;strong&gt;eval()&lt;/strong&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;mysql&amp;gt; CALL common_schema.&lt;strong&gt;eval&lt;/strong&gt;(&#39;SELECT &lt;strong&gt;sql_kill_query&lt;/strong&gt; FROM common_schema.processlist_grantees JOIN INFORMATION_SCHEMA.PROCESSLIST USING(id) WHERE TIME &amp;gt; 10 AND is_super = 0 AND is_repl = 0&#39;);&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href=&#34;http://common-schema.googlecode.com/svn/trunk/common_schema/doc/html/processlist_grantees.html&#34;&gt;Read more&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;candidate_keys&lt;/h4&gt;
&lt;p&gt;A view which lists the candidate keys for tables and provides ranking for those keys, based on some simple heuristics.&lt;/p&gt;
&lt;p&gt;This view uses  the same algorithm as that used by &lt;a href=&#34;http://openarkkit.googlecode.com/svn/trunk/openarkkit/doc/html/oak-chunk-update.html&#34;&gt;oak-chunk-update&lt;/a&gt; and &lt;a href=&#34;http://openarkkit.googlecode.com/svn/trunk/openarkkit/doc/html/oak-online-alter-table.html&#34;&gt;oak-online-alter-table&lt;/a&gt;, tools in the &lt;a href=&#34;http://code.openark.org/forge/openark-kit&#34;&gt;openark kit&lt;/a&gt;. So it provides with a way to choose the best candidate key to walk through a table. At current, a table&#39;s &lt;strong&gt;PRIMARY KEY&lt;/strong&gt; is always considered to be best, because of InnoDB&#39;s structure of clustered index. But I intend to change that as well and provide general recommendation about candidate keys (so for example, I would be able to recommend that the &lt;strong&gt;PRIMARY KEY&lt;/strong&gt; is not optimal for some table).&lt;/p&gt;
&lt;p&gt;Actually, after a discussion initiated by Giuseppe and Roland, starting &lt;a href=&#34;http://datacharmer.blogspot.com/2011/09/finding-tables-without-primary-keys.html&#34;&gt;here&lt;/a&gt; and continuing on mail, there are more checks to be made for candidate keys, and I suspect the next version of &lt;em&gt;candidate_keys&lt;/em&gt; will be more informational.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://common-schema.googlecode.com/svn/trunk/common_schema/doc/html/candidate_keys.html&#34;&gt;Read more&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;easter_day()&lt;/h4&gt;
&lt;p&gt;Many thanks to &lt;a href=&#34;http://rpbouman.blogspot.com/&#34;&gt;Roland Bouman&lt;/a&gt; who suggested his code for calculating easter day for a given year. &lt;em&gt;Weehee!&lt;/em&gt; This is the first contribution to &lt;em&gt;common_schema&lt;/em&gt;! &lt;a href=&#34;http://common-schema.googlecode.com/svn/trunk/common_schema/doc/html/time_functions.html#easter_day&#34;&gt;Read more&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;Get it&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;common_schema&lt;/em&gt; is an open source project. It is released under the BSD license.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://code.google.com/p/common-schema/&#34;&gt;Find it here&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Announcing common_schema: common views &amp; routines for MySQL</title>
      <link>/blog/2011/07/13/announcing-common_schema-common-views-routines-for-mysql/</link>
      <pubDate>Wed, 13 Jul 2011 06:25:24 +0000</pubDate>
      
      <guid>/blog/2011/07/13/announcing-common_schema-common-views-routines-for-mysql/</guid>
      <description>&lt;p&gt;Today I have released &lt;a title=&#34;common_schema&#34; href=&#34;http://code.openark.org/forge/common_schema&#34;&gt;common_schema&lt;/a&gt;, a utility schema for MySQL which includes many views and functions, and is aimed to be installed on any MySQL server.&lt;/p&gt;
&lt;h4&gt;What does it do?&lt;/h4&gt;
&lt;p&gt;There are views answering for all sorts of useful information: stuff related to schema analysis, data dimensions, monitoring, processes &amp;amp; transactions, security, internals... There are basic functions answering for common needs.&lt;/p&gt;
&lt;p&gt;Some of the views/routines simply formalize those queries we tend to write over and over again. Others take the place of external tools, answering complex questions via SQL and metadata. Still others help out with SQL generation.&lt;/p&gt;
&lt;p&gt;Here are a few highlights:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Did you know you can work out &lt;a href=&#34;http://common-schema.googlecode.com/svn/trunk/common_schema/doc/html/global_status_diff_nonzero.html&#34;&gt;simple monitoring&lt;/a&gt; of your server with a &lt;em&gt;query&lt;/em&gt;?  There&#39;s a view to do that for you.&lt;/li&gt;
&lt;li&gt;How about showing just &lt;a href=&#34;http://common-schema.googlecode.com/svn/trunk/common_schema/doc/html/processlist_top.html&#34;&gt;the good parts of the processlist&lt;/a&gt;?&lt;/li&gt;
&lt;li&gt;Does your schema have &lt;a href=&#34;http://common-schema.googlecode.com/svn/trunk/common_schema/doc/html/redundant_keys.html&#34;&gt;redundant keys&lt;/a&gt;?&lt;/li&gt;
&lt;li&gt;Or InnoDB tables with &lt;a href=&#34;http://common-schema.googlecode.com/svn/trunk/common_schema/doc/html/no_pk_innodb_tables.html&#34;&gt;no PRIMARY KEY&lt;/a&gt;?&lt;/li&gt;
&lt;li&gt;Is AUTO_INCREMENT &lt;a href=&#34;http://common-schema.googlecode.com/svn/trunk/common_schema/doc/html/auto_increment_columns.html&#34;&gt;running out of space&lt;/a&gt;?&lt;/li&gt;
&lt;li&gt;Can I get the SQL statements to &lt;a href=&#34;http://common-schema.googlecode.com/svn/trunk/common_schema/doc/html/sql_foreign_keys.html&#34;&gt;generate my FOREIGN KEYs&lt;/a&gt;? To drop them?&lt;/li&gt;
&lt;li&gt;And can we finally get &lt;a href=&#34;http://common-schema.googlecode.com/svn/trunk/common_schema/doc/html/sql_show_grants.html&#34;&gt;SHOW GRANTS for all accounts&lt;/a&gt;, and as an &lt;a href=&#34;http://common-schema.googlecode.com/svn/trunk/common_schema/doc/html/sql_grants.html&#34;&gt;SQL query&lt;/a&gt;?&lt;/li&gt;
&lt;li&gt;Ever needed a &lt;a href=&#34;http://common-schema.googlecode.com/svn/trunk/common_schema/doc/html/general_functions.html#crc64&#34;&gt;64 bit CRC function&lt;/a&gt;?&lt;/li&gt;
&lt;li&gt;And aren&#39;t you tired of writing the cumbersome SUBSTRING_INDEX(SUBSTRING_INDEX(str, &#39;,&#39;, 3), &#39;,&#39;, -1)? &lt;a href=&#34;http://common-schema.googlecode.com/svn/trunk/common_schema/doc/html/string_functions.html#split_token&#34;&gt;There&#39;s an alternative&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There&#39;s more. Take a look at the &lt;a href=&#34;http://common-schema.googlecode.com/svn/trunk/common_schema/doc/html/introduction.html&#34;&gt;common_schema documentation&lt;/a&gt; for full listing. And it&#39;s evolving: I&#39;ve got quite a few ideas already for future components.&lt;/p&gt;
&lt;p&gt;Some of these views rely on heavyweight INFORMATION_SCHEMA tables. You should be aware of the impact and &lt;a href=&#34;http://common-schema.googlecode.com/svn/trunk/common_schema/doc/html/risks.html&#34;&gt;risks&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;What do I need to install?&lt;/h4&gt;
&lt;p&gt;There&#39;s no script or executable file. It&#39;s just a schema. The distribution in an SQL file which generates &lt;em&gt;common_schema&lt;/em&gt;. Much like a dump file.&lt;/p&gt;
&lt;h4&gt;&lt;!--more--&gt;What are the system requirements?&lt;/h4&gt;
&lt;p&gt;It&#39;s just between you and your MySQL. There are currently three distribution files, dedicated for different versions of MySQL (and allowing for increased functionality):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;common_schema_mysql_51&lt;/strong&gt;: fits all MySQL &amp;gt;= 5.1 distributions&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;common_schema_innodb_plugin&lt;/strong&gt;: fits MySQL &amp;gt;= 5.1, with InnoDB plugin + INFORMATION_SCHEMA tables enabled&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;common_schema_percona_server&lt;/strong&gt;: fits Percona Server &amp;gt;= 5.1&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Refer to the &lt;a rel=&#34;nofollow&#34; href=&#34;http://common-schema.googlecode.com/svn/trunk/common_schema/doc/html/download.html&#34;&gt;documentation&lt;/a&gt; for more details.&lt;/p&gt;
&lt;h4&gt;What are the terms of use?&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;common_schema&lt;/em&gt; is released under the &lt;a href=&#34;http://www.opensource.org/licenses/bsd-license.php&#34;&gt;BSD license&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Where can I download it?&lt;/h4&gt;
&lt;p&gt;On the &lt;a href=&#34;http://code.google.com/p/common-schema/&#34;&gt;common_schema project page&lt;/a&gt;. Enjoy it!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Reasons to use AUTO_INCREMENT columns on InnoDB</title>
      <link>/blog/2011/03/22/reasons-to-use-auto_increment-columns-on-innodb/</link>
      <pubDate>Tue, 22 Mar 2011 08:31:18 +0000</pubDate>
      
      <guid>/blog/2011/03/22/reasons-to-use-auto_increment-columns-on-innodb/</guid>
      <description>&lt;p&gt;An InnoDB table must have a primary key (one is created if you don&#39;t do it yourself). You may have a &lt;a href=&#34;http://en.wikipedia.org/wiki/Natural_key&#34;&gt;natural key&lt;/a&gt; at hand. Stop! Allow me to suggest an AUTO_INCREMENT may be better.&lt;/p&gt;
&lt;p&gt;Why should one add an AUTO_INCREMENT PRIMARY KEY on a table on which there&#39;s a natural key? Isn&#39;t an AUTO_INCREMENT a pseudo key, meaning, it doesn&#39;t have any explicit relation to the row data, other than it is a number and unique?&lt;/p&gt;
&lt;p&gt;Yes, indeed so. Nevertheless, consider:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Natural keys are many times multi-columned.&lt;/li&gt;
&lt;li&gt;Multi column PRIMARY KEYs make for larger keys, and make for bloated secondary keys as well. You may be wasting space for storing the additional AUTO_INCREMENT column, but you may gain space back on secondary keys.&lt;/li&gt;
&lt;li&gt;Multi column PRIMARY KEYs make for more locks. See also &lt;a href=&#34;http://code.openark.org/blog/mysql/reducing-locks-by-narrowing-primary-key&#34;&gt;this post&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;InnoDB INSERTs work considerably faster when worked in ascending PRIMARY KEY order. Can you ensure your natural key is in such order?&lt;/li&gt;
&lt;li&gt;Even though an AUTO_INCREMENT makes for an INSERT bottleneck (values must be given serially), it is in particular helpful to InnoDB by ensuring PRIMARY KEY values are in ascending order.&lt;/li&gt;
&lt;li&gt;AUTO_INCEMENT makes for chronological resolution. You &lt;em&gt;know&lt;/em&gt; what came first, and what came next.&lt;/li&gt;
&lt;li&gt;In many datasets, more recent entries are often being accessed more, and are therefore &#34;hotter&#34;. By using AUTO_INCREMENT, you&#39;re ensuring that recent entries are grouped together within the B+ Tree. This means less random I/O when looking for recent data.&lt;/li&gt;
&lt;li&gt;A numerical key is in particular helpful in splitting your table (and tasks on your table) into smaller chunks. I write &lt;a href=&#34;http://code.google.com/p/openarkkit/&#34;&gt;tools&lt;/a&gt; which can work out with any PRIMARY KEY combination, but it&#39;s easier to work with numbers.&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>Multi condition UPDATE query</title>
      <link>/blog/2011/01/27/multi-condition-update-query/</link>
      <pubDate>Thu, 27 Jan 2011 10:30:24 +0000</pubDate>
      
      <guid>/blog/2011/01/27/multi-condition-update-query/</guid>
      <description>&lt;p&gt;A simple question I&#39;ve been asked:&lt;/p&gt;
&lt;p&gt;Is it possible to merge two &lt;strong&gt;UPDATE&lt;/strong&gt; queries, each on different &lt;strong&gt;WHERE&lt;/strong&gt; conditions, into a single query?&lt;/p&gt;
&lt;p&gt;For example, is it possible to merge the following two &lt;strong&gt;UPDATE&lt;/strong&gt; statements into one?&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;mysql&amp;gt; &lt;strong&gt;UPDATE&lt;/strong&gt; film &lt;strong&gt;SET&lt;/strong&gt; rental_duration=rental_duration+1 &lt;strong&gt;WHERE&lt;/strong&gt; rating = &#39;G&#39;;
Query OK, 178 rows affected (0.01 sec)

mysql&amp;gt; &lt;strong&gt;UPDATE&lt;/strong&gt; film &lt;strong&gt;SET&lt;/strong&gt; rental_rate=rental_rate-0.5 &lt;strong&gt;WHERE&lt;/strong&gt; length &amp;lt; 90;
Query OK, 320 rows affected (0.01 sec)
&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;To verify our tests, we take a checksum:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;mysql&amp;gt; pager md5sum
PAGER set to &#39;md5sum&#39;
mysql&amp;gt; &lt;strong&gt;SELECT&lt;/strong&gt; film_id, title, rental_duration, rental_rate &lt;strong&gt;FROM&lt;/strong&gt; film &lt;strong&gt;ORDER BY&lt;/strong&gt; film_id;
c2d253c3919efaa6d11487b1fd5061f3  -
&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;Obviously, the following query is &lt;strong&gt;incorrect&lt;/strong&gt;:&lt;!--more--&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;mysql&amp;gt; &lt;strong&gt;UPDATE&lt;/strong&gt; film &lt;strong&gt;SET&lt;/strong&gt; rental_duration=rental_duration+1, rental_rate=rental_rate-0.5  &lt;strong&gt;WHERE&lt;/strong&gt; rating = &#39;G&#39; &lt;strong&gt;OR&lt;/strong&gt; length &amp;lt; 90;
Query OK, 431 rows affected (0.03 sec)

mysql&amp;gt; pager md5sum
PAGER set to &#39;md5sum&#39;
mysql&amp;gt; &lt;strong&gt;SELECT&lt;/strong&gt; film_id, title, rental_duration, rental_rate &lt;strong&gt;FROM&lt;/strong&gt; film &lt;strong&gt;ORDER BY&lt;/strong&gt; film_id;
09d450806e2cd7fa78a83ac5bef72d2b  -
&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;h4&gt;Motivation&lt;/h4&gt;
&lt;p&gt;Why would you want to do that?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;While it may seem strange, the merge can be logically (application-wise) perfectly reasonable.&lt;/li&gt;
&lt;li&gt;The &lt;strong&gt;UPDATE&lt;/strong&gt; may be time consuming - perhaps it requires full table scan on a large table. Doing it with one scan is faster than two scans.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;The solution&lt;/h4&gt;
&lt;p&gt;Use a condition for the &lt;strong&gt;SET&lt;/strong&gt; clauses, optionally drop the &lt;strong&gt;WHERE&lt;/strong&gt; conditions.&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;&lt;strong&gt;UPDATE&lt;/strong&gt;
 film
&lt;strong&gt;SET&lt;/strong&gt;
 rental_duration=&lt;strong&gt;IF&lt;/strong&gt;(rating = &#39;G&#39;, rental_duration+1, rental_duration),
 rental_rate=&lt;strong&gt;IF&lt;/strong&gt;(length &amp;lt; 90, rental_rate-0.5, rental_rate)
;

mysql&amp;gt; pager md5sum
PAGER set to &#39;md5sum&#39;
mysql&amp;gt; &lt;strong&gt;SELECT&lt;/strong&gt; film_id, title, rental_duration, rental_rate &lt;strong&gt;FROM&lt;/strong&gt; film &lt;strong&gt;ORDER BY&lt;/strong&gt; film_id;
c2d253c3919efaa6d11487b1fd5061f3  -
&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;The above query necessarily does a full table scan. If there&#39;s a benefit to using indexes in the &lt;strong&gt;WHERE&lt;/strong&gt; clause, it may still be applied, using an &lt;strong&gt;OR&lt;/strong&gt; condition:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;&lt;strong&gt;UPDATE&lt;/strong&gt;
 film
&lt;strong&gt;SET&lt;/strong&gt;
 rental_duration=&lt;strong&gt;IF&lt;/strong&gt;(rating = &#39;G&#39;, rental_duration+1, rental_duration),
 rental_rate=&lt;strong&gt;IF&lt;/strong&gt;(length &amp;lt; 90, rental_rate-0.5, rental_rate)
&lt;strong&gt;WHERE&lt;/strong&gt;
 rating = &#39;G&#39;
 OR length &amp;lt; 90
;
&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;If there is a computational overhead to the &lt;strong&gt;IF&lt;/strong&gt; statement, I have not noticed it. This kind of solution plays well when each of the distinct queries requires a full scan, on large tables.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Simple guideline for choosing appropriate InnoDB PRIMARY KEYs</title>
      <link>/blog/2010/10/21/simple-guideline-for-choosing-appropriate-innodb-primary-keys/</link>
      <pubDate>Thu, 21 Oct 2010 07:52:45 +0000</pubDate>
      
      <guid>/blog/2010/10/21/simple-guideline-for-choosing-appropriate-innodb-primary-keys/</guid>
      <description>&lt;p&gt;Risking some flames, I&#39;d like to suggest only two options for choosing &lt;strong&gt;PRIMARY KEY&lt;/strong&gt;s for InnoDB tables. I suggest they should cover 99% (throwing numbers around) of cases.&lt;/p&gt;
&lt;h4&gt;PRIMARY KEY cases&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;An integer (SMALLINT / INT / BIGINT), possibly &lt;strong&gt;AUTO_INCREMENT&lt;/strong&gt; column.&lt;/li&gt;
&lt;li&gt;The combination of two columns on a many-to-many connecting table (e.g. &lt;strong&gt;film_actor&lt;/strong&gt;, which connects &lt;strong&gt;film&lt;/strong&gt;s to &lt;strong&gt;actor&lt;/strong&gt;s), the two columns being the &lt;strong&gt;PRIMARY KEY&lt;/strong&gt;s of respective data tables. This rule may be extended to 3-way relation tables.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;A short recap: an InnoDB must have a &lt;strong&gt;PRIMARY KEY&lt;/strong&gt;. It will pick one if you don&#39;t offer it. It can pick a really bad &lt;strong&gt;UNIQUE KEY&lt;/strong&gt; (e.g. &lt;strong&gt;website_url(255)&lt;/strong&gt;) or make one up using InnoDB internal row ids. If you don&#39;t have a good candidate, an &lt;strong&gt;AUTO_INCREMENT PRIMARY KEY&lt;/strong&gt; is probably the easiest way out.&lt;/p&gt;
&lt;p&gt;A 2-column combination for a many-to-many connection table is common and viable. The &lt;strong&gt;PRIMARY KEY&lt;/strong&gt; will not only provide with good join access method, but will also provide with the required &lt;strong&gt;UNIQUE&lt;/strong&gt; constraint.&lt;/p&gt;
&lt;p&gt;An integer-based &lt;strong&gt;PRIMARY KEY&lt;/strong&gt; will make for more compact &amp;amp; shallow index tree structures, which leads to less I/O and page reads.&lt;/p&gt;
&lt;p&gt;An &lt;strong&gt;AUTO_INCREMENT&lt;/strong&gt; will allow for ascending &lt;strong&gt;PRIMARY KEY&lt;/strong&gt; order of &lt;strong&gt;INSERT&lt;/strong&gt;, which is InnoDB-friendly: index pages will be more utilized, less fragmented.&lt;!--more--&gt;&lt;/p&gt;
&lt;h4&gt;Exceptions&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;You have a partitioned table, e.g. on date range.&lt;/strong&gt; With partitioned tables, every UNIQUE KEY, including the PRIMARY KEY, must include partitioning columns. In such case you will have to extend the PRIMARY KEY.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;The only key on your table is a unique constraint on some column, e.g. UNIQUE KRY (url).&lt;/strong&gt; On one hand, it seems wasteful to create &lt;em&gt;another&lt;/em&gt; column (e.g. AUTO_INCREMENT) to use as PRIMARY KEY. On the other hand, I&#39;ve seen many cases where this kind of PK didn&#39;t hold up. At some point there was need for another index. Or some method had to be devised for chunking up table data (&lt;a href=&#34;http://code.openark.org/forge/openark-kit/oak-chunk-update&#34;&gt;oak-chunk-update&lt;/a&gt; can do that even with non-integer PKs). I&#39;m reluctant to use such keys as PRIMARY.&lt;/li&gt;
&lt;li&gt;I&#39;m sure there are others.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Umm...&lt;/h4&gt;
&lt;p&gt;I wrote the draft for this post a while ago. And then came &lt;a href=&#34;http://mituzas.lt/2010/07/30/on-primary-keys/&#34;&gt;Domas&lt;/a&gt; and ruined it. &lt;a href=&#34;http://bugs.mysql.com/bug.php?id=55656&#34;&gt;Wait for&lt;/a&gt; &lt;strong&gt;5.1.52&lt;/strong&gt;?&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Thoughts and ideas for Online Schema Change</title>
      <link>/blog/2010/10/07/thoughts-and-ideas-for-online-schema-change/</link>
      <pubDate>Thu, 07 Oct 2010 10:29:10 +0000</pubDate>
      
      <guid>/blog/2010/10/07/thoughts-and-ideas-for-online-schema-change/</guid>
      <description>&lt;p&gt;Here&#39;s a few thoughts on current status and further possibilities for Facebook&#39;s &lt;a href=&#34;http://www.facebook.com/note.php?note_id=430801045932&#34;&gt;Online Schema Change&lt;/a&gt; (OSC) tool. I&#39;ve had these thoughts for months now, pondering over improving &lt;a href=&#34;../../forge/openark-kit/oak-online-alter-table&#34;&gt;oak-online-alter-table&lt;/a&gt; but haven&#39;t got around to implement them nor even write them down. Better late than never.&lt;/p&gt;
&lt;p&gt;The tool has some limitations. Some cannot be lifted, some could. Quoting from the &lt;a href=&#34;http://www.facebook.com/notes/mysql-at-facebook/online-schema-change-for-mysql/430801045932&#34;&gt;announcement&lt;/a&gt; and looking at the code, I add a few comments. I conclude with a general opinion on the tool&#39;s abilities.&lt;/p&gt;
&lt;h4&gt;&#34;The original table must have PK. Otherwise an error is returned.&#34;&lt;/h4&gt;
&lt;p&gt;This restriction could be lifted: it&#39;s enough that the table has a UNIQUE KEY. My original &lt;em&gt;oak-online-alter-table&lt;/em&gt; handled that particular case. As far as I see from their code, the Facebook code would work just as well with any unique key.&lt;/p&gt;
&lt;p&gt;However, this restriction is of no real interest. As we&#39;re mostly interested in InnoDB tables, and since any InnoDB table &lt;em&gt;should have&lt;/em&gt; a PRIMARY KEY, we shouldn&#39;t care too much.&lt;/p&gt;
&lt;h4&gt;&#34;No foreign keys should exist. Otherwise an error is returned.&#34;&lt;/h4&gt;
&lt;p&gt;Tricky stuff. With &lt;em&gt;oak-online-alter-table&lt;/em&gt;, changes to the original table were immediately reflected in the &lt;em&gt;ghost&lt;/em&gt; table. With InnoDB tables, that meant same transaction. And although I never got to update the text and code, there shouldn&#39;t be a reason for not using child-side foreign keys (the child-side is the table on which the FK constraint is defined).&lt;/p&gt;
&lt;p&gt;The Facebook patch works differently: it captures changes and writes them to a &lt;strong&gt;delta&lt;/strong&gt; table,  to be later (asynchronously) analyzed and make for a &lt;em&gt;replay&lt;/em&gt; of actions on the &lt;em&gt;ghost&lt;/em&gt; table.&lt;!--more--&gt;&lt;/p&gt;
&lt;p&gt;So in the Facebook code, some cases will lead to undesired behavior. Consider two tables, &lt;strong&gt;country&lt;/strong&gt; and &lt;strong&gt;city&lt;/strong&gt;, with city holding a RESTRICT/NO ACTION foreign key on &lt;strong&gt;country&lt;/strong&gt;&#39;s id. Now consider the scenario:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Rows from &lt;strong&gt;city&lt;/strong&gt; are DELETEd, where the country Id is Spain&#39;s.
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;city&lt;/strong&gt;&#39;s ghost table is still unaffected, Spain&#39;s cities are still there.&lt;/li&gt;
&lt;li&gt;A change is written to the delta table to mark these rows for deletion.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;A DELETE is issued on &lt;strong&gt;country&lt;/strong&gt;&#39;s Spain record.
&lt;ul&gt;
&lt;li&gt;The DELETE should work, from the user&#39;s perspective&lt;/li&gt;
&lt;li&gt;But it will fail: city&#39;s ghost table has not received the changes yet. There&#39;s still matching rows. The NO ACTION constraint will fail the DELETE statement.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Now, this does not lead to corruption, just to seemingly unreasonable behavior on the database part. This behavior is probably undesired. NO ACTION constraint won&#39;t do.&lt;/p&gt;
&lt;p&gt;However, with CASCADE or SET NULL options, there is less of an issue: operations on the parent table (e.g. &lt;strong&gt;country&lt;/strong&gt;) cannot fail. We must make sure operations on the ghost table make it consistent with the original table (e.g. &lt;strong&gt;city&lt;/strong&gt;).&lt;/p&gt;
&lt;p&gt;Consider the following scenario:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;A new country is created, called &#34;Sleepyland&#34;. An INSERT is made to &lt;strong&gt;country&lt;/strong&gt;.
&lt;ul&gt;
&lt;li&gt;Both &lt;strong&gt;city&lt;/strong&gt; and &lt;strong&gt;city&lt;/strong&gt;&#39;s ghost are immediately aware of it.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;A new town is created and INSERTed to &lt;strong&gt;city&lt;/strong&gt;. The town is called &#34;Naphaven&#34;.
&lt;ul&gt;
&lt;li&gt;The change takes time to propagate to &lt;strong&gt;city&lt;/strong&gt;&#39;s ghost table.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Meanwhile, we realized we made a mistake. We&#39;ve been had. There&#39;s no such city nor country.
&lt;ol&gt;
&lt;li&gt;We DELETE &#34;Naphaven&#34; from &lt;strong&gt;city&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;We DELETE &#34;Sleepyland&#34; from &lt;strong&gt;country&lt;/strong&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Note that &lt;strong&gt;city&lt;/strong&gt;&#39;s ghost table still hasn&#39;t caught up with the changes.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Eventually, the INSERT statement for &#34;Naphaven&#34; reaches &lt;strong&gt;city&lt;/strong&gt;&#39;s ghost table.
&lt;ul&gt;
&lt;li&gt;What should happen now? The INSERT cannot succeed.&lt;/li&gt;
&lt;li&gt;Will this fail the entire process?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Looking at the PHP code, I see that changes written on the &lt;strong&gt;delta&lt;/strong&gt; table are blindly replayed on the ghost table.&lt;/p&gt;
&lt;p&gt;Since the process is asynchronous, this should not be the case. We can solve the above if we use INSERT IGNORE instead of INSERT. The statement will fail without failing anything else. The row cannot exist, and that&#39;s because the original row does not exist anymore.&lt;/p&gt;
&lt;p&gt;Unlike a replication corruption, this does not lead to accumulation mistakes. The &lt;strong&gt;replay&lt;/strong&gt; is static, somewhat like in &lt;em&gt;binary log format&lt;/em&gt;. Changes are &lt;em&gt;just written&lt;/em&gt;, regardless of existing data.&lt;/p&gt;
&lt;p&gt;I have given this considerable thought, and I can&#39;t say I&#39;ve covered all the possible scenario. However I believe that with proper use of INSERT IGNORE and REPLACE INTO (two statements I heavily relied on with &lt;em&gt;oak-online-alter-table&lt;/em&gt;), correctness can be achieved.&lt;/p&gt;
&lt;p&gt;There&#39;s the small pain of re-generating the foreign key definition on the &#34;ghost&#34; table (&lt;strong&gt;CREATE TABLE LIKE ...&lt;/strong&gt; does not copy FK definitions). And since foreign key names are unique, a new name must be picked up. Not pretty, but perfectly doable.&lt;/p&gt;
&lt;h4&gt;&#34;No AFTER_{INSERT/UPDATE/DELETE} triggers must exist.&#34;&lt;/h4&gt;
&lt;p&gt;It would be nicer if MySQL had an ALTER TRIGGER statement. There isn&#39;t such statement. If there were such an atomic statement, then we would be able to rewrite the trigger, so as to add our own code to the &lt;em&gt;end of the trigger&#39;s code&lt;/em&gt;. Yuck. Would be even nicer if we were &lt;a href=&#34;http://code.openark.org/blog/mysql/triggers-use-case-compilation-part-ii&#34;&gt;allowed to have multiple triggers&lt;/a&gt; of same event.&lt;/p&gt;
&lt;p&gt;So, we are left with DROP and CREATE triggers. Alas, this makes for a short period where the trigger does not exist. Bad. The easy solution would be to LOCK WRITE the table, but apparently you can&#39;t DROP the trigger (*) when the table is locked. Sigh.&lt;/p&gt;
&lt;p&gt;(*) Happened to me, apparently to Facebook too; With latest 5.1 (5.1.51) version this actually works. With 5.0 it didn&#39;t use to; this needs more checking.&lt;/p&gt;
&lt;h4&gt;Use of INFORMATION_SCHEMA&lt;/h4&gt;
&lt;p&gt;As with oak-online-alter-table, the OSC checks for triggers, indexes, column by searching on the INFORMATION_SCHEMA tables. This makes for nice SQL for getting the exact listing and types of PRIMARY KEY columns, whether or not AFTER triggers exist, and so on.&lt;/p&gt;
&lt;p&gt;I&#39;ve always considered this to be the weak part of &lt;a href=&#34;http://code.openark.org/forge/openark-kit&#34;&gt;openark-kit&lt;/a&gt;, that it relies on INFORMATION_SCHEMA so much. It&#39;s easier, it&#39;s cleaner, it&#39;s even &lt;em&gt;more correct&lt;/em&gt; to work that way -- but it just puts too much locks. I think Baron Schwartz (and now Daniel Nichter) did amazing work on analyzing table schemata by parsing the SHOW CREATE TABLE and other SHOW commands regex-wise with &lt;a href=&#34;http://www.maatkit.org/&#34;&gt;Maatkit&lt;/a&gt;. It&#39;s a crazy work! Had I written &lt;em&gt;openark-kit&lt;/em&gt; in Perl, I would have just import their code. But I&#39;m too &lt;span style=&#34;text-decoration: line-through;&#34;&gt;lazy&lt;/span&gt; busy to do the conversion from Perl to Python, and rewrite that code, what with all the debugging.&lt;/p&gt;
&lt;p&gt;OSC is written in PHP. Again, much conversion work. I think performance-wise this is an important step to make.&lt;/p&gt;
&lt;h4&gt;A word for the critics&lt;/h4&gt;
&lt;p&gt;Finally, a word for the critics. I&#39;ve read some Facebook/MySQL bashing comments and wish to relate.&lt;/p&gt;
&lt;p&gt;In his &lt;a href=&#34;http://www.theregister.co.uk/2010/09/21/facebook_online_schema_change_for_mysql/&#34;&gt;interview to The Register&lt;/a&gt;, Mark Callaghan gave the example that &#34;Open Schema Change lets the company update indexes without user downtime, according to Callaghan&#34;.&lt;/p&gt;
&lt;p&gt;PostgreSQL was mentioned for being able to add index with only read locks taken, or being able to do the work with no locks using CREATE INDEX CONCURRENTLY. I wish MySQL had that feature! Yes, MySQL has a lot to improve upon, and the latest PostgreSQL 9.0 brings valuable new features. (Did I make it clear I have no intention of bashing PostgreSQL? If not, please re-read this paragraph until convinced).&lt;/p&gt;
&lt;p&gt;Bashing related to the notion of MySQL being so poor that Facebook used an even poorer mechanism to work out the ALTER TABLE.&lt;/p&gt;
&lt;p&gt;Well, allow me to add a few words: the CREATE INDEX is by far not the only thing you can achieve with OSC (although it may be Facebook&#39;s major concern). You should be able to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Add columns&lt;/li&gt;
&lt;li&gt;Drop columns&lt;/li&gt;
&lt;li&gt;Convert character sets&lt;/li&gt;
&lt;li&gt;Modify column types&lt;/li&gt;
&lt;li&gt;Add partitioning&lt;/li&gt;
&lt;li&gt;Reorganize partitioning&lt;/li&gt;
&lt;li&gt;Compress the table&lt;/li&gt;
&lt;li&gt;Otherwise changing table format&lt;/li&gt;
&lt;li&gt;Heck, you could even modify the storage engine! (To other transactional engine)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These are giant steps. How easy would it be to write these down into the database? It only takes a few weeks time to work out a working solution with reasonable limitations, just using the resources the MySQL server provides you with. The &lt;a href=&#34;http://www.facebook.com/MySQLatFacebook&#34;&gt;MySQL@Facebook team&lt;/a&gt; should be given credit for that.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>How often should you use OPTIMIZE TABLE? - followup</title>
      <link>/blog/2010/10/04/how-often-should-you-use-optimize-table-followup/</link>
      <pubDate>Mon, 04 Oct 2010 10:07:45 +0000</pubDate>
      
      <guid>/blog/2010/10/04/how-often-should-you-use-optimize-table-followup/</guid>
      <description>&lt;p&gt;This post follows up on Baron&#39;s &lt;a href=&#34;http://www.xaprb.com/blog/2010/02/07/how-often-should-you-use-optimize-table/&#34;&gt;How often should you use OPTIMIZE TABLE?&lt;/a&gt;. I had the opportunity of doing some massive purging of data from large tables, and was interested to see the impact of the &lt;strong&gt;OPTIMIZE&lt;/strong&gt; operation on table&#39;s indexes. I worked on some production data I was authorized to provide as example.&lt;/p&gt;
&lt;h4&gt;The use case&lt;/h4&gt;
&lt;p&gt;I&#39;ll present a single use case here. The table at hand is a compressed InnoDB table used for logs. I&#39;ve rewritten some column names for privacy:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;mysql&amp;gt; show create table logs \G

Create Table: CREATE TABLE `logs` (
 `id` int(11) NOT NULL AUTO_INCREMENT,
 `name` varchar(20) CHARACTER SET ascii COLLATE ascii_bin NOT NULL,
 `ts` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
 `origin` varchar(64) CHARACTER SET ascii COLLATE ascii_bin NOT NULL,
 `message` text NOT NULL,
 `level` tinyint(11) NOT NULL DEFAULT &#39;0&#39;,
 `s` char(16) CHARACTER SET ascii COLLATE ascii_bin NOT NULL DEFAULT &#39;&#39;,
 PRIMARY KEY (`id`),
 KEY `s` (`s`),
 KEY `name` (`name`,`ts`),
 KEY `origin` (`origin`,`ts`)
) ENGINE=InnoDB AUTO_INCREMENT=186878729 DEFAULT CHARSET=utf8 ROW_FORMAT=COMPRESSED KEY_BLOCK_SIZE=8&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;The table had log records starting &lt;strong&gt;2010-08-23&lt;/strong&gt; and up till &lt;strong&gt;2010-09-02&lt;/strong&gt; noon. Table status:&lt;!--more--&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;mysql&amp;gt; show table status like &#39;logs&#39;\G
*************************** 1. row ***************************
           Name: logs
         Engine: InnoDB
        Version: 10
     Row_format: Compressed
           Rows: 22433048
 Avg_row_length: 206
    Data_length: 4625285120
Max_data_length: 0
   Index_length: 1437073408
      Data_free: 4194304
 Auto_increment: 186878920
    Create_time: 2010-08-24 18:10:49
    Update_time: NULL
     Check_time: NULL
      Collation: utf8_general_ci
       Checksum: NULL
 Create_options: row_format=COMPRESSED KEY_BLOCK_SIZE=8
        Comment:&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;(A bit puzzled on the &lt;strong&gt;Create_time&lt;/strong&gt;; the table was taken from an LVM snapshot of another server, so it existed for a very long time before. Not sure why the &lt;strong&gt;Create_time&lt;/strong&gt; field is as it is here; I assume the MySQL upgrade marked it so, did not have the time nor need to look into it).&lt;/p&gt;
&lt;p&gt;I was using &lt;a href=&#34;http://www.percona.com/downloads/Percona-Server-5.1/&#34;&gt;Percona-Server-5.1.47-11.2&lt;/a&gt;, and so was able to look at the index statistics for that table:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;mysql&amp;gt; SELECT * FROM information_schema.INNODB_INDEX_STATS WHERE table_name=&#39;logs&#39;;
+--------------+------------+--------------+--------+----------------+------------+------------+
| table_schema | table_name | index_name   | fields | row_per_keys   | index_size | leaf_pages |
+--------------+------------+--------------+--------+----------------+------------+------------+
| newsminer    | logs       | PRIMARY      |      1 | 1              |     282305 |     246856 |
| newsminer    | logs       | s            |      2 | 17, 1          |      38944 |      33923 |
| newsminer    | logs       | name         |      3 | 2492739, 10, 2 |      22432 |      19551 |
| newsminer    | logs       | origin       |      3 | 1303, 4, 1     |      26336 |      22931 |
+--------------+------------+--------------+--------+----------------+------------+------------+&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;h4&gt;Status after massive purge&lt;/h4&gt;
&lt;p&gt;My first requirement was to purge out all record up to &lt;strong&gt;2010-09-01 00:00:00&lt;/strong&gt;. I did so in small chunks, using &lt;a href=&#34;http://code.openark.org/forge/openark-kit&#34;&gt;openark kit&lt;/a&gt;&#39;s oak-chunk-update (same can be achieved with &lt;a href=&#34;http://www.maatkit.org/&#34;&gt;maatkit&lt;/a&gt;&#39;s mk-archiver). The process purged &lt;strong&gt;1000&lt;/strong&gt; rows at a time, with some sleep in between, and ran for about a couple of hours. It may be interesting to note that since ts is in &lt;a href=&#34;http://code.openark.org/blog/mysql/monotonic-functions-sql-and-mysql&#34;&gt;monotonically ascending&lt;/a&gt; values, purging of old rows also means purging of lower PKs, which means we&#39;re trimming the PK tree from left.&lt;/p&gt;
&lt;p&gt;Even while purging took place, I could see the index_size/leaf_pages values dropping, until, finally:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;mysql&amp;gt; SELECT * FROM information_schema.INNODB_INDEX_STATS WHERE table_name=&#39;logs&#39;;
+--------------+------------+--------------+--------+--------------+------------+------------+
| table_schema | table_name | index_name   | fields | row_per_keys | index_size | leaf_pages |
+--------------+------------+--------------+--------+--------------+------------+------------+
| newsminer    | logs       | PRIMARY      |      1 | 1            |      40961 |      35262 |
| newsminer    | logs       | s            |      2 | 26, 1        |      34440 |       3798 |
| newsminer    | logs       | name         |      3 | 341011, 4, 1 |       4738 |       2774 |
| newsminer    | logs       | origin       |      3 | 341011, 4, 2 |      10178 |       3281 |
+--------------+------------+--------------+--------+--------------+------------+------------+&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;The number of deleted rows was roughly &lt;strong&gt;85%&lt;/strong&gt; of total rows, so down to &lt;strong&gt;15%&lt;/strong&gt; number of rows.&lt;/p&gt;
&lt;h4&gt;Status after OPTIMIZE TABLE&lt;/h4&gt;
&lt;p&gt;Time to see whether &lt;strong&gt;OPTIMIZE&lt;/strong&gt; really optimizes! Will it reduce number of leaf pages in PK? In secondary keys?&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;mysql&amp;gt; OPTIMIZE TABLE logs;
...
mysql&amp;gt; SELECT * FROM information_schema.INNODB_INDEX_STATS WHERE table_name=&#39;logs&#39;;
+--------------+------------+--------------+--------+--------------+------------+------------+
| table_schema | table_name | index_name   | fields | row_per_keys | index_size | leaf_pages |
+--------------+------------+--------------+--------+--------------+------------+------------+
| newsminer    | logs       | PRIMARY      |      1 | 1            |      40436 |      35323 |
| newsminer    | logs       | s            |      2 | 16, 1        |       5489 |       4784 |
| newsminer    | logs       | name         |      3 | 335813, 7, 1 |       3178 |       2749 |
| newsminer    | logs       | origin       |      3 | 335813, 5, 2 |       3951 |       3446 |
+--------------+------------+--------------+--------+--------------+------------+------------+
4 rows in set (0.00 sec)&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;The above shows no significant change in either of the indexes: not for &lt;strong&gt;index_size&lt;/strong&gt;, not for &lt;strong&gt;leaf_pages&lt;/strong&gt;, not for statistics (&lt;strong&gt;row_per_keys&lt;/strong&gt;). The &lt;strong&gt;OPTIMIZE&lt;/strong&gt; did not reduce index size. It did not reduce the number of index pages (&lt;strong&gt;leaf_pages&lt;/strong&gt; are the major factor here). Some &lt;strong&gt;leaff_pages&lt;/strong&gt; values have even increased, but in small enough margin to consider as equal.&lt;/p&gt;
&lt;p&gt;Index-wise, the above example does not show an advantage to using &lt;strong&gt;OPTIMIZE&lt;/strong&gt;. I confess, I was surprised. And for the better. This indicates InnoDB makes good merging of index pages after massive purging.&lt;/p&gt;
&lt;h4&gt;So, no use for OPTIMIZE?&lt;/h4&gt;
&lt;p&gt;Think again: file system-wise, things look different.&lt;/p&gt;
&lt;p&gt;Before purging of data:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;bash:~# ls -l logs.* -h
-rw-r----- 1 mysql mysql 8.6K 2010-08-15 17:40 logs.frm
-rw-r----- 1 mysql mysql 2.9G 2010-09-02 14:01 logs.ibd&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;After purging of data:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;bash:~# ls -l logs.* -h
-rw-r----- 1 mysql mysql 8.6K 2010-08-15 17:40 logs.frm
-rw-r----- 1 mysql mysql 2.9G 2010-09-02 14:21 logs.ibd&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;Recall that InnoDB never releases table space back to file system!&lt;/p&gt;
&lt;p&gt;After &lt;strong&gt;OPTIMIZE&lt;/strong&gt; on table:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;bash:~# ls -l logs.* -h
-rw-rw---- 1 mysql mysql 8.6K 2010-09-02 14:26 logs.frm
-rw-rw---- 1 mysql mysql 428M 2010-09-02 14:43 logs.ibd&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;On &lt;strong&gt;innodb_file_per_table&lt;/strong&gt; an &lt;strong&gt;OPTIMIZE&lt;/strong&gt; creates a new table space, and the old one gets destroyed. Space goes back to file system. Don&#39;t know about you; I like to have my file system with as much free space as possible.&lt;/p&gt;
&lt;h4&gt;Need to verify&lt;/h4&gt;
&lt;p&gt;I&#39;ve tested Percona Server, since this is where I can find &lt;strong&gt;INNODB_INDEX_STATS&lt;/strong&gt;. But this begs the following questions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Perhaps the results only apply for Percona Server? (I&#39;m guessing not).&lt;/li&gt;
&lt;li&gt;Or only for InnoDB plugin? Does the same hold for &#34;builtin&#34; InnoDB? (dunno)&lt;/li&gt;
&lt;li&gt;Only on &amp;gt;= 5.1? (Maybe; 5.0 is becoming rare now anyway)&lt;/li&gt;
&lt;li&gt;Only on InnoDB (Well, of course this test is storage engine dependent!)&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Conclusion&lt;/h4&gt;
&lt;p&gt;The use case above is a particular example. Other use cases may include tables where deletions often occur in middle of table (remember we were trimming the tree from left side only). Other yet may need to handle &lt;strong&gt;UPDATE&lt;/strong&gt;s to indexed columns. I have some more operations to do here, with larger tables (e.g. &lt;strong&gt;40GB&lt;/strong&gt; compressed). If anything changes, I&#39;ll drop a note.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Table refactoring &amp; application version upgrades, Part II</title>
      <link>/blog/2010/08/12/table-refactoring-application-version-upgrades-part-ii/</link>
      <pubDate>Thu, 12 Aug 2010 05:24:06 +0000</pubDate>
      
      <guid>/blog/2010/08/12/table-refactoring-application-version-upgrades-part-ii/</guid>
      <description>&lt;p&gt;Continuing &lt;a href=&#34;http://code.openark.org/blog/mysql/table-refactoring-application-version-upgrades-part-i&#34;&gt;Table refactoring &amp;amp; application version upgrades, Part I&lt;/a&gt;, we now discuss code &amp;amp; database upgrades which require &lt;strong&gt;DROP&lt;/strong&gt; operations. As before, we break apart the upgrade process into sequential steps, each involving either the application or the database, but not both.&lt;/p&gt;
&lt;p&gt;As I&#39;ll show, DROP operations are significantly simpler than creation operations. Interestingly, it&#39;s the same as in life.&lt;/p&gt;
&lt;h4&gt;DROP COLUMN&lt;/h4&gt;
&lt;p&gt;A column turns to be redundant, unused. Before it is dropped from the database, we must ensure no one is using it anymore. The steps are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;App: &lt;strong&gt;V1&lt;/strong&gt; -&amp;gt; &lt;strong&gt;V2&lt;/strong&gt;. Remove all references to column; make sure no queries use said column.&lt;/li&gt;
&lt;li&gt;DB: &lt;strong&gt;V1&lt;/strong&gt; -&amp;gt; &lt;strong&gt;V2&lt;/strong&gt; (possibly failover from &lt;strong&gt;M1&lt;/strong&gt; to &lt;strong&gt;M2&lt;/strong&gt;), change is &lt;strong&gt;DROP COLUMN&lt;/strong&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;DROP INDEX&lt;/h4&gt;
&lt;p&gt;A possibly simpler case here. Why would you drop an index? Is it because you found out you never use it anymore? Then all you have to do is just drop it.&lt;/p&gt;
&lt;p&gt;Or perhaps you don&#39;t need the functionality the index supports anymore? Then first drop the functionality:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;(optional) App: &lt;strong&gt;V1&lt;/strong&gt; -&amp;gt; &lt;strong&gt;V2&lt;/strong&gt;. Discard using functionality which relies on index.&lt;/li&gt;
&lt;li&gt;DB: &lt;strong&gt;V1&lt;/strong&gt; -&amp;gt; &lt;strong&gt;V2&lt;/strong&gt; (possibly failover from &lt;strong&gt;M1&lt;/strong&gt; to &lt;strong&gt;M2&lt;/strong&gt;), change is &lt;strong&gt;DROP INDEX&lt;/strong&gt;. Check out InnoDB Plugin here.&lt;!--more--&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;DROP UNIQUE INDEX&lt;/h4&gt;
&lt;p&gt;When using Master-Slave failover for table refactoring, we&#39;re now removing a constraint from the slave. Since the master is more constrained than the slave, there is no problem here. It&#39;s mostly the same as with a normal DROP INDEX, with a minor addition:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;(optional) App: &lt;strong&gt;V1&lt;/strong&gt; -&amp;gt; &lt;strong&gt;V2&lt;/strong&gt;. Discard using functionality which relies on index.&lt;/li&gt;
&lt;li&gt;DB: &lt;strong&gt;V1&lt;/strong&gt; -&amp;gt; &lt;strong&gt;V2&lt;/strong&gt; (possibly failover from &lt;strong&gt;M1&lt;/strong&gt; to &lt;strong&gt;M2&lt;/strong&gt;), change is &lt;strong&gt;DROP INDEX&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;(optional) App: &lt;strong&gt;V2&lt;/strong&gt; -&amp;gt; &lt;strong&gt;V3&lt;/strong&gt;. Enable functionality that inserts duplicates.&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;DROP FOREIGN KEY&lt;/h4&gt;
&lt;p&gt;Again, we are removing a constraint.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;DB: &lt;strong&gt;V1&lt;/strong&gt; -&amp;gt; &lt;strong&gt;V2&lt;/strong&gt; (possibly failover from &lt;strong&gt;M1&lt;/strong&gt; to &lt;strong&gt;M2&lt;/strong&gt;), change is &lt;strong&gt;DROP INDEX&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;(optional) App: &lt;strong&gt;V2&lt;/strong&gt; -&amp;gt; &lt;strong&gt;V3&lt;/strong&gt;. Enable functionality that conflicts with removed constraint. I mean, if you really know what you are doing.&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;DROP TABLE&lt;/h4&gt;
&lt;p&gt;The very simple steps are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;App: &lt;strong&gt;V1&lt;/strong&gt; -&amp;gt; &lt;strong&gt;V2&lt;/strong&gt;. Make sure no reference to table is made.&lt;/li&gt;
&lt;li&gt;DB: &lt;strong&gt;V1&lt;/strong&gt; -&amp;gt; &lt;strong&gt;V2&lt;/strong&gt;. Issue a &lt;strong&gt;DROP TABLE&lt;/strong&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;With &lt;strong&gt;ext3&lt;/strong&gt; dropping a large table is no less than a nightmare. Not only does the action take long time, it also locks down the table cache, which very quickly leads to having dozens of queries hang. &lt;strong&gt;xfs&lt;/strong&gt; is a good alternative.&lt;/p&gt;
&lt;h4&gt;Conclusion&lt;/h4&gt;
&lt;p&gt;We looked at single table operations, coupled with application upgrades. By carefully looking at the process breakdown, multiple changes can be addressed with ease and safety. Not all operations are completely safe when used with replication failover. But they are mostly safe if you have some trust in your code.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Table refactoring &amp; application version upgrades, Part I</title>
      <link>/blog/2010/08/10/table-refactoring-application-version-upgrades-part-i/</link>
      <pubDate>Tue, 10 Aug 2010 14:36:28 +0000</pubDate>
      
      <guid>/blog/2010/08/10/table-refactoring-application-version-upgrades-part-i/</guid>
      <description>&lt;p&gt;A developer&#39;s major concern is: &lt;em&gt;How do I do application &amp;amp; database upgrades with minimal downtime? How do I synchronize between a DB&#39;s version upgrade and an application&#39;s version upgrade?&lt;br /&gt;
&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;I will break down the discussion into types of database refactoring operations, and I will limit to single table refactoring. The discussion will try to understand the need for refactoring and will dictate the steps towards a successful upgrade.&lt;/p&gt;
&lt;h4&gt;Reader prerequisites&lt;/h4&gt;
&lt;p&gt;I will assume MySQL to be the underlying database. To take a major component out of the equation: we may need to deal with very large tables, for which an &lt;strong&gt;ALTER&lt;/strong&gt; command may take long hours. I will assume familiarity with Master-Master (Active-Passive) replication, with possible use of &lt;a href=&#34;http://mysql-mmm.org/&#34;&gt;MMM for MySQL&lt;/a&gt;. When I describe &#34;Failover from &lt;strong&gt;M1&lt;/strong&gt; to &lt;strong&gt;M2&lt;/strong&gt;&#34;, I mean &#34;Make the &lt;strong&gt;ALTER&lt;/strong&gt; changes on &lt;strong&gt;M2&lt;/strong&gt; (passive), then switch your application from &lt;strong&gt;M1&lt;/strong&gt; to &lt;strong&gt;M2&lt;/strong&gt; (change of IPs, VIP, etc.), promoting &lt;strong&gt;M2&lt;/strong&gt; to active position, then apply same changes on &lt;strong&gt;M1&lt;/strong&gt; (now passive) or completely rebuild it&#34;.&lt;/p&gt;
&lt;p&gt;Phew, a one sentence description of M-M usage...&lt;/p&gt;
&lt;p&gt;I also assume the reader&#39;s understanding that a table&#39;s schema can be different on master &amp;amp; slave, which is the basis for the &#34;use replication for refactoring&#34; trick. But it cannot be too different, or, to be precise, the two schemata must both support the ongoing queries for the table.&lt;/p&gt;
&lt;p&gt;A full discussion of the above is beyond the scope of this post.&lt;/p&gt;
&lt;h4&gt;Types of refactoring needs&lt;/h4&gt;
&lt;p&gt;As I limit this discussion to single table refactoring,we can look at major refactoring operations and their impact on application &amp;amp; upgrades. We will discuss ADD/DROP COLUMN, ADD/DROP INDEX, ADD/DROP UNIQUE INDEX, ADD/DROP FOREIGN KEY, ADD/DROP TABLE.&lt;/p&gt;
&lt;p&gt;We will assume the database and application are both in Version #1 (&lt;strong&gt;V1&lt;/strong&gt;), and need to be upgraded to &lt;strong&gt;V2&lt;/strong&gt; or greater.&lt;!--more--&gt;&lt;/p&gt;
&lt;h4&gt;ADD INDEX&lt;/h4&gt;
&lt;p&gt;Starting with the easier actions. Why would you add an index? Either:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;There is some existing query which can be optimized by the new query&lt;/li&gt;
&lt;li&gt;Or there is some new functionality which issues a query for which the new index is required.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Adding an index is an easy action in that the table&#39;s data does not really change.&lt;/p&gt;
&lt;p&gt;In case &lt;strong&gt;#1&lt;/strong&gt;, all you need to do is to add the new index (if the table is large, fail over from &lt;strong&gt;M1&lt;/strong&gt; to &lt;strong&gt;M2&lt;/strong&gt;). There is no application upgrade, so all that happens is that the database upgrades &lt;strong&gt;V1 &lt;/strong&gt;-&amp;gt;&lt;strong&gt; V2&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;In case &lt;strong&gt;#2&lt;/strong&gt;, the database must be prepared with new schema before the new functionality/query is introduced (since it depends on the existence of the index). The steps, therefore, are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;DB: &lt;strong&gt;V1&lt;/strong&gt; -&amp;gt; &lt;strong&gt;V2&lt;/strong&gt; (possibly failover from &lt;strong&gt;M1&lt;/strong&gt; to &lt;strong&gt;M2&lt;/strong&gt;)&lt;/li&gt;
&lt;li&gt;(Sometime later) App: &lt;strong&gt;V1&lt;/strong&gt; -&amp;gt; &lt;strong&gt;V2&lt;/strong&gt;. Application will issue queries which utilize the new index.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The application does not have to be upgraded at the same instant the DB gets upgraded. In fact, we&#39;ll see that this is a typical scenario: we can separate upgrades into smaller steps, which allow for time lapse. One &lt;em&gt;could&lt;/em&gt; work out steps &lt;strong&gt;1&lt;/strong&gt; &amp;amp; &lt;strong&gt;2&lt;/strong&gt; together, but that would take an extra effort.&lt;/p&gt;
&lt;h4&gt;ADD COLUMN&lt;/h4&gt;
&lt;p&gt;This must be one of the most common table schema upgrades: a new property is needed on the application side. It must be supported by the database. Perhaps a new field in some Java Object, with Hibernate mapping that field onto a new column. Or maybe the new column is there for purpose of de-normalization.&lt;/p&gt;
&lt;p&gt;This is also a more complicated task. Let&#39;s look at the required steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;DB: &lt;strong&gt;V1&lt;/strong&gt; -&amp;gt; &lt;strong&gt;V2&lt;/strong&gt; (possibly failover from &lt;strong&gt;M1&lt;/strong&gt; to &lt;strong&gt;M2&lt;/strong&gt;), change is &lt;strong&gt;ADD COLUMN&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;App: &lt;strong&gt;V1&lt;/strong&gt; -&amp;gt; &lt;strong&gt;V2&lt;/strong&gt;. Change is: provide column value for newly &lt;strong&gt;INSERT&lt;/strong&gt;ed rows.&lt;/li&gt;
&lt;li&gt;If needed, retroactively update column values for all pre-existing rows.&lt;/li&gt;
&lt;li&gt;App: &lt;strong&gt;V2&lt;/strong&gt; -&amp;gt; &lt;strong&gt;V3&lt;/strong&gt;. Application begins to use (read, &lt;strong&gt;SELECT&lt;/strong&gt;) new column.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The above procedure assumes that the new column must have some calculated value. A 10-million rows table must now be updated, to have the correct values filled in. So we ask of the application to start filling in data for new rows, which makes the invalid row set static. We can just take a &#34;from row&#34; and a &#34;to row&#34; and fill in the missing column&#39;s value for those rows. Only when all rows contain valid values can we let the application start using that row. This makes for &lt;em&gt;two&lt;/em&gt; application upgrades.&lt;/p&gt;
&lt;p&gt;If you&#39;re content with just a static &lt;strong&gt;DEFAULT&lt;/strong&gt; value, then step &lt;strong&gt;3&lt;/strong&gt; can be skipped, and step &lt;strong&gt;4&lt;/strong&gt; can be merged with step &lt;strong&gt;2&lt;/strong&gt;.&lt;/p&gt;
&lt;h4&gt;ADD UNIQUE INDEX&lt;/h4&gt;
&lt;p&gt;This is an altogether different case than the normal &lt;strong&gt;ADD INDEX&lt;/strong&gt;, even though they may seem similar. And the case is particularly different when using Master-Slave failover for rebuilding the table.&lt;/p&gt;
&lt;p&gt;Consider the case where we add a &lt;strong&gt;UNIQUE INDEX&lt;/strong&gt; on a slave. Some &lt;strong&gt;INSERT&lt;/strong&gt; query executes on the master, successfully, and is logged to the binary log. The slave picks it up, tries to execute it, to find that it fails on a DUPLICATE KEY error.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;UNIQUE INDEX&lt;/strong&gt; is a constraint, and it makes the slave more constrained than the master. This is a delicate situation. Here how to (mostly) work it out:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;App: &lt;strong&gt;V1&lt;/strong&gt; -&amp;gt; &lt;strong&gt;V2&lt;/strong&gt;. Change &lt;strong&gt;INSERT&lt;/strong&gt; queries on relevant table to &lt;strong&gt;INSERT IGNORE&lt;/strong&gt; or &lt;strong&gt;REPLACE&lt;/strong&gt; queries, whichever is more appropriate.&lt;/li&gt;
&lt;li&gt;DB: &lt;strong&gt;V1&lt;/strong&gt; -&amp;gt; &lt;strong&gt;V2&lt;/strong&gt; (possibly failover from &lt;strong&gt;M1&lt;/strong&gt; to &lt;strong&gt;M2&lt;/strong&gt;), change is &lt;strong&gt;ADD UNIQUE KEY&lt;/strong&gt; (and while at it, a tip: are you aware of &lt;a href=&#34;http://dev.mysql.com/doc/refman/5.1/en/alter-table.html&#34;&gt;ALTER IGNORE TABLE&lt;/a&gt;?)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The change of query ensures that the query will succeed on the slave (either by silently doing nothing or by actually replacing content). It also means that the slave can now have different data than the master. Of course, it you trust your application to never &lt;strong&gt;INSERT&lt;/strong&gt; duplicates, you can sleep better.&lt;/p&gt;
&lt;p&gt;We do not handle &lt;strong&gt;UPDATE&lt;/strong&gt; statements here.&lt;/p&gt;
&lt;h4&gt;ADD CONSTRAINT FOREIGN KEY&lt;/h4&gt;
&lt;p&gt;As with &lt;strong&gt;ADD UNIQUE INDEX&lt;/strong&gt;, there is a new constraint here. A slave becomes more constrained than the master. But we now have to make sure &lt;strong&gt;INSERT&lt;/strong&gt;, &lt;strong&gt;UPDATE&lt;/strong&gt; and &lt;strong&gt;DELETE&lt;/strong&gt; statements all go peacefully (well, it also depends on the type of &lt;strong&gt;ON DELETE&lt;/strong&gt; and &lt;strong&gt;ON UPDATE&lt;/strong&gt; property of the FK).&lt;/p&gt;
&lt;p&gt;The steps would be:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;DB: &lt;strong&gt;V1&lt;/strong&gt; -&amp;gt; &lt;strong&gt;V2&lt;/strong&gt; (possibly failover from &lt;strong&gt;M1&lt;/strong&gt; to &lt;strong&gt;M2&lt;/strong&gt;), change is &lt;strong&gt;ADD CONSTRAINT FOREIGN KEY&lt;/strong&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;And then cross your fingers or have trust in your application. If the table is small enough, one does not have to use replication to do the refactoring, and life is simpler. Just execute the &lt;strong&gt;ALTER&lt;/strong&gt; on the active master, and continue with your life.&lt;/p&gt;
&lt;h4&gt;CREATE TABLE&lt;/h4&gt;
&lt;p&gt;This is a simple case, since the table is new. The steps are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;DB: &lt;strong&gt;V1&lt;/strong&gt; -&amp;gt; &lt;strong&gt;V2&lt;/strong&gt; (no need to use slaves here)&lt;/li&gt;
&lt;li&gt;App: &lt;strong&gt;V1&lt;/strong&gt; -&amp;gt; &lt;strong&gt;V2&lt;/strong&gt;. Application will start using new table.&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;Conslusion&lt;/h4&gt;
&lt;p&gt;Having such steps formalized help with development management and database management. It makes clear what is expected of the application, and what is expected of the database. The breaking down of these operations into sequential steps allows us to work more slowly; make preparation work; work within our own working hours; get a chance to see the family.&lt;/p&gt;
&lt;p&gt;In this post we took a look at &#34;creation&#34; refactoring changes. New columns, new keys, new constraints. In the next part of this article, we&#39;ll discuss &lt;strong&gt;DROP&lt;/strong&gt; operations.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>SQL: forcing single row tables integrity</title>
      <link>/blog/2010/06/22/sql-forcing-single-row-tables-integrity/</link>
      <pubDate>Tue, 22 Jun 2010 06:58:51 +0000</pubDate>
      
      <guid>/blog/2010/06/22/sql-forcing-single-row-tables-integrity/</guid>
      <description>&lt;p&gt;Single row tables are used in various cases. Such tables can be used for &#34;preferences&#34; or &#34;settings&#34;; for managing counters (e.g. summary tables), for general-purpose administration tasks (e.g. heartbeat table) etc.&lt;/p&gt;
&lt;p&gt;The problem with single row tables is that, well, they must have s single row. And the question is: &lt;em&gt;how can you force them to have just one row?&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;The half-baked solution&lt;/h4&gt;
&lt;p&gt;The common solution is to create a &lt;strong&gt;PRIMARY KEY&lt;/strong&gt; and always use the same value for that key. In addition, using &lt;strong&gt;REPLACE&lt;/strong&gt; or &lt;strong&gt;INSERT INTO ON DUPLICATE KEY UPDATE&lt;/strong&gt; helps out in updating the row. For example:&lt;/p&gt;
&lt;blockquote&gt;&lt;pre&gt;
CREATE TABLE heartbeat (
 id int NOT NULL PRIMARY KEY,
 ts datetime NOT NULL
 );
&lt;/pre&gt;&lt;/blockquote&gt;
&lt;p&gt;The above table definition is taken from &lt;a href=&#34;http://www.maatkit.org/doc/mk-heartbeat.html&#34;&gt;mk-heartbeat&lt;/a&gt;. It should be noted that &lt;em&gt;mk-heartbeat&lt;/em&gt; in itself does not require that the table has a single row, so it is not the target of this post. I&#39;m taking the above table definition as a very simple example.&lt;/p&gt;
&lt;p&gt;So, we assume we want this table to have a single row, for whatever reasons we have. We would usually do:&lt;/p&gt;
&lt;blockquote&gt;&lt;pre&gt;
REPLACE INTO heartbeat (id, ts) VALUES (1, NOW());
&lt;/pre&gt;&lt;/blockquote&gt;
&lt;p&gt;or&lt;/p&gt;
&lt;blockquote&gt;&lt;pre&gt;
INSERT INTO heartbeat (id, ts) VALUES (1, NOW()) ON DUPLICATE KEY UPDATE ts = NOW();
&lt;/pre&gt;&lt;/blockquote&gt;
&lt;p&gt;Why is the above a &lt;em&gt;&#34;half baked solution&#34;&lt;/em&gt;? Because it is up to the application to make sure it reuses the same &lt;strong&gt;PRIMARY KEY&lt;/strong&gt; value. There is nothing in the database to prevent the following:&lt;!--more--&gt;&lt;/p&gt;
&lt;blockquote&gt;&lt;pre&gt;
REPLACE INTO heartbeat (id, ts) VALUES (73, NOW()); -- Ooops
&lt;/pre&gt;&lt;/blockquote&gt;
&lt;p&gt;One may claim that &lt;em&gt;&#34;my application has good integrity&#34;&lt;/em&gt;. That may be the case; but I would then raise the question: &lt;em&gt;why, then, would you need &lt;strong&gt;FOREIGN KEY&lt;/strong&gt;s&lt;/em&gt;? Of course, many people don&#39;t use &lt;strong&gt;FOREIGN KEY&lt;/strong&gt;s, but I think the message is clear.&lt;/p&gt;
&lt;h4&gt;A heavyweight solution&lt;/h4&gt;
&lt;p&gt;Triggers &lt;a href=&#34;http://code.openark.org/blog/mysql/triggers-use-case-compilation-part-i&#34;&gt;can help out&lt;/a&gt;. But really, this is an overkill.&lt;/p&gt;
&lt;h4&gt;A solution&lt;/h4&gt;
&lt;p&gt;I purpose a solution where, much like &lt;strong&gt;FOREIGN KEY&lt;/strong&gt;s, the database will force the integrity of the table; namely, have it contain &lt;em&gt;at most one row&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;For this solution to work, we will need a strict &lt;strong&gt;sql_mode&lt;/strong&gt;. I&#39;ll show later what happens when using a relaxed &lt;strong&gt;sql_mode&lt;/strong&gt;:&lt;/p&gt;
&lt;blockquote&gt;&lt;pre&gt;
SET sql_mode=&#39;STRICT_ALL_TABLES&#39;; -- Session scope for the purpose of this article
&lt;/pre&gt;&lt;/blockquote&gt;
&lt;p&gt;Here&#39;s a new table definition:&lt;/p&gt;
&lt;blockquote&gt;&lt;pre&gt;
CREATE TABLE heartbeat (
 integrity_keeper ENUM(&#39;&#39;) NOT NULL PRIMARY KEY,
 ts datetime NOT NULL
);
&lt;/pre&gt;&lt;/blockquote&gt;
&lt;p&gt;Let&#39;s see what happens now:&lt;/p&gt;
&lt;blockquote&gt;&lt;pre&gt;
mysql&amp;gt; INSERT INTO heartbeat (ts) VALUES (NOW());
Query OK, 1 row affected (0.00 sec)
mysql&amp;gt; INSERT INTO heartbeat (ts) VALUES (NOW());
ERROR 1062 (23000): Duplicate entry &#39;&#39; for key &#39;PRIMARY&#39;
mysql&amp;gt; INSERT INTO heartbeat (integrity_keeper, ts) VALUES (&#39;&#39;, NOW());
ERROR 1062 (23000): Duplicate entry &#39;&#39; for key &#39;PRIMARY&#39;
mysql&amp;gt; INSERT INTO heartbeat (integrity_keeper, ts) VALUES (0, NOW());
ERROR 1265 (01000): Data truncated for column &#39;integrity_keeper&#39; at row 1
mysql&amp;gt; INSERT INTO heartbeat (integrity_keeper, ts) VALUES (1, NOW());
ERROR 1062 (23000): Duplicate entry &#39;&#39; for key &#39;PRIMARY&#39;
mysql&amp;gt; REPLACE INTO heartbeat (ts) VALUES (NOW());
Query OK, 2 rows affected (0.00 sec)
mysql&amp;gt; INSERT INTO heartbeat (ts) VALUES (NOW()) ON DUPLICATE KEY UPDATE ts = NOW();
Query OK, 0 rows affected (0.00 sec)
mysql&amp;gt; SELECT * FROM heartbeat;
+------------------+---------------------+
| integrity_keeper | ts                  |
+------------------+---------------------+
|                  | 2010-06-15 09:12:19 |
+------------------+---------------------+
&lt;/pre&gt;&lt;/blockquote&gt;
&lt;p&gt;So the trick is to create a &lt;strong&gt;PRIMARY KEY&lt;/strong&gt; column which is only allowed a single value.&lt;/p&gt;
&lt;p&gt;The above shows I cannot force another row into the table: the schema will prevent me from doing so. Mission accomplished.&lt;/p&gt;
&lt;h4&gt;Further thoughts&lt;/h4&gt;
&lt;p&gt;The &lt;strong&gt;CHECK&lt;/strong&gt; keyword is the real solution to this problem (and other  problems). However, it is ignored by MySQL.&lt;/p&gt;
&lt;p&gt;It is interesting to note that with a relaxed &lt;strong&gt;sql_mode&lt;/strong&gt;, the &lt;strong&gt;INSERT INTO heartbeat (integrity_keeper, ts) VALUES (0, NOW());&lt;/strong&gt; query succeeds. Why? The default &lt;strong&gt;ENUM&lt;/strong&gt; value is &lt;strong&gt;1&lt;/strong&gt;, and, being in relaxed mode, &lt;strong&gt;0&lt;/strong&gt; is allowed in, even though it is not a valid value (Argh!).&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Reducing locks by narrowing primary key</title>
      <link>/blog/2010/05/04/reducing-locks-by-narrowing-primary-key/</link>
      <pubDate>Tue, 04 May 2010 08:46:01 +0000</pubDate>
      
      <guid>/blog/2010/05/04/reducing-locks-by-narrowing-primary-key/</guid>
      <description>&lt;p&gt;In a period of two weeks, I had two cases with the exact same symptoms.&lt;/p&gt;
&lt;p&gt;Database users were experiencing low responsiveness. DBAs were seeing locks occurring on seemingly normal tables. In particular, looking at Innotop, it seemed that &lt;strong&gt;INSERT&lt;/strong&gt;s were causing the locks.&lt;/p&gt;
&lt;p&gt;In both cases, tables were InnoDB. In both cases, there was a &lt;strong&gt;PRIMARY KEY&lt;/strong&gt; on the combination of all &lt;strong&gt;5&lt;/strong&gt; columns. And in both cases, there was no clear explanation as for why the &lt;strong&gt;PRIMARY KEY&lt;/strong&gt; was chosen as such.&lt;/p&gt;
&lt;h4&gt;Choosing a proper PRIMARY KEY&lt;/h4&gt;
&lt;p&gt;Especially with InnoDB, which uses clustered index structure, the &lt;strong&gt;PRIMARY KEY&lt;/strong&gt; is of particular importance. Besides the fact that a bloated &lt;strong&gt;PRIMARY KEY&lt;/strong&gt; bloats the entire clustered index and secondary keys (see: &lt;a href=&#34;http://code.openark.org/blog/mysql/the-depth-of-an-index-primer&#34;&gt;The depth of an index: primer&lt;/a&gt;), it is also a source for locks. It&#39;s true that any &lt;strong&gt;UNIQUE KEY&lt;/strong&gt; can serve as a &lt;strong&gt;PRIMARY KEY&lt;/strong&gt;. But not all such keys are good candidates.&lt;!--more--&gt;&lt;/p&gt;
&lt;h4&gt;Reducing the locks&lt;/h4&gt;
&lt;p&gt;In both described cases, the solution was to add an &lt;strong&gt;AUTO_INCREMENT&lt;/strong&gt; column to serve as the &lt;strong&gt;PRIMARY KEY&lt;/strong&gt;, and have that &lt;strong&gt;5&lt;/strong&gt; column combination under a secondary &lt;strong&gt;UNIQUE KEY&lt;/strong&gt;. The impact was immediate: no further locks on that table were detected, and query responsiveness turned very high.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Monotonic functions, SQL and MySQL</title>
      <link>/blog/2010/02/09/monotonic-functions-sql-and-mysql/</link>
      <pubDate>Tue, 09 Feb 2010 09:47:21 +0000</pubDate>
      
      <guid>/blog/2010/02/09/monotonic-functions-sql-and-mysql/</guid>
      <description>&lt;blockquote&gt;In mathematics, a &lt;strong&gt;monotonic function&lt;/strong&gt; (or &lt;strong&gt;monotone function&lt;/strong&gt;) is a function which preserves the given order. [&lt;a href=&#34;http://en.wikipedia.org/wiki/Monotonic_function&#34;&gt;Wikipedia&lt;/a&gt;]&lt;/blockquote&gt;
&lt;p&gt;To be more precise, a function &lt;em&gt;f&lt;/em&gt; is monotonic increasing, if for every &lt;em&gt;x ≤ y&lt;/em&gt; it holds that &lt;em&gt;f(x) ≤ f(y)&lt;/em&gt;. &lt;em&gt;f&lt;/em&gt; is said to be strictly monotonic increasing is for every &lt;em&gt;x &amp;lt; y&lt;/em&gt; it holds that &lt;em&gt;f(x) &amp;lt; f(y)&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;So, if we follow values in some order, we say that &lt;em&gt;f&lt;/em&gt; is monotonic increasing if &lt;em&gt;f&lt;/em&gt;&#39;s value never decreases (it either increases or stays the same), and we say that &lt;em&gt;f&lt;/em&gt; is strictly increasing if &lt;em&gt;f&lt;/em&gt;&#39;s value is always changes &#34;upwards&#34;.&lt;/p&gt;
&lt;p&gt;Monotonic functions play an important role in SQL. To discuss monotonic functions in SQL we must first determine what the &lt;em&gt;order&lt;/em&gt; is, and then, what the &lt;em&gt;function&lt;/em&gt; is.&lt;/p&gt;
&lt;p&gt;Well, they both change according to our point of view. Let&#39;s look at some examples. Take a look at the following table:&lt;!--more--&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;CREATE TABLE `log` (
 `&lt;strong&gt;id&lt;/strong&gt;` int(11) NOT NULL &lt;strong&gt;AUTO_INCREMENT&lt;/strong&gt;,
 `&lt;strong&gt;ts&lt;/strong&gt;` &lt;strong&gt;timestamp&lt;/strong&gt; NOT NULL &lt;strong&gt;DEFAULT CURRENT_TIMESTAMP&lt;/strong&gt;,
 `error_level` tinyint(4) DEFAULT NULL,
 `subject` varchar(32) DEFAULT NULL,
 `description` varchar(255) DEFAULT NULL,
 PRIMARY KEY (`id`)
)&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;In the above &lt;strong&gt;log&lt;/strong&gt; table, log entries are added with &lt;strong&gt;id&lt;/strong&gt; and &lt;strong&gt;ts&lt;/strong&gt; getting automatically evaluated. Assuming no dirty hacks occur, we can expect that &lt;strong&gt;ts&lt;/strong&gt; in &lt;em&gt;monotonic&lt;/em&gt; by order of &lt;strong&gt;id&lt;/strong&gt;. That is, as &lt;strong&gt;id&lt;/strong&gt; increases, so does &lt;strong&gt;ts&lt;/strong&gt;. Is is possible that we get the same &lt;strong&gt;ts&lt;/strong&gt; for a few rows (it is not unique), but once it increases, it never decreases again.&lt;/p&gt;
&lt;h4&gt;Why is this interesting?&lt;/h4&gt;
&lt;p&gt;Because it simplifies common problems.&lt;/p&gt;
&lt;p&gt;For example, it simplifies a search for a given &lt;strong&gt;ts&lt;/strong&gt; value, when no index exists on the &lt;strong&gt;ts&lt;/strong&gt; column. If we were to look for a log entry from &lt;strong&gt;&#39;2009-02-07 11:58:00&#39;&lt;/strong&gt; by simple &lt;strong&gt;SELECT&lt;/strong&gt;, we would have to use a full table scan. But, by knowing that &lt;strong&gt;ts&lt;/strong&gt; is monotonic, we can also use &lt;a href=&#34;http://en.wikipedia.org/wiki/Binary_search_algorithm&#34;&gt;binary search&lt;/a&gt; on &lt;strong&gt;id&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;As another example, it simplifies the task of purging all rows up to last midnight. Instead of issuing &lt;strong&gt;&#34;DELETE FROM log WHERE ts &amp;lt; DATE(NOW())&#34;&lt;/strong&gt;, thus using, again, full table scan plus locking all rows (depending on storage engine), we can use other methods:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We can detect the &lt;strong&gt;id&lt;/strong&gt; for the first row which holds the condition using binary search, then &lt;strong&gt;&#34;DELETE FROM log WHERE id &amp;lt; ###&#34;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Or we can slowly work our way in ascending &lt;strong&gt;id&lt;/strong&gt; order, issuing something like &lt;strong&gt;&#34;DELETE FROM log WHERE ts &amp;lt; DATE(NOW()) ORDER BY id ASC LIMIT 1000&#34;&lt;/strong&gt;, and stop once the &lt;strong&gt;ROW_COUNT()&lt;/strong&gt; is less than &lt;strong&gt;1000&lt;/strong&gt;. We know we need not advance any further, without needing to compute anything. We thus block less, while retaining correctness of our operation.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Monotonic functions in MySQL&lt;/h4&gt;
&lt;p&gt;When we iterate InnoDB tables (as in full table scan), we know that rows are iterated in ascending &lt;strong&gt;PRIMARY KEY&lt;/strong&gt; order &lt;a name=&#34;note1m&#34; href=&#34;#note1&#34;&gt;[¹]&lt;/a&gt;. So the &lt;strong&gt;PRIMARY KEY&lt;/strong&gt; dictates the order by which monotonic functions are evaluated.&lt;/p&gt;
&lt;p&gt;With MyISAM, rows are iterated according to internal storage order. It has nothing to do with &lt;strong&gt;PRIMARY KEY&lt;/strong&gt;s (though depending on &lt;strong&gt;concurrent_insert&lt;/strong&gt; this can be somewhat controlled). It also has nothing to do with chronological order. Newer rows may capture space held by older rows.&lt;/p&gt;
&lt;p&gt;But MyISAM allows for &lt;strong&gt;ALTER TABLE ... ORDER BY ...&lt;/strong&gt; syntax, which allows us to do a one-time sort of the table. Assuming no writes shortly thereafter, a full table scan will iterate the rows according to specified order.&lt;/p&gt;
&lt;h4&gt;Monotonic functions and indexes&lt;/h4&gt;
&lt;p&gt;A column which is indexed dictates a monotonic function by index order.&lt;/p&gt;
&lt;p&gt;Wait. Isn&#39;t that obvious? Of course: if we index a column, then the index sorts by that column, and the column is ascending by the index order which is,... itself.&lt;/p&gt;
&lt;p&gt;I call that trivial, but it does interest us: because, while mathematically there may be nothing significant here, we do care about this order when we have index scans. So, if we can force an index scan on our query, then we can anticipate the order by which rows are processed; we now have some order by which to evaluate monotonic functions.&lt;/p&gt;
&lt;p&gt;OK, maybe I made it sound more complicated than it really is. Monotonic functions work well when the &lt;em&gt;order&lt;/em&gt; by which they are monotonic is some indexed column(s). The &lt;strong&gt;AUTO_INCREMENT PRIMARY KEY&lt;/strong&gt; we saw in the &lt;strong&gt;log&lt;/strong&gt; example above, is perhaps the most trivial case.&lt;/p&gt;
&lt;p&gt;While MySQL does not support function indexes, if the function we consider is monotonic, we still benefit from adding an index on the raw column.&lt;/p&gt;
&lt;h4&gt;Other examples of monotonic functions&lt;/h4&gt;
&lt;p&gt;So, where else can we find them? Timestamp columns are probably the most common (this post holds true until time travel to the past is introduced).&lt;/p&gt;
&lt;p&gt;But also summaries: like a reporting table which lists down some ever-ascending value (the number of books ever sold in our store; trip mileage; hop counter; etc.).&lt;/p&gt;
&lt;p&gt;I&#39;ve seen many cases (though difficult to illustrate in this scope) when foreign key values are in ascending order. A very brief example is a 1-1 relation between two denormalized tables, where the tables ids do not necessarily have to match, but is always ascending).&lt;/p&gt;
&lt;p&gt;And Baron&#39;s &lt;a href=&#34;http://www.xaprb.com/blog/2010/01/22/my-wishlist-for-sql-the-until-clause/&#34;&gt;wishlist for SQL&lt;/a&gt; can also benefit from monotonic functions.&lt;/p&gt;
&lt;h4&gt;Conclusion&lt;/h4&gt;
&lt;p&gt;When a monotonic function is present, it brings an added value to our schema and query design. It allows for less indexing; quicker operations. Look for these. I&#39;ve only discussed increasing functions. Indeed, MySQL&#39;s indexes are always increasing (they cannot be defined in decreasing order), but query simplifications work just as well for monotonic decreasing functions.&lt;/p&gt;
&lt;p&gt;&lt;a name=&#34;note1&#34; href=&#34;#note1m&#34;&gt;[¹]&lt;/a&gt; I&#39;ve actually seen a different behavior on temporary InnoDB tables and on compressed InnoDB Plugin tables; I&#39;ll write on this on another occasion.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>