<!DOCTYPE html>
<html lang="en">

<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="">
  <meta name="generator" content="Hugo 0.16" />

  <title>Performance &middot; code.openark.org</title>

  
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pure/0.6.0/pure-min.css">

  <!--[if lte IE 8]>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pure/0.6.0/grids-responsive-old-ie-min.css">
  <![endif]-->
  <!--[if gt IE 8]><!-->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pure/0.6.0/grids-responsive-min.css">
  <!--<![endif]-->

  <!--[if lte IE 8]>
  <link rel="stylesheet" href="/css/side-menu-old-ie.css">
  <![endif]-->
  <!--[if gt IE 8]><!-->
  <link rel="stylesheet" href="/blog/css/side-menu.css">
  <!--<![endif]-->

  <link rel="stylesheet" href="/blog/css/blackburn.css">

  
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">

  
  <link href="https://fonts.googleapis.com/css?family=Raleway" rel="stylesheet" type="text/css">

  
  
  <link rel="alternate" type="application/rss+xml" title="code.openark.org" href="/blog/tags/performance/index.xml" />
  

  
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.1.0/styles/androidstudio.min.css">
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.1.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  

  <link rel="shortcut icon" href="/blogimg/favicon.ico" type="image/x-icon" />

  
  

</head>


<body>
<div id="layout">

  
<a href="#menu" id="menuLink" class="menu-link">
  
  <span></span>
</a>
<div id="menu">

  

  <a class="pure-menu-heading brand" href="/blog">openark.org</a>


  <div class="pure-menu">
    <ul class="pure-menu-list">
      
      
        <li class="pure-menu-item">
          <a class="pure-menu-link" href="/blog/"><i class='fa fa-home fa-fw'></i>Home</a>
      
        </li>
      
      
        <li class="pure-menu-item">
          <a class="pure-menu-link" href="/blog/post/"><i class='fa fa-list fa-fw'></i>Posts</a>
      
        </li>
      
      
        <li class="pure-menu-item">
          <a class="pure-menu-link" href="/blog/page/about"><i class='fa fa-user fa-fw'></i>About</a>
      
        </li>
      
      
      <li class="pure-menu-item">
        <a class="pure-menu-link" href="/blog/tags/performance/index.xml"><i class="fa fa-rss fa-fw"></i>RSS</a>
      </li>
      
    </ul>
  </div>

  <div class="pure-menu social">
  <ul class="pure-menu-list">
    <li class="pure-menu-item">
      <a class="pure-menu-link">
        Shlomi Noach
        
          <div class="avatar-container">
        	  <div class="avatar-img-border">
                <img class="avatar-img" src="/blog/images/shlomi-noach.png" alt="code.openark.org" />
        	  </div>
        	</div>
        
      </a>
    </li>

    
    <li class="pure-menu-item">
      <a class="pure-menu-link" href="https://github.com/shlomi-noach" target="_blank"><i class="fa fa-github-square fa-fw"></i>GitHub</a>
    </li>
    

    

    
    <li class="pure-menu-item">
      <a class="pure-menu-link" href="https://twitter.com/ShlomiNoach" target="_blank"><i class="fa fa-twitter-square fa-fw"></i>Twitter</a>
    </li>
    

    

    

    

    

    

    

    

    

    

    

    

    

    

    
    <li class="pure-menu-item">
      <a class="pure-menu-link" href="https://linkedin.com/in/shlominoach" target="_blank"><i class="fa fa-linkedin-square fa-fw"></i>LinkedIn</a>
    </li>
    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

  </ul>
</div>





  <div>
  <div class="small-print">
    <small>&copy; 2016. All rights reserved.</small>
  </div>
  <div class="small-print">
    <small>Built with&nbsp;<a href="https://gohugo.io/" target="_blank">Hugo</a></small>
    <small>Theme&nbsp;<a href="https://github.com/yoshiharuyamashita/blackburn" target="_blank">Blackburn</a></small>
  </div>
</div>

</div>


  <div id="main">


<div class="header">
  <h1>Performance</h1>
</div>

<div class="content">
  
    <article>
  <header>
    <h2><a href="/blog/2016/08/01/introducing-gh-ost-triggerless-online-schema-migrations/">Introducing gh-ost: triggerless online schema migrations</a></h2>

    <div class="post-meta">

  <div>

    <time>01 Aug 2016</time>
  </div>

  

  

  <div>
    
      
      
        <span>
          
            <a class="post-taxonomy-category" href="/blogcategories/mysql">MySQL</a>
          
        </span>
      
    

    
      
      
        <span>
          <i class="fa fa-tags fa-fw"></i>
          
            <a class="post-taxonomy-tag" href="/blogtags/development">Development</a>&nbsp;&#47;
          
            <a class="post-taxonomy-tag" href="/blogtags/github">GitHub</a>&nbsp;&#47;
          
            <a class="post-taxonomy-tag" href="/blogtags/open-source">Open Source</a>&nbsp;&#47;
          
            <a class="post-taxonomy-tag" href="/blogtags/operations">operations</a>&nbsp;&#47;
          
            <a class="post-taxonomy-tag" href="/blogtags/performance">Performance</a>&nbsp;&#47;
          
            <a class="post-taxonomy-tag" href="/blogtags/tools">tools</a>
          
        </span>
      
    
  </div>

</div>


  </header>

  <p>
  I'm thoroughly happy to introduce gh-ost: triggerless, controllable, auditable, testable, trusted online schema change tool released today by GitHub. gh-ost now powers our production schema migrations. We hit some serious limitations using pt-online-schema-change on our large volume, high traffic tables, to the effect of driving our database to a near grinding halt or even to the extent of causing outages. With gh-ost, we are now able to migrate our busiest tables at any time, peak hours and heavy workloads included, without causing impact to our service.
  </p>

  
  <footer>
    <a href="/blog/2016/08/01/introducing-gh-ost-triggerless-online-schema-migrations/">Read more<i class="fa fa-angle-double-right fa-fw"></i></a>
  </footer>
  
</article>

  
    <article>
  <header>
    <h2><a href="/blog/2014/04/20/the-mystery-of-mysql-5-6-excessive-buffer-pool-flushing/">The mystery of MySQL 5.6 excessive buffer pool flushing</a></h2>

    <div class="post-meta">

  <div>

    <time>20 Apr 2014</time>
  </div>

  

  

  <div>
    
      
      
        <span>
          
            <a class="post-taxonomy-category" href="/blogcategories/mysql">MySQL</a>
          
        </span>
      
    

    
      
      
        <span>
          <i class="fa fa-tags fa-fw"></i>
          
            <a class="post-taxonomy-tag" href="/blogtags/innodb">InnoDB</a>&nbsp;&#47;
          
            <a class="post-taxonomy-tag" href="/blogtags/performance">Performance</a>&nbsp;&#47;
          
            <a class="post-taxonomy-tag" href="/blogtags/replication">Replication</a>
          
        </span>
      
    
  </div>

</div>


  </header>

  <p>
  <p><p>I&rsquo;m experimenting with upgrading to MySQL <strong>5.6</strong> and am experiencing an unexplained increase in disk I/O utilization. After discussing this with several people I&rsquo;m publishing in the hope that someone has an enlightenment on this.</p>
<p>We have a few dozens servers in a normal replication topology. On this particular replication topology we&rsquo;ve already evaluated that <strong>STATEMENT</strong> based replication is faster than <strong>ROW</strong> based replication, and so we use <strong>SBR</strong>. We have two different workloads on our slaves, applied by two different HAProxy groups, on three different data centres. Hardware-wise, servers of two groups use either Virident SSD cards or normal SAS spindle disks.</p>
<p>Our servers are I/O bound. A common query used by both workloads looks up data that does not necessarily have a hotspot, and is very large in volume. DML is low, and we only have a few hundred statements per second executed on master (and propagated through replication).</p>
<p>We have upgraded <strong>6</strong> servers from all datacenters to <strong>5.6</strong>, both on SSD and spindle disks, and are experiencing the following phenomena:</p>

  </p>

  
  <footer>
    <a href="/blog/2014/04/20/the-mystery-of-mysql-5-6-excessive-buffer-pool-flushing/">Read more<i class="fa fa-angle-double-right fa-fw"></i></a>
  </footer>
  
</article>

  
    <article>
  <header>
    <h2><a href="/blog/2013/10/23/tokudb-configuration-variables-of-interest/">TokuDB configuration variables of interest</a></h2>

    <div class="post-meta">

  <div>

    <time>23 Oct 2013</time>
  </div>

  

  

  <div>
    
      
      
        <span>
          
            <a class="post-taxonomy-category" href="/blogcategories/mysql">MySQL</a>
          
        </span>
      
    

    
      
      
        <span>
          <i class="fa fa-tags fa-fw"></i>
          
            <a class="post-taxonomy-tag" href="/blogtags/configuration">Configuration</a>&nbsp;&#47;
          
            <a class="post-taxonomy-tag" href="/blogtags/innodb">InnoDB</a>&nbsp;&#47;
          
            <a class="post-taxonomy-tag" href="/blogtags/performance">Performance</a>&nbsp;&#47;
          
            <a class="post-taxonomy-tag" href="/blogtags/tokudb">TokuDB</a>
          
        </span>
      
    
  </div>

</div>


  </header>

  <p>
  <p><p>During our experiments I came upon a few TokuDB variables of interest; if you are using TokuDB you might want to look into these:</p>
<ul>
<li>
<h4>tokudb_analyze_time</h4>
</li>
</ul>
<p style="padding-left: 30px;">This is a boundary on the number of seconds an <strong>ANALYZE TABLE</strong> will operate on each index on each partition on a TokuDB table.</p>
<p style="padding-left: 30px;">That is, if <strong>tokudb_analyze_time = 5</strong>, and your table has <strong>4</strong> indexes (including <strong>PRIMARY</strong>) and <strong>7</strong> partitions, then the total runtime is limited to <strong>5*4*7 = 140</strong> seconds.</p>
<p style="padding-left: 30px;">Default in <strong>7.1.0</strong>: <strong>5</strong> seconds</p>
<ul>
<li>
<h4>tokudb_cache_size</h4>
</li>
</ul>
<p style="padding-left: 30px;">Similar to <strong>innodb_buffer_pool_size</strong>, this variable sets the amount of memory allocated by TokuDB for caching pages. Like InnoDB the table is clustered within the index, so the cache includes pages for both indexes and data.</p>
<p style="padding-left: 30px;">Default: <strong>50%</strong> of total memory</p>
<ul>
<li>
<h4>tokudb_directio</h4>
</li>
</ul>
<p style="padding-left: 30px;">Boolean, values are <strong>0/1</strong>. Setting <strong>tokudb_directio = 1</strong> is like specifying <strong>innodb_flush_method = O_DIRECT</strong>. Which in turn means the OS should not cache pages requested by TokuDB. Default: <strong>0</strong>.</p>
<p style="padding-left: 30px;">Now here&rsquo;s the interesting part: we are used to tell InnoDB to get the most memory we can provide (because we want it to cache as much as it can) and to avoid OS caching (because that would mean a page would appear both in the buffer pool and in OS memory, which is a waste). So the following setup is common:</p>

  </p>

  
  <footer>
    <a href="/blog/2013/10/23/tokudb-configuration-variables-of-interest/">Read more<i class="fa fa-angle-double-right fa-fw"></i></a>
  </footer>
  
</article>

  
    <article>
  <header>
    <h2><a href="/blog/2013/09/03/converting-an-olap-database-to-tokudb-part-1/">Converting an OLAP database to TokuDB, part 1</a></h2>

    <div class="post-meta">

  <div>

    <time>03 Sep 2013</time>
  </div>

  

  

  <div>
    
      
      
        <span>
          
            <a class="post-taxonomy-category" href="/blogcategories/mysql">MySQL</a>
          
        </span>
      
    

    
      
      
        <span>
          <i class="fa fa-tags fa-fw"></i>
          
            <a class="post-taxonomy-tag" href="/blogtags/common_schema">common_schema</a>&nbsp;&#47;
          
            <a class="post-taxonomy-tag" href="/blogtags/compression">compression</a>&nbsp;&#47;
          
            <a class="post-taxonomy-tag" href="/blogtags/innodb">InnoDB</a>&nbsp;&#47;
          
            <a class="post-taxonomy-tag" href="/blogtags/open-source">Open Source</a>&nbsp;&#47;
          
            <a class="post-taxonomy-tag" href="/blogtags/performance">Performance</a>&nbsp;&#47;
          
            <a class="post-taxonomy-tag" href="/blogtags/queryscript">QueryScript</a>&nbsp;&#47;
          
            <a class="post-taxonomy-tag" href="/blogtags/tokudb">TokuDB</a>
          
        </span>
      
    
  </div>

</div>


  </header>

  <p>
  <p><p>This is the first in a series of posts describing my impressions of converting a large OLAP server to TokuDB. There&rsquo;s a lot to tell, and the experiment is not yet complete, so this is an ongoing blogging. In this post I will describe the case at hand and out initial reasons for looking at TokuDB.</p>
<p>Disclosure: I have no personal interests and no company interests; we did get friendly, useful and free advice from Tokutek engineers. TokuDB is open source and free to use, though commercial license is also available.</p>
<h4>The case at hand</h4>
<p>We have a large and fast growing DWH MySQL setup. This data warehouse is but one component in a larger data setup, which includes Hadoop, Cassandra and more. For online dashboards and most reports, MySQL is our service. We populate this warehouse mainly via Hive/Hadoop. Thus, we have an hourly load of data from Hive, as well as a larger daily load.</p>
<p>There are some updates on the data, but the majority of writes are just <strong>mysqlimport</strong>s of Hive queries.</p>
<p>Usage of this database is OLAP: no concurrency issues here; we have some should-be-fast-running queries issued by our dashboards, as well as ok-to-run-longer queries issued for reports.</p>
<p>Our initial and most burning trouble is with size. Today we use <strong>COMPRESSED</strong> InnoDB tables (<strong>KEY_BLOCK_SIZE</strong> is default, i.e. <strong>8</strong>). Our data volume sums right now at about <strong>2TB</strong>. I happen to know this translates as <strong>4TB</strong> of uncompressed data.</p>
<p>However growth of data is accelerating. A year ago we would capture a dozen GB per month. Today it is a <strong>100GB</strong> per month, and by the end of this year it may climb to <strong>150GB</strong> per month or more.</p>
<p>Our data is not sharded. We have a simple replication topology of some <strong>6</strong> servers. Machines are quite generous as detailed following. And yet, we will be running out of resources shortly: disk space (total <strong>2.7TB</strong>) is now running low and is expected to run out in about six months. One of my first tasks in Outbrain is to find a solution to our DWH growth problem. The solution could be sharding; it could be a commercial DWH product; anything that works.</p>

  </p>

  
  <footer>
    <a href="/blog/2013/09/03/converting-an-olap-database-to-tokudb-part-1/">Read more<i class="fa fa-angle-double-right fa-fw"></i></a>
  </footer>
  
</article>

  
    <article>
  <header>
    <h2><a href="/blog/2012/06/27/delete-dont-insert/">DELETE, don&#39;t INSERT</a></h2>

    <div class="post-meta">

  <div>

    <time>27 Jun 2012</time>
  </div>

  

  

  <div>
    
      
      
        <span>
          
            <a class="post-taxonomy-category" href="/blogcategories/mysql">MySQL</a>
          
        </span>
      
    

    
      
      
        <span>
          <i class="fa fa-tags fa-fw"></i>
          
            <a class="post-taxonomy-tag" href="/blogtags/common_schema">common_schema</a>&nbsp;&#47;
          
            <a class="post-taxonomy-tag" href="/blogtags/openark-kit">openark kit</a>&nbsp;&#47;
          
            <a class="post-taxonomy-tag" href="/blogtags/performance">Performance</a>&nbsp;&#47;
          
            <a class="post-taxonomy-tag" href="/blogtags/queryscript">QueryScript</a>
          
        </span>
      
    
  </div>

</div>


  </header>

  <p>
  <p><p>Have just read <a href="http://blog.9minutesnooze.com/insert-delete/">INSERT, Don’t DELETE</a> by Aaron Brown, and have some lengthy response, which is why I write this post instead of commenting on said post.</p>
<p>I wish to offer my counter thought and suggest that <strong>DELETE</strong>s are probably the better choice.</p>
<p>Aaron suggests that, when one wishes to purge rows from some table, a trick can be used: instead of <strong>DELETE</strong>ing unwanted rows, one can <strong>INSERT</strong> &ldquo;good&rdquo; rows into a new table, then switch over with <strong>RENAME</strong> (but please read referenced post for complete details).</p>
<p>I respectfully disagree on several points discussed.</p>
<h4>Lockdown</h4>
<p>The fact one needs to block writes during the time of creation of new table is problematic: you need to essentially turn off parts of your application. The posts suggests one could use a slave - but this solution is far from being trivial as well. To switch over, you yet again need to turn off access to DB, even if for a short while.</p>

  </p>

  
  <footer>
    <a href="/blog/2012/06/27/delete-dont-insert/">Read more<i class="fa fa-angle-double-right fa-fw"></i></a>
  </footer>
  
</article>

  
    <article>
  <header>
    <h2><a href="/blog/2011/11/01/self-throttling-mysql-queries/">Self throttling MySQL queries</a></h2>

    <div class="post-meta">

  <div>

    <time>01 Nov 2011</time>
  </div>

  

  

  <div>
    
      
      
        <span>
          
            <a class="post-taxonomy-category" href="/blogcategories/mysql">MySQL</a>
          
        </span>
      
    

    
      
      
        <span>
          <i class="fa fa-tags fa-fw"></i>
          
            <a class="post-taxonomy-tag" href="/blogtags/hack">Hack</a>&nbsp;&#47;
          
            <a class="post-taxonomy-tag" href="/blogtags/myisam">MyISAM</a>&nbsp;&#47;
          
            <a class="post-taxonomy-tag" href="/blogtags/performance">Performance</a>&nbsp;&#47;
          
            <a class="post-taxonomy-tag" href="/blogtags/sql">SQL</a>&nbsp;&#47;
          
            <a class="post-taxonomy-tag" href="/blogtags/stored-routines">Stored routines</a>
          
        </span>
      
    
  </div>

</div>


  </header>

  <p>
  <p><p>Recap on the problem:</p>
<ul>
<li>A query takes a long time to complete.</li>
<li>During this time it makes for a lot of I/O.</li>
<li>Query&rsquo;s I/O overloads the db, making for other queries run slow.</li>
</ul>
<p>I introduce the notion of self-throttling queries: queries that go to sleep, by themselves, throughout the runtime. The sleep period means the query does not perform I/O at that time, which then means other queries can have their chance to execute.</p>
<p>I present two approaches:</p>
<ul>
<li>The naive approach: for every <strong>1,000</strong> rows, the query sleep for <strong>1</strong> second</li>
<li>The factor approach: for every <strong>1,000</strong> rows, the query sleeps for the amount of time it took to iterate those <strong>1,000</strong> rows (effectively doubling the total runtime of the query).</p>

  </p>

  
  <footer>
    <a href="/blog/2011/11/01/self-throttling-mysql-queries/">Read more<i class="fa fa-angle-double-right fa-fw"></i></a>
  </footer>
  
</article>

  
    <article>
  <header>
    <h2><a href="/blog/2011/02/02/on-generating-unique-ids-using-last_insert_id-and-other-tools/">On generating unique IDs using LAST_INSERT_ID() and other tools</a></h2>

    <div class="post-meta">

  <div>

    <time>02 Feb 2011</time>
  </div>

  

  

  <div>
    
      
      
        <span>
          
            <a class="post-taxonomy-category" href="/blogcategories/mysql">MySQL</a>
          
        </span>
      
    

    
      
      
        <span>
          <i class="fa fa-tags fa-fw"></i>
          
            <a class="post-taxonomy-tag" href="/blogtags/memcached">memcached</a>&nbsp;&#47;
          
            <a class="post-taxonomy-tag" href="/blogtags/nosql">NoSQL</a>&nbsp;&#47;
          
            <a class="post-taxonomy-tag" href="/blogtags/performance">Performance</a>&nbsp;&#47;
          
            <a class="post-taxonomy-tag" href="/blogtags/query-cache">Query Cache</a>
          
        </span>
      
    
  </div>

</div>


  </header>

  <p>
  <p><p>There&rsquo;s a <a href="http://dev.mysql.com/doc/refman/5.1/en/information-functions.html#function_last-insert-id">trick</a> for using <strong>LAST_INSERT_ID()</strong> to generate sequences in MySQL. Quoting from the Manual:</p>
<blockquote>
<ol type="1">
<li>Create a table to hold the sequence counter and initialize               it:
<pre>mysql&gt; <strong><code>CREATE TABLE sequence (id INT NOT NULL);</code></strong>
mysql&gt; <strong><code>INSERT INTO sequence VALUES (0);</code></strong>
</pre>
</li>
<li>Use the table to generate sequence numbers like this:
<pre>mysql&gt; <strong><code>UPDATE sequence SET id=LAST_INSERT_ID(id+1);</code></strong>
mysql&gt; <strong><code>SELECT LAST_INSERT_ID();</code></strong>
</pre>
</li>
</ol>
</blockquote>
<p>This trick calls for trouble.</p>
<h4>Contention</h4>
<p>A customer was using this trick to generate unique session IDs for his JBoss sessions. These IDs would eventually be written back to the database in the form of log events. Business go well, and one day the customer adds three new JBoss servers (doubling the amount of webapps). All of a sudden, nothing works quite as it used to. All kinds of queries take long seconds to complete; load average becomes very high.</p>

  </p>

  
  <footer>
    <a href="/blog/2011/02/02/on-generating-unique-ids-using-last_insert_id-and-other-tools/">Read more<i class="fa fa-angle-double-right fa-fw"></i></a>
  </footer>
  
</article>

  
    <article>
  <header>
    <h2><a href="/blog/2010/10/04/how-often-should-you-use-optimize-table-followup/">How often should you use OPTIMIZE TABLE? - followup</a></h2>

    <div class="post-meta">

  <div>

    <time>04 Oct 2010</time>
  </div>

  

  

  <div>
    
      
      
        <span>
          
            <a class="post-taxonomy-category" href="/blogcategories/mysql">MySQL</a>
          
        </span>
      
    

    
      
      
        <span>
          <i class="fa fa-tags fa-fw"></i>
          
            <a class="post-taxonomy-tag" href="/blogtags/indexing">Indexing</a>&nbsp;&#47;
          
            <a class="post-taxonomy-tag" href="/blogtags/innodb">InnoDB</a>&nbsp;&#47;
          
            <a class="post-taxonomy-tag" href="/blogtags/performance">Performance</a>
          
        </span>
      
    
  </div>

</div>


  </header>

  <p>
  <p><p>This post follows up on Baron&rsquo;s <a href="http://www.xaprb.com/blog/2010/02/07/how-often-should-you-use-optimize-table/">How often should you use OPTIMIZE TABLE?</a>. I had the opportunity of doing some massive purging of data from large tables, and was interested to see the impact of the <strong>OPTIMIZE</strong> operation on table&rsquo;s indexes. I worked on some production data I was authorized to provide as example.</p>
<h4>The use case</h4>
<p>I&rsquo;ll present a single use case here. The table at hand is a compressed InnoDB table used for logs. I&rsquo;ve rewritten some column names for privacy:</p>
<blockquote>
<pre>mysql&gt; show create table logs \G</p>

<p>Create Table: CREATE TABLE <code>logs</code> (
 <code>id</code> int(11) NOT NULL AUTO_INCREMENT,
 <code>name</code> varchar(20) CHARACTER SET ascii COLLATE ascii_bin NOT NULL,
 <code>ts</code> timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
 <code>origin</code> varchar(64) CHARACTER SET ascii COLLATE ascii_bin NOT NULL,
 <code>message</code> text NOT NULL,
 <code>level</code> tinyint(11) NOT NULL DEFAULT &lsquo;0&rsquo;,
 <code>s</code> char(16) CHARACTER SET ascii COLLATE ascii_bin NOT NULL DEFAULT &ldquo;,
 PRIMARY KEY (<code>id</code>),
 KEY <code>s</code> (<code>s</code>),
 KEY <code>name</code> (<code>name</code>,<code>ts</code>),
 KEY <code>origin</code> (<code>origin</code>,<code>ts</code>)
) ENGINE=InnoDB AUTO_INCREMENT=186878729 DEFAULT CHARSET=utf8 ROW_FORMAT=COMPRESSED KEY_BLOCK_SIZE=8</pre>
</blockquote>
<p>The table had log records starting <strong>2010-08-23</strong> and up till <strong>2010-09-02</strong> noon. Table status:</p>

  </p>

  
  <footer>
    <a href="/blog/2010/10/04/how-often-should-you-use-optimize-table-followup/">Read more<i class="fa fa-angle-double-right fa-fw"></i></a>
  </footer>
  
</article>

  
    <article>
  <header>
    <h2><a href="/blog/2010/05/20/views-better-performance-with-condition-pushdown/">Views: better performance with condition pushdown</a></h2>

    <div class="post-meta">

  <div>

    <time>20 May 2010</time>
  </div>

  

  

  <div>
    
      
      
        <span>
          
            <a class="post-taxonomy-category" href="/blogcategories/mysql">MySQL</a>
          
        </span>
      
    

    
      
      
        <span>
          <i class="fa fa-tags fa-fw"></i>
          
            <a class="post-taxonomy-tag" href="/blogtags/execution-plan">Execution plan</a>&nbsp;&#47;
          
            <a class="post-taxonomy-tag" href="/blogtags/performance">Performance</a>&nbsp;&#47;
          
            <a class="post-taxonomy-tag" href="/blogtags/sql">SQL</a>&nbsp;&#47;
          
            <a class="post-taxonomy-tag" href="/blogtags/stored-routines">Stored routines</a>&nbsp;&#47;
          
            <a class="post-taxonomy-tag" href="/blogtags/syntax">Syntax</a>
          
        </span>
      
    
  </div>

</div>


  </header>

  <p>
  <p><p>Justin&rsquo;s <a href="http://www.mysqlperformanceblog.com/2010/05/19/a-workaround-for-the-performance-problems-of-temptable-views/">A workaround for the performance problems of TEMPTABLE views</a> post on <a href="http://www.mysqlperformanceblog.com/">mysqlperformanceblog.com</a> reminded me of a solution I once saw on a customer&rsquo;s site.</p>
<p>The customer was using nested views structure, up to depth of some 8-9 views. There were a lot of aggregations along the way, and even the simplest query resulted with a LOT of subqueries, temporary tables, and vast amounts of data, even if only to return with a couple of rows.</p>
<p>While we worked to solve this, a developer showed me his own trick. His trick is now impossible to implement, but there&rsquo;s a hack around this.</p>
<p>Let&rsquo;s use the world database to illustrate. Look at the following view definition:</p>

  </p>

  
  <footer>
    <a href="/blog/2010/05/20/views-better-performance-with-condition-pushdown/">Read more<i class="fa fa-angle-double-right fa-fw"></i></a>
  </footer>
  
</article>

  
    <article>
  <header>
    <h2><a href="/blog/2010/05/04/reducing-locks-by-narrowing-primary-key/">Reducing locks by narrowing primary key</a></h2>

    <div class="post-meta">

  <div>

    <time>04 May 2010</time>
  </div>

  

  

  <div>
    
      
      
        <span>
          
            <a class="post-taxonomy-category" href="/blogcategories/mysql">MySQL</a>
          
        </span>
      
    

    
      
      
        <span>
          <i class="fa fa-tags fa-fw"></i>
          
            <a class="post-taxonomy-tag" href="/blogtags/indexing">Indexing</a>&nbsp;&#47;
          
            <a class="post-taxonomy-tag" href="/blogtags/innodb">InnoDB</a>&nbsp;&#47;
          
            <a class="post-taxonomy-tag" href="/blogtags/performance">Performance</a>
          
        </span>
      
    
  </div>

</div>


  </header>

  <p>
  <p><p>In a period of two weeks, I had two cases with the exact same symptoms.</p>
<p>Database users were experiencing low responsiveness. DBAs were seeing locks occurring on seemingly normal tables. In particular, looking at Innotop, it seemed that <strong>INSERT</strong>s were causing the locks.</p>
<p>In both cases, tables were InnoDB. In both cases, there was a <strong>PRIMARY KEY</strong> on the combination of all <strong>5</strong> columns. And in both cases, there was no clear explanation as for why the <strong>PRIMARY KEY</strong> was chosen as such.</p>
<h4>Choosing a proper PRIMARY KEY</h4>
<p>Especially with InnoDB, which uses clustered index structure, the <strong>PRIMARY KEY</strong> is of particular importance. Besides the fact that a bloated <strong>PRIMARY KEY</strong> bloats the entire clustered index and secondary keys (see: <a href="http://code.openark.org/blog/mysql/the-depth-of-an-index-primer">The depth of an index: primer</a>), it is also a source for locks. It&rsquo;s true that any <strong>UNIQUE KEY</strong> can serve as a <strong>PRIMARY KEY</strong>. But not all such keys are good candidates.</p>

  </p>

  
  <footer>
    <a href="/blog/2010/05/04/reducing-locks-by-narrowing-primary-key/">Read more<i class="fa fa-angle-double-right fa-fw"></i></a>
  </footer>
  
</article>

  
    <article>
  <header>
    <h2><a href="/blog/2010/01/14/misimproving-performance-problems-with-insert-delayed/">Misimproving performance problems with INSERT DELAYED</a></h2>

    <div class="post-meta">

  <div>

    <time>14 Jan 2010</time>
  </div>

  

  

  <div>
    
      
      
        <span>
          
            <a class="post-taxonomy-category" href="/blogcategories/mysql">MySQL</a>
          
        </span>
      
    

    
      
      
        <span>
          <i class="fa fa-tags fa-fw"></i>
          
            <a class="post-taxonomy-tag" href="/blogtags/performance">Performance</a>&nbsp;&#47;
          
            <a class="post-taxonomy-tag" href="/blogtags/sql">SQL</a>
          
        </span>
      
    
  </div>

</div>


  </header>

  <p>
  INSERT DELAYED may come in handy when using MyISAM tables. It may in particular be useful for log tables, where one is required to issue frequent INSERTs on one hand, but does not usually want or need to wait for DB response on the other hand. It may even offer some performance boost, by aggregating such frequent INSERTs in a single thread. But it is NOT a performance solution. That is, in a case I've seen, database performance was poor.
  </p>

  
  <footer>
    <a href="/blog/2010/01/14/misimproving-performance-problems-with-insert-delayed/">Read more<i class="fa fa-angle-double-right fa-fw"></i></a>
  </footer>
  
</article>

  
    <article>
  <header>
    <h2><a href="/blog/2009/12/01/on-restoring-a-single-table-from-mysqldump/">On restoring a single table from mysqldump</a></h2>

    <div class="post-meta">

  <div>

    <time>01 Dec 2009</time>
  </div>

  

  

  <div>
    
      
      
        <span>
          
            <a class="post-taxonomy-category" href="/blogcategories/mysql">MySQL</a>
          
        </span>
      
    

    
      
      
        <span>
          <i class="fa fa-tags fa-fw"></i>
          
            <a class="post-taxonomy-tag" href="/blogtags/backup">Backup</a>&nbsp;&#47;
          
            <a class="post-taxonomy-tag" href="/blogtags/books">Books</a>&nbsp;&#47;
          
            <a class="post-taxonomy-tag" href="/blogtags/mysqldump">mysqldump</a>&nbsp;&#47;
          
            <a class="post-taxonomy-tag" href="/blogtags/performance">Performance</a>&nbsp;&#47;
          
            <a class="post-taxonomy-tag" href="/blogtags/scripts">scripts</a>
          
        </span>
      
    
  </div>

</div>


  </header>

  <p>
  Following Restore one table from an ALL database dump and Restore a Single Table From mysqldump, I would like to add my own thoughts and comments on the subject. I also wish to note performance issues with the two suggested solutions, and offer improvements. Problem relevance While the problem is interesting, I just want to note that it is relevant in very specific database dimensions. Too small - and it doesn't matter how you solve it (e.g.
  </p>

  
  <footer>
    <a href="/blog/2009/12/01/on-restoring-a-single-table-from-mysqldump/">Read more<i class="fa fa-angle-double-right fa-fw"></i></a>
  </footer>
  
</article>

  
    <article>
  <header>
    <h2><a href="/blog/2009/11/12/performance-analysis-with-mycheckpoint/">Performance analysis with mycheckpoint</a></h2>

    <div class="post-meta">

  <div>

    <time>12 Nov 2009</time>
  </div>

  

  

  <div>
    
      
      
        <span>
          
            <a class="post-taxonomy-category" href="/blogcategories/mysql">MySQL</a>
          
        </span>
      
    

    
      
      
        <span>
          <i class="fa fa-tags fa-fw"></i>
          
            <a class="post-taxonomy-tag" href="/blogtags/analysis">Analysis</a>&nbsp;&#47;
          
            <a class="post-taxonomy-tag" href="/blogtags/innodb">InnoDB</a>&nbsp;&#47;
          
            <a class="post-taxonomy-tag" href="/blogtags/monitoring">Monitoring</a>&nbsp;&#47;
          
            <a class="post-taxonomy-tag" href="/blogtags/mycheckpoint">mycheckpoint</a>&nbsp;&#47;
          
            <a class="post-taxonomy-tag" href="/blogtags/performance">Performance</a>
          
        </span>
      
    
  </div>

</div>


  </header>

  <p>
  <p><p><a href="http://code.openark.org/forge/mycheckpoint">mycheckpoint</a> (see <a href="http://code.openark.org/blog/mysql/announcing-mycheckpoint-lightweight-sql-oriented-monitoring-for-mysql">announcement</a>) allows for both graph presentation and quick SQL access to monitored &amp; analyzed data. I&rsquo;d like to show the power of combining them both.</p>
<h4>InnoDB performance</h4>
<p>Taking a look at one of the most important InnoDB metrics: the read hit ratio (we could get the same graph by looking at the <a href="http://code.openark.org/forge/mycheckpoint/documentation/generating-html-reports">HTML report</a>):</p>
<blockquote>
<pre>SELECT innodb_read_hit_percent FROM sv_report_chart_sample \G
*************************** 1. row ***************************
innodb_read_hit_percent: <a href="http://chart.apis.google.com/chart?cht=lc&amp;chs=400x200&amp;chts=303030,12&amp;chtt=Nov+10,+11:40++-++Nov+11,+08:55+(0+days,+21+hours)&amp;chdl=innodb_read_hit_percent&amp;chdlp=b&amp;chco=ff8c00&amp;chd=s:400664366P6674y7176677677u467773y64ux166666764366646y616666666666644444434444s6u4S331444404433341334433646777666666074736777r1777767764776666F667777617777777777777777yaRi776776mlf667676xgx776766rou67767777u37797777x76676776u6A737464y67467761777666643u66446&amp;chxt=x,y&amp;chxr=1,99.60,100.00&amp;chxl=0:||Nov+10,+15:55|Nov+10,+20:10|Nov+11,+00:25|Nov+11,+04:40|&amp;chxs=0,505050,10">http://chart.apis.google.com/chart?cht=lc&amp;chs=400x200&amp;chts=303030,12&amp;chtt=Nov+10,+11:40++-++Nov+11,+08:55+(0+days,+21+hours)&amp;chdl=innodb_read_hit_percent&amp;chdlp=b&amp;chco=ff8c00&amp;chd=s:400664366P6674y7176677677u467773y64ux166666764366646y616666666666644444434444s6u4S331444404433341334433646777666666074736777r1777767764776666F667777617777777777777777yaRi776776mlf667676xgx776766rou67767777u37797777x76676776u6A737464y67467761777666643u66446&amp;chxt=x,y&amp;chxr=1,99.60,100.00&amp;chxl=0:||Nov+10,+15:55|Nov+10,+20:10|Nov+11,+00:25|Nov+11,+04:40|&amp;chxs=0,505050,10</a></pre>
</blockquote>
<blockquote>
<pre /><img class="alignnone" title="innodb_read_hit_percent" src="/blog/assets/chart?cht=lc&amp;chs=400x200&amp;chts=303030,12&amp;chtt=Nov+10,+11:40++-++Nov+11,+08:55+(0+days,+21+hours)&amp;chdl=innodb_read_hit_percent&amp;chdlp=b&amp;chco=ff8c00&amp;chd=s:400664366P6674y7176677677u467773y64ux166666764366646y616666666666644444434444s6u4S331444404433341334433646777666666074736777r1777767764776666F667777617777777777777777yaRi776776mlf667676xgx776766rou67767777u37797777x76676776u6A737464y67467761777666643u66446&amp;chxt=x,y&amp;chxr=1,99.60,100.00&amp;chxl=0:||Nov+10,+15:55|Nov+10,+20:10|Nov+11,+00:25|Nov+11,+04:40|&amp;chxs=0,505050,10" alt="" width="400" height="200" />
</blockquote>
We see that read hit is usually high, but occasionally drops low, down to 99.7, or even 99.6. But it seems like most of the time we are above 99.95% read hit ratio. It&rsquo;s hard to tell about 99.98%.
<h4>Can we know for sure?</h4>
We can stress our eyes, yet be certain of little. It&rsquo;s best if we just query for the metrics! <em>mycheckpoint</em> provides with all data, accessible by simple SQL queries:</p>

  </p>

  
  <footer>
    <a href="/blog/2009/11/12/performance-analysis-with-mycheckpoint/">Read more<i class="fa fa-angle-double-right fa-fw"></i></a>
  </footer>
  
</article>

  
    <article>
  <header>
    <h2><a href="/blog/2009/10/20/how-to-calculate-a-good-innodb-log-file-size-recap/">How to calculate a good InnoDB log file size - recap</a></h2>

    <div class="post-meta">

  <div>

    <time>20 Oct 2009</time>
  </div>

  

  

  <div>
    
      
      
        <span>
          
            <a class="post-taxonomy-category" href="/blogcategories/mysql">MySQL</a>
          
        </span>
      
    

    
      
      
        <span>
          <i class="fa fa-tags fa-fw"></i>
          
            <a class="post-taxonomy-tag" href="/blogtags/information_schema">INFORMATION_SCHEMA</a>&nbsp;&#47;
          
            <a class="post-taxonomy-tag" href="/blogtags/innodb">InnoDB</a>&nbsp;&#47;
          
            <a class="post-taxonomy-tag" href="/blogtags/performance">Performance</a>&nbsp;&#47;
          
            <a class="post-taxonomy-tag" href="/blogtags/sql">SQL</a>
          
        </span>
      
    
  </div>

</div>


  </header>

  <p>
  <p><p>Following Baron Schwartz&rsquo; post: <a href="http://www.mysqlperformanceblog.com/2008/11/21/how-to-calculate-a-good-innodb-log-file-size/">How to calculate a good InnoDB log file size</a>, which shows how to make an estimate for the InnoDB log file size, and based on <a href="http://code.openark.org/blog/mysql/sql-querying-for-status-difference-over-time">SQL: querying for status difference over time</a>, I&rsquo;ve written a query to run on MySQL 5.1, which, upon sampling 60 seconds of status, estimates the InnoDB transaction log bytes that are expected to be written in the period of 1 hour.</p>
<p><em>Recap</em>: this information can be useful if you&rsquo;re looking for a good <strong>innodb_log_file_size</strong> value, such that will not pose too much I/O (smaller values will make for more frequent flushes), not will make for a too long recovery time (larger values mean more transactions to recover upon crash).</p>
<p>It is assumed that the 60 seconds period represents an average system load, not some activity spike period. Edit the sleep time and factors as you will to sample longer or shorter periods.</p>

  </p>

  
  <footer>
    <a href="/blog/2009/10/20/how-to-calculate-a-good-innodb-log-file-size-recap/">Read more<i class="fa fa-angle-double-right fa-fw"></i></a>
  </footer>
  
</article>

  
    <article>
  <header>
    <h2><a href="/blog/2009/09/27/high-performance-mysql-a-book-to-re-read/">High Performance MySQL - a book to re-read</a></h2>

    <div class="post-meta">

  <div>

    <time>27 Sep 2009</time>
  </div>

  

  

  <div>
    
      
      
        <span>
          
            <a class="post-taxonomy-category" href="/blogcategories/mysql">MySQL</a>
          
        </span>
      
    

    
      
      
        <span>
          <i class="fa fa-tags fa-fw"></i>
          
            <a class="post-taxonomy-tag" href="/blogtags/books">Books</a>&nbsp;&#47;
          
            <a class="post-taxonomy-tag" href="/blogtags/performance">Performance</a>
          
        </span>
      
    
  </div>

</div>


  </header>

  <p>
  I first read High Performance MySQL, 2nd edition about a year ago, when it first came out. I since re-read a few pages on occasion. In my previous posts I've suggested ways to improve upon the common ranking solution. Very innovative stuff! Or... so I thought. I happened to browse through the book today, and a section on User Variables caught my eye. "Let's see if I get get some insight", I thought to myself.
  </p>

  
  <footer>
    <a href="/blog/2009/09/27/high-performance-mysql-a-book-to-re-read/">Read more<i class="fa fa-angle-double-right fa-fw"></i></a>
  </footer>
  
</article>

  
    <article>
  <header>
    <h2><a href="/blog/2009/09/10/innodb-is-dead-long-live-innodb/">InnoDB is dead. Long live InnoDB!</a></h2>

    <div class="post-meta">

  <div>

    <time>10 Sep 2009</time>
  </div>

  

  

  <div>
    
      
      
        <span>
          
            <a class="post-taxonomy-category" href="/blogcategories/mysql">MySQL</a>
          
        </span>
      
    

    
      
      
        <span>
          <i class="fa fa-tags fa-fw"></i>
          
            <a class="post-taxonomy-tag" href="/blogtags/innodb">InnoDB</a>&nbsp;&#47;
          
            <a class="post-taxonomy-tag" href="/blogtags/performance">Performance</a>
          
        </span>
      
    
  </div>

</div>


  </header>

  <p>
  <p><p>I find myself converting more and more customers&rsquo; databases to InnoDB plugin. In one case, it was a last resort: disk space was running out, and plugin&rsquo;s compression released 75% space; in another, a slow disk made for IO bottlenecks, and plugin&rsquo;s improvements &amp; compression alleviated the problem; in yet another, I used the above to fight replication lag on a stubborn slave.</p>
<p>In all those case, I needed to justify the move to &ldquo;new technology&rdquo;. The questions &ldquo;Is it GA? Is it stable?&rdquo; are being asked a lot. Well, just a few days ago the MySQL 5.1 distribution started shipping with InnoDB plugin 1.0.4. That gives some weight to the stability question when facing a doubtful customer.</p>
<p>But I realized <em>that wasn&rsquo;t the point</em>.</p>
<p></p>

  </p>

  
  <footer>
    <a href="/blog/2009/09/10/innodb-is-dead-long-live-innodb/">Read more<i class="fa fa-angle-double-right fa-fw"></i></a>
  </footer>
  
</article>

  
    <article>
  <header>
    <h2><a href="/blog/2009/05/26/sql-finding-a-users-countryregion-based-on-ip/">SQL: finding a user&#39;s country/region based on IP</a></h2>

    <div class="post-meta">

  <div>

    <time>26 May 2009</time>
  </div>

  

  

  <div>
    
      
      
        <span>
          
            <a class="post-taxonomy-category" href="/blogcategories/mysql">MySQL</a>
          
        </span>
      
    

    
      
      
        <span>
          <i class="fa fa-tags fa-fw"></i>
          
            <a class="post-taxonomy-tag" href="/blogtags/indexing">Indexing</a>&nbsp;&#47;
          
            <a class="post-taxonomy-tag" href="/blogtags/performance">Performance</a>&nbsp;&#47;
          
            <a class="post-taxonomy-tag" href="/blogtags/sql">SQL</a>
          
        </span>
      
    
  </div>

</div>


  </header>

  <p>
  <p><p>I&rsquo;ve encountered the same problem twice for different customers, so I guess it&rsquo;s worth a discussion.</p>
<p>A common task for web applications is to find out the country/region of a user, based on her IP address, as can be detected in the HTTP request. Depending on the country of origin, the website can translate dates for different time zones, can change locale settings, and, perhaps most commonly, show advertisements in her native language.</p>
<p>To start with, there&rsquo;s a table which lists the IP ranges per country/region. Let&rsquo;s assume we&rsquo;re only dealing with IPv4:</p>
<blockquote>
<pre>CREATE TABLE regions_ip_range (
  regions_ip_range_id INT UNSIGNED AUTO_INCREMENT,
  country VARCHAR(64) CHARSET utf8,
  region VARCHAR(64) CHARSET utf8,
  start_ip INT UNSIGNED,
  end_ip INT UNSIGNED,
  …
  PRIMARY KEY(regions_ip_range_id),
  &hellip;
);</pre>
</blockquote>
<p>The table is fixed, and is populated. Now the question arises: how do we query this table, and which indexes should be created?</p>
<h4>The wrong way</h4>
<p>The form I&rsquo;ve encountered is as follows: an index is declared on regions_ip_range:</p>
<blockquote>
<pre>KEY ip_range_idx (start_ip, end_ip)</pre>
</blockquote>
<p>And the query goes like this:</p>
<blockquote>
<pre>SELECT * FROM regions_ip_range
WHERE my_ip BETWEEN start_ip AND end_ip</pre>
</blockquote>
<p></p>

  </p>

  
  <footer>
    <a href="/blog/2009/05/26/sql-finding-a-users-countryregion-based-on-ip/">Read more<i class="fa fa-angle-double-right fa-fw"></i></a>
  </footer>
  
</article>

  
    <article>
  <header>
    <h2><a href="/blog/2009/05/21/reasons-to-use-innodb_file_per_table/">Reasons to use innodb_file_per_table</a></h2>

    <div class="post-meta">

  <div>

    <time>21 May 2009</time>
  </div>

  

  

  <div>
    
      
      
        <span>
          
            <a class="post-taxonomy-category" href="/blogcategories/mysql">MySQL</a>
          
        </span>
      
    

    
      
      
        <span>
          <i class="fa fa-tags fa-fw"></i>
          
            <a class="post-taxonomy-tag" href="/blogtags/configuration">Configuration</a>&nbsp;&#47;
          
            <a class="post-taxonomy-tag" href="/blogtags/innodb">InnoDB</a>&nbsp;&#47;
          
            <a class="post-taxonomy-tag" href="/blogtags/mysqldump">mysqldump</a>&nbsp;&#47;
          
            <a class="post-taxonomy-tag" href="/blogtags/performance">Performance</a>
          
        </span>
      
    
  </div>

</div>


  </header>

  <p>
  <p><p>When working with InnoDB, you have two ways for managing the tablespace storage:</p>
<ol>
<li>Throw everything in one big file (optionally split).</li>
<li>Have one file per table.</li>
</ol>
<p>I will discuss the advantages and disadvantages of the two options, and will strive to convince that <strong>innodb_file_per_table</strong> is preferable.</p>
<h4>A single tablespace</h4>
<p>Having everything in one big file means all tables and indexes, from <em>all schemes</em>, are &lsquo;mixed&rsquo; together in that file.</p>
<p>This allows for the following nice property: free space can be shared between different tables and different schemes. Thus, if I purge many rows from my <strong>log</strong> table, the now unused space can be occupied by new rows of any other table.</p>
<p>This same nice property also translates to a not so nice one: data can be greatly fragmented across the tablespace.</p>
<p>An annoying property of InnoDB&rsquo;s tablespaces is that they never shrink. So after purging those rows from the <strong>log</strong> table, the tablespace file (usually <strong>ibdata1</strong>) still keeps the same storage. It does not release storage to the file system.</p>
<p>I&rsquo;ve seen more than once how certain tables are left unwatched, growing until disk space reaches 90% and SMS notifications start beeping all around.</p>

  </p>

  
  <footer>
    <a href="/blog/2009/05/21/reasons-to-use-innodb_file_per_table/">Read more<i class="fa fa-angle-double-right fa-fw"></i></a>
  </footer>
  
</article>

  
</div>

</div>
</div>
<script src="/blogjs/ui.js"></script>


<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'Your Google Analytics tracking ID', 'auto');
  ga('send', 'pageview');

</script>



</body>
</html>
