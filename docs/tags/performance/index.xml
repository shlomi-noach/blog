<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Performance on code.openark.org</title>
    <link>/blog/tags/performance/</link>
    <description>Recent content in Performance on code.openark.org</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2016. All rights reserved.</copyright>
    <lastBuildDate>Mon, 01 Aug 2016 19:19:00 +0000</lastBuildDate>
    <atom:link href="/blog/tags/performance/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Introducing gh-ost: triggerless online schema migrations</title>
      <link>/blog/2016/08/01/introducing-gh-ost-triggerless-online-schema-migrations/</link>
      <pubDate>Mon, 01 Aug 2016 19:19:00 +0000</pubDate>
      
      <guid>/blog/2016/08/01/introducing-gh-ost-triggerless-online-schema-migrations/</guid>
      <description>&lt;p&gt;I&#39;m thoroughly happy to introduce &lt;a href=&#34;https://github.com/github/gh-ost&#34;&gt;&lt;strong&gt;gh-ost&lt;/strong&gt;&lt;/a&gt;: triggerless, controllable, auditable, testable, trusted online schema change tool &lt;a href=&#34;http://githubengineering.com/gh-ost-github-s-online-migration-tool-for-mysql/&#34;&gt;released today by GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;gh-ost&lt;/em&gt; now powers our production schema migrations. We hit some serious limitations using &lt;a href=&#34;https://www.percona.com/doc/percona-toolkit/2.2/pt-online-schema-change.html&#34;&gt;pt-online-schema-change&lt;/a&gt; on our large volume, high traffic tables, to the effect of driving our database to a near grinding halt or even to the extent of causing outages. With &lt;em&gt;gh-ost&lt;/em&gt;, we are now able to migrate our busiest tables at any time, peak hours and heavy workloads included, without causing impact to our service.&lt;/p&gt;
&lt;p&gt;gh-ost supports testing in production. It goes a long way to build trust, both in integrity and in control. Are your databases just too busy and you cannot run existing online-schema-change tools? Have you suffered outages due to migrations? Are you tired of babysitting migrations that run up to 3:00am? Tired of being the only one tailing logs? Please, take a look at &lt;em&gt;gh-ost&lt;/em&gt;. I believe it changes online migration paradigm.&lt;/p&gt;
&lt;p&gt;For a more thorough overview, please read the &lt;a href=&#34;http://githubengineering.com/gh-ost-github-s-online-migration-tool-for-mysql/&#34;&gt;announcement&lt;/a&gt; on the GitHub Engineering Blog, and proceed to the &lt;a href=&#34;https://github.com/github/gh-ost/blob/master/README.md&#34;&gt;documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;gh-ost&lt;/em&gt; is open sourced under the MIT license.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>The mystery of MySQL 5.6 excessive buffer pool flushing</title>
      <link>/blog/2014/04/20/the-mystery-of-mysql-5-6-excessive-buffer-pool-flushing/</link>
      <pubDate>Sun, 20 Apr 2014 07:16:13 +0000</pubDate>
      
      <guid>/blog/2014/04/20/the-mystery-of-mysql-5-6-excessive-buffer-pool-flushing/</guid>
      <description>&lt;p&gt;I&#39;m experimenting with upgrading to MySQL &lt;strong&gt;5.6&lt;/strong&gt; and am experiencing an unexplained increase in disk I/O utilization. After discussing this with several people I&#39;m publishing in the hope that someone has an enlightenment on this.&lt;/p&gt;
&lt;p&gt;We have a few dozens servers in a normal replication topology. On this particular replication topology we&#39;ve already evaluated that &lt;strong&gt;STATEMENT&lt;/strong&gt; based replication is faster than &lt;strong&gt;ROW&lt;/strong&gt; based replication, and so we use &lt;strong&gt;SBR&lt;/strong&gt;. We have two different workloads on our slaves, applied by two different HAProxy groups, on three different data centres. Hardware-wise, servers of two groups use either Virident SSD cards or normal SAS spindle disks.&lt;/p&gt;
&lt;p&gt;Our servers are I/O bound. A common query used by both workloads looks up data that does not necessarily have a hotspot, and is very large in volume. DML is low, and we only have a few hundred statements per second executed on master (and propagated through replication).&lt;/p&gt;
&lt;p&gt;We have upgraded &lt;strong&gt;6&lt;/strong&gt; servers from all datacenters to &lt;strong&gt;5.6&lt;/strong&gt;, both on SSD and spindle disks, and are experiencing the following phenomena:&lt;!--more--&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A substantial increase in disk I/O utilization. See a &lt;strong&gt;10&lt;/strong&gt; day breakdown (upgrade is visible on &lt;strong&gt;04/14&lt;/strong&gt;) this goes on like this many days later:&lt;br /&gt;
&lt;blockquote&gt;&lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2014/04/5.5-to-5.6-disk-utilization-10-days.png&#34;&gt;&lt;img class=&#34;alignnone wp-image-6845 size-full&#34; src=&#34;/blog/blog/assets/5.5-to-5.6-disk-utilization-10-days.png&#34; alt=&#34;5.5-to-5.6-disk-utilization-10-days&#34; width=&#34;700&#34; height=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;A substantial increase in InnoDB buffer pool pages flush: Mr. Blue is our newly upgraded server; it joins Mr. Green upgraded a couple weeks ago. Mr. Red is still &lt;strong&gt;5.5&lt;/strong&gt;. This is the only MySQL graph that I could directly relate to the increase in I/O:&lt;br /&gt;
&lt;blockquote&gt;&lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2014/04/5.5-to-5.6-rise-in-innodb-buffer-pool-pages-flushed.png&#34;&gt;&lt;img class=&#34;alignnone size-full wp-image-6848&#34; src=&#34;/blog/blog/assets/5.5-to-5.6-rise-in-innodb-buffer-pool-pages-flushed.png&#34; alt=&#34;5.5-to-5.6-rise-in-innodb-buffer-pool-pages-flushed&#34; width=&#34;700&#34; height=&#34;350&#34; /&gt;&lt;/a&gt;&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;No change in workload (it takes some 60 minutes for caches to warm up, so measuring after that time). Same equal share of serving as dictated by HAProxy. Same amount of queries. Same amount of everything.&lt;/li&gt;
&lt;li&gt;Faster replication speed, on single thread - that&#39;s the good part! We see &lt;strong&gt;30%&lt;/strong&gt; and more improvement in replication speed. Tested by stopping &lt;strong&gt;SLAVE SQL_THREAD&lt;/strong&gt; for a number of pre-defined minutes, then measuring time it took for slave to catch up, up to 10 seconds lag. The results vary depending on the time of day and serving workload on slaves, but it is &lt;em&gt;consistently far faster&lt;/em&gt; with &lt;strong&gt;5.6&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The faster replication speed motivates us to continue with the experiment, and is of a significant factor in our decision. However we are concerned about the I/O utilization and excessive flushing.&lt;/p&gt;
&lt;p&gt;The above graphs depict the &lt;strong&gt;5.6&lt;/strong&gt; status without any configuration changes as compared to &lt;strong&gt;5.5&lt;/strong&gt;. I took some days to reconfigure the following variables, with no change to the rate of flushed pages (though some changes visible in double-wite buffer writes):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;innodb_log_file_size=128M/2G&lt;/li&gt;
&lt;li&gt;innodb_adaptive_flushing:=0/1&lt;/li&gt;
&lt;li&gt;innodb_adaptive_flushing_lwm:=0/70&lt;/li&gt;
&lt;li&gt;innodb_max_dirty_pages_pct := 75/90&lt;/li&gt;
&lt;li&gt;innodb_flush_neighbors:=0/1&lt;/li&gt;
&lt;li&gt;innodb_max_dirty_pages_pct_lwm:=75/90&lt;/li&gt;
&lt;li&gt;innodb_old_blocks_time:=0/1000&lt;/li&gt;
&lt;li&gt;innodb_io_capacity:=50/100/200&lt;/li&gt;
&lt;li&gt;innodb_io_capacity_max:=50/100/1000&lt;/li&gt;
&lt;li&gt;relay_log_info_repository:=&#39;table&#39;/&#39;file&#39;&lt;/li&gt;
&lt;li&gt;master_info_repository:=&#39;table&#39;/&#39;file&#39;&lt;/li&gt;
&lt;li&gt;default_tmp_storage_engine:=&#39;myisam&#39;/&#39;innodb&#39;&lt;/li&gt;
&lt;li&gt;eq_range_index_dive_limit:=0/10&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And more... Have done patient one-by-one or combinations of the above where it made sense. As you see I began with the usual suspects and moved on to more esoteric stuff. I concentrated on new variables introduced in &lt;strong&gt;5.6&lt;/strong&gt;, or ones where the defaults have changed, or ones we have explicitly changed the defaults from.&lt;/p&gt;
&lt;p&gt;The above is consistent on all upgraded servers. On SSD the disk utilization is lower, but still concerning.&lt;/p&gt;
&lt;p&gt;Our use case is very different from the one &lt;a href=&#34;http://yoshinorimatsunobu.blogspot.co.il/2013/12/single-thread-performance-regression-in.html&#34;&gt;presented by Yoshinori Matsunobu&lt;/a&gt;. and apparently not too many have experienced upgrading to &lt;strong&gt;5.6&lt;/strong&gt;. I&#39;m hoping someone might shed some light.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>TokuDB configuration variables of interest</title>
      <link>/blog/2013/10/23/tokudb-configuration-variables-of-interest/</link>
      <pubDate>Wed, 23 Oct 2013 19:42:12 +0000</pubDate>
      
      <guid>/blog/2013/10/23/tokudb-configuration-variables-of-interest/</guid>
      <description>&lt;p&gt;During our experiments I came upon a few TokuDB variables of interest; if you are using TokuDB you might want to look into these:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h4&gt;tokudb_analyze_time&lt;/h4&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p style=&#34;padding-left: 30px;&#34;&gt;This is a boundary on the number of seconds an &lt;strong&gt;ANALYZE TABLE&lt;/strong&gt; will operate on each index on each partition on a TokuDB table.&lt;/p&gt;
&lt;p style=&#34;padding-left: 30px;&#34;&gt;That is, if &lt;strong&gt;tokudb_analyze_time = 5&lt;/strong&gt;, and your table has &lt;strong&gt;4&lt;/strong&gt; indexes (including &lt;strong&gt;PRIMARY&lt;/strong&gt;) and &lt;strong&gt;7&lt;/strong&gt; partitions, then the total runtime is limited to &lt;strong&gt;5*4*7 = 140&lt;/strong&gt; seconds.&lt;/p&gt;
&lt;p style=&#34;padding-left: 30px;&#34;&gt;Default in &lt;strong&gt;7.1.0&lt;/strong&gt;: &lt;strong&gt;5&lt;/strong&gt; seconds&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h4&gt;tokudb_cache_size&lt;/h4&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p style=&#34;padding-left: 30px;&#34;&gt;Similar to &lt;strong&gt;innodb_buffer_pool_size&lt;/strong&gt;, this variable sets the amount of memory allocated by TokuDB for caching pages. Like InnoDB the table is clustered within the index, so the cache includes pages for both indexes and data.&lt;/p&gt;
&lt;p style=&#34;padding-left: 30px;&#34;&gt;Default: &lt;strong&gt;50%&lt;/strong&gt; of total memory&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h4&gt;tokudb_directio&lt;/h4&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p style=&#34;padding-left: 30px;&#34;&gt;Boolean, values are &lt;strong&gt;0/1&lt;/strong&gt;. Setting &lt;strong&gt;tokudb_directio = 1&lt;/strong&gt; is like specifying &lt;strong&gt;innodb_flush_method = O_DIRECT&lt;/strong&gt;. Which in turn means the OS should not cache pages requested by TokuDB. Default: &lt;strong&gt;0&lt;/strong&gt;.&lt;/p&gt;
&lt;p style=&#34;padding-left: 30px;&#34;&gt;Now here&#39;s the interesting part: we are used to tell InnoDB to get the most memory we can provide (because we want it to cache as much as it can) and to avoid OS caching (because that would mean a page would appear both in the buffer pool and in OS memory, which is a waste). So the following setup is common:&lt;!--more--&gt;&lt;/p&gt;
&lt;blockquote style=&#34;padding-left: 30px;&#34;&gt;
&lt;pre style=&#34;padding-left: 30px;&#34;&gt;&lt;strong&gt;innodb_buffer_pool_size&lt;/strong&gt; = [as much as you can allocate while leaving room for connection memory]G
&lt;strong&gt;innodb_flush_method&lt;/strong&gt; = O_DIRECT&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p style=&#34;padding-left: 30px;&#34;&gt;And my first instinct was to do the same for TokuDB. But after speaking to Gerry Narvaja of Tokutek, I realized it was not that simple. The reason TokuDB&#39;s default memory allocation is &lt;strong&gt;50%&lt;/strong&gt; and not, say, &lt;strong&gt;90%&lt;/strong&gt;, is that OS cache caches the data in compressed form, while TokuDB cache caches data in uncompressed form. Which means if you limit the TokuDB cache, you allow for more cache to the OS, that is used to cache compressed data, which means &lt;em&gt;more data&lt;/em&gt; (hopefully, pending duplicates) in memory.&lt;/p&gt;
&lt;p style=&#34;padding-left: 30px;&#34;&gt;I did try both options and did not see an obvious difference, but did not test this thoroughly. My current setup is:&lt;/p&gt;
&lt;blockquote style=&#34;padding-left: 30px;&#34;&gt;
&lt;pre style=&#34;padding-left: 30px;&#34;&gt;&lt;strong&gt;#No setup. just keep to the default for both:&lt;/strong&gt;
#tokudb_cache_size
#tokudb_directio&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h4&gt;tokudb_commit_sync&lt;/h4&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h4&gt;tokudb_fsync_log_period&lt;/h4&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p style=&#34;padding-left: 30px;&#34;&gt;These two variable are similar in essence to &lt;strong&gt;innodb_flush_log_at_trx_commit&lt;/strong&gt;, but allow for finer tuning. With &lt;strong&gt;innodb_flush_log_at_trx_commit&lt;/strong&gt; you choose between syncing the transaction log to disk upon each commit and once per second. With &lt;strong&gt;tokudb_commit_sync = 1&lt;/strong&gt; (which is default) you get transaction log sync to disk per commit. When &lt;strong&gt;tokudb_commit_sync = 0&lt;/strong&gt;, then &lt;strong&gt;tokudb_fsync_log_period&lt;/strong&gt; dictates the interval between flushes. So a value of &lt;strong&gt;tokudb_fsync_log_period = 1000&lt;/strong&gt; means once per second.&lt;/p&gt;
&lt;p style=&#34;padding-left: 30px;&#34;&gt;Since our original InnoDB installation used &lt;strong&gt;innodb_flush_log_at_trx_commit = 2&lt;/strong&gt;, our TokuDB setup is:&lt;/p&gt;
&lt;blockquote style=&#34;padding-left: 30px;&#34;&gt;
&lt;pre style=&#34;padding-left: 30px;&#34;&gt;&lt;strong&gt;tokudb_commit_sync&lt;/strong&gt; = 0
&lt;strong&gt;tokudb_fsync_log_period&lt;/strong&gt; = 1000&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h4&gt;tokudb_load_save_space&lt;/h4&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p style=&#34;padding-left: 30px;&#34;&gt;Turned on (value &lt;strong&gt;1&lt;/strong&gt;) by default as of TokuDB &lt;strong&gt;7.1.0&lt;/strong&gt;, this parameter decides whether temporary file created on bulk load operations (e.g. ALTER TABLE) are compressed or uncompressed. Do yourself a big favour (why? &lt;a href=&#34;http://code.openark.org/blog/mysql/converting-an-olap-database-to-tokudb-part-2-the-process-of-migration&#34;&gt;read here&lt;/a&gt;) and keep it on. Our setup is:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;&lt;strong&gt;tokudb_load_save_space&lt;/strong&gt; = 1&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;TokuDB&#39;s general recommendation is: don&#39;t change the variables; the engine should work well right out of the box. I like the approach (by MySQL &lt;strong&gt;5.5&lt;/strong&gt; I already lost count of InnoDB variables that can have noticeable impact; with &lt;strong&gt;5.6&lt;/strong&gt; I&#39;m all but lost). The complete list of configuration variables is found in &lt;a href=&#34;http://www.tokutek.com/wp-content/uploads/2013/10/mysql-5.5.30-tokudb-7.1.0-users-guide.pdf&#34;&gt;TokuDB&#39;s Users Guide&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Converting an OLAP database to TokuDB, part 1</title>
      <link>/blog/2013/09/03/converting-an-olap-database-to-tokudb-part-1/</link>
      <pubDate>Tue, 03 Sep 2013 09:04:12 +0000</pubDate>
      
      <guid>/blog/2013/09/03/converting-an-olap-database-to-tokudb-part-1/</guid>
      <description>&lt;p&gt;This is the first in a series of posts describing my impressions of converting a large OLAP server to TokuDB. There&#39;s a lot to tell, and the experiment is not yet complete, so this is an ongoing blogging. In this post I will describe the case at hand and out initial reasons for looking at TokuDB.&lt;/p&gt;
&lt;p&gt;Disclosure: I have no personal interests and no company interests; we did get friendly, useful and free advice from Tokutek engineers. TokuDB is open source and free to use, though commercial license is also available.&lt;/p&gt;
&lt;h4&gt;The case at hand&lt;/h4&gt;
&lt;p&gt;We have a large and fast growing DWH MySQL setup. This data warehouse is but one component in a larger data setup, which includes Hadoop, Cassandra and more. For online dashboards and most reports, MySQL is our service. We populate this warehouse mainly via Hive/Hadoop. Thus, we have an hourly load of data from Hive, as well as a larger daily load.&lt;/p&gt;
&lt;p&gt;There are some updates on the data, but the majority of writes are just &lt;strong&gt;mysqlimport&lt;/strong&gt;s of Hive queries.&lt;/p&gt;
&lt;p&gt;Usage of this database is OLAP: no concurrency issues here; we have some should-be-fast-running queries issued by our dashboards, as well as ok-to-run-longer queries issued for reports.&lt;/p&gt;
&lt;p&gt;Our initial and most burning trouble is with size. Today we use &lt;strong&gt;COMPRESSED&lt;/strong&gt; InnoDB tables (&lt;strong&gt;KEY_BLOCK_SIZE&lt;/strong&gt; is default, i.e. &lt;strong&gt;8&lt;/strong&gt;). Our data volume sums right now at about &lt;strong&gt;2TB&lt;/strong&gt;. I happen to know this translates as &lt;strong&gt;4TB&lt;/strong&gt; of uncompressed data.&lt;/p&gt;
&lt;p&gt;However growth of data is accelerating. A year ago we would capture a dozen GB per month. Today it is a &lt;strong&gt;100GB&lt;/strong&gt; per month, and by the end of this year it may climb to &lt;strong&gt;150GB&lt;/strong&gt; per month or more.&lt;/p&gt;
&lt;p&gt;Our data is not sharded. We have a simple replication topology of some &lt;strong&gt;6&lt;/strong&gt; servers. Machines are quite generous as detailed following. And yet, we will be running out of resources shortly: disk space (total &lt;strong&gt;2.7TB&lt;/strong&gt;) is now running low and is expected to run out in about six months. One of my first tasks in Outbrain is to find a solution to our DWH growth problem. The solution could be sharding; it could be a commercial DWH product; anything that works.&lt;!--more--&gt;&lt;/p&gt;
&lt;h4&gt;The approach we experiment with&lt;/h4&gt;
&lt;p&gt;It was at my initial interview that I suggested &lt;a href=&#34;http://www.tokutek.com/products/tokudb-for-mysql/&#34;&gt;TokuDB&lt;/a&gt; might be a good solution, with the primary reason of being so good with compression. And we decided to experiment with this simple (setup-wise) solution of compression. If we could compress the data even by &lt;strong&gt;50%&lt;/strong&gt;, that would buy us considerable time. And it&#39;s the simplest approach as we would need to change nothing at the application side, nor add additional frameworks.&lt;/p&gt;
&lt;p&gt;Of course, we were already using InnoDB &lt;strong&gt;COMPRESSED&lt;/strong&gt; tables. How about just improving the compression? And here I thought to myself: we can try &lt;strong&gt;KEY_BLOCK_SIZE=4&lt;/strong&gt;, which I know would generally compress by &lt;strong&gt;50%&lt;/strong&gt; as compared to &lt;strong&gt;KEY_BLOCK_SIZE=8&lt;/strong&gt; (not always, but in many use cases). We&#39;re already using InnoDB so this isn&#39;t a new beast; it will be &#34;more of the same&#34;. It would work.&lt;/p&gt;
&lt;p&gt;I got myself a dedicated machine: a slave in our production topology I am free to play with. I installed TokuDB &lt;strong&gt;7.0.1&lt;/strong&gt;, later upgraded to &lt;strong&gt;7.0.3&lt;/strong&gt;, based on MySQL &lt;strong&gt;5.5.30&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The machine is a Dell Inc. &lt;strong&gt;PowerEdge R510&lt;/strong&gt; machine, with &lt;b&gt;16&lt;/b&gt; CPUs @ &lt;b&gt;2.1 GHz&lt;/b&gt; and &lt;b&gt;126 GiB&lt;/b&gt; RAM, &lt;b&gt;16 GiB&lt;/b&gt; Swap. OS is CentOS &lt;strong&gt;5.7&lt;/strong&gt;,  kernel &lt;strong&gt;2.6.18&lt;/strong&gt;. We have RAID &lt;strong&gt;10&lt;/strong&gt; over local &lt;strong&gt;10k&lt;/strong&gt; RPM SAS disks (10x&lt;strong&gt;600GB&lt;/strong&gt; disks)&lt;/p&gt;
&lt;h4&gt;How to compare InnoDB &amp;amp; TokuDB?&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;2TB&lt;/strong&gt; of compressed data (for absolute measurement I consider it to be a &lt;strong&gt;4TB&lt;/strong&gt; worth of data) is quite a large setup. How do I do the comparison? I don&#39;t even have too much disk space here...&lt;/p&gt;
&lt;p&gt;We have tables of various size. Our largest is in itself &lt;strong&gt;1TB&lt;/strong&gt; (&lt;strong&gt;2TB&lt;/strong&gt; uncompressed) - half of the entire volume. The rest ranging &lt;strong&gt;330GB&lt;/strong&gt;, &lt;strong&gt;140GB&lt;/strong&gt;, &lt;strong&gt;120GB&lt;/strong&gt;, &lt;strong&gt;90GB&lt;/strong&gt;, &lt;strong&gt;50GB&lt;/strong&gt; and below. We have &lt;strong&gt;MONTH&lt;/strong&gt;ly partitioning schemes on most tables and obviously on our larger tables.&lt;/p&gt;
&lt;p&gt;For our smaller tables, we could just &lt;strong&gt;CREATE TABLE test_table LIKE small_table&lt;/strong&gt;, populating it and comparing compression. However, the really interesting question (and perhaps the only interesting question compression-wise) is how well would our larger (and specifically largest) tables would compress.&lt;/p&gt;
&lt;p&gt;Indeed, for our smaller tables we saw between &lt;strong&gt;20%&lt;/strong&gt; to &lt;strong&gt;70%&lt;/strong&gt; reduction in size when using stronger InnoDB compression: &lt;strong&gt;KEY_BLOCK_SIZE=4/2/1&lt;/strong&gt;. How well would that work on our larger tables? How much slower would it be?&lt;/p&gt;
&lt;p&gt;We know MySQL partitions are implemented by actual &lt;em&gt;independent&lt;/em&gt; tables. Our testing approach was: let&#39;s build a test_table from a one month worth of data (== one single partition) of our largest table. We tested:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The time it takes to load the entire partition (about &lt;strong&gt;120M&lt;/strong&gt; rows, &lt;strong&gt;100GB COMPRESSED&lt;/strong&gt; data as seen on &lt;strong&gt;.idb&lt;/strong&gt; file)&lt;/li&gt;
&lt;li&gt;The time it would take to load a single day&#39;s worth of data from Hive/Hadoop (loading real data, as does our nightly import)&lt;/li&gt;
&lt;li&gt;The time it would take for various important &lt;strong&gt;SELECT&lt;/strong&gt; query to execute on this data.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;InnoDB vs. TokuDB comparison&lt;/h4&gt;
&lt;p&gt;In this post I will only describe our impressions of compression size. I have a lot to say about TokuDB vs InnoDB partitioning and queries; this will wait till later post.&lt;/p&gt;
&lt;p&gt;So here goes:&lt;/p&gt;
&lt;table border=&#34;0&#34; cellspacing=&#34;0&#34;&gt;
&lt;colgroup width=&#34;85&#34;&gt;&lt;/colgroup&gt;
&lt;colgroup width=&#34;155&#34;&gt;&lt;/colgroup&gt;
&lt;colgroup width=&#34;152&#34;&gt;&lt;/colgroup&gt;
&lt;colgroup width=&#34;147&#34;&gt;&lt;/colgroup&gt;
&lt;colgroup width=&#34;141&#34;&gt;&lt;/colgroup&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#E6E6E6&#34; height=&#34;31&#34;&gt;&lt;b&gt;Engine&lt;/b&gt;&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#E6E6E6&#34;&gt;&lt;b&gt;Compression&lt;/b&gt;&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#E6E6E6&#34;&gt;&lt;b&gt;Time to Insert 1 month&lt;/b&gt;&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#E6E6E6&#34;&gt;&lt;b&gt;Table size (optimized)&lt;/b&gt;&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#E6E6E6&#34;&gt;&lt;b&gt;Time to import 1 day&lt;/b&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34; height=&#34;17&#34;&gt;InnoDB&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34;&gt;8k&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34;&gt;&lt;strong&gt;10.5h&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34;&gt;58GB&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34;&gt;&lt;b&gt;32m&lt;/b&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34; height=&#34;17&#34;&gt;InnoDB&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34;&gt;4k&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34;&gt;48h&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34;&gt;33GB&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34;&gt;unknown (too long)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34; height=&#34;17&#34;&gt;TokuDB&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34;&gt;quicklz&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34;&gt;14h&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34;&gt;17GB&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34;&gt;40m&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34; height=&#34;17&#34;&gt;TokuDB&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34;&gt;lzma (small/aggresive)&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34;&gt;15h&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34;&gt;&lt;b&gt;7.5GB&lt;/b&gt;&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34;&gt;42m&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Some comments and insights:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Each test was performed 3-4 times. There were no significant differences on the various cycles.&lt;/li&gt;
&lt;li&gt;The &lt;strong&gt;1&lt;/strong&gt; month insert was done courtesy &lt;a href=&#34;http://common-schema.googlecode.com/svn/trunk/common_schema/doc/html/query_script_split.html&#34;&gt;QueryScript split&lt;/a&gt;,  &lt;strong&gt;5,000&lt;/strong&gt; rows at a time, no throttling.&lt;/li&gt;
&lt;li&gt;The &lt;strong&gt;1&lt;/strong&gt; day import via &lt;em&gt;mysqlimport&lt;/em&gt;. There were multiple files imported. Each file is sorted by &lt;strong&gt;PRIMARY KEY ASC&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Isn&#39;t it nice to know that your &lt;strong&gt;100GB&lt;/strong&gt; InnoDB table actually fits within &lt;strong&gt;58GB&lt;/strong&gt; when rebuilt?&lt;/li&gt;
&lt;li&gt;For InnoDB &lt;strong&gt;flush_logs_at_trx_commit=2&lt;/strong&gt;, &lt;strong&gt;flush_method=O_DIRECT&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;I used default configuration to TokuDB -- touched nothing. More on this in later post.&lt;/li&gt;
&lt;li&gt;InnoDB &lt;strong&gt;4k&lt;/strong&gt; was &lt;em&gt;prohibitively&lt;/em&gt; slow to load data. It was so slow so as to be unacceptable. For the 1 day load it took &lt;strong&gt;1&lt;/strong&gt; hour for a mere &lt;strong&gt;20%&lt;/strong&gt; of data to load. &lt;strong&gt;1&lt;/strong&gt; hour was already marginal for our requirements; waiting for &lt;strong&gt;5&lt;/strong&gt; hours was out of the question. I tested several times, never got to wait for completion. Did I say it would just be &#34;more of the same&#34;? &lt;strong&gt;4k&lt;/strong&gt; turned to be &#34;not an option&#34;.&lt;/li&gt;
&lt;li&gt;I saw almost no difference in load time between the two TokuDB compression formats. Both somewhat (30%) longer than InnoDB to load, but comparable.&lt;/li&gt;
&lt;li&gt;TokuDB compression: nothing short of &lt;em&gt;amazing&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With InnoDB &lt;strong&gt;4k&lt;/strong&gt; being &#34;not an option&#34;, and with both TokuDB compressions being similar in load time yet so different in compression size, we are left with the following conclusion: if we want to compress more than our existing 8k (and we have to) - TokuDB&#39;s &lt;em&gt;agressive compression&lt;/em&gt; (aka small, aka lzma) is our only option.&lt;/p&gt;
&lt;h4&gt;Shameless plug&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;http://code.google.com/p/common-schema/&#34;&gt;common_schema&lt;/a&gt; turned to be quite the &#34;save the day&#34; tool here. Not only did we use it to extract 100GB of data from a large dataset and load it onto our tables, it also helped out in the ALTER process for TokuDB: at this time (&amp;lt;=&lt;strong&gt; 7.0.4&lt;/strong&gt;) TokuDB still has a bug with &lt;strong&gt;KEY_BLOCK_SIZE&lt;/strong&gt;: when this option is found in table definition, it impacts TokuDB&#39;s indexes by bloating them. This is how &lt;a href=&#34;http://common-schema.googlecode.com/svn/trunk/common_schema/doc/html/sql_alter_table_tokudb.html&#34;&gt;sql_alter_table_tokudb&lt;/a&gt; was born. Hopefully it will be redundant shortly.&lt;/p&gt;
&lt;h4&gt;More to come&lt;/h4&gt;
&lt;p&gt;Was our test fair? Should we have configure TokuDB differently? Is loading via small &lt;strong&gt;5,000&lt;/strong&gt; row chunks the right way?&lt;/p&gt;
&lt;p&gt;In the next post I will describe the process of migrating our 4TB worth of data to TokuDB, pitfalls, issues, party crushers, sport spoilers, configuration, recovery, cool behaviour and general advice you should probably want to embrace. At later stage I&#39;ll describe how our DWH looks after migration. Finally I&#39;ll share some (ongoing) insights on performance.&lt;/p&gt;
&lt;p&gt;You&#39;ll probably want to know &#34;How much is (non compressed) &lt;strong&gt;4TB&lt;/strong&gt; of data worth in TokuDB?&#34; Let&#39;s keep the suspense :)&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>DELETE, don&#39;t INSERT</title>
      <link>/blog/2012/06/27/delete-dont-insert/</link>
      <pubDate>Wed, 27 Jun 2012 07:25:09 +0000</pubDate>
      
      <guid>/blog/2012/06/27/delete-dont-insert/</guid>
      <description>&lt;p&gt;Have just read &lt;a href=&#34;http://blog.9minutesnooze.com/insert-delete/&#34;&gt;INSERT, Don’t DELETE&lt;/a&gt; by Aaron Brown, and have some lengthy response, which is why I write this post instead of commenting on said post.&lt;/p&gt;
&lt;p&gt;I wish to offer my counter thought and suggest that &lt;strong&gt;DELETE&lt;/strong&gt;s are probably the better choice.&lt;/p&gt;
&lt;p&gt;Aaron suggests that, when one wishes to purge rows from some table, a trick can be used: instead of &lt;strong&gt;DELETE&lt;/strong&gt;ing unwanted rows, one can &lt;strong&gt;INSERT&lt;/strong&gt; &#34;good&#34; rows into a new table, then switch over with &lt;strong&gt;RENAME&lt;/strong&gt; (but please read referenced post for complete details).&lt;/p&gt;
&lt;p&gt;I respectfully disagree on several points discussed.&lt;/p&gt;
&lt;h4&gt;Lockdown&lt;/h4&gt;
&lt;p&gt;The fact one needs to block writes during the time of creation of new table is problematic: you need to essentially turn off parts of your application. The posts suggests one could use a slave - but this solution is far from being trivial as well. To switch over, you yet again need to turn off access to DB, even if for a short while.&lt;!--more--&gt;&lt;/p&gt;
&lt;p&gt;A switch over to a slave is quite a big deal, in my opinion, for the mere purpose of deletion of rows.&lt;/p&gt;
&lt;h4&gt;DELETEs are easy&lt;/h4&gt;
&lt;p&gt;The DELETEs are so much easier: the first thing to note is the following: &lt;em&gt;You don&#39;t actually have to delete all the rows *at once*&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;You just need to drop some rows, right? Why waste a huge transaction that takes minutes, when you can drop the rows by chunks, one at a time?&lt;br /&gt;
For that, you can use either &lt;a href=&#34;http://www.percona.com/doc/percona-toolkit/2.1/pt-archiver.html&#34;&gt;pt-archive&lt;/a&gt; from &lt;em&gt;Percona Toolkit&lt;/em&gt;, &lt;a href=&#34;http://openarkkit.googlecode.com/svn/trunk/openarkkit/doc/html/oak-chunk-update.html&#34;&gt;oak-chunk-update&lt;/a&gt; from &lt;em&gt;openark-kit&lt;/em&gt;, or write a simple &lt;a href=&#34;http://common-schema.googlecode.com/svn/trunk/common_schema/doc/html/query_script.html&#34;&gt;QueryScript&lt;/a&gt; code with &lt;em&gt;common_schema&lt;/em&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;while (DELETE FROM title WHERE title &amp;lt;= &#39;g&#39; LIMIT 1000)
{
  throttle 1;
}&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;So, drop &lt;strong&gt;1,000&lt;/strong&gt; rows or so at a time, then sleep some time, etc. The total runtime is longer, but who cares? The impact can be reduced to be unnoticeable.&lt;/p&gt;
&lt;h4&gt;Space reclaim&lt;/h4&gt;
&lt;p&gt;You can use online table operations to rebuild your table and reclaim the disk space. Either see &lt;a href=&#34;http://openarkkit.googlecode.com/svn/trunk/openarkkit/doc/html/oak-online-alter-table.html&#34;&gt;oak-online-alter-table&lt;/a&gt; or &lt;a href=&#34;http://www.percona.com/doc/percona-toolkit/2.1/pt-online-schema-change.html&#34;&gt;pt-online-schema-change&lt;/a&gt;. Again, both work in small chunks, so no long stalls.&lt;/p&gt;
&lt;p&gt;But more on this: my usual purge scenario shows that it is repetitive. You purge, data fills again, you purge again, and so on.&lt;/p&gt;
&lt;p&gt;Which is why it doesn&#39;t make much sense to rebuild the table and reclaim the disk space: it just grows again to roughly same dimensions.&lt;br /&gt;
For a one time operation (e.g. after neglect of cleanup for long time) -- yes, absolutely, do a rebuild and reclaim. For repetitive cleanup - I don&#39;t bother.&lt;/p&gt;
&lt;h4&gt;Conclusion&lt;/h4&gt;
&lt;p&gt;Aaron does make note at the end of his post that &lt;strong&gt;DELETE&lt;/strong&gt; operations can be done online, while the &lt;strong&gt;INSERT&lt;/strong&gt; trick requires downtime, and this is a fair assessment.&lt;/p&gt;
&lt;p&gt;But just to make a point: none of the &lt;strong&gt;DELETE&lt;/strong&gt; timings are interesting. Since we are not concerned with deleting the rows in a given time (no &#34;press the red button&#34;), we can spread them over time and make the impact negligible. So not only is everything done online, it also goes unnoticed by the user. And this, I believe, is the major thing to consider.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Self throttling MySQL queries</title>
      <link>/blog/2011/11/01/self-throttling-mysql-queries/</link>
      <pubDate>Tue, 01 Nov 2011 09:55:47 +0000</pubDate>
      
      <guid>/blog/2011/11/01/self-throttling-mysql-queries/</guid>
      <description>&lt;p&gt;Recap on the problem:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A query takes a long time to complete.&lt;/li&gt;
&lt;li&gt;During this time it makes for a lot of I/O.&lt;/li&gt;
&lt;li&gt;Query&#39;s I/O overloads the db, making for other queries run slow.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I introduce the notion of self-throttling queries: queries that go to sleep, by themselves, throughout the runtime. The sleep period means the query does not perform I/O at that time, which then means other queries can have their chance to execute.&lt;/p&gt;
&lt;p&gt;I present two approaches:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The naive approach: for every &lt;strong&gt;1,000&lt;/strong&gt; rows, the query sleep for &lt;strong&gt;1&lt;/strong&gt; second&lt;/li&gt;
&lt;li&gt;The factor approach: for every &lt;strong&gt;1,000&lt;/strong&gt; rows, the query sleeps for the amount of time it took to iterate those &lt;strong&gt;1,000&lt;/strong&gt; rows (effectively doubling the total runtime of the query).&lt;!--more--&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Sample query&lt;/h4&gt;
&lt;p&gt;We use a simple, single-table scan. No aggregates (which complicate the solution considerably).&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;SELECT
  rental_id,
  TIMESTAMPDIFF(DAY, rental_date, return_date) AS rental_days
FROM
  sakila.rental
;&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;h4&gt;The naive solution&lt;/h4&gt;
&lt;p&gt;We need to know every &lt;strong&gt;1,000&lt;/strong&gt; rows. So we need to count the rows. We do that by using a counter, as follows:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;SELECT
  rental_id,
  TIMESTAMPDIFF(DAY, rental_date, return_date) AS rental_days,
  @row_counter := @row_counter + 1
FROM
  sakila.rental,
  (SELECT @row_counter := 0) sel_row_counter
;&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;A thing that bothers me, is that I wasn&#39;t asking for an additional column. I would like the result set to remain as it were; same result structure. We also want to sleep for &lt;strong&gt;1&lt;/strong&gt; second for each &lt;strong&gt;1,000&lt;/strong&gt; rows. So we merge the two together along with one of the existing columns, like this:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;SELECT
  rental_id +
    IF(
      (@row_counter := @row_counter + 1) % 1000 = 0,
      SLEEP(1), 0
    ) AS rental_id,
  TIMESTAMPDIFF(DAY, rental_date, return_date) AS rental_days
FROM
  sakila.rental,
  (SELECT @row_counter := 0) sel_row_counter
;&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;To remain faithful to &lt;a href=&#34;http://code.openark.org/blog/mysql/slides-from-my-talk-programmatic-queries-things-you-can-code-with-sql&#34;&gt;my slides&lt;/a&gt;, I rewrite as follows, and this is &lt;em&gt;the naive solution&lt;/em&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;SELECT
  rental_id +
    CASE
      WHEN &lt;strong&gt;(@row_counter := @row_counter + 1) % 1000 = 0&lt;/strong&gt; THEN &lt;strong&gt;SLEEP(1)&lt;/strong&gt;
      ELSE &lt;strong&gt;0&lt;/strong&gt;
    END AS rental_id,
  TIMESTAMPDIFF(DAY, rental_date, return_date) AS rental_days
FROM
  sakila.rental,
  (SELECT @row_counter := 0) sel_row_counter
;&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;The &lt;strong&gt;WHEN&lt;/strong&gt; clause always returns &lt;strong&gt;0&lt;/strong&gt;, so it does not affect the value of &lt;strong&gt;rental_id&lt;/strong&gt;.&lt;/p&gt;
&lt;h4&gt;The factor approach&lt;/h4&gt;
&lt;p&gt;In the factor approach we wish to keep record of query execution, every &lt;strong&gt;1,000&lt;/strong&gt; rows. I introduce a nested &lt;strong&gt;WHEN&lt;/strong&gt; statement which updates time records. I rely on &lt;strong&gt;SYSDATE()&lt;/strong&gt; to return the true time, and on &lt;strong&gt;NOW()&lt;/strong&gt; to return query execution start time.&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;SELECT
  rental_id +
    CASE
      WHEN (@row_counter := @row_counter + 1) IS NULL THEN NULL
      WHEN &lt;strong&gt;@row_counter % 1000 = 0&lt;/strong&gt; THEN
        CASE
          WHEN (@time_now := &lt;strong&gt;SYSDATE()&lt;/strong&gt;) IS NULL THEN NULL
          WHEN (@time_diff := (&lt;strong&gt;TIMESTAMPDIFF(SECOND, @chunk_start_time, @time_now)&lt;/strong&gt;)) IS NULL THEN NULL
          WHEN &lt;strong&gt;SLEEP(@time_diff)&lt;/strong&gt; IS NULL THEN NULL
          WHEN (@chunk_start_time := &lt;strong&gt;SYSDATE()&lt;/strong&gt;) IS NULL THEN NULL
          ELSE 0
        END
      ELSE 0
    END AS rental_id,
  TIMESTAMPDIFF(DAY, rental_date, return_date) AS rental_days
FROM
  sakila.rental,
  (SELECT @row_counter := 0) sel_row_counter,
  (SELECT @chunk_start_time := NOW()) sel_chunk_start_time
;&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;h4&gt;Proof&lt;/h4&gt;
&lt;p&gt;How can we prove that the queries do indeed work?&lt;/p&gt;
&lt;p&gt;We can see if the total runtime sums up to the number of sleep calls, in seconds; but how do we know that sleeps do occur at the correct times?&lt;/p&gt;
&lt;p&gt;A solution I offer is to use a stored routines which logs to a MyISAM table (a non transactional table) the exact time (using &lt;strong&gt;SYSDATE()&lt;/strong&gt;) and value per row. The following constructs are introduced:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;&lt;strong&gt;CREATE TABLE&lt;/strong&gt; test.proof(
  id INT UNSIGNED AUTO_INCREMENT PRIMARY KEY,
  dt DATETIME NOT NULL,
  msg VARCHAR(255)
) &lt;strong&gt;ENGINE=MyISAM&lt;/strong&gt;;

DELIMITER $$
&lt;strong&gt;CREATE FUNCTION&lt;/strong&gt; test.prove_it(message VARCHAR(255)) RETURNS TINYINT
DETERMINISTIC
MODIFIES SQL DATA
BEGIN
  &lt;strong&gt;INSERT INTO test.proof (dt, msg) VALUES (SYSDATE(), message); RETURN 0;&lt;/strong&gt;
END $$
DELIMITER ;&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;The &lt;strong&gt;prove_it()&lt;/strong&gt; function records the immediate time and a message into the MyISAM table, which immediately accepts the write, being non-transactional. It returns with &lt;strong&gt;0&lt;/strong&gt;, so we will now embed it within the query. Of course, the function itself incurs some overhead, but it will nevertheless convince you that &lt;strong&gt;SLEEP()&lt;/strong&gt;s do occur at the right time!&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;SELECT
  rental_id +
    CASE
      WHEN (@row_counter := @row_counter + 1) IS NULL THEN NULL
      WHEN @row_counter % 1000 = 0 THEN
        CASE
          WHEN (@time_now := SYSDATE()) IS NULL THEN NULL
          WHEN (@time_diff := (TIMESTAMPDIFF(SECOND, @chunk_start_time, @time_now))) IS NULL THEN NULL
          WHEN SLEEP(@time_diff)&lt;strong&gt; + test.prove_it(CONCAT(&#39;will sleep for &#39;, @time_diff, &#39; seconds&#39;))&lt;/strong&gt; IS NULL THEN NULL
          WHEN (@chunk_start_time := SYSDATE()) IS NULL THEN NULL
          ELSE 0
        END
      ELSE 0
    END AS rental_id,
  TIMESTAMPDIFF(DAY, rental_date, return_date) AS rental_days
FROM
  sakila.rental,
  (SELECT @row_counter := 0) sel_row_counter,
  (SELECT @chunk_start_time := NOW()) sel_chunk_start_time
;

mysql&amp;gt; SELECT * FROM test.proof;
+----+---------------------+--------------------------+
| id | dt                  | msg                      |
+----+---------------------+--------------------------+
|  1 | 2011-11-01 09:22:36 | will sleep for 1 seconds |
|  2 | 2011-11-01 09:22:36 | will sleep for 0 seconds |
|  3 | 2011-11-01 09:22:36 | will sleep for 0 seconds |
|  4 | 2011-11-01 09:22:36 | will sleep for 0 seconds |
|  5 | 2011-11-01 09:22:36 | will sleep for 0 seconds |
|  6 | 2011-11-01 09:22:36 | will sleep for 0 seconds |
|  7 | 2011-11-01 09:22:38 | will sleep for 1 seconds |
|  8 | 2011-11-01 09:22:38 | will sleep for 0 seconds |
|  9 | 2011-11-01 09:22:38 | will sleep for 0 seconds |
| 10 | 2011-11-01 09:22:38 | will sleep for 0 seconds |
| 11 | 2011-11-01 09:22:38 | will sleep for 0 seconds |
| 12 | 2011-11-01 09:22:40 | will sleep for 1 seconds |
| 13 | 2011-11-01 09:22:40 | will sleep for 0 seconds |
| 14 | 2011-11-01 09:22:40 | will sleep for 0 seconds |
| 15 | 2011-11-01 09:22:40 | will sleep for 0 seconds |
+----+---------------------+--------------------------+&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;The above query is actually very fast. Try adding &lt;strong&gt;BENCHMARK(1000,ENCODE(&#39;hello&#39;,&#39;goodbye&#39;))&lt;/strong&gt; to rental_id so as to make it slower, or just use it on a really large table, see what happens (this is what I actually used to make the query run for several seconds in the example above).&lt;/p&gt;
&lt;p&gt;Observant reads will note that the &lt;strong&gt;&#34;will sleep...&#34;&lt;/strong&gt; message actually gets written &lt;em&gt;after&lt;/em&gt; the &lt;strong&gt;SLEEP()&lt;/strong&gt; call. I leave this as it is.&lt;/p&gt;
&lt;p&gt;Another very nice treat of the code is that you don&#39;t need sub-second resolution for it to work. If you look at the above, we don&#39;t actually go to sleep every &lt;strong&gt;1,000&lt;/strong&gt; rows (&lt;strong&gt;1,000&lt;/strong&gt; is just too quick in the query -- perhaps I should have used &lt;strong&gt;10,000&lt;/strong&gt; seconds). But we &lt;em&gt;do&lt;/em&gt; make it once a second has &lt;em&gt;elapsed&lt;/em&gt;. Which means it works correctly &lt;em&gt;on average&lt;/em&gt;. Of course, the entire discussion is only of interest when a query executes for a &lt;em&gt;substantial&lt;/em&gt; number of seconds, so this is just an anecdote.&lt;/p&gt;
&lt;h4&gt;And the winner is...&lt;/h4&gt;
&lt;p&gt;Wow, this &lt;a href=&#34;http://code.openark.org/blog/mysql/contest-for-glory-write-a-self-throttling-mysql-query&#34;&gt;contest&lt;/a&gt; was anything but popular. &lt;strong&gt;&lt;a href=&#34;http://marcalff.blogspot.com/&#34;&gt;Marc Alff&lt;/a&gt;&lt;/strong&gt; is the obvious winner: he is the &lt;em&gt;only&lt;/em&gt; one to suggest a solution :)&lt;/p&gt;
&lt;p&gt;But Marc uses a very nice trick: he reads the &lt;strong&gt;PERFORMANCE_SCHEMA&lt;/strong&gt;. Now, I&#39;m not sure how the &lt;strong&gt;PERFORMANCE_SCHEMA&lt;/strong&gt; gets updated. I know that the &lt;strong&gt;INFORMATION_SCHEMA.GLOBAL_STATUS&lt;/strong&gt; table does not get updated by a query until the query completes (so you cannot expect a change in &lt;strong&gt;innodb_rows_read&lt;/strong&gt; throughout the execution of the query). I just didn&#39;t test it (homework, anyone?). If it does get updated, then we can throttle the query based on InnoDB page reads using a simple query. Otherwise, an access to &lt;strong&gt;/proc/diskstats&lt;/strong&gt; is possible, assuming no &lt;em&gt;apparmor&lt;/em&gt; or &lt;em&gt;SELinux&lt;/em&gt; are blocking us.&lt;/p&gt;
&lt;p&gt;Marc also uses a stored function, which is the &lt;em&gt;clean&lt;/em&gt; way of doing it; however I distrust the overhead incurred by s stored routine and prefer my solution (which is, admittedly, not a pretty SQL sight!).&lt;/p&gt;
&lt;p&gt;Happy throttling!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>On generating unique IDs using LAST_INSERT_ID() and other tools</title>
      <link>/blog/2011/02/02/on-generating-unique-ids-using-last_insert_id-and-other-tools/</link>
      <pubDate>Wed, 02 Feb 2011 08:50:02 +0000</pubDate>
      
      <guid>/blog/2011/02/02/on-generating-unique-ids-using-last_insert_id-and-other-tools/</guid>
      <description>&lt;p&gt;There&#39;s a &lt;a href=&#34;http://dev.mysql.com/doc/refman/5.1/en/information-functions.html#function_last-insert-id&#34;&gt;trick&lt;/a&gt; for using &lt;strong&gt;LAST_INSERT_ID()&lt;/strong&gt; to generate sequences in MySQL. Quoting from the Manual:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ol type=&#34;1&#34;&gt;
&lt;li&gt;Create a table to hold the sequence counter and initialize               it:
&lt;pre&gt;mysql&amp;gt; &lt;strong&gt;&lt;code&gt;CREATE TABLE sequence (id INT NOT NULL);&lt;/code&gt;&lt;/strong&gt;
mysql&amp;gt; &lt;strong&gt;&lt;code&gt;INSERT INTO sequence VALUES (0);&lt;/code&gt;&lt;/strong&gt;
&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;Use the table to generate sequence numbers like this:
&lt;pre&gt;mysql&amp;gt; &lt;strong&gt;&lt;code&gt;UPDATE sequence SET id=LAST_INSERT_ID(id+1);&lt;/code&gt;&lt;/strong&gt;
mysql&amp;gt; &lt;strong&gt;&lt;code&gt;SELECT LAST_INSERT_ID();&lt;/code&gt;&lt;/strong&gt;
&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;This trick calls for trouble.&lt;/p&gt;
&lt;h4&gt;Contention&lt;/h4&gt;
&lt;p&gt;A customer was using this trick to generate unique session IDs for his JBoss sessions. These IDs would eventually be written back to the database in the form of log events. Business go well, and one day the customer adds three new JBoss servers (doubling the amount of webapps). All of a sudden, nothing works quite as it used to. All kinds of queries take long seconds to complete; load average becomes very high.&lt;!--more--&gt;&lt;/p&gt;
&lt;p&gt;A short investigation reveals that a very slight load is enough to make for an accumulation of sequence-UPDATE queries. Dozens of them are active at any given time, waiting for long seconds.&lt;/p&gt;
&lt;p&gt;InnoDB or MyISAM both make for poor response times. No wonder! Everyone&#39;s contending for one lock.&lt;/p&gt;
&lt;h4&gt;Not just one&lt;/h4&gt;
&lt;p&gt;Other queries seem to hang as well. Why?&lt;/p&gt;
&lt;p&gt;It is easy to forget or let go unnoticed that there are quite a few global locks involved with each query. If query cache is activated, then any query must pass through that cache, holding the query cache mutex. There&#39;s a global mutex on MyISAM&#39;s key cache. There&#39;s one on InnoDB&#39;s buffer pool (see multiple buffer pools in InnoDB 5.5), albeit less of an overhead. And there&#39;s the table cache.&lt;/p&gt;
&lt;p&gt;When table cache is enabled, any completed query attempts to return file handles to the cache. Any new query attempts to retrieve handles from the cache. While writing to the cache (extracting, adding), the cache is locked. When everyone&#39;s busy doing the sequence-UPDATE, table cache lock is being abused. Other queries are unable to find the time to squire the lock and get on with their business.&lt;/p&gt;
&lt;h4&gt;What can be done?&lt;/h4&gt;
&lt;p&gt;One could try and increase the &lt;strong&gt;table_open_cache&lt;/strong&gt; value. That may help to some extent, and for limited time. But the more requests are made, the quicker the problem surfaces again. When, in fact, &lt;em&gt;reducing&lt;/em&gt; the &lt;strong&gt;table_open_cache&lt;/strong&gt; to zero (well, minimum value is &lt;strong&gt;1&lt;/strong&gt;) can make for a great impact. If there&#39;s nothing to fight for, everyone just get by on their own.&lt;/p&gt;
&lt;p&gt;I know the following is not a scientific explanation, but it hits me as a good comparison: when my daughter brings a friend over, and there&#39;s a couple of toys, both are happy. A third friend makes for a fight: &lt;em&gt;&#34;I saw it first! She took it from me! I was holding it!&#34;&lt;/em&gt;. Any parent knows the ultimate solution to this kind of fight: take away the toys, and have them find something else to enjoy doing. OK, sorry for this unscientific display, I had to share my daily stress.&lt;/p&gt;
&lt;p&gt;When no table cache is available, a query will go on opening the table by itself, and will not attempt to return the file handle back to the cache. The file handle will simply be destroyed. Now, usually this is not desired. Caching is good. But in our customer&#39;s case, the cost of not using a table cache was minified by the cost of having everyone fight for the sequence table. Reducing the table cache made for an immediate relaxation of the database, with observable poorer responsiveness on peak times, however way better than with large table cache.&lt;/p&gt;
&lt;h4&gt;Other tools?&lt;/h4&gt;
&lt;p&gt;I don&#39;t consider the above to be a good solution. It&#39;s just a temporary hack.&lt;/p&gt;
&lt;p&gt;I actually don&#39;t like the &lt;strong&gt;LAST_INSERT_ID()&lt;/strong&gt; trick. Moreover, I don&#39;t see that it&#39;s the database&#39;s job to provide with unique IDs. Let it do relational stuff. If generating IDs is too intensive, let someone else do it.&lt;/p&gt;
&lt;p&gt;NoSQL solutions provide such a service. &lt;a href=&#34;http://memcached.org/&#34;&gt;Memcached&lt;/a&gt;, &lt;a href=&#34;http://redis.io/&#34;&gt;redis&lt;/a&gt;, &lt;a href=&#34;http://www.mongodb.org/&#34;&gt;MongoDB&lt;/a&gt; (and probably more) all provide with increment functions. Check them out.&lt;/p&gt;
&lt;h4&gt;Application level solutions&lt;/h4&gt;
&lt;p&gt;I actually use an application level solution to generate unique IDs. I mean, there&#39;s always &lt;strong&gt;GUID()&lt;/strong&gt;, but it&#39;s result is just too long. Take a look at the following Java code:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;public class Utils {
  private static long lastUniqueNumber = 0;

  public static synchronized long uniqueNumber() {
    long unique = System.currentTimeMillis();
    if (unique &amp;lt;= lastUniqueNumber)
      unique = lastUniqueNumber + 1;
    lastUniqueNumber = unique;
    return unique;
  }
}&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;Within a Java application this above method returns with unique IDs, up to 1000 per second on average (and it can perform way more than 1000 times per second).&lt;/p&gt;
&lt;p&gt;On consequential executions of applications on the same machine one would still expect unique values due to the time-related nature of values. However, computer time changes. It&#39;s possible that &lt;strong&gt;System.currentTimeMillis()&lt;/strong&gt; would return a value already used in the past.&lt;/p&gt;
&lt;p&gt;And, what about two processes running on the same machine at the same time? Or on different machine?&lt;/p&gt;
&lt;p&gt;Which is why I use the following combination to generate my unique IDs:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Server ID (much like MySQL&#39;s server_id parameter). this could be the last byte in the server&#39;s IP address, or just 4 or 5 bits if not too many players are expected.&lt;/li&gt;
&lt;li&gt;Process ID (plain old &lt;em&gt;pid&lt;/em&gt;) which I pass to the Java runtime in the form of system properties. Any two processes running on the same machine are assured to have different IDs. Two consequently spawned processes will have different IDs. The time it would take to cycle the process IDs is way more than would make for a &#34;time glitch&#34; problem as described above&lt;/li&gt;
&lt;li&gt;Current time in milliseconds.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you have to have everything withing 64 bit (BIGINT) then you&#39;ll have to do bit manipulation, and drop some of the MSB on the milliseconds so as to overwrite with server &amp;amp; process IDs.&lt;/p&gt;
&lt;p&gt;If you are willing to have your IDs unique in the bounds of a given time frame (so, for example, a month from now you wouldn&#39;t mind reusing old IDs), then the problem is significantly easier. You may just use &#34;day of month&#34; and &#34;millis since day start&#34; and save those precious bits.&lt;/p&gt;
&lt;h4&gt;Still other?&lt;/h4&gt;
&lt;p&gt;Please share your solutions below!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>How often should you use OPTIMIZE TABLE? - followup</title>
      <link>/blog/2010/10/04/how-often-should-you-use-optimize-table-followup/</link>
      <pubDate>Mon, 04 Oct 2010 10:07:45 +0000</pubDate>
      
      <guid>/blog/2010/10/04/how-often-should-you-use-optimize-table-followup/</guid>
      <description>&lt;p&gt;This post follows up on Baron&#39;s &lt;a href=&#34;http://www.xaprb.com/blog/2010/02/07/how-often-should-you-use-optimize-table/&#34;&gt;How often should you use OPTIMIZE TABLE?&lt;/a&gt;. I had the opportunity of doing some massive purging of data from large tables, and was interested to see the impact of the &lt;strong&gt;OPTIMIZE&lt;/strong&gt; operation on table&#39;s indexes. I worked on some production data I was authorized to provide as example.&lt;/p&gt;
&lt;h4&gt;The use case&lt;/h4&gt;
&lt;p&gt;I&#39;ll present a single use case here. The table at hand is a compressed InnoDB table used for logs. I&#39;ve rewritten some column names for privacy:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;mysql&amp;gt; show create table logs \G

Create Table: CREATE TABLE `logs` (
 `id` int(11) NOT NULL AUTO_INCREMENT,
 `name` varchar(20) CHARACTER SET ascii COLLATE ascii_bin NOT NULL,
 `ts` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
 `origin` varchar(64) CHARACTER SET ascii COLLATE ascii_bin NOT NULL,
 `message` text NOT NULL,
 `level` tinyint(11) NOT NULL DEFAULT &#39;0&#39;,
 `s` char(16) CHARACTER SET ascii COLLATE ascii_bin NOT NULL DEFAULT &#39;&#39;,
 PRIMARY KEY (`id`),
 KEY `s` (`s`),
 KEY `name` (`name`,`ts`),
 KEY `origin` (`origin`,`ts`)
) ENGINE=InnoDB AUTO_INCREMENT=186878729 DEFAULT CHARSET=utf8 ROW_FORMAT=COMPRESSED KEY_BLOCK_SIZE=8&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;The table had log records starting &lt;strong&gt;2010-08-23&lt;/strong&gt; and up till &lt;strong&gt;2010-09-02&lt;/strong&gt; noon. Table status:&lt;!--more--&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;mysql&amp;gt; show table status like &#39;logs&#39;\G
*************************** 1. row ***************************
           Name: logs
         Engine: InnoDB
        Version: 10
     Row_format: Compressed
           Rows: 22433048
 Avg_row_length: 206
    Data_length: 4625285120
Max_data_length: 0
   Index_length: 1437073408
      Data_free: 4194304
 Auto_increment: 186878920
    Create_time: 2010-08-24 18:10:49
    Update_time: NULL
     Check_time: NULL
      Collation: utf8_general_ci
       Checksum: NULL
 Create_options: row_format=COMPRESSED KEY_BLOCK_SIZE=8
        Comment:&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;(A bit puzzled on the &lt;strong&gt;Create_time&lt;/strong&gt;; the table was taken from an LVM snapshot of another server, so it existed for a very long time before. Not sure why the &lt;strong&gt;Create_time&lt;/strong&gt; field is as it is here; I assume the MySQL upgrade marked it so, did not have the time nor need to look into it).&lt;/p&gt;
&lt;p&gt;I was using &lt;a href=&#34;http://www.percona.com/downloads/Percona-Server-5.1/&#34;&gt;Percona-Server-5.1.47-11.2&lt;/a&gt;, and so was able to look at the index statistics for that table:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;mysql&amp;gt; SELECT * FROM information_schema.INNODB_INDEX_STATS WHERE table_name=&#39;logs&#39;;
+--------------+------------+--------------+--------+----------------+------------+------------+
| table_schema | table_name | index_name   | fields | row_per_keys   | index_size | leaf_pages |
+--------------+------------+--------------+--------+----------------+------------+------------+
| newsminer    | logs       | PRIMARY      |      1 | 1              |     282305 |     246856 |
| newsminer    | logs       | s            |      2 | 17, 1          |      38944 |      33923 |
| newsminer    | logs       | name         |      3 | 2492739, 10, 2 |      22432 |      19551 |
| newsminer    | logs       | origin       |      3 | 1303, 4, 1     |      26336 |      22931 |
+--------------+------------+--------------+--------+----------------+------------+------------+&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;h4&gt;Status after massive purge&lt;/h4&gt;
&lt;p&gt;My first requirement was to purge out all record up to &lt;strong&gt;2010-09-01 00:00:00&lt;/strong&gt;. I did so in small chunks, using &lt;a href=&#34;http://code.openark.org/forge/openark-kit&#34;&gt;openark kit&lt;/a&gt;&#39;s oak-chunk-update (same can be achieved with &lt;a href=&#34;http://www.maatkit.org/&#34;&gt;maatkit&lt;/a&gt;&#39;s mk-archiver). The process purged &lt;strong&gt;1000&lt;/strong&gt; rows at a time, with some sleep in between, and ran for about a couple of hours. It may be interesting to note that since ts is in &lt;a href=&#34;http://code.openark.org/blog/mysql/monotonic-functions-sql-and-mysql&#34;&gt;monotonically ascending&lt;/a&gt; values, purging of old rows also means purging of lower PKs, which means we&#39;re trimming the PK tree from left.&lt;/p&gt;
&lt;p&gt;Even while purging took place, I could see the index_size/leaf_pages values dropping, until, finally:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;mysql&amp;gt; SELECT * FROM information_schema.INNODB_INDEX_STATS WHERE table_name=&#39;logs&#39;;
+--------------+------------+--------------+--------+--------------+------------+------------+
| table_schema | table_name | index_name   | fields | row_per_keys | index_size | leaf_pages |
+--------------+------------+--------------+--------+--------------+------------+------------+
| newsminer    | logs       | PRIMARY      |      1 | 1            |      40961 |      35262 |
| newsminer    | logs       | s            |      2 | 26, 1        |      34440 |       3798 |
| newsminer    | logs       | name         |      3 | 341011, 4, 1 |       4738 |       2774 |
| newsminer    | logs       | origin       |      3 | 341011, 4, 2 |      10178 |       3281 |
+--------------+------------+--------------+--------+--------------+------------+------------+&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;The number of deleted rows was roughly &lt;strong&gt;85%&lt;/strong&gt; of total rows, so down to &lt;strong&gt;15%&lt;/strong&gt; number of rows.&lt;/p&gt;
&lt;h4&gt;Status after OPTIMIZE TABLE&lt;/h4&gt;
&lt;p&gt;Time to see whether &lt;strong&gt;OPTIMIZE&lt;/strong&gt; really optimizes! Will it reduce number of leaf pages in PK? In secondary keys?&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;mysql&amp;gt; OPTIMIZE TABLE logs;
...
mysql&amp;gt; SELECT * FROM information_schema.INNODB_INDEX_STATS WHERE table_name=&#39;logs&#39;;
+--------------+------------+--------------+--------+--------------+------------+------------+
| table_schema | table_name | index_name   | fields | row_per_keys | index_size | leaf_pages |
+--------------+------------+--------------+--------+--------------+------------+------------+
| newsminer    | logs       | PRIMARY      |      1 | 1            |      40436 |      35323 |
| newsminer    | logs       | s            |      2 | 16, 1        |       5489 |       4784 |
| newsminer    | logs       | name         |      3 | 335813, 7, 1 |       3178 |       2749 |
| newsminer    | logs       | origin       |      3 | 335813, 5, 2 |       3951 |       3446 |
+--------------+------------+--------------+--------+--------------+------------+------------+
4 rows in set (0.00 sec)&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;The above shows no significant change in either of the indexes: not for &lt;strong&gt;index_size&lt;/strong&gt;, not for &lt;strong&gt;leaf_pages&lt;/strong&gt;, not for statistics (&lt;strong&gt;row_per_keys&lt;/strong&gt;). The &lt;strong&gt;OPTIMIZE&lt;/strong&gt; did not reduce index size. It did not reduce the number of index pages (&lt;strong&gt;leaf_pages&lt;/strong&gt; are the major factor here). Some &lt;strong&gt;leaff_pages&lt;/strong&gt; values have even increased, but in small enough margin to consider as equal.&lt;/p&gt;
&lt;p&gt;Index-wise, the above example does not show an advantage to using &lt;strong&gt;OPTIMIZE&lt;/strong&gt;. I confess, I was surprised. And for the better. This indicates InnoDB makes good merging of index pages after massive purging.&lt;/p&gt;
&lt;h4&gt;So, no use for OPTIMIZE?&lt;/h4&gt;
&lt;p&gt;Think again: file system-wise, things look different.&lt;/p&gt;
&lt;p&gt;Before purging of data:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;bash:~# ls -l logs.* -h
-rw-r----- 1 mysql mysql 8.6K 2010-08-15 17:40 logs.frm
-rw-r----- 1 mysql mysql 2.9G 2010-09-02 14:01 logs.ibd&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;After purging of data:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;bash:~# ls -l logs.* -h
-rw-r----- 1 mysql mysql 8.6K 2010-08-15 17:40 logs.frm
-rw-r----- 1 mysql mysql 2.9G 2010-09-02 14:21 logs.ibd&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;Recall that InnoDB never releases table space back to file system!&lt;/p&gt;
&lt;p&gt;After &lt;strong&gt;OPTIMIZE&lt;/strong&gt; on table:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;bash:~# ls -l logs.* -h
-rw-rw---- 1 mysql mysql 8.6K 2010-09-02 14:26 logs.frm
-rw-rw---- 1 mysql mysql 428M 2010-09-02 14:43 logs.ibd&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;On &lt;strong&gt;innodb_file_per_table&lt;/strong&gt; an &lt;strong&gt;OPTIMIZE&lt;/strong&gt; creates a new table space, and the old one gets destroyed. Space goes back to file system. Don&#39;t know about you; I like to have my file system with as much free space as possible.&lt;/p&gt;
&lt;h4&gt;Need to verify&lt;/h4&gt;
&lt;p&gt;I&#39;ve tested Percona Server, since this is where I can find &lt;strong&gt;INNODB_INDEX_STATS&lt;/strong&gt;. But this begs the following questions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Perhaps the results only apply for Percona Server? (I&#39;m guessing not).&lt;/li&gt;
&lt;li&gt;Or only for InnoDB plugin? Does the same hold for &#34;builtin&#34; InnoDB? (dunno)&lt;/li&gt;
&lt;li&gt;Only on &amp;gt;= 5.1? (Maybe; 5.0 is becoming rare now anyway)&lt;/li&gt;
&lt;li&gt;Only on InnoDB (Well, of course this test is storage engine dependent!)&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Conclusion&lt;/h4&gt;
&lt;p&gt;The use case above is a particular example. Other use cases may include tables where deletions often occur in middle of table (remember we were trimming the tree from left side only). Other yet may need to handle &lt;strong&gt;UPDATE&lt;/strong&gt;s to indexed columns. I have some more operations to do here, with larger tables (e.g. &lt;strong&gt;40GB&lt;/strong&gt; compressed). If anything changes, I&#39;ll drop a note.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Views: better performance with condition pushdown</title>
      <link>/blog/2010/05/20/views-better-performance-with-condition-pushdown/</link>
      <pubDate>Thu, 20 May 2010 07:17:05 +0000</pubDate>
      
      <guid>/blog/2010/05/20/views-better-performance-with-condition-pushdown/</guid>
      <description>&lt;p&gt;Justin&#39;s &lt;a href=&#34;http://www.mysqlperformanceblog.com/2010/05/19/a-workaround-for-the-performance-problems-of-temptable-views/&#34;&gt;A workaround for the performance problems of TEMPTABLE views&lt;/a&gt; post on &lt;a href=&#34;http://www.mysqlperformanceblog.com/&#34;&gt;mysqlperformanceblog.com&lt;/a&gt; reminded me of a solution I once saw on a customer&#39;s site.&lt;/p&gt;
&lt;p&gt;The customer was using nested views structure, up to depth of some 8-9 views. There were a lot of aggregations along the way, and even the simplest query resulted with a LOT of subqueries, temporary tables, and vast amounts of data, even if only to return with a couple of rows.&lt;/p&gt;
&lt;p&gt;While we worked to solve this, a developer showed me his own trick. His trick is now impossible to implement, but there&#39;s a hack around this.&lt;/p&gt;
&lt;p&gt;Let&#39;s use the world database to illustrate. Look at the following view definition:&lt;!--more--&gt;&lt;/p&gt;
&lt;blockquote&gt;&lt;pre&gt;
CREATE
  ALGORITHM=TEMPTABLE
VIEW country_languages AS
  SELECT
    Country.CODE, Country.Name AS country,
    GROUP_CONCAT(CountryLanguage.Language) AS languages
  FROM
    world.Country
    JOIN world.CountryLanguage ON (Country.CODE = CountryLanguage.CountryCode)
  GROUP BY
    Country.CODE;
&lt;/pre&gt;&lt;/blockquote&gt;
&lt;p&gt;The view presents with a list of spoken languages per country. The execution plan for querying this view looks like this:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;mysql&amp;gt; EXPLAIN SELECT * FROM country_languages;
+----+-------------+-----------------+--------+---------------+---------+---------+-----------------------------------+------+----------------------------------------------+
| id | select_type | table           | type   | possible_keys | key     | key_len | ref                               | rows | Extra                                        |
+----+-------------+-----------------+--------+---------------+---------+---------+-----------------------------------+------+----------------------------------------------+
|  1 | PRIMARY     | &amp;lt;derived2&amp;gt;      | ALL    | NULL          | NULL    | NULL    | NULL                              |  233 |                                              |
|  2 | DERIVED     | CountryLanguage | index  | PRIMARY       | PRIMARY | 33      | NULL                              |  984 | Using index; Using temporary; Using filesort |
|  2 | DERIVED     | Country         | eq_ref | PRIMARY       | PRIMARY | 3       | world.CountryLanguage.CountryCode |    1 |                                              |
+----+-------------+-----------------+--------+---------------+---------+---------+-----------------------------------+------+----------------------------------------------+
&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;And, even if we only want to filter out a single country, we still get the same plan:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;mysql&amp;gt; EXPLAIN SELECT * FROM country_languages WHERE Code=&#39;USA&#39;;
+----+-------------+-----------------+--------+---------------+---------+---------+-----------------------------------+------+----------------------------------------------+
| id | select_type | table           | type   | possible_keys | key     | key_len | ref                               | rows | Extra                                        |
+----+-------------+-----------------+--------+---------------+---------+---------+-----------------------------------+------+----------------------------------------------+
|  1 | PRIMARY     | &amp;lt;derived2&amp;gt;      | ALL    | NULL          | NULL    | NULL    | NULL                              |  233 | Using where                                  |
|  2 | DERIVED     | CountryLanguage | index  | PRIMARY       | PRIMARY | 33      | NULL                              |  984 | Using index; Using temporary; Using filesort |
|  2 | DERIVED     | Country         | eq_ref | PRIMARY       | PRIMARY | 3       | world.CountryLanguage.CountryCode |    1 |                                              |
+----+-------------+-----------------+--------+---------------+---------+---------+-----------------------------------+------+----------------------------------------------+
&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;So, we need to scan the entire country_language and country tables in order to return results for just one row.&lt;/p&gt;
&lt;h4&gt;A non-working solution&lt;/h4&gt;
&lt;p&gt;The solution offered by the developer was this:&lt;/p&gt;
&lt;blockquote&gt;&lt;pre&gt;
CREATE
  ALGORITHM=MERGE
  VIEW country_languages_non_working AS
  SELECT
    Country.CODE, Country.Name AS country,
    GROUP_CONCAT(CountryLanguage.Language) AS languages
  FROM
    world.Country
    JOIN world.CountryLanguage ON
      (Country.CODE = CountryLanguage.CountryCode)
  WHERE
    Country.CODE = @country_code
  GROUP BY Country.CODE;
&lt;/pre&gt;&lt;/blockquote&gt;
&lt;p&gt;And follow by:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;mysql&amp;gt; SET @country_code=&#39;USA&#39;;
Query OK, 0 rows affected (0.00 sec)

mysql&amp;gt; SELECT * FROM country_languages_2;
+------+---------------+----------------------------------------------------------------------------------------------------+
| CODE | country       | languages                                                                                          |
+------+---------------+----------------------------------------------------------------------------------------------------+
| USA  | United States | Chinese,English,French,German,Italian,Japanese,Korean,Polish,Portuguese,Spanish,Tagalog,Vietnamese |
+------+---------------+----------------------------------------------------------------------------------------------------+
&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;So, pushdown a &lt;strong&gt;WHERE&lt;/strong&gt; condition into the view&#39;s definition. The session variable @country_code is used to filter rows. In the above simplified code the value is assumed to be set; tweak it as you see fit (using &lt;strong&gt;IFNULL&lt;/strong&gt;, for example, or &lt;strong&gt;OR&lt;/strong&gt; statements) to allow for full scan in case the variable is undefined.&lt;/p&gt;
&lt;p&gt;This doesn&#39;t work. It used to work a couple years back; but today you cannot create a view which uses session variables or parameters. It is a restriction imposed by views.&lt;/p&gt;
&lt;h4&gt;A workaround&lt;/h4&gt;
&lt;p&gt;Justin showed a workaround using an additional table. There is another workaround which does not involve tables, but rather stored routines. Now, this is a patch, and an ugly one. It may not work in future versions of MySQL for all I know. But, here it goes:&lt;/p&gt;
&lt;blockquote&gt;&lt;pre&gt;
DELIMITER $$
CREATE DEFINER=`root`@`localhost` FUNCTION `get_session_country`() RETURNS CHAR(3)
    NO SQL
    DETERMINISTIC
BEGIN
  RETURN @country_code;
END $$
DELIMITER ;
CREATE
  ALGORITHM=MERGE
  VIEW country_languages_2 AS
  SELECT
    Country.CODE, Country.Name AS country,
    GROUP_CONCAT(CountryLanguage.Language) AS languages
  FROM
    world.Country
    JOIN world.CountryLanguage ON
      (Country.CODE = CountryLanguage.CountryCode)
  WHERE
    Country.CODE = get_session_country()
  GROUP BY Country.CODE;
&lt;/pre&gt;&lt;/blockquote&gt;
&lt;p&gt;And now:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;mysql&amp;gt; SET @country_code=&#39;USA&#39;;
Query OK, 0 rows affected (0.00 sec)

mysql&amp;gt; SELECT * FROM country_languages_2;
+------+---------------+----------------------------------------------------------------------------------------------------+
| CODE | country       | languages                                                                                          |
+------+---------------+----------------------------------------------------------------------------------------------------+
| USA  | United States | Chinese,English,French,German,Italian,Japanese,Korean,Polish,Portuguese,Spanish,Tagalog,Vietnamese |
+------+---------------+----------------------------------------------------------------------------------------------------+
1 row in set, 1 warning (0.00 sec)

mysql&amp;gt; EXPLAIN SELECT * FROM country_languages_2;
+----+-------------+-----------------+--------+---------------+---------+---------+------+------+--------------------------+
| id | select_type | table           | type   | possible_keys | key     | key_len | ref  | rows | Extra                    |
+----+-------------+-----------------+--------+---------------+---------+---------+------+------+--------------------------+
|  1 | PRIMARY     | &amp;lt;derived2&amp;gt;      | system | NULL          | NULL    | NULL    | NULL |    1 |                          |
|  2 | DERIVED     | Country         | const  | PRIMARY       | PRIMARY | 3       |      |    1 |                          |
|  2 | DERIVED     | CountryLanguage | ref    | PRIMARY       | PRIMARY | 3       |      |    8 | Using where; Using index |
+----+-------------+-----------------+--------+---------------+---------+---------+------+------+--------------------------+
&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;Since views are allowed to call stored routines (Justing used this to call upon &lt;strong&gt;CONNECTION_ID()&lt;/strong&gt;), and since stored routines can use session variables, we can take advantage and force the view into filtering out irrelevant rows before these accumulate to temporary tables and big joins.&lt;/p&gt;
&lt;p&gt;Back in the customer&#39;s office, we witnessed, what with their real data and multiple views, a reduction of query times from ~30 minutes to a few seconds.&lt;/p&gt;
&lt;h4&gt;Another kind of use&lt;/h4&gt;
&lt;p&gt;Eventually we worked to make better view definitions and query splitting, resulting in clearer code and fast queries, but this solution plays nicely into another kind of problem:&lt;/p&gt;
&lt;p&gt;Can we force different customers to see different parts of a given table? e.g., only those rows that relate to the customers?&lt;/p&gt;
&lt;p&gt;There can be many solutions: different tables; multiple views (one per customer), stored procedures, what have you. The above provides a solution, and I&#39;ve seen it in use.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Reducing locks by narrowing primary key</title>
      <link>/blog/2010/05/04/reducing-locks-by-narrowing-primary-key/</link>
      <pubDate>Tue, 04 May 2010 08:46:01 +0000</pubDate>
      
      <guid>/blog/2010/05/04/reducing-locks-by-narrowing-primary-key/</guid>
      <description>&lt;p&gt;In a period of two weeks, I had two cases with the exact same symptoms.&lt;/p&gt;
&lt;p&gt;Database users were experiencing low responsiveness. DBAs were seeing locks occurring on seemingly normal tables. In particular, looking at Innotop, it seemed that &lt;strong&gt;INSERT&lt;/strong&gt;s were causing the locks.&lt;/p&gt;
&lt;p&gt;In both cases, tables were InnoDB. In both cases, there was a &lt;strong&gt;PRIMARY KEY&lt;/strong&gt; on the combination of all &lt;strong&gt;5&lt;/strong&gt; columns. And in both cases, there was no clear explanation as for why the &lt;strong&gt;PRIMARY KEY&lt;/strong&gt; was chosen as such.&lt;/p&gt;
&lt;h4&gt;Choosing a proper PRIMARY KEY&lt;/h4&gt;
&lt;p&gt;Especially with InnoDB, which uses clustered index structure, the &lt;strong&gt;PRIMARY KEY&lt;/strong&gt; is of particular importance. Besides the fact that a bloated &lt;strong&gt;PRIMARY KEY&lt;/strong&gt; bloats the entire clustered index and secondary keys (see: &lt;a href=&#34;http://code.openark.org/blog/mysql/the-depth-of-an-index-primer&#34;&gt;The depth of an index: primer&lt;/a&gt;), it is also a source for locks. It&#39;s true that any &lt;strong&gt;UNIQUE KEY&lt;/strong&gt; can serve as a &lt;strong&gt;PRIMARY KEY&lt;/strong&gt;. But not all such keys are good candidates.&lt;!--more--&gt;&lt;/p&gt;
&lt;h4&gt;Reducing the locks&lt;/h4&gt;
&lt;p&gt;In both described cases, the solution was to add an &lt;strong&gt;AUTO_INCREMENT&lt;/strong&gt; column to serve as the &lt;strong&gt;PRIMARY KEY&lt;/strong&gt;, and have that &lt;strong&gt;5&lt;/strong&gt; column combination under a secondary &lt;strong&gt;UNIQUE KEY&lt;/strong&gt;. The impact was immediate: no further locks on that table were detected, and query responsiveness turned very high.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Misimproving performance problems with INSERT DELAYED</title>
      <link>/blog/2010/01/14/misimproving-performance-problems-with-insert-delayed/</link>
      <pubDate>Thu, 14 Jan 2010 20:58:36 +0000</pubDate>
      
      <guid>/blog/2010/01/14/misimproving-performance-problems-with-insert-delayed/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://dev.mysql.com/doc/refman/5.1/en/insert-delayed.html&#34;&gt;INSERT DELAYED&lt;/a&gt; may come in handy when using MyISAM tables. It may in particular be useful for log tables, where one is required to issue frequent INSERTs on one hand, but does not usually want or need to wait for DB response on the other hand.&lt;/p&gt;
&lt;p&gt;It may even offer some performance boost, by aggregating such frequent INSERTs in a single thread.&lt;/p&gt;
&lt;p&gt;But it is &lt;strong&gt;NOT&lt;/strong&gt; a performance solution.&lt;/p&gt;
&lt;p&gt;That is, in a case I&#39;ve seen, database performance was poor. INSERTs were taking a very long time. Lot&#39;s of locks were involved. The solution offered was to change all slow INSERTs to INSERT DELAYED. Voila! All INSERT queries now completed in no time.&lt;/p&gt;
&lt;p&gt;But the database performance remained poor. Just as poor as before, with the additional headache: nobody knew what caused the low performance.&lt;/p&gt;
&lt;p&gt;Using INSERT DELAYED to improve overall INSERT performance is like sweeping the dust under the carpet. It&#39;s still there, only you can&#39;t actually see it. When your queries are slow to return, you know which queries or which parts of your application are the immediate suspects. When everything happens in the background you lose that feeling.&lt;/p&gt;
&lt;p&gt;The slow query log, fortunately, still provides with the necessary information, and all the other metrics are just as before. Good. But it now takes a deeper level of analysis to find a problem that was previously in plain sight.&lt;/p&gt;
&lt;p&gt;So: use INSERT DELAYED carefully, don&#39;t just throw it at your slow queries like a magic potion.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>On restoring a single table from mysqldump</title>
      <link>/blog/2009/12/01/on-restoring-a-single-table-from-mysqldump/</link>
      <pubDate>Tue, 01 Dec 2009 10:25:00 +0000</pubDate>
      
      <guid>/blog/2009/12/01/on-restoring-a-single-table-from-mysqldump/</guid>
      <description>&lt;p&gt;Following &lt;a href=&#34;http://everythingmysql.ning.com/profiles/blogs/restore-one-table-from-an-all&#34;&gt;Restore one table from an ALL database dump&lt;/a&gt; and &lt;a href=&#34;http://gtowey.blogspot.com/2009/11/restore-single-table-from-mysqldump.html&#34;&gt;Restore a Single Table From mysqldump&lt;/a&gt;, I would like to add my own thoughts and comments on the subject.&lt;/p&gt;
&lt;p&gt;I also wish to note performance issues with the two suggested solutions, and offer improvements.&lt;/p&gt;
&lt;h4&gt;Problem relevance&lt;/h4&gt;
&lt;p&gt;While the problem is interesting, I just want to note that it is relevant in very specific database dimensions. Too small - and it doesn&#39;t matter how you solve it (e.g. just open vi/emacs and copy+paste). Too big - and it would not be worthwhile to restore from &lt;em&gt;mysqldump&lt;/em&gt; anyway. I would suggest that the problem is interesting in the whereabouts of a few dozen GB worth of data.&lt;/p&gt;
&lt;h4&gt;Problem recap&lt;/h4&gt;
&lt;p&gt;Given a dump file (generated by mysqldump), how do you restore a single table, without making any changes to other tables?&lt;/p&gt;
&lt;p&gt;Let&#39;s review the two referenced solutions. I&#39;ll be using the &lt;a href=&#34;http://dev.mysql.com/doc/employee/en/employee.html&#34;&gt;employees db&lt;/a&gt; on &lt;a href=&#34;https://launchpad.net/mysql-sandbox&#34;&gt;mysql-sandbox&lt;/a&gt; for testing. I&#39;ll choose a very small table to restore: &lt;strong&gt;departments&lt;/strong&gt; (only a few rows in this table).&lt;/p&gt;
&lt;h4&gt;Security based solution&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;http://everythingmysql.ning.com/profiles/blogs/restore-one-table-from-an-all&#34;&gt;&lt;strong&gt;Chris&lt;/strong&gt;&lt;/a&gt; offers to create a special purpose account, which will only have write (CREATE, INSERT, etc.) privileges on the particular table to restore. Cool hack! But, I&#39;m afraid, not too efficient, for two reasons:&lt;span id=&#34;more-1630&#34;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;MySQL needs to process all irrelevant queries (ALTER, INSERT, ...) only to disallow them due to access violation errors.&lt;/li&gt;
&lt;li&gt;Assuming restore is from remote host, we overload the network with all said irrelevant queries.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Just how inefficient? Let&#39;s time it:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;mysql&amp;gt; grant usage on *.* to &#39;restoreuser&#39;@&#39;localhost&#39;;
mysql&amp;gt; grant select on *.* to &#39;restoreuser&#39;@&#39;localhost&#39;;
mysql&amp;gt; grant all on employees.departments to &#39;restoreuser&#39;@&#39;localhost&#39;;

$ time mysql --user=restoreuser --socket=/tmp/mysql_sandbox21701.sock --force employees &amp;lt; /tmp/employees.sql
...
ERROR 1142 (42000) at line 343: INSERT command denied to user &#39;restoreuser&#39;@&#39;localhost&#39; for table &#39;titles&#39;
ERROR 1142 (42000) at line 344: ALTER command denied to user &#39;restoreuser&#39;@&#39;localhost&#39; for table &#39;titles&#39;
...
(lot&#39;s of these messages)
...

real&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;strong&gt;0m31.945s&lt;/strong&gt;
user&amp;nbsp;&amp;nbsp;&amp;nbsp; 0m6.328s
sys&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 0m0.508s&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;So, at about &lt;strong&gt;30&lt;/strong&gt; seconds to restore a 9 rows table.&lt;/p&gt;
&lt;h4&gt;Text filtering based solution.&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;http://gtowey.blogspot.com/2009/11/restore-single-table-from-mysqldump.html&#34;&gt;&lt;strong&gt;gtowey&lt;/strong&gt;&lt;/a&gt; offers parsing the dump file beforehand:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;First, parse with &lt;em&gt;grep&lt;/em&gt;, to detect rows where tables are referenced within dump file&lt;/li&gt;
&lt;li&gt;Second, parse with &lt;em&gt;sed&lt;/em&gt;, extracting relevant rows.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let&#39;s time this one:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;$ time grep -n &#39;Table structure&#39; /tmp/employees.sql
23:-- Table structure for table `departments`
48:-- Table structure for table `dept_emp`
89:-- Table structure for table `dept_manager`
117:-- Table structure for table `employees`
161:-- Table structure for table `salaries`
301:-- Table structure for table `titles`

real&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;strong&gt;0m0.397s&lt;/strong&gt;
user&amp;nbsp;&amp;nbsp;&amp;nbsp; 0m0.232s
sys&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 0m0.164s

$ time sed -n 23,48p /tmp/employees.sql | ./use employees

real&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;strong&gt;0m0.562s&lt;/strong&gt;
user&amp;nbsp;&amp;nbsp;&amp;nbsp; 0m0.380s
sys&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 0m0.176s&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;Much faster: about &lt;strong&gt;1&lt;/strong&gt; second, compared to &lt;strong&gt;30&lt;/strong&gt; seconds from above.&lt;/p&gt;
&lt;p&gt;Nevertheless, I find two issues here:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;A correctness problem: this solution somewhat assumes that there&#39;s only a single table with desired name. I say &#34;somewhat&#34; since it leaves this for the user.&lt;/li&gt;
&lt;li&gt;An efficiency problem: it reads the dump file &lt;em&gt;twice&lt;/em&gt;. First parsing it with &lt;em&gt;grep&lt;/em&gt;, then with &lt;em&gt;sed&lt;/em&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;A third solution&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;sed&lt;/em&gt; is much stronger than presented. In fact, the inquiry made by &lt;em&gt;grep&lt;/em&gt; in gtowey&#39;s solution can be easily handled by &lt;em&gt;sed&lt;/em&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;$ time sed -n &#34;/^-- Table structure for table \`departments\`/,/^-- Table structure for table/p&#34; /tmp/employees.sql | ./use employees

real&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;strong&gt;0m0.573s&lt;/strong&gt;
user&amp;nbsp;&amp;nbsp;&amp;nbsp; 0m0.416s
sys&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 0m0.152s&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;So, the &lt;strong&gt;&#34;/^-- Table structure for table \`departments\`/,/^-- Table structure for table/p&#34;&lt;/strong&gt; part tells &lt;em&gt;sed&lt;/em&gt; to only print those rows starting from the &lt;strong&gt;departments&lt;/strong&gt; table structure, and ending in the next table structure (this is for clarity: had department been the last table, there would not be a next table, but we could nevertheless solve this using other anchors).&lt;/p&gt;
&lt;p&gt;And, we only do it in &lt;strong&gt;0.57&lt;/strong&gt; seconds: about half the time of previous attempt.&lt;/p&gt;
&lt;p&gt;Now, just to be more correct, we only wish to consider the &lt;strong&gt;employees.department&lt;/strong&gt; table. So, &lt;em&gt;assuming&lt;/em&gt; there&#39;s more than one database dumped (and, by consequence, &lt;strong&gt;USE&lt;/strong&gt; statements in the dump-file), we use:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;cat /tmp/employees.sql | sed -n &#34;/^USE \`employees\`/,/^USE \`/p&#34; | sed -n &#34;/^-- Table structure for table \`departments\`/,/^-- Table structure for table/p&#34; | ./use employees&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;h4&gt;Further notes&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;All tests used warmed-up caches.&lt;/li&gt;
&lt;li&gt;The sharp eyed readers would notice that &lt;strong&gt;departments&lt;/strong&gt; is the first table in the dump file. Would that give an unfair advantage to the parsing-based restore methods? The answer is no. I&#39;ve created an &lt;strong&gt;xdepartments&lt;/strong&gt; table, to be located at the end of the dump. The difference in time is neglectful and inconclusive; we&#39;re still at ~0.58-0.59 seconds. The effect will be more visible on really large dumps; but then, so would the security-based effects.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;[&lt;strong&gt;UPDATE&lt;/strong&gt;: see also following similar post: &lt;a href=&#34;http://blog.tsheets.com/2008/tips-tricks/extract-a-single-table-from-a-mysqldump-file.html&#34;&gt;Extract a Single Table from a mysqldump File&lt;/a&gt;]&lt;/p&gt;
&lt;h4&gt;Conclusion&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;http://www.amazon.com/Classic-Shell-Scripting-Arnold-Robbins/dp/0596005954/ref=sr_1_1&#34;&gt;&lt;img class=&#34;alignright&#34; title=&#34;classic-shell-scripting&#34; src=&#34;http://code.openark.org/blog/wp-content/uploads/2009/12/classic-shell-scripting.png&#34; alt=&#34;classic-shell-scripting&#34; width=&#34;144&#34; height=&#34;189&#34; scale=&#34;0&#34;&gt;&lt;/a&gt;Its is always best to test on large datasets, to get a feel on performance.&lt;/p&gt;
&lt;p&gt;It&#39;s best to save MySQL the trouble of parsing &amp;amp; ignoring statements. Scripting utilities like &lt;em&gt;sed&lt;/em&gt;, &lt;em&gt;awk&lt;/em&gt; &amp;amp; &lt;em&gt;grep&lt;/em&gt; have been around for ages, and are well optimized. They excel at text processing.&lt;/p&gt;
&lt;p&gt;I&#39;ve used &lt;em&gt;sed&lt;/em&gt; many times in transforming dump outputs; for example, in converting MyISAM to InnoDB tables; to convert Antelope InnoDB tables to Barracuda format, etc. grep &amp;amp; awk are also very useful.&lt;/p&gt;
&lt;p&gt;May I recommend, at this point, reading &lt;a href=&#34;http://www.amazon.com/Classic-Shell-Scripting-Arnold-Robbins/dp/0596005954/ref=sr_1_1&#34;&gt;Classic Shell Scripting&lt;/a&gt;, a very easy to follow book, which lists the most popular command line utilities like &lt;em&gt;grep&lt;/em&gt;, &lt;em&gt;sed&lt;/em&gt;, &lt;em&gt;awk&lt;/em&gt;, &lt;em&gt;sort&lt;/em&gt;, (countless more) and shell scripting in general. While most of these utilities are well known, the book excels in providing suprisingly practical, simple solution to common tasks.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Performance analysis with mycheckpoint</title>
      <link>/blog/2009/11/12/performance-analysis-with-mycheckpoint/</link>
      <pubDate>Thu, 12 Nov 2009 12:47:00 +0000</pubDate>
      
      <guid>/blog/2009/11/12/performance-analysis-with-mycheckpoint/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://code.openark.org/forge/mycheckpoint&#34;&gt;mycheckpoint&lt;/a&gt; (see &lt;a href=&#34;http://code.openark.org/blog/mysql/announcing-mycheckpoint-lightweight-sql-oriented-monitoring-for-mysql&#34;&gt;announcement&lt;/a&gt;) allows for both graph presentation and quick SQL access to monitored &amp;amp; analyzed data. I&#39;d like to show the power of combining them both.&lt;/p&gt;
&lt;h4&gt;InnoDB performance&lt;/h4&gt;
&lt;p&gt;Taking a look at one of the most important InnoDB metrics: the read hit ratio (we could get the same graph by looking at the &lt;a href=&#34;http://code.openark.org/forge/mycheckpoint/documentation/generating-html-reports&#34;&gt;HTML report&lt;/a&gt;):&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;SELECT innodb_read_hit_percent FROM sv_report_chart_sample \G
*************************** 1. row ***************************
innodb_read_hit_percent: http://chart.apis.google.com/chart?cht=lc&amp;amp;chs=400x200&amp;amp;chts=303030,12&amp;amp;chtt=Nov+10,+11:40++-++Nov+11,+08:55+(0+days,+21+hours)&amp;amp;chdl=innodb_read_hit_percent&amp;amp;chdlp=b&amp;amp;chco=ff8c00&amp;amp;chd=s:400664366P6674y7176677677u467773y64ux166666764366646y616666666666644444434444s6u4S331444404433341334433646777666666074736777r1777767764776666F667777617777777777777777yaRi776776mlf667676xgx776766rou67767777u37797777x76676776u6A737464y67467761777666643u66446&amp;amp;chxt=x,y&amp;amp;chxr=1,99.60,100.00&amp;amp;chxl=0:||Nov+10,+15:55|Nov+10,+20:10|Nov+11,+00:25|Nov+11,+04:40|&amp;amp;chxs=0,505050,10&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;pre /&gt;&lt;img class=&#34;alignnone&#34; title=&#34;innodb_read_hit_percent&#34; src=&#34;/blog/blog/assets/chart?cht=lc&amp;amp;chs=400x200&amp;amp;chts=303030,12&amp;amp;chtt=Nov+10,+11:40++-++Nov+11,+08:55+(0+days,+21+hours)&amp;amp;chdl=innodb_read_hit_percent&amp;amp;chdlp=b&amp;amp;chco=ff8c00&amp;amp;chd=s:400664366P6674y7176677677u467773y64ux166666764366646y616666666666644444434444s6u4S331444404433341334433646777666666074736777r1777767764776666F667777617777777777777777yaRi776776mlf667676xgx776766rou67767777u37797777x76676776u6A737464y67467761777666643u66446&amp;amp;chxt=x,y&amp;amp;chxr=1,99.60,100.00&amp;amp;chxl=0:||Nov+10,+15:55|Nov+10,+20:10|Nov+11,+00:25|Nov+11,+04:40|&amp;amp;chxs=0,505050,10&#34; alt=&#34;&#34; width=&#34;400&#34; height=&#34;200&#34; /&gt;
&lt;/blockquote&gt;
We see that read hit is usually high, but occasionally drops low, down to 99.7, or even 99.6. But it seems like most of the time we are above 99.95% read hit ratio. It&#39;s hard to tell about 99.98%.
&lt;h4&gt;Can we know for sure?&lt;/h4&gt;
We can stress our eyes, yet be certain of little. It&#39;s best if we just query for the metrics! &lt;em&gt;mycheckpoint&lt;/em&gt; provides with all data, accessible by simple SQL queries:&lt;!--more--&gt;
&lt;blockquote&gt;
&lt;pre&gt;SELECT SUM(innodb_read_hit_percent &amp;gt; 99.95)/count(*)
  FROM sv_report_sample;
+-----------------------------------------------+
| SUM(innodb_read_hit_percent &amp;gt; 99.95)/count(*) |
+-----------------------------------------------+
|                                        0.7844 |
+-----------------------------------------------+&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;Yes, most of the time we&#39;re above 99.95% read hit ratio: but not too often!&lt;/p&gt;
&lt;p&gt;I&#39;m more interested in seeing how much time my server&#39;s above 99.98% read hit:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;SELECT SUM(innodb_read_hit_percent &amp;gt; 99.98)/count(*)
  FROM sv_report_sample;
+-----------------------------------------------+
| SUM(innodb_read_hit_percent &amp;gt; 99.98)/count(*) |
+-----------------------------------------------+
|                                        0.3554 |
+-----------------------------------------------+&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;We can see the server only has 99.98% read hit percent 35% of the time. Need to work on that!&lt;/p&gt;
&lt;h4&gt;Disk activity&lt;/h4&gt;
&lt;p&gt;Lower read hit percent means higher number of disk reads; that much is obvious. The first two following graphs present this obvious connection. But the third graph tells us another fact: with increased disk I/O, we can expect more (and longer) locks.&lt;/p&gt;
&lt;p&gt;Again, this should be very intuitive, when thinking about it this way. The problem sometimes arises when we try to analyze it the other way round: &#34;Hey! InnoDB has a lot of locks! What are we going to do about it?&#34;. Many times, people will look for answers in their &lt;em&gt;transactions&lt;/em&gt;, their &lt;em&gt;Isolation Level&lt;/em&gt;, their &lt;em&gt;LOCK IN SHARE MODE&lt;/em&gt; clauses. But the simple answer can be: &#34;There&#39;s a lot of I/O, so everything has to wait; therefore we increase the probability for locks; therefore there&#39;s more locks&#34;.&lt;/p&gt;
&lt;p&gt;The answer, then, is to reduce I/O. The usual stuff: slow queries; indexing; ... and, yes, perhaps transactions or tuning.&lt;/p&gt;
&lt;p&gt;The charts below make it quite clear that we have an issue of excessive reads -&amp;gt; less read hit -&amp;gt; increased I/O -&amp;gt; more locks.&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre /&gt;&lt;img class=&#34;alignnone&#34; title=&#34;DML&#34; src=&#34;/blog/blog/assets/chart?cht=lc&amp;amp;chs=400x200&amp;amp;chts=303030,12&amp;amp;chtt=Oct+31,+17:00++-++Nov+11,+08:00+(10+days,+15+hours)&amp;amp;chdl=com_select_psec|com_insert_psec|com_delete_psec|com_update_psec|com_replace_psec&amp;amp;chdlp=b&amp;amp;chco=ff8c00,4682b4,9acd32,dc143c,9932cc&amp;amp;chd=s:IJJJJJKHGHGHGHHHHHIIIJJJKKKLKLLIHHHIHIHIIIJJJJJKKKLLLLMIHHHHHHHIIIIIJJKKKLLLLMMIHIIIIIIIIIIJJJJKKLLLLMMIHHHIHIHIIIJJJJJKKLKLLLLIHHHIHIHIIIIJIJJJJKKKKKKHHHHHHHHHHHHIIIIJIJJJJJKHHHHHHHHHHIIJJNKLLKKLLLMSMHHIHHHIOSae9RNPJIIJJJKHGGGHGGHHHHHJJKJLKLLLMKMJHIIIIIII,EEEEEEEFEEEEEEEFEFFFFFFFFFFFFFFEEFFEEEEEFFFFFFFFFFFGFFFGFFFFEEEEFFFFFFGGFGFFFFFGEFFFEEFFFFFFFFFFFFFFFFFGEEEEEEEEFFFGGFFFFGFFFFFGEEEFFEEFFEFFEEFFFFFFFEEFEEEEEEEEEEEEEEFFEEEEFEEGEEEEEEEEEFFFFFFFFFEFFFFHEEEEEEEFFFFFFFFFFEEEEFEHEFEEEEEEEEFFFGFGGFFFFFFIEEEEEEEF,CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCDCCCCCCDDCCCCCCCCCCCCCCCDCCCCCCCCCCCCCCCCCCDCCCCECCBCBCCCCCCCCCCDCCCCCDCECCCCCCCCCCCCCCCCCCCDCCCDCCCCBBCCCCDCCDCCCCCCCCCECCCBBCCCCCCCCCCCCCCCCCCFCCBCCCCCCCCCCCCCCCCCCCCFCCCCBCCCCCCCDCCCCDCCDCCGCCCCCCCD,CBCCCBBCBBBCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCBCCCCCCCCCCCCCCBBBCCCCCCBCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCBBBBBBCBBBBBBBBCBBCCCCCCCCCCCCCCCCCCCCC,AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA&amp;amp;chxt=x,y&amp;amp;chxr=1,0,680.42&amp;amp;chxl=0:||Nov+2,+20:00|Nov+4,+23:00|Nov+7,+02:00|Nov+9,+05:00|&amp;amp;chxs=0,505050,10&#34; alt=&#34;&#34; width=&#34;400&#34; height=&#34;200&#34; /&gt;

&lt;img class=&#34;alignnone&#34; title=&#34;innodb_read_hit_percent&#34; src=&#34;/blog/blog/assets/chart?cht=lc&amp;amp;chs=400x200&amp;amp;chts=303030,12&amp;amp;chtt=Oct+31,+17:00++-++Nov+11,+08:00+(10+days,+15+hours)&amp;amp;chdl=innodb_read_hit_percent&amp;amp;chdlp=b&amp;amp;chco=ff8c00&amp;amp;chd=s:8p879mq7z1377377777z788863778839z13773877633697786888969z1379377667275377376672813167266771288716689y759121685885785236675889869232685789w63y69997989999252696878y8698878588886933368587ffpibibaTYRfVAdXjqfdmbYneRhciXYcifb6995802z56377666576877268875913278387&amp;amp;chxt=x,y&amp;amp;chxr=1,99.44,99.99&amp;amp;chxl=0:||Nov+2,+20:00|Nov+4,+23:00|Nov+7,+02:00|Nov+9,+05:00|&amp;amp;chxs=0,505050,10&#34; alt=&#34;&#34; width=&#34;400&#34; height=&#34;200&#34; /&gt;

&lt;img class=&#34;alignnone&#34; title=&#34;innodb_io&#34; src=&#34;/blog/blog/assets/chart?cht=lc&amp;amp;chs=400x200&amp;amp;chts=303030,12&amp;amp;chtt=Oct+31,+17:00++-++Nov+11,+08:00+(10+days,+15+hours)&amp;amp;chdl=innodb_buffer_pool_reads_psec|innodb_buffer_pool_pages_flushed_psec&amp;amp;chdlp=b&amp;amp;chco=ff8c00,4682b4&amp;amp;chd=s:DYDDBZVEPMJEFKFGGGEOEDDDEJDECBGBONKEFJEFFFIHFCDDCECCCBECSPLECJEFGHFLDGHEDHEEEDHCMJMGFLGHFELMDCDLEFDBRDFBKIKECIDEHEDHJHFEDGCDCDFCKJKFEJFECRGHNFCCBFBECBCCLHKFBGDEDUEGBCCEEHDCDDFBJJJFEIDFwrfpthozmqqcn3g9hYjkbpqdsvhxormohorGCBHDOLNGEHEDFFGIEFDEDKFDDDGCLIJECIDE,EEEEEGEFSWUFFEGHKIHHHGGHJIHHHHGGQbTFFEFFHGGGGFFHHHGHGFFGUdYGDEGFJKIHHHLKJJJIHHHGHZQRGFGHIHIGGGHIIIGHFFFEHYPNCEFEHHIIIKKJIJHHGIFFGbSPFGJIGFGGGEFFEFEFFEEGSIUODCDFHGGFEEGGGGGGGHFFGYPNDFFGJHIJJIJIHHGFFFEHHVSLCDGIHIHGIHGGFGFFFGIJTRSMEEFFGHHIIIHJKLKHIHGHPNNMCFGE&amp;amp;chxt=x,y&amp;amp;chxr=1,0,151.44&amp;amp;chxl=0:||Nov+2,+20:00|Nov+4,+23:00|Nov+7,+02:00|Nov+9,+05:00|&amp;amp;chxs=0,505050,10&#34; alt=&#34;&#34; width=&#34;400&#34; height=&#34;200&#34; /&gt;

&lt;img class=&#34;alignnone&#34; title=&#34;innodb_row_lock_waits_psec&#34; src=&#34;/blog/blog/assets/chart?cht=lc&amp;amp;chs=400x200&amp;amp;chts=303030,12&amp;amp;chtt=Oct+31,+17:00++-++Nov+11,+08:00+(10+days,+15+hours)&amp;amp;chdl=innodb_row_lock_waits_psec&amp;amp;chdlp=b&amp;amp;chco=ff8c00&amp;amp;chd=s:GWFGGYSHQJKFGJHIHIHNGHGGGKHHFFGGMMJFFKHHHKMIHGIIJGGFGFHGTNOGFJIHGJGKGJHGGGGFHFJFPIKFFJHJIFLKGGFIFGGEVEJGPILGFIGHJJHIJKGGFJGLIGKGJMSGGIGIGVGGQJGHHKHIGHFGLHMHFIFIGQGGIFGJHIEEHFHGKLJHGGGIYgTVXOaXabSUadW9gVfRSeaQbfalXeYcXTiGHIKHKEJEFFFGGGIGGGGHGKGGHGLGPJHGFJEG&amp;amp;chxt=x,y&amp;amp;chxr=1,0,1.42&amp;amp;chxl=0:||Nov+2,+20:00|Nov+4,+23:00|Nov+7,+02:00|Nov+9,+05:00|&amp;amp;chxs=0,505050,10&#34; alt=&#34;&#34; width=&#34;400&#34; height=&#34;200&#34; /&gt;
&lt;/blockquote&gt;
By the way, the above resulted from the fact that, due to a problematic query, all slave stopped replicating. Slaves participated in read-balancing, so when they went stale, all reads were directed at the master (the monitored node).
&lt;h4&gt;You have the metrics at your disposal&lt;/h4&gt;
Looking at the following chart:
&lt;blockquote&gt;
&lt;pre /&gt;&lt;img class=&#34;alignnone&#34; title=&#34;questions&#34; src=&#34;/blog/blog/assets/chart?cht=lc&amp;amp;chs=400x200&amp;amp;chts=303030,12&amp;amp;chtt=Nov+11,+15:15++-++Nov+12,+12:30+(0+days,+21+hours)&amp;amp;chdl=queries_psec|questions_psec|slow_queries_psec|com_commit_psec|com_set_option_psec&amp;amp;chdlp=b&amp;amp;chco=ff8c00,4682b4,9acd32,dc143c,9932cc&amp;amp;chd=s:pqpviksvvz0vuxxxjpw0mwpkkso1vhuvn0nnrtx2uisrnvoknmusomqvlyymsvpuweqslwumkomutcromzrinukvwcuzotujjto1shrtszqlu849mXenejkaZlZhcYbgciaZZegecUWhZkaYWebfaXVaecdZUZgdbSbccbcTXYeaaTYZfZeVjbnZhRdegcfYorkdmVadqenfcknkoadeuhrjcbptpkhkqkrqfjprrtmllmnqdwsusojoo0qtpwp4,abfjVQebgjaWeedkWRgcelWUcdclYUhddnaVidendUieflaUhcfkdRfefmgSjianfPkcdmfUegamfRmcfmgTgegmhQghgmgWeiepfShfhqjcqzwzfVYdbfbUXbXcYSYaYdVWUZabWTTbXeYUVZYdVTUYabYWUYbbXSWaYaYSSXZZVTVZaXZURZaWbQWZaaYWUWbaZSUadadVUcbbbTWYeabXUUcebUYbabdVUYbbdTWaaccWTddaeTWbdbgXXdci,AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA,AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA,MLOPJHNMNPLKNNNQJHOMNQJJMMMQLIPMNSLJPNNRNJPNNRKJPMNQNHONNRNIPPLROHQNMRNJOOMRNHRMNRNJONOROHOPOROKNPNTOIPOOTOMQUSUNJKNLOLJKMKMKIKLKMJJILLMKIIMKNJJJLKNIJJKKMKJIKLMKHJLKLKIIKKLJIJLKKKJHLLKLHJLKLKJIKLLKIJLMLMKJMMMMIJKMLLKIJMNLJKMLMMJJKLMNIKLLMMKIMMLNIKMMMOKKNMP&amp;amp;chxt=x,y&amp;amp;chxr=1,0,916.47&amp;amp;chxl=0:||Nov+11,+19:30|Nov+11,+23:45|Nov+12,+04:00|Nov+12,+08:15|&amp;amp;chxs=0,505050,10&#34; alt=&#34;&#34; width=&#34;400&#34; height=&#34;200&#34; /&gt;
&lt;/blockquote&gt;
It appears that there&#39;s no slow queries. But this may be misleading: perhaps there&#39;s just a little, that don&#39;t show due to the chart&#39;s large scale?

One could argue that this is the chart&#39;s fault. Perhaps there should be a distinct chart for &#34;slow queries percent&#34;. Perhaps I&#39;ll add one. But we can&#39;t have special charts for everything. It&#39;s would be too tiresome to look at hundreds of charts.

Anyway, my point is: let&#39;s verify just how many slow queries we have:
&lt;blockquote&gt;
&lt;pre&gt;SELECT slow_queries_psec FROM sv_hour ORDER BY id DESC;
+-------------------+
| slow_queries_psec |
+-------------------+
|              3.05 |
|              3.83 |
|              4.39 |
|              4.03 |
|              3.86 |
|              3.56 |
|              3.73 |
|              3.79 |
|              3.58 |
|              3.55 |
...
+-------------------+&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;So, between 3 and 4 slow queries per second. It doesn&#39;t look too good in this light. Checking on the percentage of slow queries (of total questions):&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;SELECT ROUND(100*slow_queries_diff/questions_diff, 1) AS slow_queries_percent
  FROM sv_hour ORDER BY id DESC LIMIT 10;&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;Or, since the above calculation is pre-defined in the reports tables:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;SELECT slow_queries_percent FROM sv_report_hour_recent;
+----------------------+
| slow_queries_percent |
+----------------------+
|                  0.8 |
|                  1.0 |
|                  1.2 |
|                  1.2 |
|                  1.1 |
|                  1.0 |
|                  1.1 |
|                  1.1 |
|                  1.0 |
...
+----------------------+&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;h4&gt;Accessible data&lt;/h4&gt;
&lt;p&gt;This is what I&#39;ve been trying to achieve with &lt;em&gt;mycheckpoint&lt;/em&gt;. As a DBA, consultant and SQL geek I find that direct SQL access works best for me. It&#39;s like loving command line interface over GUI tools. Direct SQL gives you so much more control and information.&lt;/p&gt;
&lt;p&gt;Charting is important, since it&#39;s easy to watch and get first impressions, or find extreme changes. But beware of relying on charts all the time. Scale issues, misleading human interpretation, technology limitations - all these make charts inaccurate.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://code.openark.org/forge/mycheckpoint&#34;&gt;mycheckpoint&lt;/a&gt; allows for both methods, and, I believe, intuitively so.&lt;/p&gt;
&lt;p&gt;&amp;lt;/propaganda&amp;gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>How to calculate a good InnoDB log file size - recap</title>
      <link>/blog/2009/10/20/how-to-calculate-a-good-innodb-log-file-size-recap/</link>
      <pubDate>Tue, 20 Oct 2009 21:04:40 +0000</pubDate>
      
      <guid>/blog/2009/10/20/how-to-calculate-a-good-innodb-log-file-size-recap/</guid>
      <description>&lt;p&gt;Following Baron Schwartz&#39; post: &lt;a href=&#34;http://www.mysqlperformanceblog.com/2008/11/21/how-to-calculate-a-good-innodb-log-file-size/&#34;&gt;How to calculate a good InnoDB log file size&lt;/a&gt;, which shows how to make an estimate for the InnoDB log file size, and based on &lt;a href=&#34;http://code.openark.org/blog/mysql/sql-querying-for-status-difference-over-time&#34;&gt;SQL: querying for status difference over time&lt;/a&gt;, I&#39;ve written a query to run on MySQL 5.1, which, upon sampling 60 seconds of status, estimates the InnoDB transaction log bytes that are expected to be written in the period of 1 hour.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Recap&lt;/em&gt;: this information can be useful if you&#39;re looking for a good &lt;strong&gt;innodb_log_file_size&lt;/strong&gt; value, such that will not pose too much I/O (smaller values will make for more frequent flushes), not will make for a too long recovery time (larger values mean more transactions to recover upon crash).&lt;/p&gt;
&lt;p&gt;It is assumed that the 60 seconds period represents an average system load, not some activity spike period. Edit the sleep time and factors as you will to sample longer or shorter periods.&lt;!--more--&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;&lt;strong&gt;SELECT&lt;/strong&gt;
  innodb_os_log_written_per_minute*60
    &lt;strong&gt;AS&lt;/strong&gt; estimated_innodb_os_log_written_per_hour,
  CONCAT(ROUND(innodb_os_log_written_per_minute*60/1024/1024, 1), &#39;MB&#39;)
    &lt;strong&gt;AS&lt;/strong&gt; estimated_innodb_os_log_written_per_hour_mb
&lt;strong&gt;FROM&lt;/strong&gt;
  (&lt;strong&gt;SELECT&lt;/strong&gt; &lt;strong&gt;SUM&lt;/strong&gt;(value) &lt;strong&gt;AS&lt;/strong&gt; innodb_os_log_written_per_minute &lt;strong&gt;FROM&lt;/strong&gt; (
    &lt;strong&gt;SELECT&lt;/strong&gt; -VARIABLE_VALUE &lt;strong&gt;AS&lt;/strong&gt; value
      &lt;strong&gt;FROM&lt;/strong&gt; INFORMATION_SCHEMA.GLOBAL_STATUS
      &lt;strong&gt;WHERE&lt;/strong&gt; VARIABLE_NAME = &#39;innodb_os_log_written&#39;
    &lt;strong&gt;UNION ALL&lt;/strong&gt;
    &lt;strong&gt;SELECT&lt;/strong&gt; SLEEP(60)
      &lt;strong&gt;FROM&lt;/strong&gt; DUAL
    &lt;strong&gt;UNION ALL&lt;/strong&gt;
    &lt;strong&gt;SELECT&lt;/strong&gt; VARIABLE_VALUE
      &lt;strong&gt;FROM&lt;/strong&gt; INFORMATION_SCHEMA.GLOBAL_STATUS
      &lt;strong&gt;WHERE&lt;/strong&gt; VARIABLE_NAME = &#39;innodb_os_log_written&#39;
  ) s1
) s2
;&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;Sample output:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;+------------------------------------------+---------------------------------------------+
| estimated_innodb_os_log_written_per_hour | estimated_innodb_os_log_written_per_hour_mb |
+------------------------------------------+---------------------------------------------+
|                                584171520 | 557.1MB                                     |
+------------------------------------------+---------------------------------------------+&lt;/pre&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>High Performance MySQL - a book to re-read</title>
      <link>/blog/2009/09/27/high-performance-mysql-a-book-to-re-read/</link>
      <pubDate>Sun, 27 Sep 2009 09:56:59 +0000</pubDate>
      
      <guid>/blog/2009/09/27/high-performance-mysql-a-book-to-re-read/</guid>
      <description>&lt;p&gt;I first read &lt;a href=&#34;http://www.amazon.com/High-Performance-MySQL-Optimization-Replication/dp/0596101716&#34;&gt;High Performance MySQL, 2nd edition&lt;/a&gt; about a year ago, when it first came out. I since re-read a few pages on occasion.&lt;/p&gt;
&lt;p&gt;In my previous posts I&#39;ve suggested ways to improve upon the common ranking solution. Very innovative stuff! Or... so I thought.&lt;/p&gt;
&lt;p&gt;I happened to browse through the book today, and a section on User Variables caught my eye. &#34;&lt;em&gt;Let&#39;s see if I get get some insight&lt;/em&gt;&#34;, I thought to myself. Imagine my surprise when I realized almost everything I&#39;ve suggested is discussed in this modest section, black on white, sitting on my bookshelf for over a year!&lt;/p&gt;
&lt;p&gt;I have read it a year back, have forgotten all about it, have re-invented stuff already solved and discussed... Oh, for more brain capacity...&lt;/p&gt;
&lt;p&gt;To be honest, this has happened to me more than once in the past few months; I&#39;m taking the habit of browsing the web when I&#39;m looking for answers to my problems; I forget that this book contains the answers to so many common, practical MySQL problems, and does so in a very direct and helpful manner.&lt;/p&gt;
&lt;p&gt;So, yet again, thumbs up to &lt;em&gt;High Performance MySQL&lt;/em&gt;. Really a must book. Get it if you haven&#39;t already!&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>