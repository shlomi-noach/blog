<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Innodb on code.openark.org</title>
    <link>/blog/tags/innodb/</link>
    <description>Recent content in Innodb on code.openark.org</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2016. All rights reserved.</copyright>
    <lastBuildDate>Sun, 20 Apr 2014 07:16:13 +0000</lastBuildDate>
    <atom:link href="/blog/tags/innodb/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>The mystery of MySQL 5.6 excessive buffer pool flushing</title>
      <link>/blog/2014/04/20/the-mystery-of-mysql-5-6-excessive-buffer-pool-flushing/</link>
      <pubDate>Sun, 20 Apr 2014 07:16:13 +0000</pubDate>
      
      <guid>/blog/2014/04/20/the-mystery-of-mysql-5-6-excessive-buffer-pool-flushing/</guid>
      <description>&lt;p&gt;I&#39;m experimenting with upgrading to MySQL &lt;strong&gt;5.6&lt;/strong&gt; and am experiencing an unexplained increase in disk I/O utilization. After discussing this with several people I&#39;m publishing in the hope that someone has an enlightenment on this.&lt;/p&gt;
&lt;p&gt;We have a few dozens servers in a normal replication topology. On this particular replication topology we&#39;ve already evaluated that &lt;strong&gt;STATEMENT&lt;/strong&gt; based replication is faster than &lt;strong&gt;ROW&lt;/strong&gt; based replication, and so we use &lt;strong&gt;SBR&lt;/strong&gt;. We have two different workloads on our slaves, applied by two different HAProxy groups, on three different data centres. Hardware-wise, servers of two groups use either Virident SSD cards or normal SAS spindle disks.&lt;/p&gt;
&lt;p&gt;Our servers are I/O bound. A common query used by both workloads looks up data that does not necessarily have a hotspot, and is very large in volume. DML is low, and we only have a few hundred statements per second executed on master (and propagated through replication).&lt;/p&gt;
&lt;p&gt;We have upgraded &lt;strong&gt;6&lt;/strong&gt; servers from all datacenters to &lt;strong&gt;5.6&lt;/strong&gt;, both on SSD and spindle disks, and are experiencing the following phenomena:&lt;!--more--&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A substantial increase in disk I/O utilization. See a &lt;strong&gt;10&lt;/strong&gt; day breakdown (upgrade is visible on &lt;strong&gt;04/14&lt;/strong&gt;) this goes on like this many days later:&lt;br /&gt;
&lt;blockquote&gt;&lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2014/04/5.5-to-5.6-disk-utilization-10-days.png&#34;&gt;&lt;img class=&#34;alignnone wp-image-6845 size-full&#34; src=&#34;/blog/blog/assets/5.5-to-5.6-disk-utilization-10-days.png&#34; alt=&#34;5.5-to-5.6-disk-utilization-10-days&#34; width=&#34;700&#34; height=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;A substantial increase in InnoDB buffer pool pages flush: Mr. Blue is our newly upgraded server; it joins Mr. Green upgraded a couple weeks ago. Mr. Red is still &lt;strong&gt;5.5&lt;/strong&gt;. This is the only MySQL graph that I could directly relate to the increase in I/O:&lt;br /&gt;
&lt;blockquote&gt;&lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2014/04/5.5-to-5.6-rise-in-innodb-buffer-pool-pages-flushed.png&#34;&gt;&lt;img class=&#34;alignnone size-full wp-image-6848&#34; src=&#34;/blog/blog/assets/5.5-to-5.6-rise-in-innodb-buffer-pool-pages-flushed.png&#34; alt=&#34;5.5-to-5.6-rise-in-innodb-buffer-pool-pages-flushed&#34; width=&#34;700&#34; height=&#34;350&#34; /&gt;&lt;/a&gt;&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;No change in workload (it takes some 60 minutes for caches to warm up, so measuring after that time). Same equal share of serving as dictated by HAProxy. Same amount of queries. Same amount of everything.&lt;/li&gt;
&lt;li&gt;Faster replication speed, on single thread - that&#39;s the good part! We see &lt;strong&gt;30%&lt;/strong&gt; and more improvement in replication speed. Tested by stopping &lt;strong&gt;SLAVE SQL_THREAD&lt;/strong&gt; for a number of pre-defined minutes, then measuring time it took for slave to catch up, up to 10 seconds lag. The results vary depending on the time of day and serving workload on slaves, but it is &lt;em&gt;consistently far faster&lt;/em&gt; with &lt;strong&gt;5.6&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The faster replication speed motivates us to continue with the experiment, and is of a significant factor in our decision. However we are concerned about the I/O utilization and excessive flushing.&lt;/p&gt;
&lt;p&gt;The above graphs depict the &lt;strong&gt;5.6&lt;/strong&gt; status without any configuration changes as compared to &lt;strong&gt;5.5&lt;/strong&gt;. I took some days to reconfigure the following variables, with no change to the rate of flushed pages (though some changes visible in double-wite buffer writes):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;innodb_log_file_size=128M/2G&lt;/li&gt;
&lt;li&gt;innodb_adaptive_flushing:=0/1&lt;/li&gt;
&lt;li&gt;innodb_adaptive_flushing_lwm:=0/70&lt;/li&gt;
&lt;li&gt;innodb_max_dirty_pages_pct := 75/90&lt;/li&gt;
&lt;li&gt;innodb_flush_neighbors:=0/1&lt;/li&gt;
&lt;li&gt;innodb_max_dirty_pages_pct_lwm:=75/90&lt;/li&gt;
&lt;li&gt;innodb_old_blocks_time:=0/1000&lt;/li&gt;
&lt;li&gt;innodb_io_capacity:=50/100/200&lt;/li&gt;
&lt;li&gt;innodb_io_capacity_max:=50/100/1000&lt;/li&gt;
&lt;li&gt;relay_log_info_repository:=&#39;table&#39;/&#39;file&#39;&lt;/li&gt;
&lt;li&gt;master_info_repository:=&#39;table&#39;/&#39;file&#39;&lt;/li&gt;
&lt;li&gt;default_tmp_storage_engine:=&#39;myisam&#39;/&#39;innodb&#39;&lt;/li&gt;
&lt;li&gt;eq_range_index_dive_limit:=0/10&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And more... Have done patient one-by-one or combinations of the above where it made sense. As you see I began with the usual suspects and moved on to more esoteric stuff. I concentrated on new variables introduced in &lt;strong&gt;5.6&lt;/strong&gt;, or ones where the defaults have changed, or ones we have explicitly changed the defaults from.&lt;/p&gt;
&lt;p&gt;The above is consistent on all upgraded servers. On SSD the disk utilization is lower, but still concerning.&lt;/p&gt;
&lt;p&gt;Our use case is very different from the one &lt;a href=&#34;http://yoshinorimatsunobu.blogspot.co.il/2013/12/single-thread-performance-regression-in.html&#34;&gt;presented by Yoshinori Matsunobu&lt;/a&gt;. and apparently not too many have experienced upgrading to &lt;strong&gt;5.6&lt;/strong&gt;. I&#39;m hoping someone might shed some light.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>TokuDB configuration variables of interest</title>
      <link>/blog/2013/10/23/tokudb-configuration-variables-of-interest/</link>
      <pubDate>Wed, 23 Oct 2013 19:42:12 +0000</pubDate>
      
      <guid>/blog/2013/10/23/tokudb-configuration-variables-of-interest/</guid>
      <description>&lt;p&gt;During our experiments I came upon a few TokuDB variables of interest; if you are using TokuDB you might want to look into these:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h4&gt;tokudb_analyze_time&lt;/h4&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p style=&#34;padding-left: 30px;&#34;&gt;This is a boundary on the number of seconds an &lt;strong&gt;ANALYZE TABLE&lt;/strong&gt; will operate on each index on each partition on a TokuDB table.&lt;/p&gt;
&lt;p style=&#34;padding-left: 30px;&#34;&gt;That is, if &lt;strong&gt;tokudb_analyze_time = 5&lt;/strong&gt;, and your table has &lt;strong&gt;4&lt;/strong&gt; indexes (including &lt;strong&gt;PRIMARY&lt;/strong&gt;) and &lt;strong&gt;7&lt;/strong&gt; partitions, then the total runtime is limited to &lt;strong&gt;5*4*7 = 140&lt;/strong&gt; seconds.&lt;/p&gt;
&lt;p style=&#34;padding-left: 30px;&#34;&gt;Default in &lt;strong&gt;7.1.0&lt;/strong&gt;: &lt;strong&gt;5&lt;/strong&gt; seconds&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h4&gt;tokudb_cache_size&lt;/h4&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p style=&#34;padding-left: 30px;&#34;&gt;Similar to &lt;strong&gt;innodb_buffer_pool_size&lt;/strong&gt;, this variable sets the amount of memory allocated by TokuDB for caching pages. Like InnoDB the table is clustered within the index, so the cache includes pages for both indexes and data.&lt;/p&gt;
&lt;p style=&#34;padding-left: 30px;&#34;&gt;Default: &lt;strong&gt;50%&lt;/strong&gt; of total memory&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h4&gt;tokudb_directio&lt;/h4&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p style=&#34;padding-left: 30px;&#34;&gt;Boolean, values are &lt;strong&gt;0/1&lt;/strong&gt;. Setting &lt;strong&gt;tokudb_directio = 1&lt;/strong&gt; is like specifying &lt;strong&gt;innodb_flush_method = O_DIRECT&lt;/strong&gt;. Which in turn means the OS should not cache pages requested by TokuDB. Default: &lt;strong&gt;0&lt;/strong&gt;.&lt;/p&gt;
&lt;p style=&#34;padding-left: 30px;&#34;&gt;Now here&#39;s the interesting part: we are used to tell InnoDB to get the most memory we can provide (because we want it to cache as much as it can) and to avoid OS caching (because that would mean a page would appear both in the buffer pool and in OS memory, which is a waste). So the following setup is common:&lt;!--more--&gt;&lt;/p&gt;
&lt;blockquote style=&#34;padding-left: 30px;&#34;&gt;
&lt;pre style=&#34;padding-left: 30px;&#34;&gt;&lt;strong&gt;innodb_buffer_pool_size&lt;/strong&gt; = [as much as you can allocate while leaving room for connection memory]G
&lt;strong&gt;innodb_flush_method&lt;/strong&gt; = O_DIRECT&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p style=&#34;padding-left: 30px;&#34;&gt;And my first instinct was to do the same for TokuDB. But after speaking to Gerry Narvaja of Tokutek, I realized it was not that simple. The reason TokuDB&#39;s default memory allocation is &lt;strong&gt;50%&lt;/strong&gt; and not, say, &lt;strong&gt;90%&lt;/strong&gt;, is that OS cache caches the data in compressed form, while TokuDB cache caches data in uncompressed form. Which means if you limit the TokuDB cache, you allow for more cache to the OS, that is used to cache compressed data, which means &lt;em&gt;more data&lt;/em&gt; (hopefully, pending duplicates) in memory.&lt;/p&gt;
&lt;p style=&#34;padding-left: 30px;&#34;&gt;I did try both options and did not see an obvious difference, but did not test this thoroughly. My current setup is:&lt;/p&gt;
&lt;blockquote style=&#34;padding-left: 30px;&#34;&gt;
&lt;pre style=&#34;padding-left: 30px;&#34;&gt;&lt;strong&gt;#No setup. just keep to the default for both:&lt;/strong&gt;
#tokudb_cache_size
#tokudb_directio&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h4&gt;tokudb_commit_sync&lt;/h4&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h4&gt;tokudb_fsync_log_period&lt;/h4&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p style=&#34;padding-left: 30px;&#34;&gt;These two variable are similar in essence to &lt;strong&gt;innodb_flush_log_at_trx_commit&lt;/strong&gt;, but allow for finer tuning. With &lt;strong&gt;innodb_flush_log_at_trx_commit&lt;/strong&gt; you choose between syncing the transaction log to disk upon each commit and once per second. With &lt;strong&gt;tokudb_commit_sync = 1&lt;/strong&gt; (which is default) you get transaction log sync to disk per commit. When &lt;strong&gt;tokudb_commit_sync = 0&lt;/strong&gt;, then &lt;strong&gt;tokudb_fsync_log_period&lt;/strong&gt; dictates the interval between flushes. So a value of &lt;strong&gt;tokudb_fsync_log_period = 1000&lt;/strong&gt; means once per second.&lt;/p&gt;
&lt;p style=&#34;padding-left: 30px;&#34;&gt;Since our original InnoDB installation used &lt;strong&gt;innodb_flush_log_at_trx_commit = 2&lt;/strong&gt;, our TokuDB setup is:&lt;/p&gt;
&lt;blockquote style=&#34;padding-left: 30px;&#34;&gt;
&lt;pre style=&#34;padding-left: 30px;&#34;&gt;&lt;strong&gt;tokudb_commit_sync&lt;/strong&gt; = 0
&lt;strong&gt;tokudb_fsync_log_period&lt;/strong&gt; = 1000&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;h4&gt;tokudb_load_save_space&lt;/h4&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p style=&#34;padding-left: 30px;&#34;&gt;Turned on (value &lt;strong&gt;1&lt;/strong&gt;) by default as of TokuDB &lt;strong&gt;7.1.0&lt;/strong&gt;, this parameter decides whether temporary file created on bulk load operations (e.g. ALTER TABLE) are compressed or uncompressed. Do yourself a big favour (why? &lt;a href=&#34;http://code.openark.org/blog/mysql/converting-an-olap-database-to-tokudb-part-2-the-process-of-migration&#34;&gt;read here&lt;/a&gt;) and keep it on. Our setup is:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;&lt;strong&gt;tokudb_load_save_space&lt;/strong&gt; = 1&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;TokuDB&#39;s general recommendation is: don&#39;t change the variables; the engine should work well right out of the box. I like the approach (by MySQL &lt;strong&gt;5.5&lt;/strong&gt; I already lost count of InnoDB variables that can have noticeable impact; with &lt;strong&gt;5.6&lt;/strong&gt; I&#39;m all but lost). The complete list of configuration variables is found in &lt;a href=&#34;http://www.tokutek.com/wp-content/uploads/2013/10/mysql-5.5.30-tokudb-7.1.0-users-guide.pdf&#34;&gt;TokuDB&#39;s Users Guide&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Converting an OLAP database to TokuDB, part 3: operational stuff</title>
      <link>/blog/2013/10/14/converting-an-olap-database-to-tokudb-part-3-operational-stuff/</link>
      <pubDate>Mon, 14 Oct 2013 12:03:43 +0000</pubDate>
      
      <guid>/blog/2013/10/14/converting-an-olap-database-to-tokudb-part-3-operational-stuff/</guid>
      <description>&lt;p&gt;This is the third post in a series of posts describing our experience in migrating a large DWH server to TokuDB (see &lt;a href=&#34;http://code.openark.org/blog/mysql/converting-an-olap-database-to-tokudb-part-1&#34;&gt;1st&lt;/a&gt; and &lt;a href=&#34;http://code.openark.org/blog/mysql/converting-an-olap-database-to-tokudb-part-2-the-process-of-migration&#34;&gt;2nd&lt;/a&gt; parts). This post discusses operations; namely ALTER TABLE operations in TokuDB. We ran into quite a few use cases by this time that we can shed light on.&lt;/p&gt;
&lt;p&gt;Quick recap: we&#39;ve altered one of out DWH slaves to TokuDB, with the goal of migrating most of out servers, including the master, to TokuDB.&lt;/p&gt;
&lt;h4&gt;Adding an index&lt;/h4&gt;
&lt;p&gt;Shortly after migrating our server to TokuDB we noticed an unreasonably disproportionate slave lag on our TokuDB slave (red line in chart below) as compared to other slaves.&lt;/p&gt;
&lt;blockquote&gt;&lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2013/09/tokudb-slave-lag.png&#34;&gt;&lt;img alt=&#34;tokudb-slave-lag&#34; src=&#34;/blog/blog/assets/tokudb-slave-lag.png&#34; width=&#34;700&#34; height=&#34;329&#34; /&gt;&lt;/a&gt;&lt;/blockquote&gt;
&lt;p&gt;Quick investigation led to the fact that, coincidentally, a manual heavy-duty operation was just taking place, which updated some year&#39;s worth of data retroactively. OK, but why so slow on TokuDB? Another quick investigation led to an apples vs. oranges problem: as depicted in &lt;a href=&#34;http://code.openark.org/blog/mysql/converting-an-olap-database-to-tokudb-part-1&#34;&gt;part 1&lt;/a&gt;, our original setup included MONTHly partitioning on our larger tables, whereas we could not do the same in TokuDB, where we settled for YEARly partitioning.&lt;/p&gt;
&lt;p&gt;The heavy-duty operation included a query that was relying on the MONTHly partitioning to do reasonable pruning: a &lt;strong&gt;WHERE&lt;/strong&gt; condition on a date column did the right partition pruning; but where on InnoDB that would filter &lt;strong&gt;1&lt;/strong&gt; month&#39;s worth of data, on TokuDB it would filter &lt;strong&gt;1&lt;/strong&gt; &lt;em&gt;year&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Wasn&#39;t it suggested that TokuDB has online table operations? I decided to give it a shot, and add a proper index on our date column (I actually created a compound index, but irrelevant).&lt;/p&gt;
&lt;p&gt;It took &lt;strong&gt;13&lt;/strong&gt; minutes to add an index on a &lt;strong&gt;1GB&lt;/strong&gt; TokuDB table (approx. &lt;strong&gt;20GB&lt;/strong&gt; InnoDB uncompressed equivalent):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;strong&gt;ALTER&lt;/strong&gt; was non blocking: table was unlocked at that duration&lt;/li&gt;
&lt;li&gt;The client issuing the &lt;strong&gt;ALTER&lt;/strong&gt; &lt;em&gt;was&lt;/em&gt; blocked (I thought it would happen completely in the background) -- but who cares?&lt;/li&gt;
&lt;li&gt;I would say &lt;strong&gt;13&lt;/strong&gt; minutes is fast&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Not surprisingly adding the index eliminated the problem altogether.&lt;/p&gt;
&lt;h4&gt;Modifying a PRIMARY KEY&lt;/h4&gt;
&lt;p&gt;It was suggested by our DBA that there was a long time standing need to modify our &lt;strong&gt;PRIMARY KEY&lt;/strong&gt;. It was impossible to achieve with our InnoDB setup (not enough disk space for the operation, would take weeks to complete if we did have the disk space). Would it be possible to modify our TokuDB tables? On some of our medium-sized tables we issued an &lt;strong&gt;ALTER&lt;/strong&gt; of the form:&lt;!--more--&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;ALTER TABLE my_table DROP PRIMARY KEY, ADD PRIMARY KEY (c1, c2, c3, ...);&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;Time-wise the operation completed in good time. We did note, however, that the &lt;a href=&#34;http://common-schema.googlecode.com/svn/trunk/common_schema/doc/html/tokudb_file_map.html&#34;&gt;disk space consumed by the new table&lt;/a&gt; was &lt;em&gt;doubled&lt;/em&gt;. Was it due to the fact we added two columns to our PK? Did that account for the bloated space? I did not believe so, and decided to rebuild the table:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;OPTIMIZE TABLE my_table&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;Nope. Space not reduced. However we were unconvinced and &lt;a href=&#34;https://groups.google.com/forum/#!topic/tokudb-user/ow47QY2pcgU&#34;&gt;asked&lt;/a&gt;. As usual, we got quick response from the Tokutek team; this was a bug: while our original table used the TOKUDB_SMALL row format (high compression), the table rebuild reset it to TOKUDB_FAST (normal compression), which makes for roughly twice the file size. The bug was filed as: &lt;a href=&#34;https://github.com/Tokutek/ft-engine/issues/107&#34;&gt;alter table operations that rebuild the table lose the original tokudb compression&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Now, we &lt;em&gt;were&lt;/em&gt; altering the &lt;strong&gt;PRIMARY KEY&lt;/strong&gt;. We were not expecting an online operation anyhow, and didn&#39;t mind blocking the table; hence the solution was simple: make sure to spceify the row format:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;ALTER TABLE my_table DROP PRIMARY KEY, ADD PRIMARY KEY (c1, c2, c3, ...) ENGINE=TokuDB ROW_FORMAT=TOKUDB_SMALL;&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;This worked in terms of disk space -- but we only later realized it would still make us trouble.&lt;/p&gt;
&lt;h4&gt;Modifying a PRIMARY KEY on our largest table&lt;/h4&gt;
&lt;p&gt;We moved on to our largest table: originally &lt;strong&gt;1TB&lt;/strong&gt; InnoDB &lt;strong&gt;COMPRESSED&lt;/strong&gt;, worth of &lt;strong&gt;2TB&lt;/strong&gt; uncompressed. With TokuDB it went down to &lt;strong&gt;100GB&lt;/strong&gt;. Converting this table to TokuDB took about &lt;strong&gt;40&lt;/strong&gt; hours, which is just fast. We issued an ALTAR TABLE modifying the PRIMARY KEY as above and waited.&lt;/p&gt;
&lt;p&gt;The operation did not complete after &lt;strong&gt;40&lt;/strong&gt; hours. Nor after &lt;strong&gt;3&lt;/strong&gt; days. By day &lt;strong&gt;4&lt;/strong&gt; we thought we might look into this. Fortunately, TokuDB is friendly on &lt;strong&gt;SHOW PROCESSLIST&lt;/strong&gt; and provides you with useful information, such as &#34;&lt;strong&gt;Fetched about 1234567890 rows, loading data still remains&lt;/strong&gt;&#34;. Yikes! We extrapolated the values to realize it would take &lt;strong&gt;2&lt;/strong&gt; &lt;em&gt;weeks&lt;/em&gt; to complete! Weekend went by and we decided to find a better way. Again, posting on the tokudb-user group, we got a definitive answer: a table rebuild does not utilize the &lt;em&gt;bulk loader&lt;/em&gt; (you really want to be friends with the bulk loader, it&#39;s the process that loads your data quickly).&lt;/p&gt;
&lt;p&gt;And so we chose to &lt;strong&gt;KILL&lt;/strong&gt; the &lt;strong&gt;ALTER&lt;/strong&gt; process and go another way; again, &lt;strong&gt;KILL&lt;/strong&gt;s are very easy with TokuDB &lt;strong&gt;ALTER&lt;/strong&gt; operations: took &lt;strong&gt;3&lt;/strong&gt; minutes to abort this week old operation. The alternative operation was:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;CREATE TABLE my_table_New LIKE my_table;
ALTER TABLE my_table_New DROP PRIMARY KEY, ADD PRIMARY KEY (c1, c2, c3, ...) ENGINE=TokuDB ROW_FORMAT=TOKUDB_SMALL;
INSERT INTO my_table_New SELECT * FROM my_table;
RENAME TABLE my_table TO my_table_Old, my_table_New TO my_table;
DROP TABLE my_table_Old;&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;The &lt;strong&gt;INSERT INTO ... SELECT&lt;/strong&gt; operation does use the bulk loader when you do it on an empty table. It completed within merely &lt;strong&gt;30&lt;/strong&gt; hours. Hurrah!&lt;/p&gt;
&lt;h4&gt;DROPping a TABLE&lt;/h4&gt;
&lt;p&gt;It was an immediate operation to drop our &#34;Old&#34; table -- subsecond. Nothing like your InnoDB DROP.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Converting an OLAP database to TokuDB, part 1</title>
      <link>/blog/2013/09/03/converting-an-olap-database-to-tokudb-part-1/</link>
      <pubDate>Tue, 03 Sep 2013 09:04:12 +0000</pubDate>
      
      <guid>/blog/2013/09/03/converting-an-olap-database-to-tokudb-part-1/</guid>
      <description>&lt;p&gt;This is the first in a series of posts describing my impressions of converting a large OLAP server to TokuDB. There&#39;s a lot to tell, and the experiment is not yet complete, so this is an ongoing blogging. In this post I will describe the case at hand and out initial reasons for looking at TokuDB.&lt;/p&gt;
&lt;p&gt;Disclosure: I have no personal interests and no company interests; we did get friendly, useful and free advice from Tokutek engineers. TokuDB is open source and free to use, though commercial license is also available.&lt;/p&gt;
&lt;h4&gt;The case at hand&lt;/h4&gt;
&lt;p&gt;We have a large and fast growing DWH MySQL setup. This data warehouse is but one component in a larger data setup, which includes Hadoop, Cassandra and more. For online dashboards and most reports, MySQL is our service. We populate this warehouse mainly via Hive/Hadoop. Thus, we have an hourly load of data from Hive, as well as a larger daily load.&lt;/p&gt;
&lt;p&gt;There are some updates on the data, but the majority of writes are just &lt;strong&gt;mysqlimport&lt;/strong&gt;s of Hive queries.&lt;/p&gt;
&lt;p&gt;Usage of this database is OLAP: no concurrency issues here; we have some should-be-fast-running queries issued by our dashboards, as well as ok-to-run-longer queries issued for reports.&lt;/p&gt;
&lt;p&gt;Our initial and most burning trouble is with size. Today we use &lt;strong&gt;COMPRESSED&lt;/strong&gt; InnoDB tables (&lt;strong&gt;KEY_BLOCK_SIZE&lt;/strong&gt; is default, i.e. &lt;strong&gt;8&lt;/strong&gt;). Our data volume sums right now at about &lt;strong&gt;2TB&lt;/strong&gt;. I happen to know this translates as &lt;strong&gt;4TB&lt;/strong&gt; of uncompressed data.&lt;/p&gt;
&lt;p&gt;However growth of data is accelerating. A year ago we would capture a dozen GB per month. Today it is a &lt;strong&gt;100GB&lt;/strong&gt; per month, and by the end of this year it may climb to &lt;strong&gt;150GB&lt;/strong&gt; per month or more.&lt;/p&gt;
&lt;p&gt;Our data is not sharded. We have a simple replication topology of some &lt;strong&gt;6&lt;/strong&gt; servers. Machines are quite generous as detailed following. And yet, we will be running out of resources shortly: disk space (total &lt;strong&gt;2.7TB&lt;/strong&gt;) is now running low and is expected to run out in about six months. One of my first tasks in Outbrain is to find a solution to our DWH growth problem. The solution could be sharding; it could be a commercial DWH product; anything that works.&lt;!--more--&gt;&lt;/p&gt;
&lt;h4&gt;The approach we experiment with&lt;/h4&gt;
&lt;p&gt;It was at my initial interview that I suggested &lt;a href=&#34;http://www.tokutek.com/products/tokudb-for-mysql/&#34;&gt;TokuDB&lt;/a&gt; might be a good solution, with the primary reason of being so good with compression. And we decided to experiment with this simple (setup-wise) solution of compression. If we could compress the data even by &lt;strong&gt;50%&lt;/strong&gt;, that would buy us considerable time. And it&#39;s the simplest approach as we would need to change nothing at the application side, nor add additional frameworks.&lt;/p&gt;
&lt;p&gt;Of course, we were already using InnoDB &lt;strong&gt;COMPRESSED&lt;/strong&gt; tables. How about just improving the compression? And here I thought to myself: we can try &lt;strong&gt;KEY_BLOCK_SIZE=4&lt;/strong&gt;, which I know would generally compress by &lt;strong&gt;50%&lt;/strong&gt; as compared to &lt;strong&gt;KEY_BLOCK_SIZE=8&lt;/strong&gt; (not always, but in many use cases). We&#39;re already using InnoDB so this isn&#39;t a new beast; it will be &#34;more of the same&#34;. It would work.&lt;/p&gt;
&lt;p&gt;I got myself a dedicated machine: a slave in our production topology I am free to play with. I installed TokuDB &lt;strong&gt;7.0.1&lt;/strong&gt;, later upgraded to &lt;strong&gt;7.0.3&lt;/strong&gt;, based on MySQL &lt;strong&gt;5.5.30&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The machine is a Dell Inc. &lt;strong&gt;PowerEdge R510&lt;/strong&gt; machine, with &lt;b&gt;16&lt;/b&gt; CPUs @ &lt;b&gt;2.1 GHz&lt;/b&gt; and &lt;b&gt;126 GiB&lt;/b&gt; RAM, &lt;b&gt;16 GiB&lt;/b&gt; Swap. OS is CentOS &lt;strong&gt;5.7&lt;/strong&gt;,  kernel &lt;strong&gt;2.6.18&lt;/strong&gt;. We have RAID &lt;strong&gt;10&lt;/strong&gt; over local &lt;strong&gt;10k&lt;/strong&gt; RPM SAS disks (10x&lt;strong&gt;600GB&lt;/strong&gt; disks)&lt;/p&gt;
&lt;h4&gt;How to compare InnoDB &amp;amp; TokuDB?&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;2TB&lt;/strong&gt; of compressed data (for absolute measurement I consider it to be a &lt;strong&gt;4TB&lt;/strong&gt; worth of data) is quite a large setup. How do I do the comparison? I don&#39;t even have too much disk space here...&lt;/p&gt;
&lt;p&gt;We have tables of various size. Our largest is in itself &lt;strong&gt;1TB&lt;/strong&gt; (&lt;strong&gt;2TB&lt;/strong&gt; uncompressed) - half of the entire volume. The rest ranging &lt;strong&gt;330GB&lt;/strong&gt;, &lt;strong&gt;140GB&lt;/strong&gt;, &lt;strong&gt;120GB&lt;/strong&gt;, &lt;strong&gt;90GB&lt;/strong&gt;, &lt;strong&gt;50GB&lt;/strong&gt; and below. We have &lt;strong&gt;MONTH&lt;/strong&gt;ly partitioning schemes on most tables and obviously on our larger tables.&lt;/p&gt;
&lt;p&gt;For our smaller tables, we could just &lt;strong&gt;CREATE TABLE test_table LIKE small_table&lt;/strong&gt;, populating it and comparing compression. However, the really interesting question (and perhaps the only interesting question compression-wise) is how well would our larger (and specifically largest) tables would compress.&lt;/p&gt;
&lt;p&gt;Indeed, for our smaller tables we saw between &lt;strong&gt;20%&lt;/strong&gt; to &lt;strong&gt;70%&lt;/strong&gt; reduction in size when using stronger InnoDB compression: &lt;strong&gt;KEY_BLOCK_SIZE=4/2/1&lt;/strong&gt;. How well would that work on our larger tables? How much slower would it be?&lt;/p&gt;
&lt;p&gt;We know MySQL partitions are implemented by actual &lt;em&gt;independent&lt;/em&gt; tables. Our testing approach was: let&#39;s build a test_table from a one month worth of data (== one single partition) of our largest table. We tested:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The time it takes to load the entire partition (about &lt;strong&gt;120M&lt;/strong&gt; rows, &lt;strong&gt;100GB COMPRESSED&lt;/strong&gt; data as seen on &lt;strong&gt;.idb&lt;/strong&gt; file)&lt;/li&gt;
&lt;li&gt;The time it would take to load a single day&#39;s worth of data from Hive/Hadoop (loading real data, as does our nightly import)&lt;/li&gt;
&lt;li&gt;The time it would take for various important &lt;strong&gt;SELECT&lt;/strong&gt; query to execute on this data.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;InnoDB vs. TokuDB comparison&lt;/h4&gt;
&lt;p&gt;In this post I will only describe our impressions of compression size. I have a lot to say about TokuDB vs InnoDB partitioning and queries; this will wait till later post.&lt;/p&gt;
&lt;p&gt;So here goes:&lt;/p&gt;
&lt;table border=&#34;0&#34; cellspacing=&#34;0&#34;&gt;
&lt;colgroup width=&#34;85&#34;&gt;&lt;/colgroup&gt;
&lt;colgroup width=&#34;155&#34;&gt;&lt;/colgroup&gt;
&lt;colgroup width=&#34;152&#34;&gt;&lt;/colgroup&gt;
&lt;colgroup width=&#34;147&#34;&gt;&lt;/colgroup&gt;
&lt;colgroup width=&#34;141&#34;&gt;&lt;/colgroup&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#E6E6E6&#34; height=&#34;31&#34;&gt;&lt;b&gt;Engine&lt;/b&gt;&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#E6E6E6&#34;&gt;&lt;b&gt;Compression&lt;/b&gt;&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#E6E6E6&#34;&gt;&lt;b&gt;Time to Insert 1 month&lt;/b&gt;&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#E6E6E6&#34;&gt;&lt;b&gt;Table size (optimized)&lt;/b&gt;&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#E6E6E6&#34;&gt;&lt;b&gt;Time to import 1 day&lt;/b&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34; height=&#34;17&#34;&gt;InnoDB&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34;&gt;8k&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34;&gt;&lt;strong&gt;10.5h&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34;&gt;58GB&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34;&gt;&lt;b&gt;32m&lt;/b&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34; height=&#34;17&#34;&gt;InnoDB&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34;&gt;4k&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34;&gt;48h&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34;&gt;33GB&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34;&gt;unknown (too long)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34; height=&#34;17&#34;&gt;TokuDB&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34;&gt;quicklz&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34;&gt;14h&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34;&gt;17GB&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34;&gt;40m&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34; height=&#34;17&#34;&gt;TokuDB&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34;&gt;lzma (small/aggresive)&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34;&gt;15h&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34;&gt;&lt;b&gt;7.5GB&lt;/b&gt;&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34;&gt;42m&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Some comments and insights:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Each test was performed 3-4 times. There were no significant differences on the various cycles.&lt;/li&gt;
&lt;li&gt;The &lt;strong&gt;1&lt;/strong&gt; month insert was done courtesy &lt;a href=&#34;http://common-schema.googlecode.com/svn/trunk/common_schema/doc/html/query_script_split.html&#34;&gt;QueryScript split&lt;/a&gt;,  &lt;strong&gt;5,000&lt;/strong&gt; rows at a time, no throttling.&lt;/li&gt;
&lt;li&gt;The &lt;strong&gt;1&lt;/strong&gt; day import via &lt;em&gt;mysqlimport&lt;/em&gt;. There were multiple files imported. Each file is sorted by &lt;strong&gt;PRIMARY KEY ASC&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Isn&#39;t it nice to know that your &lt;strong&gt;100GB&lt;/strong&gt; InnoDB table actually fits within &lt;strong&gt;58GB&lt;/strong&gt; when rebuilt?&lt;/li&gt;
&lt;li&gt;For InnoDB &lt;strong&gt;flush_logs_at_trx_commit=2&lt;/strong&gt;, &lt;strong&gt;flush_method=O_DIRECT&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;I used default configuration to TokuDB -- touched nothing. More on this in later post.&lt;/li&gt;
&lt;li&gt;InnoDB &lt;strong&gt;4k&lt;/strong&gt; was &lt;em&gt;prohibitively&lt;/em&gt; slow to load data. It was so slow so as to be unacceptable. For the 1 day load it took &lt;strong&gt;1&lt;/strong&gt; hour for a mere &lt;strong&gt;20%&lt;/strong&gt; of data to load. &lt;strong&gt;1&lt;/strong&gt; hour was already marginal for our requirements; waiting for &lt;strong&gt;5&lt;/strong&gt; hours was out of the question. I tested several times, never got to wait for completion. Did I say it would just be &#34;more of the same&#34;? &lt;strong&gt;4k&lt;/strong&gt; turned to be &#34;not an option&#34;.&lt;/li&gt;
&lt;li&gt;I saw almost no difference in load time between the two TokuDB compression formats. Both somewhat (30%) longer than InnoDB to load, but comparable.&lt;/li&gt;
&lt;li&gt;TokuDB compression: nothing short of &lt;em&gt;amazing&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With InnoDB &lt;strong&gt;4k&lt;/strong&gt; being &#34;not an option&#34;, and with both TokuDB compressions being similar in load time yet so different in compression size, we are left with the following conclusion: if we want to compress more than our existing 8k (and we have to) - TokuDB&#39;s &lt;em&gt;agressive compression&lt;/em&gt; (aka small, aka lzma) is our only option.&lt;/p&gt;
&lt;h4&gt;Shameless plug&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;http://code.google.com/p/common-schema/&#34;&gt;common_schema&lt;/a&gt; turned to be quite the &#34;save the day&#34; tool here. Not only did we use it to extract 100GB of data from a large dataset and load it onto our tables, it also helped out in the ALTER process for TokuDB: at this time (&amp;lt;=&lt;strong&gt; 7.0.4&lt;/strong&gt;) TokuDB still has a bug with &lt;strong&gt;KEY_BLOCK_SIZE&lt;/strong&gt;: when this option is found in table definition, it impacts TokuDB&#39;s indexes by bloating them. This is how &lt;a href=&#34;http://common-schema.googlecode.com/svn/trunk/common_schema/doc/html/sql_alter_table_tokudb.html&#34;&gt;sql_alter_table_tokudb&lt;/a&gt; was born. Hopefully it will be redundant shortly.&lt;/p&gt;
&lt;h4&gt;More to come&lt;/h4&gt;
&lt;p&gt;Was our test fair? Should we have configure TokuDB differently? Is loading via small &lt;strong&gt;5,000&lt;/strong&gt; row chunks the right way?&lt;/p&gt;
&lt;p&gt;In the next post I will describe the process of migrating our 4TB worth of data to TokuDB, pitfalls, issues, party crushers, sport spoilers, configuration, recovery, cool behaviour and general advice you should probably want to embrace. At later stage I&#39;ll describe how our DWH looks after migration. Finally I&#39;ll share some (ongoing) insights on performance.&lt;/p&gt;
&lt;p&gt;You&#39;ll probably want to know &#34;How much is (non compressed) &lt;strong&gt;4TB&lt;/strong&gt; of data worth in TokuDB?&#34; Let&#39;s keep the suspense :)&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Easy SELECT COUNT(*) with split()</title>
      <link>/blog/2013/06/08/easy-select-count-with-split/</link>
      <pubDate>Sat, 08 Jun 2013 06:41:13 +0000</pubDate>
      
      <guid>/blog/2013/06/08/easy-select-count-with-split/</guid>
      <description>&lt;p&gt;The two conservative ways of getting the number of rows in an InnoDB table are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;SELECT COUNT(*) FROM my_table&lt;/strong&gt;:&lt;br /&gt;
provides with an accurate number, but makes for a long running transaction which take ages on large tables. Long transactions make for locks&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SELECT TABLE_ROWS FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA=&#39;my_schema&#39; AND TABLE_NAME=&#39;my_table&#39;&lt;/strong&gt;, or get same info via &lt;strong&gt;SHOW TABLE STATUS&lt;/strong&gt;.&lt;br /&gt;
Gives immediate response, but the value can be &lt;em&gt;way off&lt;/em&gt;; it can be two times as large as real value, or half the value. For query execution plans this may be a &#34;good enough&#34; estimation, but typically you just can&#39;t trust it for your own purposes.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Get a good estimate using chunks&lt;/h4&gt;
&lt;p&gt;You can get a good estimate by calculating the total number of rows in steps. Walk the table 1,000 rows at a time, and keep a counter. Each chunk is its own transaction, so, if the table is modified while counting, the final value does not make for an accurate account at any point in time. Typically this should be a far better estimate than &lt;strong&gt;TABLE_ROWS&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://common-schema.googlecode.com/svn/trunk/common_schema/doc/html/query_script_split.html&#34;&gt;QueryScript&#39;s split()&lt;/a&gt; construct provides you with the means to work this out. Consider this script:&lt;!--more--&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;set @total := 0;

split(SELECT COUNT(*) FROM world.City INTO @chunk) {
  set @total = @total + @chunk;
}

select @total;&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;split()&lt;/strong&gt; breaks the above &lt;strong&gt;SELECT COUNT(*)&lt;/strong&gt; into distinct chunks, like:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;SELECT COUNT(*) FROM world.City WHERE ((((`City`.`ID` &amp;gt; &#39;3000&#39;))) AND (((`City`.`ID` &amp;lt; &#39;4000&#39;)) OR ((`City`.`ID` = &#39;4000&#39;)))) INTO @chunk&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;You can make this a one liner like this:&lt;/p&gt;
&lt;blockquote&gt;call common_schema.run(&#34;set @total := 0;split(SELECT COUNT(*) FROM world.City INTO @chunk) set @total = @total + @chunk; select @total;&#34;);&lt;/blockquote&gt;
&lt;p&gt;If you like to watch the progress, add some verbose:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;call common_schema.run(&#34;set @total := 0;split(SELECT COUNT(*) FROM world.City INTO @chunk) {set @total = @total + @chunk; select $split_step, @total} select @total;&#34;);&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;em&gt;QueryScript&lt;/em&gt; is available via &lt;a href=&#34;https://code.google.com/p/common-schema/&#34;&gt;common_schema&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Converting compressed InnoDB tables to TokuDB 7.0.1</title>
      <link>/blog/2013/06/05/converting-compressed-innodb-tables-to-tokudb-7-0-1/</link>
      <pubDate>Wed, 05 Jun 2013 09:10:12 +0000</pubDate>
      
      <guid>/blog/2013/06/05/converting-compressed-innodb-tables-to-tokudb-7-0-1/</guid>
      <description>&lt;p&gt;Or: how to make it work in TokuDB version &lt;strong&gt;7.0.1&lt;/strong&gt;. This is a follow up on a &lt;a href=&#34;https://groups.google.com/forum/?fromgroups=#!topic/tokudb-user/hLlHwlp2AL0&#34;&gt;discussion on the tokudb-user group&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Background&lt;/h4&gt;
&lt;p&gt;I wanted to test TokuDB&#39;s compression. I took a staging machine of mine, with production data, and migrated it from &lt;strong&gt;Percona Server 5.5&lt;/strong&gt; To &lt;strong&gt;MariaDB 5.5+TokuDB 7.0.1&lt;/strong&gt;. Migration went well, no problems.&lt;/p&gt;
&lt;p&gt;To my surprise, when I converted tables from InnoDB to TokuDB, I saw an &lt;em&gt;increase&lt;/em&gt; in table file size on disk. As explained by Tim Callaghan, this was due to TokuDB interpreting my compressed table&#39;s &lt;strong&gt;&#34;KEY_BLOCK_SIZE=4&#34;&lt;/strong&gt; as an instruction for TokuDB&#39;s page size. TokuDB should be using &lt;strong&gt;4MB&lt;/strong&gt; block size, but thinks it&#39;s being instructed to use &lt;strong&gt;4KB&lt;/strong&gt;. Problem is, you &lt;a href=&#34;http://bugs.mysql.com/bug.php?id=67727&#34;&gt;can&#39;t get rid of table options&lt;/a&gt;. When one converts a table to InnoDB in &lt;strong&gt;ROW_FORMAT=COMPACT&lt;/strong&gt;, or even to MyISAM, the &lt;strong&gt;KEY_BLOCK_SIZE&lt;/strong&gt; option keeps lurking in the dark.&lt;/p&gt;
&lt;p&gt;So until this is hopefully resolved in TokuDB&#39;s next version, here&#39;s a way to go around the problem.&lt;!--more--&gt;&lt;/p&gt;
&lt;h4&gt;The case at hand&lt;/h4&gt;
&lt;p&gt;Consider the following table:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt; CREATE TABLE `t` (
  `id` int(10) unsigned NOT NULL AUTO_INCREMENT,
  `c1` int(10) unsigned NOT NULL DEFAULT &#39;0&#39;,
  `c2` int(10) unsigned NOT NULL DEFAULT &#39;0&#39;,
  `c3` int(10) unsigned NOT NULL DEFAULT &#39;0&#39;,
  `c4` timestamp NOT NULL DEFAULT &#39;0000-00-00 00:00:00&#39;,
  `c5` timestamp NOT NULL DEFAULT &#39;0000-00-00 00:00:00&#39;,
  `c6` smallint(10) unsigned NOT NULL DEFAULT &#39;0&#39;,
  `c7` smallint(10) unsigned NOT NULL DEFAULT &#39;0&#39;,
  `c8` smallint(10) unsigned NOT NULL DEFAULT &#39;0&#39;,
  `c9` smallint(10) unsigned NOT NULL DEFAULT &#39;0&#39;,
  `c10` smallint(10) unsigned NOT NULL DEFAULT &#39;0&#39;,
  `c11` smallint(10) NOT NULL DEFAULT &#39;0&#39;,
  `c12` smallint(10) NOT NULL DEFAULT &#39;0&#39;,
  `c13` smallint(10) NOT NULL DEFAULT &#39;0&#39;,
  `c14` smallint(10) NOT NULL DEFAULT &#39;0&#39;,
  `ct` text NOT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `c1c4` (`c1`,`c4`),
  KEY `c4` (`c4`)
) ENGINE=InnoDB AUTO_INCREMENT=4688271 DEFAULT CHARSET=utf8 ROW_FORMAT=COMPRESSED KEY_BLOCK_SIZE=4&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;Note that it is in &lt;strong&gt;COMPRESSED&lt;/strong&gt; format, with &lt;strong&gt;KEY_BLOCK_SIZE=4&lt;/strong&gt;. It mostly has &lt;strong&gt;INT&lt;/strong&gt; columns, so I don&#39;t expect it to compress by much.&lt;/p&gt;
&lt;p&gt;On disk, the &lt;strong&gt;.ibd&lt;/strong&gt; file amounts to &lt;strong&gt;160MB&lt;/strong&gt;. Table has&lt;strong&gt; &lt;/strong&gt;&lt;strong&gt;3,587,488&lt;/strong&gt; rows. Same table in InnoDB COMPACT row format amounts to &lt;strong&gt;412MB&lt;/strong&gt; on disk.&lt;/p&gt;
&lt;p&gt;Converting the table to TokuDB with aggressive compression resulted with:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;mysql&amp;gt; alter table t engine=tokudb row_format=tokudb_lzma;
Query OK, 3587488 rows affected (29 min 48.79 sec)
Records: 3587488  Duplicates: 0  Warnings:&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;And over &lt;strong&gt;873MB&lt;/strong&gt; of combined files on disk! Also note it took nearly &lt;strong&gt;30&lt;/strong&gt; minutes to &lt;strong&gt;ALTER&lt;/strong&gt;. Clearly this is not the expected outcome.&lt;/p&gt;
&lt;h4&gt;Attempt to make it work&lt;/h4&gt;
&lt;p&gt;I tried both the following approaches:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;alter table t engine=tokudb row_format=tokudb_lzma key_block_size=4096&lt;/strong&gt;: thought it would fool TokuDB to think it should create a 4M key block size.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;alter table t engine=tokudb row_format=tokudb_lzma key_block_size=0&lt;/strong&gt;: try and reset the key block size.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Both the above attempts resulted with same bloat in resulting table.&lt;/p&gt;
&lt;p&gt;The reason? When ALTERing a table with a nother &lt;strong&gt;KEY_BLOCK_SIZE&lt;/strong&gt;, the keys on the table remain with their old &lt;strong&gt;KEY_BLOCK_SIZE&lt;/strong&gt;. They are unaffected by the &lt;strong&gt;ALTER&lt;/strong&gt;. As suggested by &lt;em&gt;Nail Kashapov&lt;/em&gt;, indexes must be rebuilt as well.&lt;/p&gt;
&lt;h4&gt;Making it work&lt;/h4&gt;
&lt;p&gt;The next &lt;strong&gt;ALTER&lt;/strong&gt; modifies the &lt;strong&gt;KEY_BLOCK_SIZE&lt;/strong&gt; &lt;em&gt;and&lt;/em&gt; rebuilds all the indexes on the table:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;mysql&amp;gt; alter table t drop primary key, add primary key(id), drop key c1c4, add unique key `c1c4` (c1, c4), drop key c4, add key `c4` (c4), engine=tokudb row_format=tokudb_lzma key_block_size=0;
Query OK, 3587488 rows affected (2 min 7.97 sec)
Records: 3587488  Duplicates: 0  Warnings: 0&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;Yep! Runtime seems much more agreeable. Total size on disk? Little over &lt;strong&gt;26M&lt;/strong&gt;. Did I say I wasn&#39;t expecting good reduction in terms of compression?&lt;/p&gt;
&lt;p&gt;Have done the same for multiple tables; compression is consistently strong (e.g. &lt;strong&gt;16MB&lt;/strong&gt; InnoDB compressed -&amp;gt; &lt;strong&gt;3.5MB&lt;/strong&gt; TokuDB aggressive, &lt;strong&gt;548MB&lt;/strong&gt; InnoDB non-compressed -&amp;gt; &lt;strong&gt;36MB&lt;/strong&gt; TokuDB aggressive), on varying table schemata. Very impressive reduction in disk space!&lt;/p&gt;
&lt;h4&gt;Conclusion&lt;/h4&gt;
&lt;p&gt;Next version of TokuDB is expected to ignore the &lt;strong&gt;KEY_BLOCK_SIZE&lt;/strong&gt; table option; until then converting compressed tables to TokuDB is a pain in terms of the syntax -- but worthwhile in terms of disk space.&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>State of InnDB Online DDL in MySQL 5.6.9-RC (good news included)</title>
      <link>/blog/2012/12/18/state-of-inndb-online-ddl-in-mysql-5-6-9-rc-good-news-included/</link>
      <pubDate>Tue, 18 Dec 2012 13:21:12 +0000</pubDate>
      
      <guid>/blog/2012/12/18/state-of-inndb-online-ddl-in-mysql-5-6-9-rc-good-news-included/</guid>
      <description>&lt;p&gt;&lt;strong&gt;5.6.9-RC&lt;/strong&gt; is &lt;a href=&#34;https://blogs.oracle.com/MySQL/entry/mysql_5_6_9_release&#34;&gt;out&lt;/a&gt;, and I was curious to see how the &lt;a href=&#34;http://dev.mysql.com/doc/refman/5.6/en/innodb-online-ddl.html&#34;&gt;online DDL&lt;/a&gt; has improved since &lt;a href=&#34;http://code.openark.org/blog/mysql/state-of-inndb-online-ddl-in-mysql-5-6-8-rc&#34;&gt;my 5.6.8 review&lt;/a&gt;. I also owe James Day this review, since he came up with results inconsistent with my own.&lt;/p&gt;
&lt;p&gt;We both agreed the dataset I was using was too small, but I got similar results even on larger scale. Then some time passed, and &lt;strong&gt;5.6.9&lt;/strong&gt; was announced.&lt;/p&gt;
&lt;p&gt;So for the &lt;strong&gt;5.6.9&lt;/strong&gt; test I took one of my real tables on production. It is not extremely large: it&#39;s a ~ &lt;strong&gt;300MB&lt;/strong&gt; &lt;strong&gt;.ibd&lt;/strong&gt; file, in the following format:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;mysql&amp;gt; show create table tbl \G

CREATE TABLE `tbl` (
  `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT,
  `a` varchar(255) CHARACTER SET utf8 NOT NULL DEFAULT &#39;&#39;,
  `w` smallint(11) NOT NULL DEFAULT &#39;0&#39;,
  `d` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  `icount` smallint(5) unsigned NOT NULL DEFAULT &#39;0&#39;,
  PRIMARY KEY (`id`) KEY_BLOCK_SIZE=8,
  UNIQUE KEY `u_idx` (`a`,`w`,`d`) KEY_BLOCK_SIZE=8,
  KEY `d` (`d`) KEY_BLOCK_SIZE=8
) ENGINE=InnoDB AUTO_INCREMENT=16960441 DEFAULT CHARSET=latin1 ROW_FORMAT=COMPRESSED KEY_BLOCK_SIZE=16&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;Got some &lt;strong&gt;2.5M&lt;/strong&gt; rows in the table; desktop machine, &lt;strong&gt;64&lt;/strong&gt; bit Linux, mysqlsandbox.&lt;/p&gt;
&lt;p&gt;I have crossed several DDL statements with several DML statements. The DDL statements in this test are (&lt;strong&gt;ALTER TABLE...&lt;/strong&gt;):&lt;!--more--&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;ROW_FORMAT=COMPACT&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;AUTO_INCREMENT=16960441&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ADD INDEX (w)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DROP INDEX w&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ADD COLUMN c CHAR(1) NOT NULL&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DROP COLUMN c&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The DML statements are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;select max(id) from test.tbl;&lt;/strong&gt; -- this queries the AUTO_INCREMENT value, which is of course a PRIMARY KEY&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;select min(d) from test.tbl;&lt;/strong&gt; -- there is an index on d, and normal execution plan is to optimize table away and just use the index&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;select min(icount) from test.tbl;&lt;/strong&gt; -- there is no index on icount, and full table scan is required&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;update test.tbl set d = d + interval 1 second where id = 8057370;&lt;/strong&gt; -- the UPDATE uses the PRIMARY KEY&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;update test.tbl set d = d + interval 1 second where icount = 200;&lt;/strong&gt; -- will affect &lt;strong&gt;4&lt;/strong&gt; rows, but requires full scan.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The results?&lt;/p&gt;
&lt;table border=&#34;0&#34; cellspacing=&#34;0&#34;&gt;
&lt;colgroup width=&#34;243&#34;&gt;&lt;/colgroup&gt;
&lt;colgroup width=&#34;92&#34;&gt;&lt;/colgroup&gt;
&lt;colgroup width=&#34;131&#34;&gt;&lt;/colgroup&gt;
&lt;colgroup span=&#34;5&#34; width=&#34;85&#34;&gt;&lt;/colgroup&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#E6E6FF&#34; height=&#34;47&#34;&gt;&lt;strong&gt;ALTER TABLE...&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#E6E6FF&#34;&gt;&lt;strong&gt;Time (sec)&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#E6E6FF&#34;&gt;&lt;strong&gt;General comments&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#E6E6FF&#34;&gt;&lt;strong&gt;select max PK&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#E6E6FF&#34;&gt;&lt;strong&gt;select min by index&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#E6E6FF&#34;&gt;&lt;strong&gt;select min by full scan&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#E6E6FF&#34;&gt;&lt;strong&gt;update by PK&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#E6E6FF&#34;&gt;&lt;strong&gt;update by full scan&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34; height=&#34;17&#34;&gt;ROW_FORMAT=COMPACT&lt;/td&gt;
&lt;td align=&#34;RIGHT&#34; bgcolor=&#34;#FFFFCC&#34;&gt;183&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34;&gt;&lt;span style=&#34;color: #00ae00;&#34;&gt;online&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34;&gt;&lt;span style=&#34;color: #00ae00;&#34;&gt;online&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34;&gt;&lt;span style=&#34;color: #00ae00;&#34;&gt;online&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34;&gt;&lt;span style=&#34;color: #00ae00;&#34;&gt;online&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34;&gt;&lt;span style=&#34;color: #00ae00;&#34;&gt;online&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34; height=&#34;16&#34;&gt;AUTO_INCREMENT=16960441&lt;/td&gt;
&lt;td align=&#34;RIGHT&#34; bgcolor=&#34;#FFFFCC&#34;&gt;0.24&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34;&gt;[Instant operation]&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34;&gt;n/a&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34;&gt;n/a&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34;&gt;n/a&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34;&gt;n/a&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34;&gt;n/a&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34; height=&#34;17&#34;&gt;ADD INDEX (w)&lt;/td&gt;
&lt;td align=&#34;RIGHT&#34; bgcolor=&#34;#FFFFCC&#34;&gt;21&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34;&gt;&lt;span style=&#34;color: #00ae00;&#34;&gt;online&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34;&gt;&lt;span style=&#34;color: #00ae00;&#34;&gt;online&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34;&gt;&lt;span style=&#34;color: #00ae00;&#34;&gt;online&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34;&gt;&lt;span style=&#34;color: #00ae00;&#34;&gt;online&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34;&gt;&lt;span style=&#34;color: #00ae00;&#34;&gt;online&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34; height=&#34;16&#34;&gt;DROP INDEX w&lt;/td&gt;
&lt;td align=&#34;RIGHT&#34; bgcolor=&#34;#FFFFCC&#34;&gt;0.1&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34;&gt;[Instant operation]&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34;&gt;n/a&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34;&gt;n/a&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34;&gt;n/a&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34;&gt;n/a&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34;&gt;n/a&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34; height=&#34;17&#34;&gt;ADD COLUMN c CHAR(1) NOT NULL&lt;/td&gt;
&lt;td align=&#34;RIGHT&#34; bgcolor=&#34;#FFFFCC&#34;&gt;103&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34;&gt;&lt;span style=&#34;color: #00ae00;&#34;&gt;online&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34;&gt;&lt;span style=&#34;color: #00ae00;&#34;&gt;online&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34;&gt;&lt;span style=&#34;color: #00ae00;&#34;&gt;online&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34;&gt;&lt;span style=&#34;color: #00ae00;&#34;&gt;online&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34;&gt;&lt;span style=&#34;color: #00ae00;&#34;&gt;online&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34; height=&#34;17&#34;&gt;DROP COLUMN c&lt;/td&gt;
&lt;td align=&#34;RIGHT&#34; bgcolor=&#34;#FFFFCC&#34;&gt;110&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34;&gt;&lt;span style=&#34;color: #00ae00;&#34;&gt;online&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34;&gt;&lt;span style=&#34;color: #00ae00;&#34;&gt;online&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34;&gt;&lt;span style=&#34;color: #00ae00;&#34;&gt;online&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34;&gt;&lt;span style=&#34;color: #00ae00;&#34;&gt;online&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;LEFT&#34; bgcolor=&#34;#FFFFCC&#34;&gt;&lt;span style=&#34;color: #00ae00;&#34;&gt;online&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4&gt;Notes&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;All operations were online: operations did not wait for &lt;strong&gt;ALTER&lt;/strong&gt; to complete.&lt;/li&gt;
&lt;li&gt;I executed all operations multiple times during each &lt;strong&gt;ALTER&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;In addition, I executed operations from another client.&lt;/li&gt;
&lt;li&gt;Some operations were fast, others sometimes took as long as &lt;strong&gt;7.34&lt;/strong&gt; seconds to complete. This is no small matter: the time it took for each DML was indeterministic, and longer than what it would usually take it. That&#39;s perfectly understandable. Just note that some operations took exceedingly long time to complete. My understanding is that the &lt;strong&gt;ALTER&lt;/strong&gt; operations happens in chunks. DML statements are allowed in between these chunks. This is the reason why on smaller tables there didn&#39;t seem to be any &#34;online&#34; statement: the chunks were just too large in relation to table size. And so, and this is still my own understanding, your query may get lucky or unlucky depending on the exact moment it has been issued.&lt;/li&gt;
&lt;li&gt;I did not try it with &lt;strong&gt;FOREIGN KEY&lt;/strong&gt;s. I previously concluded that foreign keys were a no-go for online DDL. I&#39;m not sure if this is still the case. Another time for this test - but it must take place.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Conclusions&lt;/h4&gt;
&lt;p&gt;Still RC - but for the first time the online DDL seem to deliver what&#39;s promised. I&#39;m very happy to see this.&lt;/p&gt;
&lt;p&gt;I am yet to understand how the &lt;strong&gt;ALTER&lt;/strong&gt; works via replication. With single threaded replication I would assume it&#39;s back to &#34;wait till I&#39;m done&#34; on the slave, in which case the &lt;em&gt;&#34;online&#34;&lt;/em&gt; term is not there yet. Even on multi-threaded replication DML on same schema would hang. I&#39;m happy to be corrected on this by an authority.&lt;/p&gt;
&lt;p&gt;My predicament is that &lt;a href=&#34;http://openarkkit.googlecode.com/svn/trunk/openarkkit/doc/html/oak-online-alter-table.html&#34;&gt;oak-online-alter-table&lt;/a&gt; or &lt;a href=&#34;http://www.percona.com/doc/percona-toolkit/2.1/pt-online-schema-change.html&#34;&gt;pt-online-schema-change&lt;/a&gt; are here to stay for the next couple of years at least. Some operations, like partitioning, are not supported by current online InnoDB DDL. Also, these scripts allow you some control over the speed at which the &lt;strong&gt;ALTER&lt;/strong&gt; process works, allowing for pre-defined sleep time in between chunks, so as to let the server - and its slaves - recover their breath.&lt;/p&gt;
&lt;p&gt;Nonetheless, big kudos for the InnoDB team at Oracle for pulling this one out!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Killing InnoDB idle transactions</title>
      <link>/blog/2012/12/04/killing-innodb-idle-transactions/</link>
      <pubDate>Tue, 04 Dec 2012 14:23:12 +0000</pubDate>
      
      <guid>/blog/2012/12/04/killing-innodb-idle-transactions/</guid>
      <description>&lt;p&gt;The issue of terminating long-time idle open InnoDB transaction has been discussed recently by many. I wish to add my share, by proposing a quick and clean solution via &lt;a href=&#34;http://code.google.com/p/common-schema/&#34;&gt;common_schema&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;common_schema &lt;strong&gt;1.2&lt;/strong&gt;&lt;/em&gt; provides with the &lt;a href=&#34;http://common-schema.googlecode.com/svn/trunk/common_schema/doc/html/innodb_transactions.html&#34;&gt;&lt;strong&gt;innodb_transactions&lt;/strong&gt;&lt;/a&gt; view, which relies on &lt;strong&gt;INNODB_TRX&lt;/strong&gt; - one of the InnoDB Plugin views in &lt;strong&gt;INFORMATION_SCHEMA&lt;/strong&gt; - as well as on &lt;strong&gt;PROCESSLIST&lt;/strong&gt;, and so is able to determine with certainty that a transaction has been idle for a long time.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;innodb_transactions&lt;/strong&gt; offers us with a &lt;strong&gt;sql_kill_query&lt;/strong&gt; column, which produces a &lt;strong&gt;&#39;KILL QUERY 12345&#39;&lt;/strong&gt; type of value. So we can:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;SELECT &lt;strong&gt;sql_kill_query&lt;/strong&gt; FROM &lt;strong&gt;innodb_transactions&lt;/strong&gt; WHERE &lt;strong&gt;trx_idle_seconds &amp;gt;= 10; 
&lt;/strong&gt;+-------------------+
| sql_kill_query    |
+-------------------+
| KILL QUERY 292509 |
| KILL QUERY 292475 |
+-------------------+&lt;strong&gt; &lt;/strong&gt;&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;em&gt;common_schema&lt;/em&gt;&#39;s useful &lt;a href=&#34;http://common-schema.googlecode.com/svn/trunk/common_schema/doc/html/eval.html&#34;&gt;&lt;strong&gt;eval()&lt;/strong&gt;&lt;/a&gt; routine allows us to actually invoke those &lt;strong&gt;KILL&lt;/strong&gt; statements, all in a one-liner:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;call &lt;strong&gt;eval&lt;/strong&gt;(&lt;span style=&#34;color: #003366;&#34;&gt;&#39;SELECT &lt;strong&gt;sql_kill_query&lt;/strong&gt; FROM innodb_transactions WHERE &lt;strong&gt;trx_idle_seconds &amp;gt;= 10&lt;/strong&gt;&#39;&lt;/span&gt;);&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;h4&gt;Technical details&lt;!--more--&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;trx_idle_seconds&lt;/strong&gt; notes the time, in seconds, the transaction has been idle, or 0 if the transaction is not idle at all.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;sql_kill_query&lt;/strong&gt; is a self-generated SQL query which kills the running query, e.g. &lt;strong&gt;&#39;KILL QUERY 12345&#39;&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;eval()&lt;/strong&gt; takes a query as text, retrieves the SQL resulting column, and executes it live.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Background details&lt;/h4&gt;
&lt;p&gt;The connection between &lt;strong&gt;INNODB_TRX&lt;/strong&gt; and &lt;strong&gt;PROCESSLIST&lt;/strong&gt; is not synchronous. It is possible that by the time one is querying &lt;strong&gt;INNODB_TRX&lt;/strong&gt;, &lt;strong&gt;PROCESSLIST&lt;/strong&gt; data may change (e.g. next query is already replacing the one you were considering in &lt;strong&gt;INNODB_TRX&lt;/strong&gt;). But in our case it is of little consequence: we are interested in transactions that have been idle for quite some time. Say, &lt;strong&gt;10&lt;/strong&gt; seconds. So we are not troubled by having &lt;strong&gt;200&lt;/strong&gt; queries per second changing under our hands.&lt;/p&gt;
&lt;p&gt;If the transaction has been asleep for &lt;strong&gt;10&lt;/strong&gt; seconds, and we decide to kill it, well, it is possible that just as we kill it it will turn active again. It&#39;s a risk we take no matter what kind of solution we apply, since there&#39;s no atomic &#34;get-status-and-kill&#34; operation on InnoDB transactions.&lt;/p&gt;
&lt;p&gt;The above solution is manual: one must invoke the query which kills the idle transactions. This is as opposed to a built-in server feature which does the same. Events can used to semi-automate this: one can call upon this query once every &lt;strong&gt;10&lt;/strong&gt; seconds, for example.&lt;/p&gt;
&lt;p&gt;See the many related and inspiring solutions below:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://mysqlblog.fivefarmers.com/2012/08/28/identifying-and-killing-blocking-transactions-in-innodb/&#34;&gt;Identifying and killing blocking transactions in InnoDB&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.markleith.co.uk/2011/05/31/finding-and-killing-long-running-innodb-transactions-with-events/&#34;&gt;Finding and killing long running InnoDB transactions with Events&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://datacharmer.blogspot.co.il/2008/10/using-event-scheduler-to-purge-process.html&#34;&gt;Using the event scheduler to purge the process list&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.mysqlperformanceblog.com/2011/03/08/how-to-debug-long-running-transactions-in-mysql/&#34;&gt;How to debug long-running transactions in MySQL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://yoshinorimatsunobu.blogspot.co.il/2011/04/tracking-long-running-transactions-in.html&#34;&gt;Tracking long running transactions in MySQL&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>State of InnDB Online DDL in MySQL 5.6.8-RC</title>
      <link>/blog/2012/11/20/state-of-inndb-online-ddl-in-mysql-5-6-8-rc/</link>
      <pubDate>Tue, 20 Nov 2012 11:49:14 +0000</pubDate>
      
      <guid>/blog/2012/11/20/state-of-inndb-online-ddl-in-mysql-5-6-8-rc/</guid>
      <description>&lt;p&gt;&lt;strong&gt;5.6.8-rc&lt;/strong&gt; is out, and so I&#39;m following up on InnoDB&#39;s online DDL new feature: the ability to SELECT, INSERT, DELETE, UPDATE a table even while an ALTER TABLE is executing on same table.&lt;/p&gt;
&lt;h4&gt;The brief summary&lt;/h4&gt;
&lt;p&gt;Not as advertised; many things can&#39;t be done.&lt;/p&gt;
&lt;h4&gt;The longer review&lt;/h4&gt;
&lt;p&gt;I&#39;m using &lt;strong&gt;5.6.8-rc 64bit&lt;/strong&gt; binary distribution for Linux, installed via &lt;a href=&#34;http://mysqlsandbox.net/&#34;&gt;mysqlsandbox&lt;/a&gt;. My hardware is irrelevant, but the fact I&#39;m testing on my laptop assists me in that &lt;strong&gt;ALTER TABLE&lt;/strong&gt; operations take a while, so that I&#39;m able to easily type commands in two terminals and have the time to watch them being executed. Query cache is disabled.&lt;!--more--&gt;&lt;/p&gt;
&lt;p&gt;I&#39;m using the sakila sample database, and in particular I&#39;m working with the rental table. Here&#39;s the table definition:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;CREATE TABLE `rental` (
  `rental_id` int(11) NOT NULL AUTO_INCREMENT,
  `rental_date` datetime NOT NULL,
  `inventory_id` mediumint(8) unsigned NOT NULL,
  `customer_id` smallint(5) unsigned NOT NULL,
  `return_date` datetime DEFAULT NULL,
  `staff_id` tinyint(3) unsigned NOT NULL,
  `last_update` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  PRIMARY KEY (`rental_id`),
  UNIQUE KEY `rental_date` (`rental_date`,`inventory_id`,`customer_id`),
  KEY `idx_fk_inventory_id` (`inventory_id`),
  KEY `idx_fk_customer_id` (`customer_id`),
  KEY `idx_fk_staff_id` (`staff_id`),
  CONSTRAINT `fk_rental_staff` FOREIGN KEY (`staff_id`) REFERENCES `staff` (`staff_id`) ON UPDATE CASCADE,
  CONSTRAINT `fk_rental_inventory` FOREIGN KEY (`inventory_id`) REFERENCES `inventory` (`inventory_id`) ON UPDATE CASCADE,
  CONSTRAINT `fk_rental_customer` FOREIGN KEY (`customer_id`) REFERENCES `customer` (`customer_id`) ON UPDATE CASCADE
) ENGINE=InnoDB AUTO_INCREMENT=16050 DEFAULT CHARSET=utf8&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;Highlights for the table: &lt;strong&gt;AUTO_INCREMENT PRIMARY KEY&lt;/strong&gt;, some columns indexed, some not, and Foreign Keys in place. Pretty much a standard table. It contains &lt;strong&gt;16,044&lt;/strong&gt; rows. Row format is &lt;strong&gt;COMPACT&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;What I want to know is: which DDL commands allow for which online DML commands?&lt;/p&gt;
&lt;p&gt;So, on terminal #1 I will issue queries like:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;node1 5.6.8-rc-log sakila&amp;gt; alter table &lt;strong&gt;sakila.rental&lt;/strong&gt; ROW_FORMAT=COMPACT &lt;strong&gt;/* or whatever */&lt;/strong&gt;;
Query OK, 0 rows affected (10.57 sec)
Records: 0  Duplicates: 0  Warnings: 0&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;And during the above operation, I will execute the following on terminal #2:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;select max(rental_id) from sakila.rental;&lt;/strong&gt; this queries the AUTO_INCREMENT value, which is of course a PRIMARY KEY&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;select min(rental_date) from sakila.rental&lt;/strong&gt;; there is an index on rental_date, and normal execution plan is to optimize table away and just use the index&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;select min(return_date) from sakila.rental&lt;/strong&gt;; there is no index on return_date, and full table scan is required&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;update rental set return_date = return_date + interval 1 second where rental_id=3&lt;/strong&gt;; the UPDATE uses the PRIMARY KEY&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;update rental set return_date = return_date + interval 1 second where return_date = NOW()&lt;/strong&gt;; won&#39;t actually affect anything, but requires full scan.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;So here are the results:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;+-------------------------------------------------------------+-------+---------------------------+---------------+---------------------+-------------------------+--------------+---------------------+
| ALTER statement                                             | Time  | General comments          | select max PK | select min by index | select min by full scan | update by PK | update by full scan |
+-------------------------------------------------------------+-------+---------------------------+---------------+---------------------+-------------------------+--------------+---------------------+
| ROW_FORMAT=COMPACT                                          | 10.92 |                           | &lt;span style=&#34;color: #008000;&#34;&gt;Instant&lt;/span&gt;       | &lt;span style=&#34;color: #008000;&#34;&gt;Instant&lt;/span&gt;             | &lt;span style=&#34;color: #008000;&#34;&gt;Instant&lt;/span&gt;                 | &lt;span style=&#34;color: #800000;&#34;&gt;blocked&lt;/span&gt;      | &lt;span style=&#34;color: #800000;&#34;&gt;blocked&lt;/span&gt;             |
| AUTO_INCREMENT=16051                                        |  0.06 | Instant, no table rebuild | N/A           | N/A                 | N/A                     | N/A          | N/A                 |
| ADD INDEX(last_update)                                      |  2.37 |                           | &lt;span style=&#34;color: #800000;&#34;&gt;blocked&lt;/span&gt;       | &lt;span style=&#34;color: #800000;&#34;&gt;blocked&lt;/span&gt;             | &lt;span style=&#34;color: #800000;&#34;&gt;blocked&lt;/span&gt;                 | &lt;span style=&#34;color: #800000;&#34;&gt;blocked&lt;/span&gt;      | &lt;span style=&#34;color: #800000;&#34;&gt;blocked&lt;/span&gt;             |
| ADD INDEX(last_update), ALGORITHM=INPLACE                   |  1.83 |                           | &lt;span style=&#34;color: #800000;&#34;&gt;blocked&lt;/span&gt;       | &lt;span style=&#34;color: #800000;&#34;&gt;blocked&lt;/span&gt;             | &lt;span style=&#34;color: #800000;&#34;&gt;blocked&lt;/span&gt;                 | &lt;span style=&#34;color: #800000;&#34;&gt;blocked&lt;/span&gt;      | &lt;span style=&#34;color: #800000;&#34;&gt;blocked&lt;/span&gt;             |
| ADD INDEX(last_update), ALGORITHM=INPLACE, LOCK=NONE        |  0.00 | ERROR 1235 (42000): ...   | N/A           | N/A                 | N/A                     | N/A          | N/A                 |
| ADD COLUMN c CHAR(1) NOT NULL                               | 11.20 |                           | &lt;span style=&#34;color: #008000;&#34;&gt;Instant&lt;/span&gt;       | &lt;span style=&#34;color: #008000;&#34;&gt;Instant&lt;/span&gt;             | &lt;span style=&#34;color: #008000;&#34;&gt;Instant&lt;/span&gt;                 | &lt;span style=&#34;color: #800000;&#34;&gt;blocked&lt;/span&gt;      | &lt;span style=&#34;color: #800000;&#34;&gt;blocked&lt;/span&gt;             |
| ADD COLUMN c CHAR(1) NOT NULL, ALGORITHM=INPLACE, LOCK=NONE |  0.00 | ERROR 1235 (42000): .     | N/A           | N/A                 | N/A                     | N/A          | N/A                 |
+-------------------------------------------------------------+-------+---------------------------+---------------+---------------------+-------------------------+--------------+---------------------+&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;Rather surprising, I would say.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;None&lt;/em&gt; of my tests resolved with online write (&lt;strong&gt;UPDATE&lt;/strong&gt;). At best I could get online read (&lt;strong&gt;SEELCT&lt;/strong&gt;).&lt;br /&gt;
&lt;strong&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;AUTO_INCREMENT&lt;/strong&gt; is instantaneous. High time for that! It&#39;s just some number in the &lt;strong&gt;.frm&lt;/strong&gt; file, never understood the need for table rebuild.&lt;/li&gt;
&lt;li&gt;Apparently &lt;strong&gt;ADD COLUMN&lt;/strong&gt; is &lt;em&gt;more online&lt;/em&gt; than &lt;strong&gt;ADD INDEX&lt;/strong&gt;, and I&#39;ve tested this again and again and again to make sure I was doing it right. This is quite weird, even according to the &lt;a href=&#34;http://dev.mysql.com/doc/refman/5.6/en/innodb-online-ddl.html&#34;&gt;docs&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;In none of the above tests (and others, non listed), have I been able to specify &lt;strong&gt;LOCK=NONE&lt;/strong&gt;. It&#39;s always &lt;strong&gt;ERROR 1235 (42000): This version of MySQL doesn&#39;t yet support &#39;alter table sakila.rental &amp;lt;whatever&amp;gt;, algorithm=inplace, lock=none&#39;&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So what&#39;s so online about this? Online reads are nice, but most everyone cannot accept blocking writes (for same reason no one would use &lt;em&gt;mysqlhotcopy&lt;/em&gt;, also so wrongly named). This leaves us again with &lt;a href=&#34;http://openarkkit.googlecode.com/svn/trunk/openarkkit/doc/html/oak-online-alter-table.html&#34;&gt;oak-online-alter-table&lt;/a&gt; and &lt;a href=&#34;http://www.percona.com/doc/percona-toolkit/2.1/pt-online-schema-change.html&#34;&gt;pt-online-schema-change&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;The butler did it&lt;/h4&gt;
&lt;p&gt;Apologies to the butler, the &lt;strong&gt;FOREIGN KEY&lt;/strong&gt;s did it. Let&#39;s try the same again without foreign keys:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;node1 5.6.8-rc-log sakila&amp;gt; create table rental2 like rental;
node1 5.6.8-rc-log sakila&amp;gt; insert into rental2 select * from rental;
node1 5.6.8-rc-log sakila&amp;gt; rename table rental to rental_old, rental2 to rental;
Query OK, 0 rows affected (0.31 sec)&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;Here are the results:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;+-------------------------------------------------------------+-------+---------------------------+----------------+---------------------+-------------------------+----------------+---------------------+
| ALTER statement                                             | Time  | General comments          | select max PK  | select min by index | select min by full scan | update by PK   | update by full scan |
+-------------------------------------------------------------+-------+---------------------------+----------------+---------------------+-------------------------+----------------+---------------------+
| ROW_FORMAT=COMPACT                                          | 11.03 |                           | &lt;span style=&#34;color: #008000;&#34;&gt;Instant&lt;/span&gt;        | &lt;span style=&#34;color: #008000;&#34;&gt;Instant&lt;/span&gt;             | &lt;span style=&#34;color: #008000;&#34;&gt;Instant&lt;/span&gt;                 | &lt;span style=&#34;color: #008000;&#34;&gt;Instant&lt;/span&gt;        | &lt;span style=&#34;color: #008000;&#34;&gt;Instant&lt;/span&gt;             |
| AUTO_INCREMENT=16051                                        |  0.05 | Instant, no table rebuild | N/A            | N/A                 | N/A                     | N/A            | N/A                 |
| ADD INDEX(last_update)                                      |  2.04 |                           | * &lt;span style=&#34;color: #ff6600;&#34;&gt;Inconsistent&lt;/span&gt; | * &lt;span style=&#34;color: #ff6600;&#34;&gt;Inconsistent&lt;/span&gt;      | * &lt;span style=&#34;color: #ff6600;&#34;&gt;Inconsistent&lt;/span&gt;          | &lt;span style=&#34;color: #800000;&#34;&gt;blocked&lt;/span&gt;        | &lt;span style=&#34;color: #800000;&#34;&gt;blocked&lt;/span&gt;             |
| ADD INDEX(last_update), ALGORITHM=INPLACE, LOCK=NONE        |  3.14 |                           | * &lt;span style=&#34;color: #ff6600;&#34;&gt;Inconsistent&lt;/span&gt; | * &lt;span style=&#34;color: #ff6600;&#34;&gt;Inconsistent&lt;/span&gt;      | * &lt;span style=&#34;color: #ff6600;&#34;&gt;Inconsistent&lt;/span&gt;          | &lt;span style=&#34;color: #800000;&#34;&gt;blocked&lt;/span&gt;        | &lt;span style=&#34;color: #800000;&#34;&gt;blocked&lt;/span&gt;             |
| ADD COLUMN c CHAR(1) NOT NULL                               |    ** |                           | * &lt;span style=&#34;color: #ff6600;&#34;&gt;Inconsistent&lt;/span&gt; | * &lt;span style=&#34;color: #ff6600;&#34;&gt;Inconsistent&lt;/span&gt;      | * &lt;span style=&#34;color: #ff6600;&#34;&gt;Inconsistent&lt;/span&gt;          | * &lt;span style=&#34;color: #ff6600;&#34;&gt;Inconsistent&lt;/span&gt; | * &lt;span style=&#34;color: #ff6600;&#34;&gt;Inconsistent&lt;/span&gt;      |
| ADD COLUMN c CHAR(1) NOT NULL, ALGORITHM=INPLACE, LOCK=NONE |    ** |                           | * &lt;span style=&#34;color: #ff6600;&#34;&gt;Inconsistent&lt;/span&gt; | * &lt;span style=&#34;color: #ff6600;&#34;&gt;Inconsistent&lt;/span&gt;      | * &lt;span style=&#34;color: #ff6600;&#34;&gt;Inconsistent&lt;/span&gt;          | * &lt;span style=&#34;color: #ff6600;&#34;&gt;Inconsistent&lt;/span&gt; | * &lt;span style=&#34;color: #ff6600;&#34;&gt;Inconsistent&lt;/span&gt;      |
+-------------------------------------------------------------+-------+---------------------------+----------------+---------------------+-------------------------+----------------+---------------------+&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;What&#39;s going on here?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;ALGORITHM=INPLACE, LOCK=NONE&lt;/strong&gt; is accepted! Bad, bad foreign keys!&lt;br /&gt;
&lt;strong&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;* ADD INDEX&lt;/strong&gt; usually allows for concurrent reads, but after repeated tests &lt;strong&gt;SELECT&lt;/strong&gt;s start to block. Then they don&#39;t work concurrently anymore until table is recreated. But even that not always, so I&#39;m not sure what the inconsistency is.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;* ADD COLUMN&lt;/strong&gt; is still more concurrent than &lt;strong&gt;ADD INDEX&lt;/strong&gt;, and actually allows for concurrent writes! Though, inconsistently. Sometimes it does not allow for concurrent writes.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;** ADD COLUMN&lt;/strong&gt; runtime highly affected by concurrent queries. It wents as high as &lt;strong&gt;45&lt;/strong&gt; seconds on my laptop. Now, to make things clear, I&#39;m not running an automated benchmark here: I&#39;m copying+pasting the statements from my editor to the mysql CLI. So, maybe &lt;strong&gt;10&lt;/strong&gt; or &lt;strong&gt;15&lt;/strong&gt;&lt;strong&gt;SELECT&lt;/strong&gt; and &lt;strong&gt;UPDATE&lt;/strong&gt; queries executes. How does that justify &lt;strong&gt;35&lt;/strong&gt; seconds delay in table rebuild?&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Some conclusions:&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;The documentation does not specify anything about &lt;strong&gt;FOREIGN KEY&lt;/strong&gt;s crashing the party. It should.&lt;/li&gt;
&lt;li&gt;The documentation specifically mentions the &lt;strong&gt;ADD/DROP INDEX&lt;/strong&gt; statements to be online. &lt;strong&gt;ADD INDEX&lt;/strong&gt; is less online than &lt;strong&gt;ADD COLUMN&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Everything is still shaky. Sometimes things work, sometimes they don&#39;t.&lt;/li&gt;
&lt;li&gt;Runtimes are unproportionally affected by concurrent queries.&lt;/li&gt;
&lt;li&gt;For the meantime, I keep to my online alter table scripts. Been using them for &lt;strong&gt;3.5&lt;/strong&gt; years now.&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>InnoDB DDL: kudos to quick responders on bugs.mysql.com</title>
      <link>/blog/2012/10/18/innodb-ddl-kudos-to-quick-responders-on-bugs-mysql-com/</link>
      <pubDate>Thu, 18 Oct 2012 18:55:29 +0000</pubDate>
      
      <guid>/blog/2012/10/18/innodb-ddl-kudos-to-quick-responders-on-bugs-mysql-com/</guid>
      <description>&lt;p&gt;Continuing my &lt;a href=&#34;http://code.openark.org/blog/mysql/experimenting-with-5-6-innodb-online-ddl-bugs-included&#34;&gt;experiments with 5.6 InnoDB online DDL&lt;/a&gt;, a bug which I&#39;ve opened, and another which I commented on were quickly answered and explained by the Oracle/MySQL team.&lt;/p&gt;
&lt;p&gt;On both accounts I&#39;m happy to acknowledge the issue is resolved; in both cases I failed to produce a real bug scenario. Good lesson. &lt;em&gt;Kudos for quick and informative responses!&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;What&#39;s left of my experiment, then? Still a lot to check.&lt;/p&gt;
&lt;p&gt;I am mainly still confused with which operations exactly can use &lt;strong&gt;LOCK=NONE&lt;/strong&gt; (allowing for updated to table while &lt;strong&gt;ALTER&lt;/strong&gt;ing). So far I am only able to produce &lt;strong&gt;ALTER&lt;/strong&gt;s with &lt;strong&gt;LOCK=SHARED&lt;/strong&gt;, meaning table is readable, but cannot be updated.&lt;/p&gt;
&lt;p&gt;I will want to test speeds. I&#39;ve so far been content with slow response times for queries over altered tables. How well will that endure under heavy load?&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Experimenting with 5.6 InnoDB Online DDL (bugs included)</title>
      <link>/blog/2012/10/18/experimenting-with-5-6-innodb-online-ddl-bugs-included/</link>
      <pubDate>Thu, 18 Oct 2012 14:41:46 +0000</pubDate>
      
      <guid>/blog/2012/10/18/experimenting-with-5-6-innodb-online-ddl-bugs-included/</guid>
      <description>&lt;p&gt;MySQL &lt;strong&gt;5.6&lt;/strong&gt; offers the groundbreaking online DDL operations for InnoDB. Most common use cases will enjoy this feature, and the need for &lt;a href=&#34;http://openarkkit.googlecode.com/svn/trunk/openarkkit/doc/html/oak-online-alter-table.html&#34;&gt;online alter table&lt;/a&gt; scripts will decrease. This is a killer feature!&lt;/p&gt;
&lt;p&gt;I&#39;ve put this new feature to the usability test. How did it go? Not too well, I&#39;m afraid.&lt;/p&gt;
&lt;p&gt;[Updates to this text inline], also see &lt;a href=&#34;http://code.openark.org/blog/mysql/innodb-ddl-kudos-to-quick-responders-on-bugs-mysql-com&#34;&gt;this followup&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;sakila &amp;amp; DDL&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;http://dev.mysql.com/doc/sakila/en/index.html&#34;&gt;sakila&lt;/a&gt; is still a very useful database. I say &#34;still&#34; because it is not very large, and computing power is getting stronger; yet on my laptop some operations can still take many seconds to complete, which is just fine for my tests.&lt;/p&gt;
&lt;p&gt;Sakila tables are mostly InnoDB, and rental being the largest, I do:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;node1 (sakila) &amp;gt; &lt;strong&gt;alter table sakila.rental engine=InnoDB;&lt;/strong&gt;
Query OK, 16044 rows affected (&lt;strong&gt;6.94&lt;/strong&gt; sec)
Records: 16044  Duplicates: 0  Warnings: 0&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;So what can be executed during these &lt;strong&gt;6.94&lt;/strong&gt; seconds? In a second terminal, I try the following:&lt;!--more--&gt;&lt;/p&gt;
&lt;h4&gt;Meta&lt;/h4&gt;
&lt;blockquote&gt;
&lt;pre&gt;node1 (sakila) &amp;gt; show create table sakila.rental\G
*************************** 1. row ***************************
       Table: rental
Create Table: CREATE TABLE `rental` (
  `rental_id` int(11) NOT NULL AUTO_INCREMENT,
  `rental_date` datetime NOT NULL,
  `inventory_id` mediumint(8) unsigned NOT NULL,
  `customer_id` smallint(5) unsigned NOT NULL,
  `return_date` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  `staff_id` tinyint(3) unsigned NOT NULL,
  `last_update` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  PRIMARY KEY (`rental_id`),
  UNIQUE KEY `rental_date` (`rental_date`,`inventory_id`,`customer_id`),
  KEY `idx_fk_inventory_id` (`inventory_id`),
  KEY `idx_fk_customer_id` (`customer_id`),
  KEY `idx_fk_staff_id` (`staff_id`),
  CONSTRAINT `fk_rental_customer` FOREIGN KEY (`customer_id`) REFERENCES `customer` (`customer_id`) ON UPDATE CASCADE,
  CONSTRAINT `fk_rental_inventory` FOREIGN KEY (`inventory_id`) REFERENCES `inventory` (`inventory_id`) ON UPDATE CASCADE,
  CONSTRAINT `fk_rental_staff` FOREIGN KEY (`staff_id`) REFERENCES `staff` (`staff_id`) ON UPDATE CASCADE
) ENGINE=InnoDB AUTO_INCREMENT=16050 DEFAULT CHARSET=utf8
1 row in set (&lt;strong&gt;1.08 sec&lt;/strong&gt;)&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;1.08&lt;/strong&gt; seconds for &lt;strong&gt;SHOW CREATE TABLE&lt;/strong&gt;. Consider: up till &lt;strong&gt;5.5&lt;/strong&gt; you can&#39;t run &lt;strong&gt;SHOW CREATE TABLE&lt;/strong&gt; while an &lt;strong&gt;ALTER&lt;/strong&gt; was running on that table.&lt;/p&gt;
&lt;h4&gt;Read&lt;/h4&gt;
&lt;p&gt;While ALTER TABLE runs, I execute:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;node1 (sakila) &amp;gt; select min(rental_date), max(return_date) from sakila.rental;
+---------------------+---------------------+
| min(rental_date)    | max(return_date)    |
+---------------------+---------------------+
| 2005-05-24 22:53:30 | 2005-09-02 02:35:22 |
+---------------------+---------------------+
1 row in set (2.77 sec)&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;So &lt;strong&gt;2.77&lt;/strong&gt; seconds for a query which uses a full table scan to return. I&#39;m not measuring performance here; am satisfies that query did actually succeed even while table was being altered.&lt;/p&gt;
&lt;h4&gt;Read &amp;amp; bug&lt;/h4&gt;
&lt;p&gt;But, unfortunately, being the type of geek who likes to make trouble, I am also able to consistently fail the &lt;strong&gt;ALTER TABLE&lt;/strong&gt;. Hang it, actually:&lt;/p&gt;
&lt;p&gt;See session &lt;strong&gt;#1&lt;/strong&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;node1 (sakila) &amp;gt; alter table sakila.rental engine=innodb; 

... (waiting forever)&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;And session &lt;strong&gt;#2&lt;/strong&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;node1 (sakila) &amp;gt; show processlist;
+----+----------+-----------+--------+---------+------+---------------------------------+-----------------------------------------+
| Id | User     | Host      | db     | Command | Time | State                           | Info                                    |
+----+----------+-----------+--------+---------+------+---------------------------------+-----------------------------------------+
|  6 | msandbox | localhost | sakila | Query   |  &lt;strong&gt;219&lt;/strong&gt; | &lt;strong&gt;Waiting for table metadata lock&lt;/strong&gt; | &lt;strong&gt;alter table sakila.rental engine=innodb&lt;/strong&gt; |
|  4 | msandbox | localhost | sakila | Query   |    0 | init                            | show processlist                        |
+----+----------+-----------+--------+---------+------+---------------------------------+-----------------------------------------+&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;Read all about it in &lt;a href=&#34;http://bugs.mysql.com/bug.php?id=67286&#34;&gt;bug report #67286&lt;/a&gt; .&lt;/p&gt;
&lt;h4&gt;Write: not so simple&lt;/h4&gt;
&lt;p&gt;The following &lt;strong&gt;UPDATE&lt;/strong&gt; query hangs till the &lt;strong&gt;ALTER&lt;/strong&gt; process is over:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;node1 (sakila) &amp;gt; update sakila.rental set return_date=now() where rental_id = floor(rand()*100);
Query OK, 3 rows affected, 1 warning (6.10 sec)&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;No online DDL for writes?&lt;/p&gt;
&lt;p&gt;Was I unfair? Is &#34;ENGINE=InnoDB&#34; really an online DDL operation? OK, let&#39;s try with:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;alter table sakila.rental &lt;strong&gt;row_format=compact&lt;/strong&gt;;&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;Which is documented as one of the supported online DDL operations. Same.&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;http://dev.mysql.com/doc/refman/5.6/en/innodb-online-ddl.html&#34;&gt;manual&lt;/a&gt; says I can define the &lt;strong&gt;ALGORITHM&lt;/strong&gt; and the &lt;strong&gt;LOCK&lt;/strong&gt; properties for the &lt;strong&gt;ALTER TABLE&lt;/strong&gt; operation. But is gives no example, so I try my own:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;node1 (sakila) &amp;gt; alter table sakila.rental row_format=compact &lt;strong&gt;ALGORITHM=INPLACE LOCK=NONE&lt;/strong&gt;;
ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near &#39;ALGORITHM=INPLACE LOCK=NONE&#39; at line 1&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;Ummm.... then maybe:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;node1 (sakila) &amp;gt; alter table sakila.rental &lt;strong&gt;ALGORITHM=INPLACE LOCK=NONE&lt;/strong&gt; row_format=compact;
ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near &#39;LOCK=NONE row_format=compact&#39; at line 1&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;OK, how about:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;node1 (sakila) &amp;gt; alter table sakila.rental &lt;strong&gt;ALGORITHM=INPLACE&lt;/strong&gt; row_format=compact &lt;strong&gt;LOCK=NONE&lt;/strong&gt;;
ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near &#39;row_format=compact LOCK=NONE&#39; at line 1&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;Reading, rereading, re-verifying &lt;a href=&#34;http://dev.mysql.com/doc/refman/5.6/en/alter-table.html&#34;&gt;the manual&lt;/a&gt; -- I am typing a valid statement! What&#39;s wrong here?&lt;/p&gt;
&lt;p&gt;Yes, I&#39;m on &lt;strong&gt;5.6.7-rc-log&lt;/strong&gt;. No, I can&#39;t find, in &lt;strong&gt;5.6&lt;/strong&gt; documentation and slides from &lt;a href=&#34;https://oracleus.activeevents.com/connect/search.ww?event=openworld#loadSearch-event=openworld&amp;amp;searchPhrase=&amp;amp;searchType=session&amp;amp;tc=0&amp;amp;sortBy=&amp;amp;p=&amp;amp;i%2810942%29=15982&amp;amp;i%2811425%29=&amp;amp;i%2810053%29=&amp;amp;i%2811404%29=&amp;amp;i%2811562%29=&amp;amp;i%2811488%29=&amp;amp;i%2810089%29=&amp;amp;i%2811840%29=&#34;&gt;MySQL connect&lt;/a&gt;, any code sample that actually uses &lt;strong&gt;ALGORITHM&lt;/strong&gt; and &lt;strong&gt;LOCK&lt;/strong&gt; (!?)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;[UPDATE]&lt;/strong&gt;, as Marc Alff point out, I did in fact use the wrong syntax, and was missing commas. The right syntax is:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;node1 (sakila) &amp;gt; &lt;strong&gt;alter table sakila.rental row_format=compact, algorithm=inplace, lock=none;&lt;/strong&gt;
ERROR 1235 (42000): This version of MySQL doesn&#39;t yet support &#39;alter table sakila.rental row_format=compact, algorithm=inplace, lock=none&#39;&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;Unfortunately this still results with an error. Another attempt shows that:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;node1 (sakila) &amp;gt; alter table sakila.rental row_format=compact, algorithm=inplace, lock=shared;
Query OK, 0 rows affected (11.08 sec)&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;works well. So, apparently, you can only run &lt;em&gt;this type&lt;/em&gt; of &lt;strong&gt;ALTER TABLE&lt;/strong&gt; a with a &lt;strong&gt;SHARED&lt;/strong&gt; lock. The bad news?&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;node1 (sakila) &amp;gt; alter table sakila.rental &lt;strong&gt;add index(return_date)&lt;/strong&gt;, algorithm=inplace, lock=&lt;strong&gt;none&lt;/strong&gt;;
ERROR 1235 (42000): This version of MySQL doesn&#39;t yet support &#39;alter table sakila.rental add index(return_date), algorithm=inplace, lock=none&#39;
node1 (sakila) &amp;gt; alter table sakila.rental &lt;strong&gt;add column c char&lt;/strong&gt;, algorithm=inplace, lock=&lt;strong&gt;none&lt;/strong&gt;;
ERROR 1235 (42000): This version of MySQL doesn&#39;t yet support &#39;alter table sakila.rental add column c char, algorithm=inplace, lock=none&#39;&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;So I&#39;m not sure as yet what kind of DDL operations are available with &lt;strong&gt;LOCK=NONE&lt;/strong&gt;.&lt;/p&gt;
&lt;h4&gt;Conclusion&lt;/h4&gt;
&lt;p&gt;Little success with online DDL. SHARED-only is many times as good as completely blocked.&lt;/p&gt;
&lt;p&gt;My personal conclusion is (and I do take into account &lt;strong&gt;5.6&lt;/strong&gt; is RC at this time, not GA): &lt;em&gt;not there yet!&lt;/em&gt; Stick to &lt;a href=&#34;http://openarkkit.googlecode.com/svn/trunk/openarkkit/doc/html/introduction.html&#34;&gt;openark-kit&lt;/a&gt;, &lt;a href=&#34;http://www.percona.com/doc/percona-toolkit/2.1/&#34;&gt;Percona-toolkit&lt;/a&gt; or &lt;a href=&#34;http://www.facebook.com/notes/mysql-at-facebook/online-schema-change-for-mysql/430801045932&#34;&gt;Facebook OSC&lt;/a&gt; for some time. They all provide with online-alter-table operations via external scripts.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Thoughts on MySQL 5.6 new replication features</title>
      <link>/blog/2012/10/15/thoughts-on-mysql-5-6-new-replication-features/</link>
      <pubDate>Mon, 15 Oct 2012 09:50:39 +0000</pubDate>
      
      <guid>/blog/2012/10/15/thoughts-on-mysql-5-6-new-replication-features/</guid>
      <description>&lt;p&gt;After playing a little bit with MySQL &lt;strong&gt;5.6&lt;/strong&gt; (RC), and following closely on Giuseppe&#39;s &lt;a href=&#34;http://datacharmer.blogspot.co.il/2012/08/mysql-56-replication-gotchas-and-bugs.html&#34;&gt;MySQL 5.6 replication gotchas (and bugs)&lt;/a&gt;, I was having some thoughts.&lt;/p&gt;
&lt;p&gt;These are shared for a few reasons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Maybe I didn&#39;t understand it well, and someone could correct me&lt;/li&gt;
&lt;li&gt;Or I understood it well, and my input could be of service to the developers&lt;/li&gt;
&lt;li&gt;Or it could be of service to the users&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;InnoDB tables in mysql schema&lt;/h4&gt;
&lt;p&gt;The introduction of InnoDB tables in &lt;strong&gt;mysql&lt;/strong&gt; makes for crash-safe replication information: the exact replication position (master log file+pos, relay log file+pos etc.) is updated on InnoDB tables; with &lt;strong&gt;innodb_flush_logs_at_trx_commit=1&lt;/strong&gt; this means replication status is durable and consistent with server data. This is great news!&lt;/p&gt;
&lt;p&gt;However, the introduction of InnoDB tables to the mysql schema also breaks some common usage on installation and setup of MySQL servers. You can&#39;t just drop your &lt;strong&gt;ib_data1&lt;/strong&gt; file upon dump+restore, since it also contains internal data. Giuseppe outlines the workaround for that.&lt;/p&gt;
&lt;p&gt;I was thinking: would it be possible to have a completely different tablespace for MySQL&#39;s internal InnoDB tables? That could be a single tablespace file (who cares about file-per-table on a few internal tables). And I&#39;m throwing an idea without being intimate with the internals: you know how it is possible to span the shared tablespace across multiple files, as in:&lt;!--more--&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;[mysqld]
innodb_data_file_path=ibdata1:50M;ibdata2:50M:autoextend&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;Would it be possible to, for example, force the first file in this setup to be the internal database? It would look like:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;[mysqld]
innodb_data_file_path=&lt;strong&gt;ibdata_internal_do_not_touch&lt;/strong&gt;:2M;&lt;strong&gt;ibdata1_this_one_is_yours&lt;/strong&gt;:50M:autoextend&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;Only the user would not have to actually set this thing up: the internal tablespace would be there by default (and always first).&lt;/p&gt;
&lt;p&gt;Then we would be able to drop our own table space as much as we would like to, but never touch the internal tablespace. It would always extend into our own &lt;strong&gt;ibdata1&lt;/strong&gt; file.&lt;/p&gt;
&lt;p&gt;I&#39;m wondering if I&#39;m making sense at all and if this is possible.&lt;/p&gt;
&lt;h4&gt;GTID and settings&lt;/h4&gt;
&lt;p&gt;The fact that you have to specify both &lt;strong&gt;gtid_mode=ON&lt;/strong&gt; as well as &lt;strong&gt;disable-gtid-unsafe-statements&lt;/strong&gt; is a bit of a bummer. I wouldn&#39;t mind as much if error messages would be informative. But as it turned out, when I wanted to test GTID I did the following:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;mysql&amp;gt; STOP SLAVE;
mysql&amp;gt; change master to MASTER_AUTO_POSITION=1;
ERROR 1777 (HY000): CHANGE MASTER TO MASTER_AUTO_POSITION = 1 can only be executed when GTID_MODE = ON.

-- OK, setting &lt;strong&gt;gtid_mode=ON&lt;/strong&gt; in config file, restarting server.
--
-- &lt;strong&gt;Oooops&lt;/strong&gt;, server won&#39;t restart!
-- Getting this error message in log: &lt;strong&gt;&#34;--gtid-mode=UPGRADE_STEP_1 or --gtid-mode=UPGRADE_STEP_2 are not yet supported&#34;&lt;/strong&gt;
-- What?&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;Checking up on Giuseppe&#39;s post I realized I didn&#39;t set the &lt;strong&gt;disable-gtid-unsafe-statements&lt;/strong&gt; param. But this was not mentioned on the above &lt;strong&gt;ERROR 1777&lt;/strong&gt;, and the log error was quite cryptic.&lt;/p&gt;
&lt;p&gt;TODO: just mention this &lt;em&gt;other&lt;/em&gt; variable.&lt;/p&gt;
&lt;h4&gt;GTID, internal InnoDB tables &amp;amp; wreckage&lt;/h4&gt;
&lt;p&gt;OK, I managed to completely crash my replication setup. I setup GTID, and then:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;set global master_info_repository:=&#39;table&#39;;
set global relay_log_info_repository=&#39;table&#39;;&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;Then shut down mysql; I wanted to see how reverting back to &lt;strong&gt;gtid_mode=OFF&lt;/strong&gt; works. Oh, I didn&#39;t set the two params in the config file, so their effect was lost.&lt;/p&gt;
&lt;p&gt;Starting mysql, I get:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;ERROR 1794 (HY000) at line 1: Slave is not configured or failed to initialize properly. You must at least set --server-id to enable either a master or a slave. Additional error messages can be found in the MySQL error log.&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;The error log says:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;121015  9:38:58 [ERROR] Error creating master info: Multiple replication metadata repository instances found with data in them. Unable to decide which is the correct one to choose.
121015  9:38:58 [ERROR] Failed to create or recover replication info repository.
121015  9:38:58 [Note] Check error log for additional messages. You will not be able to start replication until the issue is resolved and the server restarted.&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;What&#39;s interesting is that the data is still in the tables:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;mysql&amp;gt; select * from mysql.slave_master_info\G
*************************** 1. row ***************************
       Number_of_lines: 23
       Master_log_name: mysql-bin.000003
        Master_log_pos: 2623
                  Host: 127.0.0.1
             User_name: rsandbox
         User_password: rsandbox
                  Port: 14701
         Connect_retry: 60
           Enabled_ssl: 0
                Ssl_ca: 
            Ssl_capath: 
              Ssl_cert: 
            Ssl_cipher: 
               Ssl_key: 
Ssl_verify_server_cert: 0
             Heartbeat: 1800
                  Bind: 
    Ignored_server_ids: 0
                  Uuid: 10fa73da-13ac-11e2-bdcd-0024e8cd3122
           Retry_count: 86400
               Ssl_crl: 
           Ssl_crlpath: 
 Enabled_auto_position: 1&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;I&#39;ve tried restarting, setting variables in the config file, changing them dynamically. To no avail.&lt;/p&gt;
&lt;p&gt;No, I haven&#39;t filed a bug report yet.&lt;/p&gt;
&lt;p&gt;These are still my first steps into 5.6 replication and my very first impressions.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>MySQL 5.6 new features: the user&#39;s perspective</title>
      <link>/blog/2012/10/05/mysql-5-6-new-features-the-users-perspective/</link>
      <pubDate>Fri, 05 Oct 2012 09:20:20 +0000</pubDate>
      
      <guid>/blog/2012/10/05/mysql-5-6-new-features-the-users-perspective/</guid>
      <description>&lt;p&gt;This is a yet-another compilation of the new MySQL &lt;strong&gt;5.6&lt;/strong&gt; feature set. It is not a complete drill down. This list reflects what I believe to be the interesting new features user and usability -wise.&lt;/p&gt;
&lt;p&gt;For example, I won&#39;t be listing InnoDB&#39;s split of kernel mutex. I&#39;m assuming it can have a great impact on overall performance due to reducing lock contention; but usability-wise, this is very internal.&lt;/p&gt;
&lt;p&gt;The complication is an aggregate of the many announcements and other complications published earlier on. See a reference at the end of this post.&lt;/p&gt;
&lt;p&gt;Do note I am not using &lt;strong&gt;5.6&lt;/strong&gt; as yet; it is in RC, not GA. I am mostly excited just to write down this list.&lt;/p&gt;
&lt;h4&gt;InnoDB&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Online &lt;strong&gt;ALTER TABLE&lt;/strong&gt;: if there is one major new feature in &lt;strong&gt;5.6&lt;/strong&gt; you would want to upgrade for, this would be it. Add columns, drop columns, rename columns, add indexes, drop indexes - now online, while your &lt;strong&gt;SELECT, INSERT, UPDATE&lt;/strong&gt; and &lt;strong&gt;DELETE&lt;/strong&gt; statements are running.&lt;/li&gt;
&lt;li&gt;Transportable tablespace files: copy+paste &lt;strong&gt;your_table.ibd&lt;/strong&gt; files with &lt;strong&gt;FLUSH TABLE FOR EXPORT&lt;/strong&gt; and &lt;strong&gt;ALTER TABLE ... IMPORT TABLESPACE&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;FULLTEXT&lt;/strong&gt;: for many, the one thing holding them back from leaving MyISAM behind. Now available in InnoDB with same syntax as with MyISAM.&lt;/li&gt;
&lt;li&gt;Memcached API: access InnoDB data via memcahced protocol, and skip the SQL interface.&lt;/li&gt;
&lt;li&gt;User defined table location: place your tables in your pre-defined location. Place other tables elsewhere. This is something I&#39;ve been asked about for ages.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;&lt;!--more--&gt;Replication&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Global Transaction IDs: I&#39;m still to fully understand the limitations: MyISAM and temporary tables make a mess; this could be a killer feature when it comes to replication: no more master log file, master log pos, relay master log file, relay log pos, read master log file, read master log pos (if I wake you up at &lt;strong&gt;3:00am&lt;/strong&gt;, will you be able to cite the difference?). Just one single unique identifier for each transaction in the binary log, so it&#39;s much easier for slaves to connect to master, or to switch over to replicate another server.&lt;/li&gt;
&lt;li&gt;Multi threaded slaves: with a thread-per-schema, and assuming complete isolation of schemas&lt;/li&gt;
&lt;li&gt;Delayed replication: a must-have, in my opinion, on any replication topology using &lt;strong&gt;3-4&lt;/strong&gt; servers.&lt;/li&gt;
&lt;li&gt;Checksums: verify shipment of binary logs from master to slave by adding a checksum on log entries.&lt;/li&gt;
&lt;li&gt;Crash safe slaves: forget about master.info not syncing to disk. Now using InnoDB for that.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Partitioning&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Partition-to-table, table-to-partition: I&#39;ve encountered the need for this in the past. In one case, it was the issue of backing up single partitions for archiving, then restoring particular partitions from the past into the existing table. Some Oracle users, upon hearing about the &lt;strong&gt;5.1&lt;/strong&gt;, &lt;strong&gt;5.5&lt;/strong&gt; partition features, were wondering about this missing feature. Their words: &#34;MySQL is still in the very early stages of managing partitions. As it grows it will have to include partition-to-table, as well as other already-standard-in-Oracle features&#34;. They were right.&lt;/li&gt;
&lt;li&gt;Reduced locking: don&#39;t you hate it when you partition by date, INSERT into the last partition, only to find out you actually acquired locks for &lt;em&gt;all&lt;/em&gt; partitions? Hopefully this is gone now (hoping I&#39;m not wrong on this?)&lt;/li&gt;
&lt;li&gt;Choose partitions in query à la &lt;strong&gt;SELECT * FROM my_table PARTITION (p7)&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Query Execution Plan&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;EXPLAIN&lt;/strong&gt; for &lt;strong&gt;UPDATE, DELETE, INSERT&lt;/strong&gt;: pretty sure everyone wanted that.&lt;/li&gt;
&lt;li&gt;Optimizing subqueries (the notorious &lt;strong&gt;WHERE IN (SELECT ...)&lt;/strong&gt;): the &lt;em&gt;&#34;MySQL does WHAT with subqueries?!?!?!&#34;&lt;/em&gt; invoking behavior of subquery execution is now hopefully met. No longer &#34;execute the subquery for each row in the outer query&#34;.&lt;/li&gt;
&lt;li&gt;Index merge optimization: this optimization was rare, in my experience. The new improvements are expected to make it more common.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;EXPLAIN FORMAT=JSON&lt;/strong&gt;: I like this kind of stuff. The Json format is much more verbose, and shows the tree-structure of the query execution plan. This would make for a great analysis tool for GUI editors!&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;PERFORMANCE_SCHEMA&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;A gazillion more tables. I am yet to drill down into &lt;strong&gt;P_S&lt;/strong&gt;. However I can already verify that &lt;strong&gt;5.6&lt;/strong&gt; introduces a lot of new tables I&#39;ve been longing for. Some are actually more fitting in &lt;strong&gt;INFORMATION_SCHEMA&lt;/strong&gt;. A lot of new metadata tables. Will write more in the future.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Some of the above features already exists in earlier releases of Google Patches for MySQL, Percona Server and MariaDB. For example, global transaction IDs and binlog checksums first appeared three and a half years ago in Google Patches, and only now re-created in MySQL. Sometimes the community is far ahead of the MySQL development. But then Oracle does its thing and makes for a great release.&lt;/p&gt;
&lt;p&gt;No doubt the above is an impressive list of enhancements to the server. Some make it a significant step into the &#34;things are getting serious here&#34; realm. I can do well with online alter table, auto replication recovery, execution plan improvements, and the many performance boosts not listed here. Not everything will work in all scenarios; but this makes for one release of MySQL I&#39;m anxious to use.&lt;/p&gt;
&lt;h4&gt;References, chronologically DESC&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://mikaelronstrom.blogspot.com/2012/10/my-personal-list-of-new-features-in.html&#34;&gt;My personal list of new features in MySQL 5.6 &lt;/a&gt;- Mikael Ronström&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://svenmysql.blogspot.com/2012/10/failover-and-flexible-replication.html&#34;&gt; Failover and Flexible Replication Topologies in MySQL 5.6 &lt;/a&gt;- Sven&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://jorgenloland.blogspot.com/2012/10/index-merge-annoyances-fixed-in-mysql-56.html&#34;&gt;Index merge annoyances fixed in MySQL 5.6 &lt;/a&gt;- Jørgen Løland&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://larsthalmann.blogspot.com/2012/10/mysql-connect-2012.html&#34;&gt;MySQL Connect 2012 &lt;/a&gt;- Lars Thalmann&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://lstigile.wordpress.com/2012/09/29/mysqlconnect-auditing-online-ddl-fk-in-cluster-and-more/&#34;&gt;MySQLConnect — Auditing, Online DDL, FK in Cluster and More &lt;/a&gt;- Lee Stigile&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.sheeri.com/content/my-thoughts-about-mysql-56&#34;&gt;My Thoughts About MySQL 5.6 &lt;/a&gt;- Sheeri K. Cabral&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blogs.oracle.com/mysqlinnodb/entry/online_alter_table_in_mysql&#34;&gt;Online ALTER TABLE in MySQL 5.6 &lt;/a&gt; - Oracle&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://dev.mysql.com/tech-resources/articles/mysql-5.6-rc.html&#34;&gt; What&#39;s New in MySQL 5.6 Release Candidate &lt;/a&gt;- Oracle&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.xaprb.com/blog/2012/08/21/a-summary-of-changes-in-mysql-5-6/&#34;&gt;A summary of changes in MySQL 5.6 &lt;/a&gt; - Baron Schwartz&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://dev.mysql.com/tech-resources/articles/whats-new-in-mysql-5.6.html&#34;&gt;What&#39;s New in MySQL 5.6&lt;/a&gt; - Oracle&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>Getting rid of huge ibdata file, no dump required, part II</title>
      <link>/blog/2012/05/30/getting-rid-of-huge-ibdata-file-no-dump-required-part-ii/</link>
      <pubDate>Wed, 30 May 2012 09:03:18 +0000</pubDate>
      
      <guid>/blog/2012/05/30/getting-rid-of-huge-ibdata-file-no-dump-required-part-ii/</guid>
      <description>&lt;p&gt;This post continues &lt;a href=&#34;http://code.openark.org/blog/mysql/getting-rid-of-huge-ibdata-file-no-dump-required&#34;&gt;Getting rid of huge ibdata file, no dump required, part I&lt;/a&gt;, where I describe way of converting your single-tablespace InnoDB database into a file-per-table one, without the pain of exporting and importing &lt;em&gt;everything at once&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;In previous part we put aside the issue of foreign keys. We address this issue now.&lt;/p&gt;
&lt;h4&gt;What if my InnoDB tables have foreign keys?&lt;/h4&gt;
&lt;p&gt;MyISAM does not support them, so you can&#39;t just &lt;strong&gt;ALTER&lt;/strong&gt; an InnoDB table to MyISAM and back into InnoDB, and expect everything to work.&lt;/p&gt;
&lt;p&gt;Alas, this calls for additional steps (i.e. additional &lt;strong&gt;ALTER&lt;/strong&gt; commands). However, these still fall well under the concept of &lt;em&gt;&#34;do it one table at a time, then take time to recover your breath and replication lag&#34;&lt;/em&gt;.&lt;/p&gt;
&lt;h4&gt;Save , drop and restore your Foreign Keys setup&lt;/h4&gt;
&lt;p&gt;You can use &lt;a href=&#34;http://code.google.com/p/common-schema/&#34;&gt;common_schema&lt;/a&gt;&#39;s  &lt;a href=&#34;http://common-schema.googlecode.com/svn/trunk/common_schema/doc/html/sql_foreign_keys.html&#34;&gt;sql_foreign_keys&lt;/a&gt; to get the full listing and create definition of your foreign keys. For example, assume we use the &lt;strong&gt;sakila&lt;/strong&gt; database:&lt;!--more--&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;SELECT create_statement FROM common_schema.sql_foreign_keys WHERE TABLE_SCHEMA=&#39;sakila&#39; INTO OUTFILE &#39;/somewhere/safe/create_foreign_keys.sql&#39;&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;(replace &lt;strong&gt;TABLE_SCHEMA=&#39;sakila&#39;&lt;/strong&gt; with whatever you want).&lt;/p&gt;
&lt;p&gt;A sample output would be something like this (&lt;em&gt;note: no semicolon on end of line&lt;/em&gt;):&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;ALTER TABLE `sakila`.`address` ADD CONSTRAINT `fk_address_city` FOREIGN KEY (`city_id`) REFERENCES `sakila`.`city` (`city_id`) ON DELETE RESTRICT ON UPDATE CASCADE
ALTER TABLE `sakila`.`city` ADD CONSTRAINT `fk_city_country` FOREIGN KEY (`country_id`) REFERENCES `sakila`.`country` (`country_id`) ON DELETE RESTRICT ON UPDATE CASCADE
ALTER TABLE `sakila`.`customer` ADD CONSTRAINT `fk_customer_address` FOREIGN KEY (`address_id`) REFERENCES `sakila`.`address` (`address_id`) ON DELETE RESTRICT ON UPDATE CASCADE
ALTER TABLE `sakila`.`customer` ADD CONSTRAINT `fk_customer_store` FOREIGN KEY (`store_id`) REFERENCES `sakila`.`store` (`store_id`) ON DELETE RESTRICT ON UPDATE CASCADE
ALTER TABLE `sakila`.`film` ADD CONSTRAINT `fk_film_language` FOREIGN KEY (`language_id`) REFERENCES `sakila`.`language` (`language_id`) ON DELETE RESTRICT ON UPDATE CASCADE
ALTER TABLE `sakila`.`film` ADD CONSTRAINT `fk_film_language_original` FOREIGN KEY (`original_language_id`) REFERENCES `sakila`.`language` (`language_id`) ON DELETE RESTRICT ON UPDATE CASCADE
...&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;Once the above is in a safe place, you will want to DROP all of your foreign keys. Again, using &lt;em&gt;common_schema&lt;/em&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;SELECT drop_statement FROM common_schema.sql_foreign_keys WHERE TABLE_SCHEMA=&#39;sakila&#39;;
+-----------------------------------------------------------------------------------+
| drop_statement                                                                    |
+-----------------------------------------------------------------------------------+
| ALTER TABLE `sakila`.`address` DROP FOREIGN KEY `fk_address_city`                 |
| ALTER TABLE `sakila`.`city` DROP FOREIGN KEY `fk_city_country`                    |
| ALTER TABLE `sakila`.`customer` DROP FOREIGN KEY `fk_customer_address`            |
| ALTER TABLE `sakila`.`customer` DROP FOREIGN KEY `fk_customer_store`              |
| ALTER TABLE `sakila`.`film` DROP FOREIGN KEY `fk_film_language`                   |
| ALTER TABLE `sakila`.`film` DROP FOREIGN KEY `fk_film_language_original`          |
| ...                                                                               |
+-----------------------------------------------------------------------------------+&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;You don&#39;t want to issue all these at once: do them one at a time, and wait for your slave to catch up.&lt;/p&gt;
&lt;p&gt;Once this is done, you can move on to the steps described in Part I of this post: converting tables to MyISAM, shutting down, removing InnoDB files, then converting back to InnoDB.&lt;/p&gt;
&lt;p&gt;And then, taking breath again, you must re-import the foreign keys. Use the &lt;strong&gt;ADD CONSTRAINT&lt;/strong&gt; commands you have saved earlier on. Again, one at a time, wait for slave to catch up.&lt;/p&gt;
&lt;p&gt;To reiterate, for each table you would take the following steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Make sure the FK definition is safely stored somewhere&lt;/li&gt;
&lt;li&gt;STOP SLAVE&lt;/li&gt;
&lt;li&gt;Drop all table&#39;s foreign keys: ALTER TABLE ... DROP FOREIGN KEY ..., DROP FOREIGN KEY ...&lt;/li&gt;
&lt;li&gt;START SLAVE&lt;/li&gt;
&lt;li&gt;Wait for slave to catch up&lt;/li&gt;
&lt;li&gt;STOP SLAVE&lt;/li&gt;
&lt;li&gt;ALTER TABLE ... ENGINE=MyISAM (*)&lt;/li&gt;
&lt;li&gt;START SLAVE&lt;/li&gt;
&lt;li&gt;Wait for slave to catch up&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;(*) Altering to MyISAM drops FK constraints, so the above could actually be done in one step. I&#39;m cautious and illustrate in two.&lt;/p&gt;
&lt;p&gt;Once all tables are altered, and InnoDB tablespace is removed, restoration is as follows: for each table,&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;STOP SLAVE&lt;/li&gt;
&lt;li&gt;ALTER TABLE ... ENGINE=InnoDB [create options]&lt;/li&gt;
&lt;li&gt;START SLAVE&lt;/li&gt;
&lt;li&gt;Wait for slave to catch up&lt;/li&gt;
&lt;li&gt;STOP SLAVE&lt;/li&gt;
&lt;li&gt;ALTER TABLE ... ADD CONSTRAINT ..., ADD CONSTRAINT ...(+)&lt;/li&gt;
&lt;li&gt;START SLAVE&lt;/li&gt;
&lt;li&gt;Wait for slave to catch up&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;(+) Alas, you can&#39;t convert to InnoDB and add constraints at the same time...&lt;/p&gt;
&lt;h4&gt;This is not entirely safe&lt;/h4&gt;
&lt;p&gt;A MyISAM slave to an InnoDB master with foreign keys is a tricky business. It really depends on the type of foreign keys you have and the use you make of them. See &lt;a title=&#34;Link to Impact of foreign keys absence on replicating slaves&#34; href=&#34;http://code.openark.org/blog/mysql/impact-of-foreign-keys-absence-on-replicating-slaves&#34; rel=&#34;bookmark&#34;&gt;Impact of foreign keys absence on replicating slaves&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Getting rid of huge ibdata file, no dump required</title>
      <link>/blog/2012/05/22/getting-rid-of-huge-ibdata-file-no-dump-required/</link>
      <pubDate>Tue, 22 May 2012 07:33:05 +0000</pubDate>
      
      <guid>/blog/2012/05/22/getting-rid-of-huge-ibdata-file-no-dump-required/</guid>
      <description>&lt;p&gt;You &lt;a href=&#34;http://code.openark.org/blog/mysql/upgrading-to-barracuda-getting-rid-of-huge-ibdata1-file&#34;&gt;have&lt;/a&gt; been &lt;a href=&#34;http://ronaldbradford.com/blog/leveraging-the-innodb-plugin-2011-02-11/&#34;&gt;told&lt;/a&gt; (guilty as charged), that the only way to get rid of the huge InnoDB tablespace file (commonly named &lt;strong&gt;ibdata1&lt;/strong&gt;), when moving to &lt;strong&gt;innodb_file_per_table&lt;/strong&gt;, is to do a logical dump of your data, completely erase everything, then import the dump.&lt;/p&gt;
&lt;p&gt;To quickly reiterate, you can only delete the &lt;strong&gt;ibdata1&lt;/strong&gt; file when no InnoDB tables exist. Delete this file with an existing InnoDB table, even a table in its own tablespace, and nothing ever works anymore.&lt;/p&gt;
&lt;h4&gt;The problem with the dump-based solution&lt;/h4&gt;
&lt;p&gt;The impact of doing a logical dump is often overwhelming. Well, the dump may be tolerable, but the restore is much longer. The real pain is that you can&#39;t do this one table at a time: you have to destroy everything before dropping the &lt;strong&gt;ibdata1&lt;/strong&gt; file; you then have to import everything.&lt;/p&gt;
&lt;p&gt;Perhaps the most common scenario is that we do the changes on a slave, so as not to completely shut down our database. This is nice; no one is aware of the shutdown process. However, Huston, we have a problem: we need to make sure we can keep up the binary logs on the master for the duration of the &lt;em&gt;entire process&lt;/em&gt;.&lt;!--more--&gt;&lt;/p&gt;
&lt;h4&gt;A semi-solution for binary logs&lt;/h4&gt;
&lt;p&gt;You may get by by keeping the &lt;strong&gt;SQL_IO_THREAD&lt;/strong&gt; running on the slave while dump is taken (SQL thread is better turned off). If you&#39;re careful, you could do the same after restarting the database: you should still be able to acquire relay logs. With row based replication becoming more common, the problem of binary logs disk space returns: the logs (rather, log entries) are just so much larger!&lt;/p&gt;
&lt;p&gt;Either way, the process can takes long days, at the end of which your slave is up, but lags for long days behind.&lt;/p&gt;
&lt;h4&gt;Wishful thought: do it one table at a time&lt;/h4&gt;
&lt;p&gt;If we could do it one table at a time, and assuming our dataset is fairly split among several tables (i.e. not all of our &lt;strong&gt;500GB&lt;/strong&gt; of data is in one huge table), life would be easier: we could work on a single table, resume replication, let the slave catch up, then do the same for the next table.&lt;/p&gt;
&lt;p&gt;How? Didn&#39;t we just say one can only drop the &lt;strong&gt;ibdata1&lt;/strong&gt; file when no InnoDB tables exist?&lt;/p&gt;
&lt;h4&gt;Solution: do it one table at a time&lt;/h4&gt;
&lt;p&gt;I&#39;m going to illustrate what seems like a longer procedure. I will later show why it is not, in fact, longer.&lt;/p&gt;
&lt;p&gt;The idea is to first convert all your tables to MyISAM (Yay! A use for MyISAM!). That is, convert your tables one table at a time, using normal &lt;strong&gt;ALTER TABLE t ENGINE=MyISAM&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Please let go of the foreign keys issue right now. I will address it later, there&#39;s a lot to be addressed.&lt;/p&gt;
&lt;p&gt;So, on a slave:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;STOP SLAVE&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;One &lt;strong&gt;ALTER TABLE ... ENGINE=MyISAM&lt;br /&gt;
&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;START SLAVE&lt;/strong&gt; again&lt;/li&gt;
&lt;li&gt;Wait for slave catch up&lt;/li&gt;
&lt;li&gt;GOTO &lt;strong&gt;1&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;What do we end up with? A MyISAM only database. What do we do with it? Why, convert it back to InnoDB, of course!&lt;/p&gt;
&lt;p&gt;But, before that, we:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Shut MySQL down&lt;/li&gt;
&lt;li&gt;Delete &lt;strong&gt;ibdata1&lt;/strong&gt; file, &lt;strong&gt;ib_logfile[01]&lt;/strong&gt; (i.e. delete all InnoDB files)&lt;/li&gt;
&lt;li&gt;Start MySQL&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;A new &lt;strong&gt;ibdata1&lt;/strong&gt; file, and new transaction log files will be created. Note: the new ibdata1 file is &lt;em&gt;small&lt;/em&gt;. Mission almost accomplished.&lt;/p&gt;
&lt;p&gt;We then:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;STOP SLAVE&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Do one &lt;strong&gt;ALTER TABLE ... ENGINE=InnoDB [ROW_FORMAT=COMPRESSED KEY_BLOCK_SIZE=8 ...]&lt;br /&gt;
&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;START SLAVE&lt;/strong&gt; again&lt;/li&gt;
&lt;li&gt;Wait for slave catch up&lt;/li&gt;
&lt;li&gt;GOTO &lt;strong&gt;1&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;What do we end up with? An InnoDB only database, with true file per table, and a small &lt;strong&gt;ibdata1&lt;/strong&gt; file. Space recovered!&lt;/p&gt;
&lt;h4&gt;The advantage of this method&lt;/h4&gt;
&lt;p&gt;The thing is, we resume replication after each table alteration. This means breaking the lag period into many smaller periods. While the &lt;em&gt;total&lt;/em&gt; runtime does not reduce, we do reduce the maximum lag time. And this makes for easier recovery: no need to store multitudes of binary logs!&lt;/p&gt;
&lt;h4&gt;So what about the foreign keys?&lt;/h4&gt;
&lt;p&gt;Phew. Continued next post.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>