<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Monitoring on code.openark.org</title>
    <link>/blog/tag/monitoring/</link>
    <description>Recent content in Monitoring on code.openark.org</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2016. All rights reserved.</copyright>
    <lastBuildDate>Fri, 07 Aug 2015 14:39:59 +0000</lastBuildDate>
    <atom:link href="/blog/tag/monitoring/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Baffling 5.7 global/status variables issues, unclean migration path</title>
      <link>/blog/mysql/baffling-5-7-globalstatus-variables-issues-unclean-migration-path/</link>
      <pubDate>Fri, 07 Aug 2015 14:39:59 +0000</pubDate>
      
      <guid>/blog/mysql/baffling-5-7-globalstatus-variables-issues-unclean-migration-path/</guid>
      <description>&lt;p&gt;MySQL &lt;strong&gt;5.7&lt;/strong&gt; introduces a change in the way we query for global variables and status variables: the &lt;strong&gt;INFORMATION_SCHEMA.(GLOBAL|SESSION)_(VARIABLES|STATUS)&lt;/strong&gt; tables are now deprecated and empty. Instead, we are to use the respective &lt;strong&gt;performance_schema.(global|session)_(variables|status)&lt;/strong&gt; tables.&lt;/p&gt;
&lt;p&gt;But the change goes farther than that; there is also a security change. Oracle created a pitfall of &lt;strong&gt;2&lt;/strong&gt; changes at the same time:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Variables/status moved to a different table&lt;/li&gt;
&lt;li&gt;Privileges required on said table&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;As an example, my non-root user gets:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;mysql&amp;gt; show session variables like &#39;tx_isolation&#39;;
ERROR 1142 (42000): SELECT command denied to user &#39;normal_user&#39;@&#39;my_host&#39; for table &#39;session_variables&#39;&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;Who gets affected by this? Nearly &lt;em&gt;everyone and everything&lt;/em&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Your Nagios will not be able to read status variables&lt;/li&gt;
&lt;li&gt;Your ORM will not be able to determine session variables&lt;/li&gt;
&lt;li&gt;Your replication user will fail connecting (see &lt;a href=&#34;http://datacharmer.blogspot.nl/2015/08/mysql-578-features-bugs-and-rumors.html&#34;&gt;this post by Giuseppe&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;And most everyone else.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The problem with the above is that involves two unrelated changes to your setup, which are not entirely simple to coordinate:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Change your app code to choose the correct schema (information_schema vs. performance_schema)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GRANT&lt;/strong&gt; the permissions on your database&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Perhaps at this point you still do not consider this to be a problem. You may be thinking: &lt;em&gt;well, let&#39;s first prepare by creating the GRANTs, and once that is in place, we can, at our leisure, modify the code&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Not so fast. Can you really that simply create those GRANTs?&lt;!--more--&gt;&lt;/p&gt;
&lt;h3&gt;Migration woes&lt;/h3&gt;
&lt;p&gt;How do you migrate to a new MySQL version? You do not reinstall all your servers. You want an easy migration path, and that path is: introduce one or two slaves of a newer version, see that everything works to your satisfaction, slowly upgrade all your other slaves, eventually switchover/upgrade your master.&lt;/p&gt;
&lt;p&gt;This should not be any different for &lt;strong&gt;5.7&lt;/strong&gt;. We would like to provision a &lt;strong&gt;5.7&lt;/strong&gt; slave in our topologies and just see that everything works. Well, we have, and things don&#39;t just work. Our Nagios stops working for that &lt;strong&gt;5.7&lt;/strong&gt; slave. &lt;em&gt;Orchestrator&lt;/em&gt; started complaining (by this time I&#39;ve &lt;a href=&#34;https://github.com/outbrain/orchestrator/releases/tag/v1.4.291&#34;&gt;already fixed it&lt;/a&gt; to be more tolerant for the &lt;strong&gt;5.7&lt;/strong&gt; problems so no crashes here).&lt;/p&gt;
&lt;p&gt;I hope you see the problem by now.&lt;/p&gt;
&lt;blockquote&gt;You cannot issue a &lt;strong&gt;GRANT SELECT ON performance_schema.global_variables TO &#39;...&#39;&lt;/strong&gt; on your &lt;strong&gt;5.6&lt;/strong&gt; master.&lt;/blockquote&gt;
&lt;p&gt;The table simply does not exist there, which means the statement will not go to binary logs, which means it will not replicate on your &lt;strong&gt;5.7&lt;/strong&gt; slave, which means you will not be able to &lt;strong&gt;SHOW GLOBAL VARIABLES&lt;/strong&gt; on your slave, which means everything remains broken.&lt;/p&gt;
&lt;p&gt;Yes, you can issue this directly on your &lt;strong&gt;5.7&lt;/strong&gt; slaves. It&#39;s &lt;em&gt;doable&lt;/em&gt;, but &lt;em&gt;undesired&lt;/em&gt;. It&#39;s ugly in terms of automation (and will quite possibly break some assumptions and sanity checks your automation uses); in terms of validity testing. It&#39;s unfriendly to GTID (make sure to &lt;strong&gt;SET SQL_LOG_BIN=0&lt;/strong&gt; before that).&lt;/p&gt;
&lt;h3&gt;WHY in the first place?&lt;/h3&gt;
&lt;p&gt;It seems like a security thing. I&#39;m not sure whether this was intended. So you prevent a &lt;strong&gt;SHOW GLOBAL VARIABLES&lt;/strong&gt; for a normal user. Makes sense. And yet:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;mysql&amp;gt; show global variables like &#39;hostname&#39;;
ERROR 1142 (42000): SELECT command denied to user &#39;normal_user&#39;@&#39;my_host&#39; for table &#39;global_variables&#39;

mysql&amp;gt; select @@global.hostname;
+---------------------+
| @@global.hostname   |
+---------------------+
| myhost.mydomain.com |
+---------------------+

mysql&amp;gt; select @@version;
+--------------+
| @@version    |
+--------------+
| 5.7.8-rc-log |
+--------------+

&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;Seems like I&#39;m allowed access to that info after all. So it&#39;s not strictly a security design decision. For status variable, I admit, I don&#39;t have a similar workaround.&lt;/p&gt;
&lt;h3&gt;Solutions?&lt;/h3&gt;
&lt;p&gt;The following are meant to be solutions, but do not really solve the problem:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;SHOW&lt;/strong&gt; commands. &lt;strong&gt;SHOW GLOBAL|SESSION VARIABLES|STATUS&lt;/strong&gt; will work properly, and will implicitly know whether to provide the results via &lt;strong&gt;information_schema&lt;/strong&gt; or &lt;strong&gt;performance_schema&lt;/strong&gt; tables.
&lt;ul&gt;
&lt;li&gt;But, aren&#39;t we meant to be happier with &lt;strong&gt;SELECT&lt;/strong&gt; queries? So that I can really do stuff that is smarter than &lt;strong&gt;LIKE &#39;variable_name%&#39;&lt;/strong&gt;?&lt;/li&gt;
&lt;li&gt;And of course you cannot use &lt;strong&gt;SHOW&lt;/strong&gt; in server side cursors. Your stored routines are in a mess now.&lt;/li&gt;
&lt;li&gt;This does not solve the GRANTs problem.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;http://dev.mysql.com/doc/refman/5.7/en/server-system-variables.html#sysvar_show_compatibility_56&#34;&gt;show_compatibility_56&lt;/a&gt;&lt;/strong&gt;: an introduced variable in &lt;strong&gt;5.7&lt;/strong&gt;, boolean. It truly is a time-travel-paradox novel in disguise, in multiple respects.
&lt;ul&gt;
&lt;li&gt;Documentation introduces it, and says it is deprecated.
&lt;ul&gt;
&lt;li&gt;time-travel-paradox :O&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;But it actually works in &lt;strong&gt;5.7.8&lt;/strong&gt; (latest)
&lt;ul&gt;
&lt;li&gt;time-travel-paradox plot thickens&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Your automation scripts do not know in advance whether your MySQL has this variable
&lt;ul&gt;
&lt;li&gt;Hence &lt;strong&gt;SELECT @@global.show_compatibility_56&lt;/strong&gt; will produce an error on &lt;strong&gt;5.6&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;But the &#34;safe&#34; way of &lt;strong&gt;SHOW GLOBAL VARIABLES LIKE &#39;show_compatibility_56&#39;&lt;/strong&gt; will fail on a privilege error on &lt;strong&gt;5.7&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;time-travel-paradox :O&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Actually advised by my colleague Simon J. Mudd, &lt;strong&gt;show_compatibility_56&lt;/strong&gt; defaults to &lt;strong&gt;OFF&lt;/strong&gt;. I &lt;em&gt;support&lt;/em&gt; this line of thought. Or else it&#39;s &lt;strong&gt;old_passwords=1&lt;/strong&gt; all over again.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;show_compatibility_56&lt;/strong&gt; doesn&#39;t solve the GRANTs problem.&lt;/li&gt;
&lt;li&gt;This does not solve any migration path. It just postpones the moment when I will hit the same problem. When I flip the variable from &lt;strong&gt;&#34;1&#34;&lt;/strong&gt; to &lt;strong&gt;&#34;0&#34;&lt;/strong&gt;, I&#39;m back at square one.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Suggestion&lt;/h3&gt;
&lt;p&gt;I claim security is not the issue, as presented above. I claim Oracle will yet again fall into the trap of no-easy-way-to-migrate-to-GTID in &lt;strong&gt;5.6&lt;/strong&gt; if the current solution is unchanged. I claim that there have been too many changes at once. Therefore, I suggest one of the alternative two flows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Flow 1&lt;/strong&gt;: keep &lt;strong&gt;information_schema&lt;/strong&gt;, later migration into &lt;strong&gt;performance_schema&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;In &lt;strong&gt;5.7&lt;/strong&gt;, &lt;strong&gt;information_schema&lt;/strong&gt; tables should still produce the data.&lt;/li&gt;
&lt;li&gt;No security constraints on &lt;strong&gt;information_schema&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Generate WARNINGs on reading from &lt;strong&gt;information_schema&lt;/strong&gt; (&#34;...this will be deprecated...&#34;)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;performance_schema &lt;/strong&gt;&lt;em&gt;also available&lt;/em&gt;. With security constraints, whatever.&lt;/li&gt;
&lt;li&gt;In &lt;strong&gt;5.8&lt;/strong&gt; remove &lt;strong&gt;information_schema&lt;/strong&gt; tables; we are left with &lt;strong&gt;performance_schema&lt;/strong&gt; only.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Flow 2&lt;/strong&gt;: easy migration into &lt;strong&gt;performance_schema&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;In &lt;strong&gt;5.7&lt;/strong&gt;, &lt;strong&gt;performance_schema&lt;/strong&gt; tables should not require any special privileges. Any user can read from them.&lt;/li&gt;
&lt;li&gt;Keep &lt;strong&gt;show_compatibility_56 &lt;/strong&gt;as it is.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SHOW&lt;/strong&gt; commands choose between &lt;strong&gt;information_schema&lt;/strong&gt; or &lt;strong&gt;performance_schema&lt;/strong&gt; on their own -- just as things are done now.&lt;/li&gt;
&lt;li&gt;In &lt;strong&gt;5.8&lt;/strong&gt;, &lt;strong&gt;performance_schema&lt;/strong&gt; tables will require &lt;strong&gt;SELECT&lt;/strong&gt; privileges.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;As always, I love the work done by the engineers; and I love how they listen to the community.&lt;/p&gt;
&lt;p&gt;Comments are most welcome. Have I missed the simple solution here? Are there even more complications to these features? Thoughts on my suggested two flows?&lt;/p&gt;
&lt;h3&gt;[UPDATE 2015-08-19]&lt;/h3&gt;
&lt;p&gt;Please &lt;a href=&#34;http://www.tocker.ca/2015/08/18/a-followup-on-show_compatibility_56.html&#34;&gt;see this followup&lt;/a&gt; by Morgan Tocker of Oracle.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Monitoring DML/slow queries with graphite</title>
      <link>/blog/mysql/monitoring-dmlslow-queries-with-graphite/</link>
      <pubDate>Sat, 19 Apr 2014 07:59:23 +0000</pubDate>
      
      <guid>/blog/mysql/monitoring-dmlslow-queries-with-graphite/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://www.percona.com/doc/percona-toolkit/2.2/pt-query-digest.html&#34;&gt;pt-query-digest&lt;/a&gt;, &lt;a href=&#34;https://github.com/box/Anemometer/wiki&#34;&gt;Anemometer&lt;/a&gt; or &lt;a href=&#34;http://code.openark.org/blog/mysql/anemomaster-dml-visibility-your-must-do-for-tomorrow&#34;&gt;&#34;Anemomaster&#34;&lt;/a&gt; do a great job of analysing your queries and giving you visibility into what&#39;s going on with your MySQL servers. However, the place where the query digests are written is just some MySQL tables on some server. Do you have monitoring/alerts on that table? How will you verify a specific query does not exceed some runtime/execution count threshold, and get notified when it does?&lt;/p&gt;
&lt;p&gt;At Outbrain we use &lt;a href=&#34;http://graphite.wikidot.com/&#34;&gt;Graphite&lt;/a&gt; to collect almost all of our data. We like it for its simplicity and for the fact it has a &#34;push&#34; strategy as opposed to &#34;pull&#34; strategy: every service/server/collectd writes (&lt;em&gt;pushes&lt;/em&gt;) its own data to Graphite, as opposed to having some centralized monitoring service trying to pull data from thousands of servers &amp;amp; services. We also have a great Graphite dashboard (developed at our company by Erez Mazor) called &lt;a href=&#34;https://github.com/ezbz/graphitus&#34;&gt;graphitus&lt;/a&gt;, which is a very sophisticated and easily configurable visualization solution (see &lt;a href=&#34;http://ezbz.github.io/graphitus/&#34;&gt;documentation&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Our love/hate relationship with &lt;em&gt;Nagios&lt;/em&gt; boil down to having a single Nagios plugin: one that reads data from Graphite. We use Nagios to generate our alerts, and dream of the day we will substitute it with something else (there&#39;s not too much love in this love/hate relationship).&lt;/p&gt;
&lt;p&gt;Graphite is a &lt;em&gt;numeric timeseries data&lt;/em&gt; monitoring solution. How do you throw MySQL query analysis into Graphite, then?&lt;/p&gt;
&lt;p&gt;The answer lies within the flexible structure of a Graphite metric entry, which is a freely composed path, such as &lt;strong&gt;collectd.hosts.us-east.myhost01.mysql.gauge-Threads_running.value&lt;/strong&gt;. Graphite does not require you to pre-define paths, and you can use anything that makes sense to you. Thus, you can use a slow query&#39;s text, for example, as part of the Graphite entry &lt;em&gt;path&lt;/em&gt;. This is not entirely simple as the graphite path limits the allowed characters. So this is what we do:&lt;!--more--&gt;&lt;/p&gt;
&lt;p&gt;Any query that is written to Graphite is transformed into a &#34;canonical form&#34;. We strip it of excessive information and write enough of it that still makes sense to us. Actually, we found out that we usually do well with just the bare bones of &#34;what type of query this is and what tables are involved&#34;. For better drill down we then go to Anemometer/Anemomaster. Hence, the canonical form of the following query:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;UPDATE my_documents SET document_owner=&#39;Wallace&#39;  WHERE document_domain=&#39;Gromit&#39;&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;is simply&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;update_my_documents&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;Thankfully the &lt;em&gt;pt-query-digest&lt;/em&gt; report tables are already timestamp based, and are already aggregated by query &#34;fingerprints&#34;. This makes writing this data to graphite just a matter of text normalizing. The following script is a slightly modified version of what we use. Do note that we have the notion of &#34;clustername&#34; which is the name of the replication topology we&#39;re looking at. We have many topologies, like OLTP, OLAP, Metadata, etc. etc. We support this notion by adding a &lt;strong&gt;clustername_max&lt;/strong&gt; column to the report tables and instructing &lt;em&gt;pt-query-digest&lt;/em&gt; fill in this value.&lt;/p&gt;
&lt;p&gt;We run the following shell script by cron every 10 minutes (based on the 10 minute interval of analysing our masters&#39; DML):&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;
#!/bin/bash
#
# This script should run on the anemomaster machine every 10 minutes, shortly after
# binary logs / relay logs are analyzed via pt-query-digest.
#
unixtime=$(date +%s)
# Get stats for the last round 10 minutes
# The query only takes one representative from each cluster
query=&amp;quot; select clustername_max, sum(ts_cnt), replace(fingerprint, &#39;\n&#39;, &#39; &#39;) from global_query_review_history join global_query_review using (checksum), (select date(now()) + interval hour(now()) hour + interval (minute(now()) div 10 *10) minute as search_to_timestamp) as search_to_timestamp_sel where ts_min &amp;gt;= search_to_timestamp - interval 10 minute and ts_min &amp;lt; search_to_timestamp and hostname_max in ( select min(hostname_max) from global_query_review_history where ts_min &amp;gt;= search_to_timestamp - interval 10 minute and ts_min &amp;lt; search_to_timestamp group by clustername_max) group by clustername_max, fingerprint order by sum(ts_cnt) desc &amp;quot;
mysql -umyself -psecret anemomaster --silent --silent --raw -e &amp;quot;$query&amp;quot; | while IFS=$&#39;\t&#39; read -r -a result_values
    do
        fingerprint_cluster=${result_values[0]} ;
        fingerprint_count=${result_values[1]} ;
        fingerprint_query=${result_values[2]} ;
        fingerprint_query=$(echo $fingerprint_query | sed -r -e &amp;quot;s/^(-- .*)]//g&amp;quot;)
        fingerprint_query=$(echo $fingerprint_query | tr &#39;\n&#39; &#39; &#39; | tr &#39;\r&#39; &#39; &#39; | tr &#39;\t&#39; &#39; &#39;)
        fingerprint_query=${fingerprint_query%%(*}
        fingerprint_query=${fingerprint_query%%,*}
        fingerprint_query=${fingerprint_query%% set *}
        fingerprint_query=${fingerprint_query%% SET *}
        fingerprint_query=${fingerprint_query%% where *}
        fingerprint_query=${fingerprint_query%% WHERE *}
        fingerprint_query=${fingerprint_query%% join *}
        fingerprint_query=${fingerprint_query%% JOIN *}
        fingerprint_query=${fingerprint_query%% using *}
        fingerprint_query=${fingerprint_query%% USING *}
        fingerprint_query=${fingerprint_query%% select *}
        fingerprint_query=${fingerprint_query%% SELECT *}
        fingerprint_query=$(echo $fingerprint_query | tr -d &amp;quot;\`&amp;quot;)
        fingerprint_query=$(echo $fingerprint_query | tr -d &amp;quot;*&amp;quot;)
        fingerprint_query=$(echo $fingerprint_query | tr -d &amp;quot;?&amp;quot;)
        fingerprint_query=$(echo $fingerprint_query | tr &amp;quot; &amp;quot; &amp;quot;_&amp;quot;)
        fingerprint_query=$(echo $fingerprint_query | tr &amp;quot;.&amp;quot; &amp;quot;__&amp;quot;)
        echo &amp;quot;data.mysql.dml.${fingerprint_cluster}.${fingerprint_query}.count ${fingerprint_count} $unixtime&amp;quot; | nc -w 1 my.graphite.server 2003
    done
&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;If you don&#39;t need the &#34;clustername stuff&#34;, modify the query to read:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;
select &#39;mysql&#39; as clustername_max, sum(ts_cnt), replace(fingerprint, &#39;\n&#39;, &#39; &#39;) from global_query_review_history join global_query_review using (checksum), (select date(now()) + interval hour(now()) hour + interval (minute(now()) div 10 *10) minute as search_to_timestamp) as search_to_timestamp_sel where ts_min &amp;gt;= search_to_timestamp - interval 10 minute and ts_min &amp;lt; search_to_timestamp and hostname_max in ( select min(hostname_max) from global_query_review_history where ts_min &amp;gt;= search_to_timestamp - interval 10 minute and ts_min &amp;lt; search_to_timestamp) group by fingerprint order by sum(ts_cnt) desc
&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;The graphite metric path will look like &lt;strong&gt;data.mysql.dml.oltp.update_my_documents.count&lt;/strong&gt;, which makes for a perpefctly valid metric to monitor, graphically visualize and get alerts on.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>&#34;Anemomaster&#34;: DML visibility. Your must-do for tomorrow</title>
      <link>/blog/mysql/anemomaster-dml-visibility-your-must-do-for-tomorrow/</link>
      <pubDate>Fri, 18 Apr 2014 13:15:09 +0000</pubDate>
      
      <guid>/blog/mysql/anemomaster-dml-visibility-your-must-do-for-tomorrow/</guid>
      <description>&lt;p&gt;Here&#39;s our take of master DML query monitoring at &lt;a href=&#34;http://www.outbrain.com/&#34;&gt;Outbrain&lt;/a&gt; (&lt;a href=&#34;https://www.percona.com/live/mysql-conference-2014/sessions/mysql-devops-outbrain&#34;&gt;presented April 2014&lt;/a&gt;). It took a half-day to code, implement, automate and deploy, and within the first hour of work we managed to catch multiple ill-doing services and scripts. You might want to try this out for yourself.&lt;/p&gt;
&lt;h4&gt;What&#39;s this about?&lt;/h4&gt;
&lt;p&gt;What queries do you monitor on your MySQL servers? Many don&#39;t monitor queries at all, and only look up slow queries on occasion, using &lt;a href=&#34;http://www.percona.com/doc/percona-toolkit/2.2/pt-query-digest.html&#34;&gt;pt-query-digest&lt;/a&gt;. Some monitor slow queries, where &lt;a href=&#34;https://github.com/box/Anemometer/wiki&#34;&gt;Anemometer&lt;/a&gt; (relying on pt-query-digest) is a very good tool. To the extreme, some monitor TCP traffic on all MySQL servers -- good for you! In between, there&#39;s a particular type of queries that are of special interest: DML (&lt;strong&gt;INSERT/UPDATE/DELETE&lt;/strong&gt;) queries issued against the master.&lt;/p&gt;
&lt;p&gt;They are of particular interest because they are only issued once against the master, yet propagate through replication topology to execute on all slaves. These queries have a direct impact on your slave lag and on your overall replication capacity. I suggest you should be familiar with your DMLs just as you are with your slow queries.&lt;/p&gt;
&lt;p&gt;In particular, we had multiple occasions in the past where all or most slaves started lagging. Frantically we would go to our metrics; yes! We would see a spike in &lt;strong&gt;com_insert&lt;/strong&gt;. Someone (some service) was obviously generating more &lt;strong&gt;INSERT&lt;/strong&gt;s than usual, at a high rate that the slaves could not keep up with. But, &lt;em&gt;which&lt;/em&gt; &lt;strong&gt;INSERT&lt;/strong&gt; was that? Blindly, we would look at the binary logs. Well, erm, what are we looking for, exactly?&lt;/p&gt;
&lt;p&gt;Two such occasions convinced us that there should be a solution, but it took some time till it hit us. We were already using &lt;em&gt;Anemometer&lt;/em&gt; for monitoring our slow logs. We can do the same for monitoring our binary logs. Thus was born &lt;em&gt;&#34;Anemomaster&#34;&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Quick recap on how Anemometer works: you issue &lt;em&gt;pt-query-digest&lt;/em&gt; on your slow logs on all MySQL hosts (we actually first ship the slow logs to a central place where we analyse them; same thing). This is done periodically, and slow logs are then rotated. You throw the output of &lt;em&gt;pt-query-digest&lt;/em&gt; to a central database (this is built in with &lt;em&gt;pt-query-digest&lt;/em&gt;; it doesn&#39;t necessarily produce human readable reports). &lt;em&gt;Anemometer&lt;/em&gt; would read this central database and visualize the slow queries.&lt;/p&gt;
&lt;h4&gt;Analysing DMLs&lt;/h4&gt;
&lt;p&gt;But then, &lt;em&gt;pt-query-digest&lt;/em&gt; doesn&#39;t only parse slow logs. It can parse binary logs. Instead of asking for total query time, we ask for query count, and on we go to establish the same mechanism, using same &lt;em&gt;pt-query-digest&lt;/em&gt; and same &lt;em&gt;Anemometer&lt;/em&gt; to store and visualize the DMLs issued on our masters.&lt;/p&gt;
&lt;p&gt;When analysing DMLs we&#39;re interested in parsing binary logs -- and it makes no sense to do the same on all slaves. All slaves just have same copy of binlog entries as the master produces. It only takes &lt;em&gt;one&lt;/em&gt; server to get an accurate picture of the DMLs on your replication topology.&lt;/p&gt;
&lt;p&gt;&lt;!--more--&gt;&lt;/p&gt;
&lt;p&gt;One server could be the master, and this can indeed be done: just &lt;strong&gt;FLUSH MASTER LOGS&lt;/strong&gt;, parse the binary logs with &lt;em&gt;pt-query-digest&lt;/em&gt;, and you&#39;re done. But like others, we tend to look at our masters as tender babies. We care for them, and do not wish to overload them unnecessarily. We chose to get the binlog entries from our slaves, instead. We also chose to get the entries from the relay logs, since these are unaffected by slave performance and as long as network is good, we can expect the relay logs to be &lt;em&gt;very&lt;/em&gt; up to date. At any given time we have two slaves that take this role (this is automated and verified). On a &lt;strong&gt;10&lt;/strong&gt; minute period we would flush the relay logs on these servers, and analyse whatever relay logs we have not analysed as yet.&lt;/p&gt;
&lt;p&gt;The script below is a slightly modified version of our own, and should work for the standard installation. Modify to fit your own data (in particular, it assumes relay logs are named &lt;strong&gt;mysqld-relay-bin&lt;/strong&gt;; &lt;strong&gt;datadir&lt;/strong&gt; is specified in &lt;strong&gt;/etc/my.cnf&lt;/strong&gt;, and please don&#39;t ask me how to do this on Windows):&lt;/p&gt;
&lt;blockquote&gt;&lt;pre&gt;
#!/bin/bash
#
# Digest latest relay logs file, write results to &amp;quot;anemomaster&amp;quot;
#
# This script can run from any machine; it only needs to execute on a single machine per mysql cluster, but for
# analysis availability it should execute on at least two hosts per cluster.
#
DATADIR=`grep datadir /etc/my.cnf|awk -F= &#39;{print $2}&#39;`
TMPDIR=/tmp
DIGESTED_RELAY_LOGS_FILE=${DATADIR}/digested_relay_logs.txt
touch $DIGESTED_RELAY_LOGS_FILE
chown mysql:mysql $DIGESTED_RELAY_LOGS_FILE
hostname=$(hostname)
echo &amp;quot;deleting old relay logs from ${TMPDIR}&amp;quot;
rm ${TMPDIR}/mysqld-relay-bin.[0-9]*
echo &amp;quot;Getting current relay log files&amp;quot;
existing_relay_log_files=$(ls -tr ${DATADIR}/mysqld-relay-bin.[0-9]* | head -n -1)
for existing_relay_log_file in $existing_relay_log_files
do
    cp -u $existing_relay_log_file $TMPDIR
done
echo &amp;quot;flushing relay logs&amp;quot;
/usr/bin/mysql -umyself -psecret -e &#39;flush relay logs\G;&#39; 2&amp;gt;/dev/null
# sleep because the log file takes some time to disappear
sleep 1
echo &amp;quot;Getting current relay log files&amp;quot;
existing_relay_log_files=$(ls -tr ${DATADIR}/mysqld-relay-bin.[0-9]* | head -n -1)
for existing_relay_log_file in $existing_relay_log_files
do
    cp -u $existing_relay_log_file $TMPDIR
done
cd $TMPDIR
for relay_log_file in mysqld-relay-bin.[0-9]*
do
    # Filter this relay log file, since it&#39;s already been digested
    grep $relay_log_file $DIGESTED_RELAY_LOGS_FILE &amp;amp;&amp;amp; rm $relay_log_file
done
for relay_log_file in mysqld-relay-bin.[0-9]*
do
    echo &amp;quot;digesting $relay_log_file&amp;quot;
    mysqlbinlog $relay_log_file | /usr/bin/pt-query-digest \
      --type binlog --order-by Query_time:cnt --group-by fingerprint --limit 100 \
      --review  P=3306,u=anemomaster,p=secret,h=anemomaster_host,D=anemomaster,t=global_query_review \
      --history P=3306,u=anemomaster,p=secret,h=anemomaster_host,D=anemomaster,t=global_query_review_history \
      --filter=&amp;quot; \$event-&amp;gt;{Bytes} = length(\$event-&amp;gt;{arg}) and \$event-&amp;gt;{hostname}=\&amp;quot;$(hostname)\&amp;quot; &amp;quot; \
      --no-report
    echo &amp;quot;$relay_log_file&amp;quot; &amp;gt;&amp;gt; $DIGESTED_RELAY_LOGS_FILE
    rm $relay_log_file
done
# make sure the file does not bloat. 20 entries is more than enough.
tail -n 20 $DIGESTED_RELAY_LOGS_FILE &amp;gt; ${TMPDIR}/DIGESTED_RELAY_LOGS_FILE
cat ${TMPDIR}/DIGESTED_RELAY_LOGS_FILE &amp;gt; $DIGESTED_RELAY_LOGS_FILE
echo &amp;quot;done&amp;quot;
&lt;/pre&gt;&lt;/blockquote&gt;
&lt;p&gt;As for Anemometer, we patched it to support multiple environments (&#34;clusters&#34;) -- but irrelevant to the DML change. If you just want to make it visualize DMLs, here&#39;s the major configuration changes to &lt;strong&gt;config.inc.php&lt;/strong&gt; (marked with &lt;strong&gt;bold&lt;/strong&gt;):&lt;/p&gt;
&lt;blockquote&gt;&lt;pre&gt;
$conf[&#39;history_defaults&#39;] = array(
	&#39;output&#39;		=&amp;gt; &#39;table&#39;,
	&#39;fact-group&#39;	=&amp;gt; &#39;date&#39;,
	&#39;fact-order&#39;	=&amp;gt; &#39;date DESC&#39;,
	&#39;fact-limit&#39; =&amp;gt; &#39;90&#39;,
	&#39;dimension-ts_min_start&#39; =&amp;gt; date(&amp;quot;Y-m-d H:i:s&amp;quot;, strtotime( &#39;-90 day&#39;)),
	&#39;dimension-ts_min_end&#39;	=&amp;gt; date(&amp;quot;Y-m-d H:i:s&amp;quot;),
	&#39;table_fields&#39; =&amp;gt; array(&#39;date&#39;, &#39;query_time_avg&#39;,&#39;ts_cnt&#39;,&#39;Query_time_sum&#39;)
);
$conf[&#39;report_defaults&#39;] = array(
	&#39;fact-group&#39;	=&amp;gt; &#39;checksum&#39;,
	&#39;fact-order&#39;	=&amp;gt; &#39;ts_cnt DESC&#39;,
	&#39;fact-limit&#39; =&amp;gt; &#39;20&#39;,
	&#39;dimension-ts_min_start&#39; =&amp;gt; date(&amp;quot;Y-m-d H:i:s&amp;quot;, strtotime( &#39;-1 day&#39;)),
	&#39;dimension-ts_min_end&#39;	=&amp;gt; date(&amp;quot;Y-m-d H:i:s&amp;quot;),
	&#39;table_fields&#39; =&amp;gt; array(&#39;checksum&#39;,&#39;snippet&#39;, &#39;query_time_avg&#39;,&#39;ts_cnt&#39;,&#39;Query_time_sum&#39;),
	&#39;dimension-pivot-hostname_max&#39; =&amp;gt; null,
	&#39;dimension-pivot-clustername_max&#39; =&amp;gt; null
);
$conf[&#39;graph_defaults&#39;] = array(
	&#39;fact-group&#39;	=&amp;gt; &#39;minute_ts&#39;,
	&#39;fact-order&#39;	=&amp;gt; &#39;minute_ts&#39;,
	&#39;fact-limit&#39; =&amp;gt; &#39;&#39;,
	&#39;dimension-ts_min_start&#39; =&amp;gt; date(&amp;quot;Y-m-d H:i:s&amp;quot;, strtotime( &#39;-7 day&#39;)),
	&#39;dimension-ts_min_end&#39;	=&amp;gt; date(&amp;quot;Y-m-d H:i:s&amp;quot;),
	&#39;table_fields&#39; =&amp;gt; array(&#39;minute_ts&#39;),
	// hack ... fix is to make query builder select the group and order fields,
	// then table fields only has to contain the plot_field
	&#39;plot_field&#39; =&amp;gt; &#39;ts_cnt&#39;,
);
&lt;/pre&gt;&lt;/blockquote&gt;
&lt;p&gt;With a &lt;strong&gt;10&lt;/strong&gt; minute rotation &amp;amp; digestion, we are able to analyze near real-time what&#39;s been done on our masters. If we see a spike in &lt;strong&gt;com_insert/com_update/com_delete&lt;/strong&gt;, or just see slave lags, we turn to &lt;em&gt;Anemomaster &lt;/em&gt;and within a couple minutes know exactly what service is guilty of abusing our database. We are also working to protect our database against abuse, but that&#39;s for another discussion.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>mycheckpoint, discontinued</title>
      <link>/blog/mysql/mycheckpoint-discontinued/</link>
      <pubDate>Thu, 06 Mar 2014 12:27:28 +0000</pubDate>
      
      <guid>/blog/mysql/mycheckpoint-discontinued/</guid>
      <description>&lt;p&gt;Time to admit to myself: &lt;a href=&#34;http://code.openark.org/forge/mycheckpoint&#34;&gt;mycheckpoint&lt;/a&gt; has to be discontinued.&lt;/p&gt;
&lt;p&gt;I started &lt;em&gt;mycheckpoint&lt;/em&gt; back in &lt;strong&gt;2009&lt;/strong&gt;, as a free &amp;amp; open source lightweight monitoring tool for MySQL. Over some years it evolved and became an actual (lightweight) monitoring solution, used by many. It has a unique and original design, which, alas, is also its bane.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;mycheckpoint&lt;/em&gt; uses the relational model &amp;amp; SQL to store and query monitored metrics. This leads to quite a sophisticated service, which can make practically anything visible to the user. The raw data is just numbers. but with some SQL-Fu one can &lt;a href=&#34;http://code.openark.org/blog/mysql/generating-google-line-charts-with-sql-part-ii&#34;&gt;generate charts out of it&lt;/a&gt;,  (&lt;a href=&#34;http://mycheckpoint.googlecode.com/svn/trunk/doc/html/sample/http/mcp_sql00/sv_report_html_brief&#34;&gt;interactive&lt;/a&gt; ones as well), human readable &lt;a href=&#34;http://mycheckpoint.googlecode.com/svn/trunk/doc/html/sample/http/mcp_sql00/sv_report_html_brief&#34;&gt;reports&lt;/a&gt; and full blown email messages. It is still the only common solution I&#39;m aware of that &lt;a href=&#34;http://code.openark.org/forge/mycheckpoint/documentation/detecting-parameters-change&#34;&gt;keeps track of variable changes&lt;/a&gt; and provides with clear &#34;what changed, when, from value &amp;amp; to_value&#34;. I caught many deployment bugs by just observing this. It&#39;s a single file that provides with full blown HTTP service, alerting, mail notifications, multi-database monitoring, custom monitoring queries, query execution time monitoring, OS metrics, ...&lt;/p&gt;
&lt;p&gt;While developing &lt;em&gt;mycheckpoint&lt;/em&gt; I learned a lot on MySQL status &amp;amp; configuration, complex SQL queries, Python, linux, packaging and more. I got a lot of feedback from users, as I still do (thank you!). Didn&#39;t always manage to fix all bugs or answer all questions.&lt;/p&gt;
&lt;p&gt;The design of &lt;em&gt;mycheckpoint&lt;/em&gt; does not meet today&#39;s reality. Heck, today there are more counters &amp;amp; variables than possible table columns. The schema-per-monitored-instance design makes for simplicity, but does not fare well with dozens or hundreds of servers to monitor. There is no cross-instance aggregation or visualization of data. The per-&lt;strong&gt;10&lt;/strong&gt; minute aggregation is too rough. &lt;em&gt;There isn&#39;t a test suite&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Some of the above issues can be fixed, and if you like, the source code is still &lt;a href=&#34;http://code.google.com/p/mycheckpoint/source/checkout&#34;&gt;freely available&lt;/a&gt;. I&#39;ll even migrate the entire SVN to GitHub at some stage. But I believe the current state might only be good for small scale deployments;  not something you would consider to scale up with.&lt;/p&gt;
&lt;p&gt;For me, there&#39;s nothing more motivating in code development than knowing the code will go public. The efforts in making the code look as best it can, as easily deployable as possibly can, with good documentation, makes for a lot of effort - but very satisfying. Open Source FTW!!!1&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Seconds_behind_master vs. Absolute slave lag</title>
      <link>/blog/mysql/seconds_behind_master-vs-absolute-slave-lag/</link>
      <pubDate>Fri, 24 Jan 2014 15:41:18 +0000</pubDate>
      
      <guid>/blog/mysql/seconds_behind_master-vs-absolute-slave-lag/</guid>
      <description>&lt;p&gt;I am unable to bring myself to trust the &lt;strong&gt;Seconds_behind_master&lt;/strong&gt; value on &lt;strong&gt;SHOW SLAVE STATUS&lt;/strong&gt;. Even with MySQL &lt;strong&gt;5.5&lt;/strong&gt;&#39;s &lt;strong&gt;CHANGE MASTER TO ... MASTER_HEARTBEAT_PERIOD&lt;/strong&gt; (good thing, applied when no traffic goes from master to slave) it&#39;s easy and common to find fluctuations in &lt;strong&gt;Seconds_behind_master&lt;/strong&gt; value.&lt;/p&gt;
&lt;p&gt;And, when sampled by your favourite monitoring tool, this often leads to &lt;em&gt;many&lt;/em&gt; false negatives.&lt;/p&gt;
&lt;p&gt;At Outbrain we use HAProxy as proxy to our slaves, on multiple clusters. More about that in a future post. What&#39;s important here is that our decision whether a slave enters or leaves a certain pool (i.e. gets UP or DOWN status in HAProxy) is based on replication lag. Taking slaves out when they are actually replicating well is bad, since this reduces the amount of serving instances. Putting slaves in the pool when they are actually lagging too much is bad as they contain invalid, irrelevant data.&lt;/p&gt;
&lt;p&gt;To top it all, even when correct, the &lt;strong&gt;Seconds_behind_master&lt;/strong&gt; value is practically irrelevant on &lt;strong&gt;2nd&lt;/strong&gt; level slaves. In a &lt;em&gt;Master -&amp;gt; Slave1 -&amp;gt; Slave2&lt;/em&gt; setup, what does it mean that &lt;em&gt;Slave2&lt;/em&gt; has &lt;strong&gt;Seconds_behind_master = 0&lt;/strong&gt;? Nothing much to the application: &lt;em&gt;Slave1&lt;/em&gt; might be lagging an hour behind the master, or may not be replicating at all. &lt;em&gt;Slave2&lt;/em&gt; might have an hour&#39;s data missing even though it says its own replication is fine.&lt;/p&gt;
&lt;p&gt;None of the above is news, and yet many fall in this pitfall. The solution is quite old as well; it is also very simple: do your own heartbeat mechanism, at your favourite time resolution, and measure slave lag by timestamp you yourself updated on the master.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Maatkit&lt;/strong&gt;/&lt;strong&gt;percona-toolkit&lt;/strong&gt; did this long time ago with &lt;strong&gt;mk-heartbeat&lt;/strong&gt;/&lt;strong&gt;&lt;a href=&#34;http://www.percona.com/doc/percona-toolkit/2.2/pt-heartbeat.html&#34;&gt;pt-heartbeat&lt;/a&gt;&lt;/strong&gt;. We&#39;re doing it in a very similar manner. The benefit is obvious. Consider the following two graphs; the first shows &lt;strong&gt;Seconds_behind_master&lt;/strong&gt;, the seconds shows our own &lt;strong&gt;Absolute_slave_lag&lt;/strong&gt; measurement.&lt;!--more--&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;[caption id=&#34;attachment_6590&#34; align=&#34;alignnone&#34; width=&#34;700&#34;]&lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2013/09/seconds_behind_master.png&#34;&gt;&lt;img class=&#34;size-full wp-image-6590 &#34; alt=&#34;seconds_behind_master&#34; src=&#34;/blog/blog/assets/seconds_behind_master.png&#34; width=&#34;700&#34; height=&#34;350&#34; /&gt;&lt;/a&gt; seconds_behind_master[/caption]&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;[caption id=&#34;attachment_6591&#34; align=&#34;alignnone&#34; width=&#34;700&#34;]&lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2013/09/absolute_slave_lag.png&#34;&gt;&lt;img class=&#34;size-full wp-image-6591&#34; alt=&#34;absolute_slave_lag&#34; src=&#34;/blog/blog/assets/absolute_slave_lag.png&#34; width=&#34;700&#34; height=&#34;350&#34; /&gt;&lt;/a&gt; absolute_slave_lag[/caption]&lt;/blockquote&gt;
&lt;p&gt;The two graphs were taken simultaneously on a set of servers. Excuse me for not having same colours for same slaves, I blame it on graphite. Some small gaps are seen here that are irrelevant to our discussion (yes, we had some graphite delivery issues).&lt;/p&gt;
&lt;p&gt;As you can see the &lt;strong&gt;Absolute_slave_lag&lt;/strong&gt; does not (and cannot!) fluctuate. With our 10 second heartbeat resolution it always shows an accurate value. In fact, within the hearbeat resolution, it show the de facto replication lag. Let&#39;s stress this one:&lt;/p&gt;
&lt;blockquote&gt;When you implement your own heartbeat mechanism, your own measured slave lag makes for the de facto slave replication lag within your heartbeat interval.&lt;/blockquote&gt;
&lt;p&gt;As another example, consider what happens when a slave stop replicating (i.e. some issued &lt;strong&gt;STOP SLAVE&lt;/strong&gt;, or replication fails). The &lt;strong&gt;Seconds_behind_master&lt;/strong&gt; value is &lt;strong&gt;NULL&lt;/strong&gt;, which is a good indication to error, and easy to monitor. But how does it present visually? Not too well. It is usually just not rendered:&lt;/p&gt;
&lt;blockquote&gt;&lt;img alt=&#34;&#34; src=&#34;/blog/blog/assets/wE9+fg3khXiVQAAAABJRU5ErkJggg==&#34; /&gt;&lt;/blockquote&gt;
&lt;p&gt;But, consider: a slave that &lt;strong&gt;STOP&lt;/strong&gt;s for &lt;strong&gt;1&lt;/strong&gt; minute for whatever reason is still only &lt;strong&gt;1&lt;/strong&gt; minute behind master. That is, it is by &lt;strong&gt;60&lt;/strong&gt; seconds up to date with master&#39;s data. If we decide a slave should be serving for up to &lt;strong&gt;5&lt;/strong&gt; minutes of lag, then our slave can still be used for serving for &lt;strong&gt;4&lt;/strong&gt; more minutes! &lt;strong&gt;Seconds_behind_master&lt;/strong&gt; does not provide us with helpful information. &lt;strong&gt;Absolute_slave_lag&lt;/strong&gt; does. Consider the above system status when measured by &lt;strong&gt;Absolute_slave_lag&lt;/strong&gt;:&lt;/p&gt;
&lt;blockquote&gt;&lt;img alt=&#34;&#34; src=&#34;/blog/blog/assets/gfzmV4p3EeQAAAABJRU5ErkJggg==&#34; /&gt;&lt;/blockquote&gt;
&lt;p&gt;We now get good insight on how far our slave is behind. Of course we monitor &lt;strong&gt;Seconds_behind_master&lt;/strong&gt; to find out replication is not working; by our HAProxy only cares about &lt;strong&gt;Absolute_slave_lag&lt;/strong&gt;.&lt;/p&gt;
&lt;h4&gt;How does it work?&lt;/h4&gt;
&lt;p&gt;Very similar to &lt;strong&gt;pt-heartbeat&lt;/strong&gt;, there&#39;s a dedicated table which we update with current timestamp. We read that timestamp on slave and compare with actual time on host.&lt;/p&gt;
&lt;p&gt;We have these DDL:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;create table my_heartbeat (
  id int unsigned not null primary key,
  master_ts timestamp,
  update_by varchar(128) default NULL
);

create or replace view my_heartbeat_status_v as
  select 
    master_ts,
    now() as time_now,
    timestampdiff(SECOND, master_ts, now()) as slave_lag_seconds,
    update_by
  from my_heartbeat
;

insert into my_heartbeat (id, master_ts, update_by) values (1, NOW(), &#39;init&#39;) on duplicate key update master_ts=NOW(), update_by=VALUES(update_by);

create event 
  update_heartbeat_event
  on schedule every 10 second starts current_timestamp 
  on completion preserve
  enable
  do  
    insert into my_heartbeat (id, master_ts, update_by) values (1, NOW(), &#39;event_scheduler&#39;) on duplicate key update master_ts=NOW(), update_by=VALUES(update_by);
;&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;We use both event scheduler as well as external script to pump the heartbeat value.&lt;/p&gt;
&lt;p&gt;On slave, utilize the view to:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;select slave_lag_seconds from my_heartbeat_status_v&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;The above query answers: &#34;how long ago was did I (the slave) get a timestamp update from the master?&#34;. The result is correct within 10 seconds resolution, in our example.&lt;/p&gt;
&lt;p&gt;Not new, but not well known. I hope the above provides you with better visibility into your replication lag.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Bash script: report largest InnoDB files</title>
      <link>/blog/mysql/bash-script-report-largest-innodb-files/</link>
      <pubDate>Thu, 19 Dec 2013 10:58:17 +0000</pubDate>
      
      <guid>/blog/mysql/bash-script-report-largest-innodb-files/</guid>
      <description>&lt;p&gt;The following script will report the largest InnoDB tables under the data directory: schema, table &amp;amp; length in bytes. The tables could be non-partitioned, in which case this is simply the size of the corresponding &lt;strong&gt;.ibd&lt;/strong&gt; file, or they can be partitioned, in which case the reported size is the sum of all partition files. It is assumed tables reside in their own tablespace files, i.e. created with &lt;strong&gt;innodb_file_per_table=1&lt;/strong&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;(
    mysql_datadir=$(grep datadir /etc/my.cnf | cut -d &#34;=&#34; -f 2)
    cd $mysql_datadir
    for frm_file in $(find . -name &#34;*.frm&#34;)
    do
        tbl_file=${frm_file//.frm/.ibd}
        table_schema=$(echo $frm_file | cut -d &#34;/&#34; -f 2)
        table_name=$(echo $frm_file | cut -d &#34;/&#34; -f 3 | cut -d &#34;.&#34; -f 1)
        if [ -f $tbl_file ]
        then
            # unpartitioned table
            file_size=$(du -cb $tbl_file 2&amp;gt; /dev/null | tail -n 1) 
        else
            # attempt partitioned innodb table
            tbl_file_partitioned=${frm_file//.frm/#*.ibd}
            file_size=$(du -cb $tbl_file_partitioned 2&amp;gt; /dev/null | tail -n 1)
        fi
        file_size=${file_size//total/}
        # Replace the below with whatever action you want to take,
        # for example, push the values into graphite.
        echo $file_size $table_schema $table_name
    done
) | sort -k 1 -nr | head -n 20&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;We use this to push table statistics to our graphite service; we keep an eye on table growth (we actually do not limit to top &lt;strong&gt;20&lt;/strong&gt; but just monitor them all). File size does not report the real table data size (this can be smaller due to tablespace fragmentation). It does give the correct information if you&#39;re concerned about disk space. For table data we also monitor &lt;strong&gt;SHOW TABLE STATUS&lt;/strong&gt; / &lt;strong&gt;INFORMATION_SCHEMA.TABLES&lt;/strong&gt;, themselves being inaccurate. Gotta go by something.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Trick: recovering from &#34;no space left on device&#34; issues with MySQL</title>
      <link>/blog/mysql/trick-recovering-from-no-space-left-on-device-issues-with-mysql/</link>
      <pubDate>Fri, 23 Aug 2013 13:25:15 +0000</pubDate>
      
      <guid>/blog/mysql/trick-recovering-from-no-space-left-on-device-issues-with-mysql/</guid>
      <description>&lt;p&gt;Just read Ronald Bradford&#39;s post on an &lt;a href=&#34;http://ronaldbradford.com/blog/unnecessary-3am-emergency-call-2013-08-23/&#34;&gt;unnecessary 3am (emergency) call&lt;/a&gt;. I sympathize! Running out of disk space makes for some weird MySQL behaviour, and in fact whenever I encounter weird behaviour I verify disk space.&lt;/p&gt;
&lt;p&gt;But here&#39;s a trick I&#39;ve been using for years to avoid such cases and to be able to recover quickly. It helped me on such events as running out of disk space during ALTER TABLEs or avoiding purging of binary logs when slave is known to be under maintenance.&lt;/p&gt;
&lt;p&gt;Ronald suggested it -- just put a dummy file in your &lt;strong&gt;@@datadir&lt;/strong&gt;! I like putting a &lt;strong&gt;1GB&lt;/strong&gt; dummy file: I typically copy+paste a &lt;strong&gt;1GB&lt;/strong&gt; binary log file and call it &lt;strong&gt;&#34;placeholder.tmp&#34;&lt;/strong&gt;. Then I forget all about it. My disk space should not run out -- if it does it&#39;s a cause for emergency. I have monitoring, but sometimes I&#39;m hoping to make an operation on &lt;strong&gt;97%&lt;/strong&gt;-&lt;strong&gt;99%&lt;/strong&gt; utilization.&lt;/p&gt;
&lt;p&gt;If I do run out of disk space: well, MySQL won&#39;t let me connect; won&#39;t complete an important statement; not sync transaction to disk -- bad situation. Not a problem in our case: we can magically recover &lt;strong&gt;1GB&lt;/strong&gt; worth of data from the &lt;strong&gt;@@datadir&lt;/strong&gt;, buying us enough time (maybe just minutes) to gracefully complete so necessary operations; connect, KILL, shutdown, abort etc.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>mycheckpoint revision 231 released</title>
      <link>/blog/mysql/mycheckpoint-revision-released/</link>
      <pubDate>Thu, 23 May 2013 14:21:52 +0000</pubDate>
      
      <guid>/blog/mysql/mycheckpoint-revision-released/</guid>
      <description>&lt;p&gt;A &lt;a href=&#34;http://code.google.com/p/mycheckpoint/&#34;&gt;new release&lt;/a&gt; for &lt;strong&gt;&lt;a href=&#34;http://code.openark.org/forge/mycheckpoint&#34;&gt;mycheckpoint&lt;/a&gt;&lt;/strong&gt;: lightweight, SQL oriented MySQL monitoring solution.&lt;/p&gt;
&lt;p&gt;If you&#39;re unfamiliar with &lt;em&gt;mycheckpoint&lt;/em&gt;, well, the one minute sales pitch is: it&#39;s a free and open source monitoring tool for MySQL, which is extremely easy to install and execute, and which includes &lt;a href=&#34;http://code.openark.org/forge/mycheckpoint/documentation/custom-monitoring&#34;&gt;custom queries&lt;/a&gt;, &lt;a href=&#34;http://code.openark.org/forge/mycheckpoint/documentation/alerts&#34;&gt;alerts&lt;/a&gt; (via &lt;a href=&#34;http://code.openark.org/forge/mycheckpoint/documentation/emails&#34;&gt;emails&lt;/a&gt;), and out of the box &lt;a href=&#34;http://code.openark.org/forge/mycheckpoint/documentation/http-web-server&#34;&gt;HTTP server&lt;/a&gt; and &lt;a href=&#34;http://mycheckpoint.googlecode.com/svn/trunk/doc/html/sample/http/mcp_sql00/sv_report_html_brief&#34;&gt;charting&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This is mostly a maintenance release, with some long-time requested features, and of course solved bugs. Here are a few highlights:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Supports MariaDB and MySQL 5.6 (issues with new variables, space padded variables, text-valued variables)&lt;/li&gt;
&lt;li&gt;Supports alerts via function invocation on monitored host (so not only checking alerts via aggregated data like &lt;strong&gt;&#39;Seconds_behind_master&#39;&lt;/strong&gt; but also by &lt;strong&gt;SELECT my_sanity_check_function()&lt;/strong&gt; on monitored instance). See &lt;a href=&#34;http://code.openark.org/forge/mycheckpoint/documentation/alerts&#34;&gt;alerts.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Supports single-running-instance via &lt;strong&gt;&#34;--single&#34;&lt;/strong&gt; command line argument&lt;/li&gt;
&lt;li&gt;Supports strict &lt;strong&gt;sql_mode&lt;/strong&gt;, including &lt;strong&gt;ONLY_FULL_GROUP_BY&lt;/strong&gt;, overcoming &lt;a href=&#34;http://bugs.mysql.com/bug.php?id=69310&#34;&gt;bug #69310&lt;/a&gt;.&lt;strong&gt;&lt;br /&gt;
&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Supports sending of pending email HTML report&lt;/li&gt;
&lt;li&gt;Better re-deployment process&lt;/li&gt;
&lt;li&gt;Better recognizing of SIGNED/UNSIGNED values&lt;/li&gt;
&lt;li&gt;Some other improvements in charting, etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;mycheckpoint&lt;/em&gt; is released under the BSD license.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://code.google.com/p/mycheckpoint/&#34;&gt;Downloads are available&lt;/a&gt; from the project&#39;s page.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>MySQL monitoring: storing, not caching</title>
      <link>/blog/mysql/mysql-monitoring-storing-not-caching/</link>
      <pubDate>Wed, 22 Feb 2012 09:44:47 +0000</pubDate>
      
      <guid>/blog/mysql/mysql-monitoring-storing-not-caching/</guid>
      <description>&lt;p&gt;I&#39;ve followed with interest on Baron&#39;s &lt;a href=&#34;http://www.mysqlperformanceblog.com/2012/02/19/why-dont-our-new-nagios-plugins-use-caching/&#34;&gt;Why don’t our new Nagios plugins use caching?&lt;/a&gt; and Sheeri&#39;s &lt;a href=&#34;http://www.sheeri.com/content/caching-monitoring-timing-everything&#34;&gt;Caching for Monitoring: Timing is Everything&lt;/a&gt;. I wish to present my take on this, from &lt;a href=&#34;http://code.openark.org/forge/mycheckpoint&#34;&gt;mycheckpoint&lt;/a&gt;&#39;s point of view.&lt;/p&gt;
&lt;p&gt;So &lt;em&gt;mycheckpoint&lt;/em&gt; works in a completely different way. On one hand, it doesn&#39;t bother with caching. On the other hand, it doesn&#39;t bother with re-reads of data.&lt;/p&gt;
&lt;p&gt;There are no staleness issues, the data is consistent as it can get (you can &lt;em&gt;never&lt;/em&gt; get a completely atomic read of everything in MySQL), and you can issue as many calculations as you want at the price of one take of monitoring. As in Sheere&#39;s example, you can run &lt;strong&gt;Threads_connected/max_connections*100&lt;/strong&gt;, mix status variables, system variables, meta-variables (e.g. Seconds_behind_master), user-created variables (e.g. number of purchases in your online shop) etc.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;mycheckpoint&lt;/em&gt;&#39;s concept is to &lt;strong&gt;store&lt;/strong&gt; data. And store it in relational format. That is, &lt;strong&gt;INSERT&lt;/strong&gt; it to a table.&lt;/p&gt;
&lt;p&gt;A sample-run generates a row, which lists all status, server, OS, user, meta variables. It&#39;s a huge row, with hundreds of columns. Columns like &lt;strong&gt;threads_connected&lt;/strong&gt;, &lt;strong&gt;max_connections&lt;/strong&gt;, &lt;strong&gt;innodb_buffer_pool_size&lt;/strong&gt;, &lt;strong&gt;seconds_behind_master&lt;/strong&gt;, etc.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;mycheckpoint&lt;/em&gt; hardly cares about these columns. It identifies them dynamically. Have you just upgraded to MySQL &lt;strong&gt;5.5&lt;/strong&gt;? Oh, there&#39;s a new bunch of server and status variables? No problem, &lt;em&gt;mycheckpoint&lt;/em&gt; will notice it doesn&#39;t have the matching columns and will add them via ALTER TABLE. There you go, now we have a place to store them.&lt;/p&gt;
&lt;p&gt;Running a formula like &lt;strong&gt;Threads_connected/max_connections*100&lt;/strong&gt; is as easy as issuing the following query:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;SELECT Threads_connected/max_connections*100 FROM status_variables WHERE id = ...&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;Hmmm. This means I can run this formula on the most recent row I&#39;ve just added. But wait, this also means I can run this formula on &lt;em&gt;any&lt;/em&gt; row I&#39;ve ever gathered.&lt;!--more--&gt;&lt;/p&gt;
&lt;p&gt;With &lt;em&gt;mycheckpoint&lt;/em&gt; you can generate graphs &lt;strong&gt;retroactively&lt;/strong&gt; using new formulas. The data is there, vanilla style. Any formula which can be calculated via SQL is good to go with. Plus, you get the benefit of cross referencing in fun ways: cross reference to the timestamp at which the sample was taken (so, for example, ignore the spikes generated at this and that timeframe due to maintenance. Don&#39;t alert me on these), to system issues like load average or CPU usage (show me the average &lt;strong&gt;Seconds_behind_master&lt;/strong&gt; when load average is over &lt;strong&gt;8&lt;/strong&gt;, or the average load average when slow query rate is over some threshold. You don&#39;t do that all the time, but when you need it, well, you can get all the insight you ever wanted.&lt;/p&gt;
&lt;p&gt;Actually storing the monitored data in an easy to access format allows one to query, re-query, re-formulate. No worries about caching, you only sample once.&lt;/p&gt;
&lt;p&gt;For completeness, all the above is relevant when the data is of numeric types. Other types are far more complicated to manage (the list of running queries is a common example).&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Announcing common_schema: common views &amp; routines for MySQL</title>
      <link>/blog/mysql/announcing-common_schema-common-views-routines-for-mysql/</link>
      <pubDate>Wed, 13 Jul 2011 06:25:24 +0000</pubDate>
      
      <guid>/blog/mysql/announcing-common_schema-common-views-routines-for-mysql/</guid>
      <description>&lt;p&gt;Today I have released &lt;a title=&#34;common_schema&#34; href=&#34;http://code.openark.org/forge/common_schema&#34;&gt;common_schema&lt;/a&gt;, a utility schema for MySQL which includes many views and functions, and is aimed to be installed on any MySQL server.&lt;/p&gt;
&lt;h4&gt;What does it do?&lt;/h4&gt;
&lt;p&gt;There are views answering for all sorts of useful information: stuff related to schema analysis, data dimensions, monitoring, processes &amp;amp; transactions, security, internals... There are basic functions answering for common needs.&lt;/p&gt;
&lt;p&gt;Some of the views/routines simply formalize those queries we tend to write over and over again. Others take the place of external tools, answering complex questions via SQL and metadata. Still others help out with SQL generation.&lt;/p&gt;
&lt;p&gt;Here are a few highlights:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Did you know you can work out &lt;a href=&#34;http://common-schema.googlecode.com/svn/trunk/common_schema/doc/html/global_status_diff_nonzero.html&#34;&gt;simple monitoring&lt;/a&gt; of your server with a &lt;em&gt;query&lt;/em&gt;?  There&#39;s a view to do that for you.&lt;/li&gt;
&lt;li&gt;How about showing just &lt;a href=&#34;http://common-schema.googlecode.com/svn/trunk/common_schema/doc/html/processlist_top.html&#34;&gt;the good parts of the processlist&lt;/a&gt;?&lt;/li&gt;
&lt;li&gt;Does your schema have &lt;a href=&#34;http://common-schema.googlecode.com/svn/trunk/common_schema/doc/html/redundant_keys.html&#34;&gt;redundant keys&lt;/a&gt;?&lt;/li&gt;
&lt;li&gt;Or InnoDB tables with &lt;a href=&#34;http://common-schema.googlecode.com/svn/trunk/common_schema/doc/html/no_pk_innodb_tables.html&#34;&gt;no PRIMARY KEY&lt;/a&gt;?&lt;/li&gt;
&lt;li&gt;Is AUTO_INCREMENT &lt;a href=&#34;http://common-schema.googlecode.com/svn/trunk/common_schema/doc/html/auto_increment_columns.html&#34;&gt;running out of space&lt;/a&gt;?&lt;/li&gt;
&lt;li&gt;Can I get the SQL statements to &lt;a href=&#34;http://common-schema.googlecode.com/svn/trunk/common_schema/doc/html/sql_foreign_keys.html&#34;&gt;generate my FOREIGN KEYs&lt;/a&gt;? To drop them?&lt;/li&gt;
&lt;li&gt;And can we finally get &lt;a href=&#34;http://common-schema.googlecode.com/svn/trunk/common_schema/doc/html/sql_show_grants.html&#34;&gt;SHOW GRANTS for all accounts&lt;/a&gt;, and as an &lt;a href=&#34;http://common-schema.googlecode.com/svn/trunk/common_schema/doc/html/sql_grants.html&#34;&gt;SQL query&lt;/a&gt;?&lt;/li&gt;
&lt;li&gt;Ever needed a &lt;a href=&#34;http://common-schema.googlecode.com/svn/trunk/common_schema/doc/html/general_functions.html#crc64&#34;&gt;64 bit CRC function&lt;/a&gt;?&lt;/li&gt;
&lt;li&gt;And aren&#39;t you tired of writing the cumbersome SUBSTRING_INDEX(SUBSTRING_INDEX(str, &#39;,&#39;, 3), &#39;,&#39;, -1)? &lt;a href=&#34;http://common-schema.googlecode.com/svn/trunk/common_schema/doc/html/string_functions.html#split_token&#34;&gt;There&#39;s an alternative&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There&#39;s more. Take a look at the &lt;a href=&#34;http://common-schema.googlecode.com/svn/trunk/common_schema/doc/html/introduction.html&#34;&gt;common_schema documentation&lt;/a&gt; for full listing. And it&#39;s evolving: I&#39;ve got quite a few ideas already for future components.&lt;/p&gt;
&lt;p&gt;Some of these views rely on heavyweight INFORMATION_SCHEMA tables. You should be aware of the impact and &lt;a href=&#34;http://common-schema.googlecode.com/svn/trunk/common_schema/doc/html/risks.html&#34;&gt;risks&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;What do I need to install?&lt;/h4&gt;
&lt;p&gt;There&#39;s no script or executable file. It&#39;s just a schema. The distribution in an SQL file which generates &lt;em&gt;common_schema&lt;/em&gt;. Much like a dump file.&lt;/p&gt;
&lt;h4&gt;&lt;!--more--&gt;What are the system requirements?&lt;/h4&gt;
&lt;p&gt;It&#39;s just between you and your MySQL. There are currently three distribution files, dedicated for different versions of MySQL (and allowing for increased functionality):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;common_schema_mysql_51&lt;/strong&gt;: fits all MySQL &amp;gt;= 5.1 distributions&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;common_schema_innodb_plugin&lt;/strong&gt;: fits MySQL &amp;gt;= 5.1, with InnoDB plugin + INFORMATION_SCHEMA tables enabled&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;common_schema_percona_server&lt;/strong&gt;: fits Percona Server &amp;gt;= 5.1&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Refer to the &lt;a rel=&#34;nofollow&#34; href=&#34;http://common-schema.googlecode.com/svn/trunk/common_schema/doc/html/download.html&#34;&gt;documentation&lt;/a&gt; for more details.&lt;/p&gt;
&lt;h4&gt;What are the terms of use?&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;common_schema&lt;/em&gt; is released under the &lt;a href=&#34;http://www.opensource.org/licenses/bsd-license.php&#34;&gt;BSD license&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Where can I download it?&lt;/h4&gt;
&lt;p&gt;On the &lt;a href=&#34;http://code.google.com/p/common-schema/&#34;&gt;common_schema project page&lt;/a&gt;. Enjoy it!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>mycheckpoint (rev 208): aggregation tables, enhanced charting, RPM distribution</title>
      <link>/blog/mysql/mycheckpoint-rev-208-aggregation-tables-enhanced-charting-rpm-distribution/</link>
      <pubDate>Mon, 08 Nov 2010 12:45:45 +0000</pubDate>
      
      <guid>/blog/mysql/mycheckpoint-rev-208-aggregation-tables-enhanced-charting-rpm-distribution/</guid>
      <description>&lt;p&gt;Revision &lt;strong&gt;208&lt;/strong&gt; of &lt;a href=&#34;../../forge/mycheckpoint&#34;&gt;mycheckpoint&lt;/a&gt;, a MySQL monitoring solution, has  been released. New and updated in this revision:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Aggregation tables&lt;/strong&gt;: aggregated data makes for fast reports on previously slow queries.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Enhanced charting&lt;/strong&gt;: interactive charts now present time stamps dynamically (see &lt;a href=&#34;http://mycheckpoint.googlecode.com/svn/trunk/doc/html/sample/http/mcp_sql00/sv_report_html_brief&#34;&gt;&lt;strong&gt;demo&lt;/strong&gt;&lt;/a&gt;); &#34;Zoom in&#34; charts are available (see &lt;a href=&#34;http://mycheckpoint.googlecode.com/svn/trunk/doc/html/sample/http/mcp_sql00/zoom/questions&#34;&gt;&lt;strong&gt;demo&lt;/strong&gt;&lt;/a&gt;) on &lt;em&gt;mycheckpoint&lt;/em&gt;&#39;s HTTP server.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;RPM distribution&lt;/strong&gt;: a &#34;noarch&#34; RPM &lt;em&gt;mycheckpoint&lt;/em&gt; build is now available.&lt;/li&gt;
&lt;li&gt;Initial work on formalizing test environment&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;mycheckpoint&lt;/em&gt; celebrates one year of existence!&lt;/p&gt;
&lt;h4&gt;Aggregation tables&lt;/h4&gt;
&lt;p&gt;I really wanted to avoid using these: everything was so more beautiful with one single dataset and dozens of supporting views (OK, the views themselves are hardly &#34;beautiful&#34;).&lt;/p&gt;
&lt;p&gt;However it was impossible (for my level of expertise) to optimize query performance what with all those views on per-hour and per-day aggregation. The GROUP BYs and the JOINs did not make it possible for condition pushdown (i.e. using MERGE algorithm) where desired.&lt;/p&gt;
&lt;p&gt;As result, mycheckpoint now manages aggregation tables: per-hour and per-day. The impact on sample taking is neglect able (making for two additional fast queries), but the impact on reading aggregated data is overwhelming. Generating a HTML full report could take a few minutes to complete. It now returns in no time. This makes charting more attractive, and allows for enhanced charting, such as zooming in on charts, as described following.&lt;/p&gt;
&lt;p&gt;Aggregation tables will automatically be created and retroactively populated upon using revision 208. There&#39;s nothing special to do; be advised that for one single execution of &lt;em&gt;mycheckpoint&lt;/em&gt;, many INSERT queries are going to be executed. Shouldn&#39;t take more than a couple minutes on commodity hardware and a few months of history.&lt;/p&gt;
&lt;p&gt;It is possible to disable aggregation tables, or make for a complete rebuild of tables; by default, though, aggregation is ON.&lt;/p&gt;
&lt;h4&gt;Enhanced charting&lt;/h4&gt;
&lt;p&gt;Two enhancements here:&lt;!--more--&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The interactive line charts already know how to update legend data as mouse hovers over them. Now they also present accurate date &amp;amp; time. This provides with fully informative charts.&lt;/li&gt;
&lt;li&gt;As with other monitoring tools, it is possible to &#34;zoom in&#34; on a chart: zooming in will present any chart in &#34;last 24 hours&#34;, &#34;last 10 days&#34; and &#34;complete history&#34; views, magnified on screen. See &lt;a href=&#34;http://mycheckpoint.googlecode.com/svn/trunk/doc/html/sample/http/mcp_sql00/zoom/questions&#34;&gt;&lt;strong&gt;demo&lt;/strong&gt;&lt;/a&gt; here.&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;RPM distribution&lt;/h4&gt;
&lt;p&gt;No excuse for this being so late, I know. But RPM distribution is now &lt;a href=&#34;http://code.google.com/p/mycheckpoint/&#34;&gt;available&lt;/a&gt;. Yeepee!&lt;/p&gt;
&lt;p&gt;This is a &lt;em&gt;noarch&lt;/em&gt; distribution, courtesy of Python&#39;s &lt;a href=&#34;http://docs.python.org/distutils/&#34;&gt;distutils&lt;/a&gt;; you should be able to install the package on any RPM supporting platform. I have only tested in on CentOS; feedback is welcome.&lt;/p&gt;
&lt;h4&gt;Future plans&lt;/h4&gt;
&lt;p&gt;Work is going on. These are the non-scheduled future tasks I see:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Monitoring InnoDB Plugin &amp;amp; XtraDB status.&lt;/li&gt;
&lt;li&gt;A proper &lt;em&gt;man&lt;/em&gt; page.&lt;/li&gt;
&lt;li&gt;Anything else that interests me &amp;amp; the users.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Try it out&lt;/h4&gt;
&lt;p&gt;Try out &lt;em&gt;mycheckpoint&lt;/em&gt;. It’s a different kind of monitoring    solution. Simple monitoring (charting) is immediate. For more  interesting results you will need basic SQL skills, and in return you’ll  get a lot   of power under your hands.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Download mycheckpoint &lt;a href=&#34;https://code.google.com/p/mycheckpoint/&#34;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Visit the project’s &lt;a href=&#34;../../forge/mycheckpoint&#34;&gt;homepage&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Browse the &lt;a href=&#34;../../forge/mycheckpoint/documentation&#34;&gt;documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Report &lt;a href=&#34;https://code.google.com/p/mycheckpoint/issues/list&#34;&gt;bugs&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;mycheckpoint&lt;/em&gt; is released under the &lt;a href=&#34;http://www.opensource.org/licenses/bsd-license.php&#34;&gt;New BSD  License&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Umm, I&#39;ll repeat this last one: &lt;em&gt;mycheckpoint&lt;/em&gt; is released under the &lt;a href=&#34;http://www.opensource.org/licenses/bsd-license.php&#34;&gt;New BSD  License&lt;/a&gt;. Still, and will continue to be. Thanks for the &lt;a href=&#34;http://code.openark.org/blog/mysql/openark-kit-facebook-online-schema-change-and-thoughts-on-open-source-licenses#comments&#34;&gt;good advice&lt;/a&gt; by Lenz, Domas and others.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>mycheckpoint (rev. 190): HTTP server; interactive charts</title>
      <link>/blog/mysql/mycheckpoint-rev-190-http-server-interactive-charts/</link>
      <pubDate>Tue, 07 Sep 2010 07:53:01 +0000</pubDate>
      
      <guid>/blog/mysql/mycheckpoint-rev-190-http-server-interactive-charts/</guid>
      <description>&lt;p&gt;Revision &lt;strong&gt;190&lt;/strong&gt; of &lt;a href=&#34;../../forge/mycheckpoint&#34;&gt;mycheckpoint&lt;/a&gt;, a MySQL monitoring solution, has  been released. New and updated in this revision:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;HTTP server&lt;/strong&gt;: &lt;em&gt;mycheckpoint&lt;/em&gt; can now act as a web server. Point your browser and start browsing through HTML reports. See mock up &lt;a href=&#34;http://code.openark.org/forge/wp-content/uploads/2010/09/r190/mcp_sql00/sv_report_html_brief.html&#34;&gt;&lt;strong&gt;demo&lt;/strong&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Interactive charts&lt;/strong&gt;: HTML line charts are now interactive, presenting with accurate data as you move over them. See &lt;a href=&#34;http://code.openark.org/forge/wp-content/uploads/2010/09/r190/mcp_sql00_samples/sv_report_html_brief.html&#34;&gt;&lt;strong&gt;sample&lt;/strong&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Enhanced auto-deploy&lt;/strong&gt;: now auto-recognizing failed upgrades.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reduced footprint&lt;/strong&gt;: much code taken out of the views, leading to faster loading times.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Better configuration file use&lt;/strong&gt;: now supporting all command line options in config file.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Remote host monitoring accessibility&lt;/strong&gt;: now supporting complete configurable accessibility details.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Bug fixes&lt;/strong&gt;: thanks to the bug reporters!&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;mycheckpoint&lt;/em&gt; is free, simple, easy to use (now easier with HTTP server) and &lt;strong&gt;useful&lt;/strong&gt;. I encourage you to try it out: even compared with other existing and emerging monitoring tools, I believe you will find it a breeze; it&#39;s low impact and lightness appealing; it&#39;s alerts mechanism assuring; its geeky SQL-based nature with ability to drill down to fine details -- geeky-kind-of-attractive.&lt;/p&gt;
&lt;p&gt;&amp;lt;/encouragement&amp;gt;&lt;/p&gt;
&lt;h4&gt;HTTP server&lt;/h4&gt;
&lt;p&gt;You can now run &lt;em&gt;mycheckpoint&lt;/em&gt; in &lt;em&gt;http&lt;/em&gt; mode:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;bash$ &lt;strong&gt;mycheckpoint http&lt;/strong&gt;&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;em&gt;mycheckpoint&lt;/em&gt; will listen on port &lt;strong&gt;12306&lt;/strong&gt;, and will present you with easy browsing through the reports of your &lt;em&gt;mycheckpoint&lt;/em&gt; databases.&lt;!--more--&gt;&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;http&lt;/em&gt; server automatically detects those schemata used by mycheckpoint, and utilizes the existing HTML views, integrating them into the greater web framework.&lt;/p&gt;
&lt;p&gt;While in &lt;em&gt;http&lt;/em&gt; mode, mycheckpoint does nothing besides serving web pages. It does not actively exercise monitoring: you must still use the usual cron jobs or other scheduled tasks by which you invoke &lt;em&gt;mycheckpoint&lt;/em&gt; for monitoring.&lt;/p&gt;
&lt;p&gt;The http server is directed at a single MySQL server, as with the following example:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;bash$ &lt;strong&gt;mycheckpoint --host=slave1.localdomain --port=3306 --http-port=12306 http&lt;/strong&gt;&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;It is assumed that this server has the monitoring schemata.&lt;/p&gt;
&lt;p&gt;See mock up &lt;a href=&#34;http://code.openark.org/forge/wp-content/uploads/2010/09/r190/mcp_sql00/sv_report_html_brief.html&#34;&gt;&lt;strong&gt;demo&lt;/strong&gt;&lt;/a&gt;. The demo uses presents with real output from a mycheckpoint HTTP server; I haven&#39;t got the means to put up a live demo.&lt;/p&gt;
&lt;h4&gt;Interactive charts&lt;/h4&gt;
&lt;p&gt;The &lt;em&gt;openark line charts&lt;/em&gt;, used in the HTML reports, are now interactive. As you scroll over, the legend presents you with series values.&lt;/p&gt;
&lt;p&gt;No more &lt;em&gt;&#34;I have this huge spike once every 4 hours, which reduces all other values to something that looks like zero but is actually NOT&#34;&lt;/em&gt;. Hover, and see the real values.&lt;/p&gt;
&lt;p&gt;See &lt;a href=&#34;http://code.openark.org/forge/wp-content/uploads/2010/09/r190/mcp_sql00_samples/sv_report_html_brief.html&#34;&gt;&lt;strong&gt;sample&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Enhanced auto-deploy&lt;/h4&gt;
&lt;p&gt;The idea with mycheckpoint is that it should know how to self upgrade the schema on version upgrade (much like automatic WordPress upgrades). mycheckpoint does bookkeeping of installed versions within the database, and upgrades by simple comparison.&lt;/p&gt;
&lt;p&gt;It now, following a couple of reported bugs, also recognizes failure of partial, failed upgrades. This adds to the automation of &lt;em&gt;mycheckpoint&lt;/em&gt;&#39;s installation.&lt;/p&gt;
&lt;h4&gt;Reduced footprint&lt;/h4&gt;
&lt;p&gt;Some of &lt;em&gt;mycheckpoint&lt;/em&gt;&#39;s views are complicated, and lead to a large amount of code in view declaration. This leads to increased table definition size (large &lt;strong&gt;.frm&lt;/strong&gt; files). There has been some work to reduce this size where possible. Work is still ongoing, but some 30% has been taken off already. This leads to faster table (view) load time.&lt;/p&gt;
&lt;h4&gt;Better configuration file use&lt;/h4&gt;
&lt;p&gt;Any argument supported on the command line is now also supported in the config style. Much like is handled with MySQL. For example, one can issue:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;mycheckpoint --monitored-host=sql02.mydb.com  --monitored-user=monitor --monitored-password=123456&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;But now also:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;mycheckpoint&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;With the following in &lt;strong&gt;/etc/mycheckpoint.cnf&lt;/strong&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;[mycheckpoint]
monitored_host     = sql02.mydb.com
monitored_user     = monitor
monitored_password = 123456
&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;Rules are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If an option is specified on command line, it takes precedence over anything else.&lt;/li&gt;
&lt;li&gt;Otherwise, if it&#39;s specified in the configuration file, value is read from file.&lt;/li&gt;
&lt;li&gt;Otherwise use default value is used.&lt;/li&gt;
&lt;li&gt;On command line, option format is&lt;strong&gt; xxx-yyy-zzz&lt;/strong&gt;: words split with dash/minus character.&lt;/li&gt;
&lt;li&gt;On configuration file, option format is &lt;strong&gt;xxx_yyy_zzz&lt;/strong&gt;: words split with underscore. Unlike MySQL configuration format, dashes cannot be used.&lt;/li&gt;
&lt;li&gt;If an option is specified multiple times on configuration file -- well -- I have the answer, but I won&#39;t tell. Just don&#39;t do it. It&#39;s bad for your health.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Future plans&lt;/h4&gt;
&lt;p&gt;Work is going on. These are the non-scheduled future tasks I see:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Monitoring InnoDB Plugin &amp;amp; XtraDB status.&lt;/li&gt;
&lt;li&gt;A proper &lt;em&gt;man&lt;/em&gt; page.&lt;/li&gt;
&lt;li&gt;Anything else that interests me &amp;amp; the users.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Try it out&lt;/h4&gt;
&lt;p&gt;Try out &lt;em&gt;mycheckpoint&lt;/em&gt;. It’s a different kind of monitoring   solution. Simple monitoring (charting) is immediate. For more interesting results you will need basic SQL skills, and in return you’ll get a lot   of power under your hands.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Download mycheckpoint &lt;a href=&#34;https://code.google.com/p/mycheckpoint/&#34;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Visit the project’s &lt;a href=&#34;../../forge/mycheckpoint&#34;&gt;homepage&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Browse the &lt;a href=&#34;../../forge/mycheckpoint/documentation&#34;&gt;documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Report &lt;a href=&#34;https://code.google.com/p/mycheckpoint/issues/list&#34;&gt;bugs&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;mycheckpoint&lt;/em&gt; is released under the &lt;a href=&#34;http://www.opensource.org/licenses/bsd-license.php&#34;&gt;New BSD  License&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>mycheckpoint (rev. 170): improved custom queries; local charting; page/swap I/O monitoring; improved HTML reports</title>
      <link>/blog/mysql/mycheckpoint-rev-170-improved-custom-queries-local-charting-pageswap-io-monitoring-improved-html-reports/</link>
      <pubDate>Fri, 16 Jul 2010 10:58:40 +0000</pubDate>
      
      <guid>/blog/mysql/mycheckpoint-rev-170-improved-custom-queries-local-charting-pageswap-io-monitoring-improved-html-reports/</guid>
      <description>&lt;p&gt;Revision &lt;strong&gt;170&lt;/strong&gt; of &lt;a href=&#34;../../forge/mycheckpoint&#34;&gt;mycheckpoint&lt;/a&gt;, a MySQL monitoring solution, has  been released. New and updated in this revision:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Improved custom queries&lt;/strong&gt;: lifting of limitations from previous, introductory revision; better HTML presentation&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Local, inline charting&lt;/strong&gt;: no rendering of Google Charts, unless explicitly requested. All charts are now rendered locally using JavaScript. No data is now sent over the network.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Page/Swap I/O monitoring&lt;/strong&gt;: now monitoring for page ins and outs, swap ins and outs (Linux only).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Improved HTML reports&lt;/strong&gt;: several improvements on presentation (see &lt;a href=&#34;http://code.openark.org/forge/wp-content/uploads/2010/07/mycheckpoint-report-brief-169.html&#34;&gt;sample&lt;/a&gt;, more follow).&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Improved custom queries&lt;/h4&gt;
&lt;p&gt;Some limitations, introduced in revision &lt;strong&gt;132&lt;/strong&gt;, are now lifted. New features are introduced.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;There is now no limit to the number of custom queries (well, an INT limit).&lt;/li&gt;
&lt;li&gt;In fact, the data tables adjust themselves to the existing custom queries in the form of auto-deploy: once a new &lt;a href=&#34;http://code.openark.org/forge/mycheckpoint/documentation/custom-monitoring&#34;&gt;custom query is added&lt;/a&gt; or an old one removed, mycheckpoint will add or remove the relevant columns from the data tables.&lt;/li&gt;
&lt;li&gt;The &lt;strong&gt;chart_order&lt;/strong&gt; column is now utilized: HTML reports which include custom query charts now order those charts according to &lt;strong&gt;chart_order&lt;/strong&gt; values. This makes for nicer reports.&lt;/li&gt;
&lt;li&gt;The standard &lt;a href=&#34;http://code.openark.org/forge/wp-content/uploads/2010/07/mycheckpoint-report-brief-169.html&#34;&gt;HTML brief report&lt;/a&gt; (&lt;strong&gt;SELECT html FROM sv_report_html_brief&lt;/strong&gt;) now automatically includes all custom charts. The HTML brief report is the report one usually wants to look at: it provides with the latest 24 hours metrics for selected values. It now becomes a centralized place for all that is interesting in the past 24 hours.&lt;/li&gt;
&lt;li&gt;Custom queries are now allowed to return &lt;strong&gt;NULL&lt;/strong&gt;, treated as a missing value. This is a bugfix from previous revisions.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Local charting&lt;/h4&gt;
&lt;p&gt;Motivation for local charting is clear: no one likes having their data being sent over the network. And no one likes Google to know about their DML values.&lt;/p&gt;
&lt;p&gt;I&#39;ve been playing around with quite a few charting solutions, and have gone into depths with two of them, adding and rewriting quite a lot of code. Eventually, I settled on my very own rendering. Here&#39;s what I&#39;ve seen &amp;amp; tested:&lt;!--more--&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://danvk.org/dygraphs/&#34;&gt;dygraphs&lt;/a&gt;: a very nice time series charting library. I&#39;ve presented a use case on &lt;a href=&#34;http://code.openark.org/blog/mysql/static-charts-vs-interactive-charts&#34;&gt;a previous post&lt;/a&gt;.
&lt;ul&gt;
&lt;li&gt;Pros: slick, easy to work with.&lt;/li&gt;
&lt;li&gt;Cons: uses HTML Canvas for rendering. This is fine on Firefox, Chrome, Safari, you name it. This isn&#39;t fine on IE, which does not support Canvas. There&#39;s &lt;a href=&#34;http://excanvas.sourceforge.net/&#34;&gt;ExplorerCanvas&lt;/a&gt;, a hack tool which converts canvas to IE&#39;s VML, but it is far from being satisfactory: it is &lt;em&gt;sloooow&lt;/em&gt;. Very, very slow. It is slow with one chart; but loading of 21 charts, as I do in some of &lt;em&gt;mycheckpoint&lt;/em&gt;&#39;s reports can take &lt;em&gt;long minutes&lt;/em&gt; on Internet explorer.&lt;/li&gt;
&lt;li&gt;Cons: Only provides with a time series chart. No scatter plots.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Because they&#39;re using ExplorerCanvas for IE, &lt;a href=&#34;http://code.google.com/p/flot/&#34;&gt;flot&lt;/a&gt;, &lt;a href=&#34;http://www.jqplot.com/&#34;&gt;jqPlot&lt;/a&gt; etc., are all unacceptable.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://g.raphaeljs.com/&#34;&gt;gRaphael&lt;/a&gt;: very slick charts based on Raphael. The original line charts are very basic, and I have invested a lot of time rewriting a great deal (you can find it all &lt;a href=&#34;http://code.google.com/p/mycheckpoint/source/browse/#svn/trunk/graphael&#34;&gt;here&lt;/a&gt;). Raphael uses VML on IE, and SVG for all other browsers.
&lt;ul&gt;
&lt;li&gt;Pros: very slick. Supports various chart types, including line (though not time-series) and scatter.&lt;/li&gt;
&lt;li&gt;Cons: &lt;em&gt;slooooooooow&lt;/em&gt; when instantiating multiple charts. Unbearably slow, both on Firefox and IE. Slow as in minutes of waiting.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In addition, all of the above solutions were quite heavyweight: at about 45KB to start with, then add ExplorerCanvas or jQuery, or Raphael as supporting libraries, these became a real burden.&lt;/p&gt;
&lt;p&gt;So, I had some time to spare (business is fine, thank you. I was a bit Ill. I&#39;m feeling well now, thank you), and was upset what with all the time I invested in the above coding. And I decided to invest even more time, and build &lt;em&gt;my own&lt;/em&gt; charts.&lt;/p&gt;
&lt;p&gt;Enter &lt;em&gt;openark-charts&lt;/em&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;&lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2010/07/mycheckpoint-report-html-screenshot.png&#34; /&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img class=&#34;alignnone size-full wp-image-2662&#34; title=&#34;mycheckpoint-report-html-screenshot&#34; src=&#34;/blog/blog/assets/mycheckpoint-report-html-screenshot.png&#34; alt=&#34;&#34; width=&#34;808&#34; height=&#34;307&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2010/07/mycheckpoint-24-7-report-html-screenshot.png&#34;&gt;&lt;img class=&#34;alignnone size-full wp-image-2663&#34; title=&#34;mycheckpoint-24-7-report-html-screenshot&#34; src=&#34;/blog/blog/assets/mycheckpoint-24-7-report-html-screenshot.png&#34; alt=&#34;&#34; width=&#34;808&#34; height=&#34;267&#34; /&gt;&lt;/a&gt;
&lt;/blockquote&gt;
&lt;p&gt;Currently, these line charts and scatter charts know how to parse a Google Image chart URL (only some features supported -- only those I&#39;m actually using with &lt;em&gt;mycheckpoint&lt;/em&gt;). These are not full blown solutions: they come to serve mycheckpoint. And they do so nicely, if I may say so. Using Canvas for most browsers, or VML for IE, these very small pieces of code (10K for line chart, 6K for scatter chart, minified) load fast, use very little memory, and do their work well.&lt;/p&gt;
&lt;p&gt;Granted, neither provides with interactive features: this is planned for the future.&lt;/p&gt;
&lt;h4&gt;Page/swap I/O monitoring&lt;/h4&gt;
&lt;p&gt;(Linux only) &lt;em&gt;mycheckpoint&lt;/em&gt; now reads &lt;strong&gt;/proc/vmstat&lt;/strong&gt; to get the &lt;em&gt;pageins&lt;/em&gt;, &lt;em&gt;pageouts&lt;/em&gt;, &lt;em&gt;swapins&lt;/em&gt; and &lt;em&gt;swapouts&lt;/em&gt; (since last reboot). I was actually looking at completely different places on the &lt;strong&gt;/proc&lt;/strong&gt; file system to get swap info, and was frustrated with the complexity involved, till I bumped on &lt;strong&gt;/proc/vmstat&lt;/strong&gt;... New tricks every day!&lt;/p&gt;
&lt;h4&gt;Improved HTML reports&lt;/h4&gt;
&lt;p&gt;This is mostly HTML make-up. Some minimal design, some more details thrown into the HTML pages (name of DB, MySQL version, &lt;em&gt;mycheckpoint&lt;/em&gt; version). A little more verbosity; all sorts of stuff which was neglected so far.&lt;/p&gt;
&lt;p&gt;Here are some &lt;span style=&#34;text-decoration: line-through;&#34;&gt;&lt;strong&gt;show off&lt;/strong&gt;&lt;/span&gt; examples of the new HTML views: &lt;a href=&#34;http://code.openark.org/forge/wp-content/uploads/2010/07/mycheckpoint-report-full-169.html&#34;&gt;[full report]&lt;/a&gt;, &lt;a href=&#34;http://code.openark.org/forge/wp-content/uploads/2010/07/mycheckpoint-report-brief-169.html&#34;&gt;[brief report]&lt;/a&gt;, &lt;a href=&#34;http://code.openark.org/forge/wp-content/uploads/2010/07/mycheckpoint-report-24-7-169.html&#34;&gt;[24/7 report]&lt;/a&gt;, &lt;a href=&#34;http://code.openark.org/forge/wp-content/uploads/2010/07/mycheckpoint-report-custom-full-169.html&#34;&gt;[custom full report]&lt;/a&gt;, &lt;a href=&#34;http://code.openark.org/forge/wp-content/uploads/2010/07/mycheckpoint-report-custom-brief-169.html&#34;&gt;[custom brief report]&lt;/a&gt;, &lt;a href=&#34;http://code.openark.org/forge/wp-content/uploads/2010/07/mycheckpoint-alert-pending-169.html&#34;&gt;[alert pending report]&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;All HTML views now utilize the new &lt;em&gt;openark-charts&lt;/em&gt;, and none renders charts with Google charts. This means when you &lt;a href=&#34;http://code.openark.org/forge/mycheckpoint/documentation/generating-html-reports&#34;&gt;use your HTML view&lt;/a&gt;, your data is safe. No data is sent over the net. All charts are rendered using Javascript, which is loaded and executed locally.&lt;/p&gt;
&lt;p&gt;But if you like, there&#39;s a [url] link next to each chart, which leads to a (online) Google chart image. Why? Because neither HTML Canvas nor VML allow for a complete rendering of the charts to an image. So this is a way for one to retrieve &amp;amp; store a chart&#39;s image. Don&#39;t use it if you see no reason for it; it&#39;s just there.&lt;/p&gt;
&lt;p&gt;And I even threw in rounded corners (IE users: only as of Windows 7).&lt;/p&gt;
&lt;h4&gt;Future plans&lt;/h4&gt;
&lt;p&gt;Work is going on. These are the non-scheduled future tasks I see:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Monitoring InnoDB Plugin &amp;amp; XtraDB status.&lt;/li&gt;
&lt;li&gt;Interactive charts. See my &lt;a href=&#34;../mysql/static-charts-vs-interactive-charts&#34;&gt;earlier  post&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;A proper &lt;em&gt;man&lt;/em&gt; page.&lt;/li&gt;
&lt;li&gt;Anything else that interests me.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Try it out&lt;/h4&gt;
&lt;p&gt;Try out &lt;em&gt;mycheckpoint&lt;/em&gt;. It’s a different kind of monitoring  solution. You will need basic SQL skills, and in return you’ll get a lot  of power under your hands.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Download mycheckpoint &lt;a href=&#34;https://code.google.com/p/mycheckpoint/&#34;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Visit the project’s &lt;a href=&#34;../../forge/mycheckpoint&#34;&gt;homepage&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Browse the &lt;a href=&#34;../../forge/mycheckpoint/documentation&#34;&gt;documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Report &lt;a href=&#34;https://code.google.com/p/mycheckpoint/issues/list&#34;&gt;bugs&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;mycheckpoint&lt;/em&gt; is released under the &lt;a href=&#34;http://www.opensource.org/licenses/bsd-license.php&#34;&gt;New BSD  License&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>mycheckpoint (rev. 132): custom monitoring, custom charts, process list dump</title>
      <link>/blog/mysql/mycheckpoint-rev-132-custom-monitoring-custom-charts-process-list-dump/</link>
      <pubDate>Fri, 04 Jun 2010 11:17:27 +0000</pubDate>
      
      <guid>/blog/mysql/mycheckpoint-rev-132-custom-monitoring-custom-charts-process-list-dump/</guid>
      <description>&lt;p&gt;Revision &lt;strong&gt;132&lt;/strong&gt; of &lt;a href=&#34;../../forge/mycheckpoint&#34;&gt;mycheckpoint&lt;/a&gt; has been released. New and updated in this revision:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Custom monitoring: monitoring &amp;amp; charting for user defined queries&lt;/li&gt;
&lt;li&gt;HTML reports for custom monitoring&lt;/li&gt;
&lt;li&gt;Process list dump upon alert notifications&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Custom monitoring &amp;amp; charts&lt;/h4&gt;
&lt;p&gt;Custom monitoring allows the user to supply with a query, the results of which will be monitored.&lt;/p&gt;
&lt;p&gt;That is, &lt;em&gt;mycheckpoint&lt;/em&gt; monitors the status variables, replication status, OS metrics. But it cannot by itself monitor one&#39;s &lt;em&gt;application&lt;/em&gt;. Which is why a user may supply with such query as:&lt;/p&gt;
&lt;blockquote&gt;&lt;pre&gt;
SELECT COUNT(*) FROM shopping_cart WHERE is_pending=1
&lt;/pre&gt;&lt;/blockquote&gt;
&lt;p&gt;Such a query will tell an online store how many customers are in the midst of shopping. There is no argument that this number is worth monitoring for. Given the above query, &lt;em&gt;mycheckpoint&lt;/em&gt; will execute it per sample, and store the query&#39;s result along with all sampled data, to be then aggregated by complex views to answer for:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What was the value per given sample?&lt;/li&gt;
&lt;li&gt;What is the value difference for each sample?&lt;/li&gt;
&lt;li&gt;What is the change per second, i.e. the rate?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;mycheckpoint goes one step forward, and explicity records another metric:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How much time did it take to take that sample?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;!--more--&gt;As another example, a query worth testing for rate:&lt;/p&gt;
&lt;blockquote&gt;&lt;pre&gt;
SELECT MAX(shopping_cart_id) FROM shopping_cart
&lt;/pre&gt;&lt;/blockquote&gt;
&lt;p&gt;The above will provide with the last id. Assuming this is &lt;strong&gt;AUTO_INCREMENT&lt;/strong&gt;, and assuming we&#39;re on &lt;strong&gt;auto_increment_increment=1&lt;/strong&gt;, two samples will allow us to get the number of created carts between those samples. Now, here&#39;s a metric I&#39;d like to read:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How many carts are created per second, for each hour of the day?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We get all these for free with mycheckpoint, which already does this analysis. All we need to provide is the query, and how we would like it to be visualized (visualization is optional, it is not the only way to diagnose monitored data) graphically:&lt;/p&gt;
&lt;blockquote&gt;&lt;pre&gt;
INSERT INTO
 custom_query (custom_query_id, enabled, query_eval, description, chart_type, chart_order)
 VALUES (0, 1, &#39;SELECT COUNT(*) FROM store.shopping_cart WHERE is_pending=1&#39;, &#39;Number of pending carts&#39;, &#39;value&#39;, 0);
INSERT INTO
 custom_query (custom_query_id, enabled, query_eval, description, chart_type, chart_order)
 VALUES (1, 1, &#39;SELECT MAX(shopping_cart_id) FROM store.shopping_cart&#39;, &#39;Created carts rate&#39;, &#39;value_psec&#39;, 0);
&lt;/pre&gt;&lt;/blockquote&gt;
&lt;p&gt;We can later query for these values, just like we do for normal monitored values:&lt;/p&gt;
&lt;blockquote&gt;&lt;pre&gt;
mysql&amp;gt; SELECT id, ts, created_tmp_tables_psec, custom_0, custom_1_psec FROM sv_sample WHERE ts &amp;gt;= NOW() - INTERVAL 1 HOUR;
+-------+---------------------+-------------------------+----------+---------------+
| id    | ts                  | created_tmp_tables_psec | custom_0 | custom_1_psec |
+-------+---------------------+-------------------------+----------+---------------+
| 50730 | 2010-05-21 19:05:01 |                   16.64 |      448 |          3.02 |
| 50731 | 2010-05-21 19:10:02 |                   20.97 |       89 |          1.73 |
| 50732 | 2010-05-21 19:15:01 |                   15.70 |      367 |          3.56 |
| 50733 | 2010-05-21 19:20:01 |                   18.32 |       54 |          1.43 |
| 50734 | 2010-05-21 19:25:01 |                   16.42 |       91 |          1.96 |
| 50735 | 2010-05-21 19:30:02 |                   21.93 |      233 |          2.11 |
| 50736 | 2010-05-21 19:35:02 |                   14.58 |      176 |          1.91 |
| 50737 | 2010-05-21 19:40:01 |                   21.61 |      168 |          1.93 |
| 50738 | 2010-05-21 19:45:01 |                   16.05 |      241 |          2.44 |
| 50739 | 2010-05-21 19:50:01 |                   19.70 |       46 |          1.19 |
| 50740 | 2010-05-21 19:55:01 |                   15.85 |      177 |          2.28 |
| 50741 | 2010-05-21 20:00:01 |                   19.04 |        8 |          0.82 |
+-------+---------------------+-------------------------+----------+---------------+
&lt;/pre&gt;&lt;/blockquote&gt;
&lt;p&gt;Of course, it is also possible to harness &lt;em&gt;mycheckpoint&lt;/em&gt;&#39;s views power to generate charts:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;mysql&amp;gt; SELECT custom_1_psec FROM sv_report_chart_sample\G
&lt;/pre&gt;
&lt;img class=&#34;alignnone&#34; title=&#34;custom_1_psec&#34; src=&#34;/blog/blog/assets/chart?cht=lc&amp;amp;chs=400x200&amp;amp;chts=303030,12&amp;amp;chtt=Latest+24+hours:+May+19,+20:10++-++May+20,+20:10&amp;amp;chf=c,s,ffffff&amp;amp;chdl=custom_1_psec&amp;amp;chdlp=b&amp;amp;chco=ff8c00&amp;amp;chd=s:QfXQmZQhXTmWVkWRobPpWUtQPVROaOOUMJPOKdJHQJFJEDJJEGCAIEFJHFFEGGDQHGJGMJPPMNZNRWR_ZUWfR_nSjuUcaXa3OgxRl4UivWZ5UhtWX4VgnUTYktiVW9WanUVxVYlgXwVdicXpb&amp;amp;chxt=x,y&amp;amp;chxr=1,0,5.120000&amp;amp;chxl=0:||+||00:00||+||04:00||+||08:00||+||12:00||+||16:00||+||20:00|&amp;amp;chxs=0,505050,10,0,lt&amp;amp;chg=4.17,25,1,2,3.47,0&amp;amp;chxp=0,3.47,7.64,11.81,15.98,20.15,24.32,28.49,32.66,36.83,41.00,45.17,49.34,53.51,57.68,61.85,66.02,70.19,74.36,78.53,82.70,86.87,91.04,95.21,99.38&#34; alt=&#34;&#34; width=&#34;400&#34; height=&#34;200&#34; /&gt;
&lt;/blockquote&gt;
The rules are:
&lt;ul&gt;
&lt;li&gt;There can (currently) only be 18 custom queries.&lt;/li&gt;
&lt;li&gt;The &lt;strong&gt;custom_query_id&lt;/strong&gt; must range 0-17 (to be lifted soon).&lt;/li&gt;
&lt;li&gt;A custom query must return with &lt;em&gt;exactly&lt;/em&gt; one row, with &lt;em&gt;exactly&lt;/em&gt; one column, which is a kind of &lt;em&gt;integer&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
Please read &lt;a href=&#34;http://code.openark.org/blog/mysql/things-to-monitor-on-mysql-the-users-perspective&#34;&gt;my earlier post&lt;/a&gt; on custom monitoring to get more background.
&lt;h4&gt;Custom monitoring HTML reports&lt;/h4&gt;
Custom monitoring comes with a HTML reports, featuring requested charts. See a &lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2010/05/mcp_custom_report-128.html&#34;&gt;sample custom report&lt;/a&gt;.
In this sample report, a few queries are monitored for value (pending rentals, pending downloads) and a few for rates (downloads per second, emails per second etc.).
Custom HTML reports come in two flavors:
&lt;ul&gt;
&lt;li&gt;Brief reports, featuring last 24 hours, as in the example above. These are handled by the &lt;strong&gt;sv_custom_html_brief&lt;/strong&gt; view.&lt;/li&gt;
&lt;li&gt;Full reports, featuring last 24 hours, last 10 days, known history. These take longer to generate, and are handled by the &lt;strong&gt;sv_custom_html&lt;/strong&gt; view.&lt;/li&gt;
&lt;/ul&gt;
The sample report was generated by issuing:
&lt;blockquote&gt;
&lt;pre&gt;SELECT html FROM sv_custom_html_brief;&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;I won&#39;t go into details here as for how this view generates the HTML code. There is a myriad of view dependencies, with many interesting tricks on the way. But do remember it&#39;s &lt;em&gt;just a view&lt;/em&gt;. You don&#39;t need an application (not even &lt;em&gt;mycheckpoint&lt;/em&gt; itself) to generate the report. All it takes is a query.&lt;/p&gt;
&lt;h4&gt;Processlist dump&lt;/h4&gt;
&lt;p&gt;When an alert notification fires (an email is prepared to inform on some alert condition), a processlist dump summary is taken and included in email report. It may be useful to understand why the slave is lagging, or exactly why there are so many active threads.&lt;/p&gt;
&lt;p&gt;The dump summary presents the processlist much as you would see it on SHOW PROCESSLIST, but only lists the active threads, noting down how many sleeping processes there are (PS, thread &amp;amp; process are the same in the terminology of MySQL connections). An example dump looks like this:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;PROCESSLIST summary:

     Id: 3
   User: system user
   Host:
     db: NULL
Command: Connect
   Time: 3168098
  State: Waiting for master to send event
   Info: NULL
-------

     Id: 4
   User: system user
   Host:
     db: prod_db
Command: Connect
   Time: 612
  State: Updating
   Info: UPDATE user SET is_offline = 1 WHERE id IN (50440010,50440011)
-------

     Id: 8916579
   User: prod_user
   Host: localhost
     db: prod_db
Command: Query
   Time: 1
  State: Sending data
   Info: INSERT IGNORE INTO archive.stat_archive (id, origin, path, ts, content
-------

     Id: 8916629
   User: mycheckpoint
   Host: localhost
     db: NULL
Command: Query
   Time: 0
  State: NULL
   Info: SHOW PROCESSLIST
-------
Sleeping: 3 processes
&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;h4&gt;Future plans&lt;/h4&gt;
&lt;p&gt;Work is going on. These are the non-scheduled future tasks I see:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Monitoring InnoDB Plugin &amp;amp; XtraDB status.&lt;/li&gt;
&lt;li&gt;Interactive charts. See my &lt;a href=&#34;http://code.openark.org/blog/mysql/static-charts-vs-interactive-charts&#34;&gt;earlier post&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Monitoring for swap activity (Linux only).&lt;/li&gt;
&lt;li&gt;Enhanced custom queries handling, including auto-deploy upon change of custom queries.&lt;/li&gt;
&lt;li&gt;A proper &lt;em&gt;man&lt;/em&gt; page.&lt;/li&gt;
&lt;li&gt;Anything else that interests me.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Try it out&lt;/h4&gt;
&lt;p&gt;Try out &lt;em&gt;mycheckpoint&lt;/em&gt;. It’s a different kind of monitoring solution. You will need basic SQL skills, and in return you&#39;ll get a lot of power under your hands.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Download mycheckpoint &lt;a href=&#34;https://code.google.com/p/mycheckpoint/&#34;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Visit the project’s &lt;a href=&#34;../../forge/mycheckpoint&#34;&gt;homepage&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Browse the &lt;a href=&#34;../../forge/mycheckpoint/documentation&#34;&gt;documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Report &lt;a href=&#34;https://code.google.com/p/mycheckpoint/issues/list&#34;&gt;bugs&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;mycheckpoint&lt;/em&gt; is released under the &lt;a href=&#34;http://www.opensource.org/licenses/bsd-license.php&#34;&gt;New BSD License&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>mycheckpoint (Rev. 118): alerts, email notifications and more</title>
      <link>/blog/mysql/mycheckpoint-rev-118-alerts-email-notifications-and-more/</link>
      <pubDate>Thu, 25 Mar 2010 08:26:34 +0000</pubDate>
      
      <guid>/blog/mysql/mycheckpoint-rev-118-alerts-email-notifications-and-more/</guid>
      <description>&lt;p&gt;Revision &lt;strong&gt;118&lt;/strong&gt; of &lt;a href=&#34;http://code.openark.org/forge/mycheckpoint&#34;&gt;mycheckpoint&lt;/a&gt; has been released. New and updated in this revision:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Conditional alerts&lt;/li&gt;
&lt;li&gt;Email notifications&lt;/li&gt;
&lt;li&gt;Revised HTML reports, including 24/7 reports.&lt;/li&gt;
&lt;li&gt;Updated documentation&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With this new revision mycheckpoint turns into a &lt;em&gt;monitoring solution&lt;/em&gt; for MySQL. One can now:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Store measure metrics&lt;/li&gt;
&lt;li&gt;Query for raw, aggregated or digested metrics&lt;/li&gt;
&lt;li&gt;Generate charts for selected metrics&lt;/li&gt;
&lt;li&gt;View HTML reports for selecetd metrics&lt;/li&gt;
&lt;li&gt;Define alerts conditions, query for pending alerts&lt;/li&gt;
&lt;li&gt;Be notified via &lt;em&gt;email&lt;/em&gt; on &lt;em&gt;raised&lt;/em&gt; or &lt;em&gt;resolved&lt;/em&gt; alerts.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Conditional alerts&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;mycheckpoint&lt;/em&gt; is &lt;em&gt;SQL oriented&lt;/em&gt;. As such, it allows for creation of alert conditions, which are nothing more than SQL conditions.&lt;/p&gt;
&lt;p&gt;&lt;!--more--&gt;For example, we wish to raise an alerts when the slave stops replicating (just ping us with an email one this happens):&lt;/p&gt;
&lt;blockquote&gt;&lt;pre&gt;INSERT INTO alert_condition (condition_eval, description, alert_delay_minutes)
  VALUES (&#39;seconds_behind_master IS NULL&#39;, &#39;Slave not replicating&#39;, 0);&lt;/pre&gt;&lt;/blockquote&gt;
&lt;p&gt;Or is too far behind (but since we do maintenance work during the night, it&#39;s OK on those hours). We only want to be notified if this goes on for &lt;strong&gt;10&lt;/strong&gt; minutes:&lt;/p&gt;
&lt;blockquote&gt;&lt;pre&gt;INSERT INTO alert_condition (condition_eval, description, alert_delay_minutes)
  VALUES (&#39;(seconds_behind_master &amp;gt; 60) AND (HOUR(ts) NOT BETWEEN 2 AND 4)&#39;, &#39;Slave lags too far behind&#39;, 10);&lt;/pre&gt;&lt;/blockquote&gt;
&lt;p&gt;We want to be notified when the &lt;strong&gt;datadir&lt;/strong&gt; mount point disk quota exceeds 95% usage. Oh, and please keep nagging us about this, as long as it is unresolved:&lt;/p&gt;
&lt;blockquote&gt;&lt;pre&gt;INSERT INTO alert_condition (condition_eval, description, repetitive_alert)
  VALUES (&#39;os_datadir_mountpoint_usage_percent &amp;gt; 95&#39;, &#39;datadir mount point is over 95%&#39;, 1);&lt;/pre&gt;&lt;/blockquote&gt;
&lt;p&gt;There&#39;s much more to alert conditions. You can generate a pending alerts report, get a textual presentation of raised and pending alerts, view the query which determines what alerts are currently raised, and more.&lt;/p&gt;
&lt;p&gt;Read more on the &lt;a href=&#34;http://code.openark.org/forge/mycheckpoint/documentation/alerts&#34;&gt;alerts documentation page&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Email notifications&lt;/h4&gt;
&lt;p&gt;Introducing email notifications, &lt;em&gt;mycheckpoint&lt;/em&gt; now:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sends email notification on alert conditions meeting. See &lt;a href=&#34;http://code.openark.org/forge/wp-content/uploads/2010/03/mycheckpoint-alerts-email-sample-113.jpeg&#34;&gt;sample email screenshot&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Sends email notification when it is unable to access the database.&lt;/li&gt;
&lt;li&gt;Sends report via mail. Currently only HTML brief report is supported. Report is attached as HTML file in email message.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Alert notifications are automatically sent by mail (once SMTP configuration is in place, see following) when an alert is &lt;em&gt;raised&lt;/em&gt; (alert condition becomes &lt;strong&gt;true&lt;/strong&gt;) or &lt;em&gt;resolved&lt;/em&gt; (alert condition turns &lt;strong&gt;false&lt;/strong&gt;).&lt;/p&gt;
&lt;p&gt;Email notifications require simple configuration for SMTP host, SMTP-from-address, SMTP-to-address. These can be made in the &lt;a href=&#34;http://code.openark.org/forge/mycheckpoint/documentation/usage#defaults_file&#34;&gt;defaults file&lt;/a&gt; (revised), or through the command line. The following example shows how one can manually send an HTML brief report:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;mycheckpoint --defaults-file=/etc/mycheckpoint.cnf &lt;strong&gt;--smtp-from&lt;/strong&gt;=monitor@my-server-company.com &lt;strong&gt;--smtp-to&lt;/strong&gt;=dba@my-server-company.com &lt;strong&gt;--smtp-host&lt;/strong&gt;=mail.my-server-company.com &lt;strong&gt;email_brief_report&lt;/strong&gt;&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;One should generally set up these parameters in the configuration file (aka &lt;em&gt;defaults file&lt;/em&gt;) and forget all about it. mycheckpoint now has a default for the defaults file, which is &lt;strong&gt;/etc/mycheckpoint.cnf&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Read more on the &lt;a href=&#34;http://code.openark.org/forge/mycheckpoint/documentation/emails&#34;&gt;emails documentation page&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Revised HTML reports&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;The brief HTML reports has been updated, see &lt;a href=&#34;http://code.openark.org/forge/wp-content/uploads/2010/03/mycheckpoint-brief-report-sample-113.html&#34;&gt;sample&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;An HTML 24/7 report as been added, see &lt;a href=&#34;../../forge/wp-content/uploads/2010/03/mycheckpoint-24-7-report-sample-107.html&#34;&gt;sample&lt;/a&gt;. This report shows the distribution of popular metrics throughout the weekdays and hours.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Full HTML reports remain slow to load. I&#39;m putting some work into this, but I&#39;m not sure I can work around the optimizer&#39;s limitations of using indexes for GROUPing through views.&lt;/p&gt;
&lt;h4&gt;Updated documentation&lt;/h4&gt;
&lt;p&gt;The documentation has been revised, with more details put into the pages. Since &lt;em&gt;mycheckpoint&lt;/em&gt; gains more and more features, I saw fit to write a &lt;a href=&#34;http://code.openark.org/forge/mycheckpoint/documentation/quick-howto&#34;&gt;Quick HOWTO&lt;/a&gt; page which gets you up to speed, no fuss around, with &lt;em&gt;mycheckpoint&lt;/em&gt;&#39;s usage and features.&lt;/p&gt;
&lt;p&gt;Read the mycheckpoint &lt;a href=&#34;http://code.openark.org/forge/mycheckpoint/documentation/quick-howto&#34;&gt;Quick HOWTO&lt;/a&gt; here.&lt;/p&gt;
&lt;h4&gt;Future plans&lt;/h4&gt;
&lt;p&gt;Work is going on. These are the non-scheduled future tasks I see:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Custom monitoring + notifications. See my &lt;a href=&#34;http://code.openark.org/blog/mysql/things-to-monitor-on-mysql-the-users-perspective&#34;&gt;earlier post&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Monitoring InnoDB Plugin &amp;amp; XtraDB status.&lt;/li&gt;
&lt;li&gt;PROCESSLIST dump on alerts.&lt;/li&gt;
&lt;li&gt;Interactive charts. See my &lt;a href=&#34;http://code.openark.org/blog/mysql/static-charts-vs-interactive-charts&#34;&gt;earlier post&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;A proper &lt;em&gt;man&lt;/em&gt; page...&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Try it out&lt;/h4&gt;
&lt;p&gt;Try out &lt;em&gt;mycheckpoint&lt;/em&gt;. It&#39;s a different kind of monitoring solution. It does not require to to have a web server or complicated dependencies. To the experienced DBA it can further provide with valuable, raw or digested information in the form of SQL accessible data. I have used it to find anomalies in passing months, doing SQL search for periods of time where several conditions applied -- it really gives you some extra power.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Download mycheckpoint &lt;a href=&#34;https://code.google.com/p/mycheckpoint/&#34;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Visit the project&#39;s &lt;a href=&#34;http://code.openark.org/forge/mycheckpoint&#34;&gt;homepage&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Browse the &lt;a href=&#34;http://code.openark.org/forge/mycheckpoint/documentation&#34;&gt;documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Report &lt;a href=&#34;https://code.google.com/p/mycheckpoint/issues/list&#34;&gt;bugs&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;mycheckpoint&lt;/em&gt; is released under the &lt;a href=&#34;http://www.opensource.org/licenses/bsd-license.php&#34;&gt;New BSD License&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;_mcePaste&#34; style=&#34;overflow: hidden; position: absolute; left: -10000px; top: 855px; width: 1px; height: 1px;&#34;&gt;http://code.openark.org/forge/mycheckpoint/documentation/quick-howto&lt;/div&gt;</description>
    </item>
    
  </channel>
</rss>