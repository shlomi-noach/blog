<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Failover on code.openark.org</title>
    <link>/blog/tag/failover/</link>
    <description>Recent content in Failover on code.openark.org</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2016. All rights reserved.</copyright>
    <lastBuildDate>Fri, 20 Nov 2015 11:41:13 +0000</lastBuildDate>
    <atom:link href="/blog/tag/failover/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>State of automated recovery via Pseudo-GTID &amp; Orchestrator @ Booking.com</title>
      <link>/blog/mysql/state-of-automated-recovery-via-pseudo-gtid-orchestrator-booking-com/</link>
      <pubDate>Fri, 20 Nov 2015 11:41:13 +0000</pubDate>
      
      <guid>/blog/mysql/state-of-automated-recovery-via-pseudo-gtid-orchestrator-booking-com/</guid>
      <description>&lt;p&gt;This post sums up some of my work on MySQL resilience and high availability at &lt;a href=&#34;http://www.booking.com&#34;&gt;Booking.com&lt;/a&gt; by presenting the current state of automated master and intermediate master recoveries via &lt;a href=&#34;http://code.openark.org/blog/mysql/refactoring-replication-topology-with-pseudo-gtid&#34;&gt;Pseudo-GTID&lt;/a&gt; &amp;amp; &lt;strong&gt;&lt;a href=&#34;https://github.com/outbrain/orchestrator&#34;&gt;Orchestrator&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Booking.com uses many different MySQL topologies, of varying vendors, configurations and workloads: Oracle MySQL, MariaDB, statement based replication, row based replication, hybrid, OLTP, OLAP, GTID (few), no GTID (most), Binlog Servers, filters, hybrid of all the above.&lt;/p&gt;
&lt;p&gt;Topologies size varies from a single server to many-many-many. Our typical topology has a master in one datacenter, a bunch of slaves in same DC, a slave in another DC acting as an intermediate master to further bunch of slaves in the other DC. Something like this, give or take:&lt;/p&gt;
&lt;blockquote&gt;&lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2015/11/booking-topology-sample.png&#34;&gt;&lt;img class=&#34;alignnone wp-image-7480 size-medium&#34; src=&#34;/blog/blog/assets/booking-topology-sample-300x169.png&#34; alt=&#34;booking-topology-sample&#34; width=&#34;300&#34; height=&#34;169&#34; /&gt;&lt;/a&gt;&lt;/blockquote&gt;
&lt;p&gt;However as we are building our third data center (with MySQL deployments mostly completed) the graph turns more complex.&lt;/p&gt;
&lt;p&gt;Two high availability questions are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What happens when an intermediate master dies? What of all its slaves?&lt;/li&gt;
&lt;li&gt;What happens when the master dies? What of the entire topology?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is not a technical drill down into the solution, but rather on overview of the state. For more, please refer to recent presentations in &lt;a href=&#34;https://speakerdeck.com/shlominoach/managing-and-visualizing-your-replication-topologies-with-orchestrator&#34;&gt;September&lt;/a&gt; and &lt;a href=&#34;https://speakerdeck.com/shlominoach/pseudo-gtid-and-easy-mysql-replication-topology-management&#34;&gt;April&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;At this time we have:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pseudo-GTID deployed on all chains
&lt;ul&gt;
&lt;li&gt;Injected every 5 seconds&lt;/li&gt;
&lt;li&gt;Using the &lt;a href=&#34;http://code.openark.org/blog/mysql/pseudo-gtid-ascending&#34;&gt;monotonically ascending&lt;/a&gt; variation&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Pseudo-GTID based automated failover for intermediate masters on all chains&lt;/li&gt;
&lt;li&gt;Pseudo-GTID based automated failover for masters on roughly 30% of the chains.
&lt;ul&gt;
&lt;li&gt;The rest of 70% of chains are set for manual failover using Pseudo-GTID.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Pseudo-GTID is in particular used for:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Salvaging slaves of a dead intermediate master&lt;/li&gt;
&lt;li&gt;Correctly grouping and connecting slaves of a dead master&lt;/li&gt;
&lt;li&gt;Routine refactoring of topologies. This includes:
&lt;ul&gt;
&lt;li&gt;Manual repointing of slaves for various operations (e.g. offloading slaves from a busy box)&lt;/li&gt;
&lt;li&gt;Automated refactoring (for example, used by our automated upgrading script, which consults with &lt;em&gt;orchestrator&lt;/em&gt;, upgrades, shuffles slaves around, updates intermediate master, suffles back...)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;(In the works), failing over binlog reader apps that audit our binary logs.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;!--more--&gt;Furthermore, Booking.com is also &lt;a href=&#34;https://www.percona.com/live/europe-amsterdam-2015/sessions/binlog-servers-bookingcom&#34;&gt;working on Binlog Servers&lt;/a&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;These take production traffic and offload masters and intermediate masters&lt;/li&gt;
&lt;li&gt;Often co-serve slaves using round-robin VIP, such that failure of one Binlog Server makes for simple slave replication self-recovery.&lt;/li&gt;
&lt;li&gt;Are interleaved alongside standard replication
&lt;ul&gt;
&lt;li&gt;At this time we have no &#34;pure&#34; Binlog Server topology in production; we always have normal intermediate masters and slaves&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;This hybrid state makes for greater complexity:
&lt;ul&gt;
&lt;li&gt;Binlog Servers are not designed to participate in a game of changing masters/intermediate master, unless &lt;a href=&#34;http://jfg-mysql.blogspot.nl/2015/09/abstracting-binlog-servers-and-mysql-master-promotion-wo-reconfiguring-slaves.html&#34;&gt;successors come from their own sub-topology&lt;/a&gt;, which is not the case today.
&lt;ul&gt;
&lt;li&gt;For example, a Binlog Server that replicates directly from the master, cannot be repointed to just any new master.&lt;/li&gt;
&lt;li&gt;But can still hold valuable binary log entries that other slaves may not.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Are not actual MySQL servers, therefore of course cannot be promoted as masters&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Orchestrator&lt;/em&gt; &amp;amp; Pseudo-GTID makes this hybrid topology still resilient:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Orchestrator&lt;/em&gt; understands the limitations on the hybrid topology and can salvage slaves of 1st tier Binlog Servers via Pseudo-GTID&lt;/li&gt;
&lt;li&gt;In the case where the Binlog Servers were the most up to date slaves of a failed master, &lt;em&gt;orchestrator&lt;/em&gt; knows to first move potential candidates under the Binlog Server and then extract them out again.&lt;/li&gt;
&lt;li&gt;At this time Binlog Servers are still unstable. Pseudo-GTID allows us to comfortably test them on a large setup with reduced fear of losing slaves.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Otherwise &lt;em&gt;orchestrator&lt;/em&gt; already understands pure Binlog Server topologies and can do master promotion. When pure binlog servers topologies will be in production &lt;em&gt;orchestrator&lt;/em&gt; will be there to watch over.&lt;/p&gt;
&lt;h3&gt;Summary&lt;/h3&gt;
&lt;p&gt;To date, Pseudo-GTID has high scores in automated failovers of our topologies; &lt;em&gt;orchestrator&#39;s&lt;/em&gt; &lt;a href=&#34;http://code.openark.org/blog/mysql/what-makes-a-mysql-server-failurerecovery-case&#34;&gt;holistic approach&lt;/a&gt; makes for reliable diagnostics; together they reduce our dependency on specific servers &amp;amp; hardware, physical location, latency implied by SAN devices.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Orchestrator &amp; Pseudo-GTID for binlog reader failover</title>
      <link>/blog/mysql/orchestrator-pseudo-gtid-for-binlog-reader-failover/</link>
      <pubDate>Thu, 19 Nov 2015 10:52:16 +0000</pubDate>
      
      <guid>/blog/mysql/orchestrator-pseudo-gtid-for-binlog-reader-failover/</guid>
      <description>&lt;p&gt;One of our internal apps at &lt;strong&gt;Booking.com&lt;/strong&gt; audits changes to our tables on various clusters. We used to use &lt;em&gt;tungsten replicator&lt;/em&gt;, but have since migrated onto our own solution.&lt;/p&gt;
&lt;p&gt;We have a binlog reader (uses &lt;a href=&#34;https://github.com/zendesk/open-replicator&#34;&gt;open-replicator&lt;/a&gt;) running on a slave. It expects Row Based Replication, hence our slave runs with &lt;strong&gt;log-slave-updates&lt;/strong&gt;, &lt;strong&gt;binlog-format=&#39;ROW&#39;&lt;/strong&gt;, to translate from the master&#39;s Statement Based Replication. The binlog reader reads what it needs to read, audits what it needs to audit, and we&#39;re happy.&lt;/p&gt;
&lt;h3&gt;However what happens if that slave dies?&lt;/h3&gt;
&lt;p&gt;In such case we need to be able to point our binlog reader to another slave, and it needs to be able to pick up auditing from the same point.&lt;/p&gt;
&lt;p&gt;This sounds an awful lot like slave repointing in case of master/intermediate master failure, and indeed the solutions are similar. However our binlog reader is not a real MySQL server and does not understands replication. It does not really replicate, it just parses binary logs.&lt;/p&gt;
&lt;p&gt;We&#39;re also not using GTID. But we &lt;em&gt;are&lt;/em&gt; using Pseudo-GTID. As it turns out, the failover solution is already built in by &lt;a href=&#34;https://github.com/outbrain/orchestrator&#34;&gt;orchestrator&lt;/a&gt;, and this is how it goes:&lt;/p&gt;
&lt;h3&gt;Normal execution&lt;/h3&gt;
&lt;p&gt;Our binlog app reads entries from the binary log. Some are of interest for auditing purposes, some are not. An occasional Pseudo-GTID entry is found, and is being stored to ZooKeeper tagged as  &#34;last seen and processed Pseudo-GTID&#34;.&lt;/p&gt;
&lt;h3&gt;Upon slave failure&lt;/h3&gt;
&lt;p&gt;We recognize the death of a slave; we have other slaves in the pool; we pick another. Now we need to find the coordinates from which to carry on.&lt;/p&gt;
&lt;p&gt;We read our &#34;last seen and processed Pseudo-GTID&#34;. Say it reads:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;drop view if exists `meta`.`_pseudo_gtid_hint__asc:56373F17:00000000012B1C8B:50EC77A1`&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;. We now issue:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;$ orchestrator &lt;strong&gt;-c find-binlog-entry&lt;/strong&gt; &lt;strong&gt;-i new.slave.fqdn.com&lt;/strong&gt; --pattern=&#39;drop view if exists `meta`.`_pseudo_gtid_hint__asc:56373F17:00000000012B1C8B:50EC77A1`&#39;&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;The output of such command are the binlog coordinates of that same entry as found in the new slave&#39;s binlogs:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;binlog.000148:43664433&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;Pseudo-GTID entries are only injected once every few seconds (&lt;strong&gt;5&lt;/strong&gt; in our case). Either:&lt;!--more--&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We are OK to reprocess up to &lt;strong&gt;5&lt;/strong&gt; seconds worth of data (and indeed we are, our mechanism is such that this merely overwrites our previous audit, no corruption happens)&lt;/li&gt;
&lt;li&gt;Or our binlog reader also keeps track of the number of events since the last processed Pseudo-GTID entry, skipping the same amount of events after failing over.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Planned failover&lt;/h3&gt;
&lt;p&gt;In case we plan to repoint our binlog reader to another slave, we can further use orchestrator&#39;s power in making an exact correlation between the binlog positions of two slaves. This has always been within its power, but only recently exposed as it own command. We can, at any stage:&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;$ sudo orchestrator &lt;strong&gt;-c correlate-binlog-pos&lt;/strong&gt; -i current.instance.fqdn.com --binlog=binlog.002011:72656109 -d some.other.instance.fqdn.com&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;The output is the binlog coordinates in &lt;strong&gt;some.other.instance.fqdn.com&lt;/strong&gt; that exactly correlate with &lt;strong&gt;binlog.002011:72656109&lt;/strong&gt; in &lt;strong&gt;current.instance.fqdn.com&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The case of failure of the binlog reader itself is also handled, but is not the subject of this blog post.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Thoughts on MaxScale automated failover (and Orchestrator)</title>
      <link>/blog/mysql/thoughts-on-maxscale-automated-failover-and-orchestrator/</link>
      <pubDate>Wed, 18 Nov 2015 11:17:48 +0000</pubDate>
      
      <guid>/blog/mysql/thoughts-on-maxscale-automated-failover-and-orchestrator/</guid>
      <description>&lt;p&gt;Having attended a talk (as part of the &lt;a href=&#34;https://blog.mariadb.org/2015-developers-meeting-amsterdam/&#34;&gt;MariaDB Developer Meeting in Amsterdam&lt;/a&gt;) about recent developments of &lt;a href=&#34;https://mariadb.com/products/mariadb-maxscale&#34;&gt;MaxScale&lt;/a&gt; in executing automated failovers, here are some (late) observations of mine.&lt;/p&gt;
&lt;p&gt;I will begin by noting that the project is stated to be pre-production, and so of course none of the below are complaints, but rather food for thought, points for action and otherwise recommendations.&lt;/p&gt;
&lt;p&gt;Some functionality of the MaxScale failover is also implemented by &lt;strong&gt;&lt;a href=&#34;https://github.com/outbrain/orchestrator&#34;&gt;orchestrator&lt;/a&gt;&lt;/strong&gt;, which I author. &lt;em&gt;Orchestrator&lt;/em&gt; was built in production environments by and for operational people. In this respect it has gained many insights and had to cope with many real-world cases, special cases &amp;amp; Murphy&#39;s law cases. This post compares logic, feature set and capabilities of the two where relevant. To some extent the below will read as &#34;hey, I&#39;ve already implemented this; shame to re-implement the same&#34;, and indeed I think that way; but it wouldn&#39;t be the first time a code of mine would just be re-implemented by someone else and I&#39;ve done the same, myself.&lt;/p&gt;
&lt;p&gt;I&#39;m describing the solution the way I understood it from the talk. If I&#39;m wrong on any account I&#39;m happy to be corrected via comments below. &lt;strong&gt;Edit:&lt;/strong&gt; &lt;em&gt;please see comment by&lt;/em&gt; &lt;a class=&#34;url&#34; href=&#34;http://www.mariadb.com/&#34; rel=&#34;external nofollow&#34;&gt;Dipti Joshi&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;General overview&lt;/h3&gt;
&lt;p&gt;The idea is that MaxScale operates as a proxy to your topology. You do not connect to your master directly, but rather through MaxScale. Thus, MaxScale acts as a proxy to your master.&lt;/p&gt;
&lt;p&gt;The next phase is that MaxScale would also auto-detect master failure, fix the topology for you, promote a new master, and will have your application unaware of all the complexity and without the app having to change setup/DNS/whatever. Of course some write downtime is implied.&lt;/p&gt;
&lt;p&gt;Now for some breakdown.&lt;/p&gt;
&lt;h3&gt;Detection&lt;/h3&gt;
&lt;p&gt;The detection of a dead master, the check by which a failover is initiated, is based on MaxScale not being able to query the master. This calls for some points for consideration:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Typically, I would see &#34;I can&#39;t connect to the master therefore failover&#34; as too hysterical, and the basis for a lot of false positives.&lt;/li&gt;
&lt;li&gt;However, since in the discussed configuration MaxScale is &lt;em&gt;the only access point&lt;/em&gt; to the master, the fact MaxScale cannot connect to the master means the master is inaccessible &lt;em&gt;de-facto&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;In light of the above, the decision makes sense - but I still hold that it would make false positives.&lt;/li&gt;
&lt;li&gt;I&#39;m unsure (I &lt;em&gt;think&lt;/em&gt; not; can anyone comment?) if MaxScale would make multiple attempts over time and only reach the conclusion after X successive failures. This would reduce the false positives.&lt;/li&gt;
&lt;li&gt;I&#39;m having a growing dislike to a &#34;check for 4 successive times then alert/failover&#34; Nagios-style behavior. &lt;em&gt;Orchestrator&lt;/em&gt; takes a different approach where it recognizes a master&#39;s death by not being able to connect to the master &lt;em&gt;as well as&lt;/em&gt; being able to connect to 1st tier slaves, check their status and observe that &lt;em&gt;they&#39;re unable to connect to the master as well&lt;/em&gt;. See &lt;a title=&#34;Permanent Link to What makes a MySQL server failure/recovery case?&#34; href=&#34;http://code.openark.org/blog/mysql/what-makes-a-mysql-server-failurerecovery-case&#34; rel=&#34;bookmark&#34;&gt;What makes a MySQL server failure/recovery case?&lt;/a&gt;. This approach still calls for further refinement (what if the master is temporarily deadlocked? Is this a failover or not?).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;!--more--&gt;&lt;/p&gt;
&lt;h3&gt;Assumed topology&lt;/h3&gt;
&lt;p&gt;MaxScale assumes the topology is all MariaDB, and all slaves are using (MariaDB) GTID replication. Well, MaxScale does not actually assumes that. It is assumed so by the &lt;a href=&#34;https://github.com/mariadb-corporation/replication-manager&#34;&gt;MariaDB Replication Manager&lt;/a&gt; which MaxScale invokes. But I&#39;m getting ahead of myself here.&lt;/p&gt;
&lt;h3&gt;Topology detection&lt;/h3&gt;
&lt;p&gt;MaxScale does not recognize the master by configuration but rather by state. It observes the servers it should observe, and concludes which is the master.&lt;/p&gt;
&lt;p&gt;I&#39;m using similar approach in &lt;em&gt;orchestrator&lt;/em&gt;. I maintain that this approach works well and opens the Chakras for complex recovery options.&lt;/p&gt;
&lt;h3&gt;Upon failure detection&lt;/h3&gt;
&lt;p&gt;When MaxScale detects failure, it invokes external scripts to fix the problem. There are some similar and different particulars here as compared to &lt;em&gt;orchestrator&lt;/em&gt;, and I will explain what&#39;s wrong with the MaxScale approach:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Although MaxScale observes the topology and understands who is the master and who isn&#39;t, the executed scripts do not. They need to re-discover everything by themselves.&lt;/li&gt;
&lt;li&gt;This implies the scripts start without memory of &#34;what was last observed&#34;. This is one of the greatest strengths of &lt;em&gt;orchestrator&lt;/em&gt;: it knows what the state was just before the failure, and, having the bigger picture, can make informed decisions.
&lt;ul&gt;
&lt;li&gt;As a nasty example, what do you do when some the first tier slaves also happen to be inaccessible at that time? What if one of those happens to further have slaves of its own?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;The MariaDB Replication Manager script (to be referenced as &lt;em&gt;repmgr&lt;/em&gt;) assumes all instances to be MariaDB with GTID.
&lt;ul&gt;
&lt;li&gt;It is also implied that all my slaves are configured with binary logs &amp;amp; log-slave-updates&lt;/li&gt;
&lt;li&gt;That&#39;s &lt;strong&gt;way too restrictive&lt;/strong&gt;.
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Orchestrator&lt;/em&gt; handles all following topologies: Oracle MySQL with/out GTID, MariaDB with/out GTID, MariaDB hybrid GTID &amp;amp; non-GTID replication, Pseudo-GTID (MySQL and/or MariaDB), hybrid normal &amp;amp; binlog servers topologies, slaves with/out log-slave-updates, hybrid Oracle &amp;amp; MariaDB &amp;amp; Binlog Servers &amp;amp; Pseudo-GTID.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;em&gt;repmgr&lt;/em&gt; is unaware of data centers &amp;amp; physical environments. You want failover to be as local to your datacenters as possible. Avoid too many cross-DC replication streams.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Failover invocation&lt;/h3&gt;
&lt;p&gt;MaxScale invokes the failover scripts &lt;em&gt;asynchronously&lt;/em&gt;. This is a major flaw imho, as the decoupling between the monitoring and acting processes leads to further problems, see further.&lt;/p&gt;
&lt;h3&gt;After failover&lt;/h3&gt;
&lt;p&gt;MaxScale continuously scans the topology and observes that some other server has been promoted. This behavior is similar to &lt;em&gt;orchestrator&#39;s&lt;/em&gt;. But the following differences are noteworthy:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Because of both the decoupling as well as the asynchronous invocation by MaxScale, it doesn&#39;t really have any idea if and how the promotion resolved.&lt;/li&gt;
&lt;li&gt;I don&#39;t know that there&#39;s any anti-flapping mechanism, nor that there could be. If MaxScale doesn&#39;t care what happened to the failover script, it shouldn&#39;t be able to keep up with flapping scenarios.&lt;/li&gt;
&lt;li&gt;Nor is there a minimal suspend period between any two failure recoveries, that I know of. MaxScale can actually have easier life than &lt;em&gt;orchestrator&lt;/em&gt; in this regard as it is (I suspect) strictly associated with &lt;em&gt;a topology&lt;/em&gt;. Not like there&#39;s a single MaxScale handling multiple topologies. So it should be very easy to keep track of failures.&lt;/li&gt;
&lt;li&gt;Or, if there is a minimal period and I&#39;m just uninformed -- what makes sure it is not smaller than the time it takes for the failover?&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Further on failover&lt;/h3&gt;
&lt;p&gt;I wish to point out that one component of the system analyses a failure scenario, and another one fixes it. I suggest this is an undesired design. The &#34;fixer&#34; must have its own ability to diagnose problems as it makes progress (or else it is naive and would fail in many production cases). And the &#34;analyzer&#34; part must have some wisdom of its own so as to suggest course of action; or understand the consequences of the recovery done by the &#34;fixer&#34;.&lt;/p&gt;
&lt;h3&gt;Use of shell scripts&lt;/h3&gt;
&lt;p&gt;Generally speaking, the use of shell scripts as external hooks is evil:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Shell scripts tend to be poorly audited&lt;/li&gt;
&lt;li&gt;With poor clarity as for what went wrong&lt;/li&gt;
&lt;li&gt;Killing them has operational difficulty (detect the shell script, find possible children, detached children)&lt;/li&gt;
&lt;li&gt;The approach of &#34;if you want something else, just write a shell script for it&#34; is nice for some things, but as the problem turns complex, you turn out to just write big parts of the solution in shell. This decouples your code to unwanted degree.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;At this time, &lt;em&gt;orchestrator&lt;/em&gt; also uses external hooks. However:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Fixing the topology happens within &lt;em&gt;orchestrator&lt;/em&gt;, not by external scripts. There is an elaborate, auditable, visible decision making.
&lt;ul&gt;
&lt;li&gt;Decision making includes data center considerations, different configuration of servers involved, servers hinted as candidates, servers configured to be ignored, servers known to be downtimed.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Leaving the external scripts with the task of managing DNS changes or what have you.
&lt;ul&gt;
&lt;li&gt;Today, at Booking.com, we have a special operational tool (called the dba tool) which does that, manages rosters, issues puppet etc. This tool is itself well audited. Granted, there is still decoupling, but information does not just get lost.&lt;/li&gt;
&lt;li&gt;Sometime in the future I suspect I will extend &lt;strong&gt;&lt;a href=&#34;https://github.com/outbrain/orchestrator-agent&#34;&gt;orchestrator-agent&lt;/a&gt;&lt;/strong&gt; to participate in failovers, which means the entire flow is within &lt;em&gt;orchestrator&#39;s&lt;/em&gt; scope.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;High availability&lt;/h3&gt;
&lt;p&gt;All the above is only available via a single MaxScale server. What happens if it dies?&lt;/p&gt;
&lt;p&gt;There is a MaxScale/pacemaker setup I&#39;m aware of. If one MaxScale dies, pacemaker takes charge and starts another on another box.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;But this means real downtime&lt;/li&gt;
&lt;li&gt;There are no multiple-MaxScale servers to load-balance on&lt;/li&gt;
&lt;li&gt;The MaxScale started by pacemaker is newly born, and does not have the big picture of the topology. It needs to go through a &#34;peaceful time&#34; to understand what&#39;s going on.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;More High Availability&lt;/h3&gt;
&lt;p&gt;At a time where MaxScale will be able to load-balance and run on multiple nodes, MariaDB will have to further tackle:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Leader election&lt;/li&gt;
&lt;li&gt;Avoiding concurrent initiation of failovers
&lt;ul&gt;
&lt;li&gt;Either via group communication&lt;/li&gt;
&lt;li&gt;Or via shared datastore&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Taking off from a failed/crashed MaxScale server&#39;s work
&lt;ul&gt;
&lt;li&gt;Or rolling it back&lt;/li&gt;
&lt;li&gt;Or just cleaning it up&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;And generally share all those little pieces of information, such as &#34;Hey, now this server is the master&#34; (are all MaxScales in complete agreement on the topology?) or &#34;I have failed over this topology, we should avoid failing it over again for the next 10 minutes&#34; and more.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The above are supported by &lt;em&gt;orchestrator&lt;/em&gt;. It provides leader election, automated leader promotion, fair recognition of various failure scenarios, picking up a failed recovery from a failed &lt;em&gt;orchestrator&lt;/em&gt;. Data is shared by a backend MySQL datastore, and before you shout &lt;em&gt;SPOF&lt;/em&gt;, make it Galera/NDB.&lt;/p&gt;
&lt;h3&gt;Further little things that can ruin your day&lt;/h3&gt;
&lt;h4&gt;How about having a delayed replica?&lt;/h4&gt;
&lt;p&gt;Here&#39;s an operational use case we had to tackle.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You have a slave configured to lag by &lt;strong&gt;24&lt;/strong&gt; hours. You know the drill: hackers / accidental &lt;strong&gt;DROP TABLE&lt;/strong&gt;...&lt;/li&gt;
&lt;li&gt;How much time will an automated tool spend on reconnecting this slave to the topology?
&lt;ul&gt;
&lt;li&gt;This could take long minutes&lt;/li&gt;
&lt;li&gt;Will your recovery hang till this is resolved?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Since &lt;em&gt;orchestrator&lt;/em&gt; heals the topology in-house, it knows how to push certain operations till after specific other operations took place. For example, &lt;em&gt;orchestrator&lt;/em&gt; wants to heal the entire topology, but pushes the delayed replicas aside, under the assumption that it will be able to fix them later (fair assumption, because they are known to be behind our promoted master); it will proceed to fix everything else, execute external hooks (change DNS etc.) and only then come back to the delayed replica. All the while, the process is audited.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Flapping ruins your day&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Not only do you want some stall period between two failovers, you also want your team to respond to a failover and acknowledge it. Or clear up the stall period having verified the source of the problem. Or force the next failover even if it comes sooner than the stall period termination.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Binlog formats&lt;/h4&gt;
&lt;p&gt;It is still not uncommon to have Statement Based Replication running. And then it is also not uncommon to have one or two slaves translating to Row Based Replication because of:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Some app that has to read ROW based format&lt;/li&gt;
&lt;li&gt;Experimenting with RBR for purposes of upgrade&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You just can&#39;t promote such a RBR slave on top of SBR slaves; it wouldn&#39;t work. &lt;em&gt;Orchestrator&lt;/em&gt; is aware of such rules. I still need to integrate this particular consideration into the promotion algorithm.&lt;/p&gt;
&lt;h4&gt;Versions&lt;/h4&gt;
&lt;p&gt;Likewise, not all your slaves are of same version. You should not promote a newer version slave on top of an older version slave. Again, &lt;em&gt;orchestrator&lt;/em&gt; will not allow putting such a topology, and again, I still need to integrate this consideration into the promotion algorithm.&lt;/p&gt;
&lt;h3&gt;In summary&lt;/h3&gt;
&lt;p&gt;There is a long way for MaxScale failover to go. When you consider the simplest, all-MariaDB-GTID-equal-slaves small topology case, things are kept simple and probably sustainable. But issues like complex topologies, flapping, special slaves, different configuration, high availability, leadership, acknowledgements, and more, call for a more advanced solution.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>What makes a MySQL server failure/recovery case?</title>
      <link>/blog/mysql/what-makes-a-mysql-server-failurerecovery-case/</link>
      <pubDate>Sat, 25 Jul 2015 09:00:03 +0000</pubDate>
      
      <guid>/blog/mysql/what-makes-a-mysql-server-failurerecovery-case/</guid>
      <description>&lt;p&gt;Or: How do you reach the conclusion your MySQL master/intermediate-master is dead and must be recovered?&lt;/p&gt;
&lt;p&gt;This is an attempt at making a holistic diagnosis of our replication topologies. The aim is to cover obvious and not-so-obvious crash scenarios, and to be able to act accordingly and heal the topology.&lt;/p&gt;
&lt;p&gt;At &lt;strong&gt;Booking.com&lt;/strong&gt; we are dealing with very large amounts of MySQL servers. We have many topologies, and many servers in each topology. &lt;a href=&#34;https://speakerdeck.com/shlominoach/pseudo-gtid-and-easy-mysql-replication-topology-management&#34;&gt;See past numbers&lt;/a&gt; to get a feel for it. At these numbers failures happen frequently. Typically we would see normal slaves failing, but occasionally -- and far more frequently than we would like to be paged for -- an intermediate master or a master would crash. But our current (and ever in transition) setup also include SANs, DNS records, VIPs, any of which can fail and bring down our topologies.&lt;/p&gt;
&lt;p&gt;Tackling issues of monitoring, disaster analysis and recovery processes, I feel safe to claim the following statements:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The fact your monitoring tool cannot access your database does not mean your database has failed.&lt;/li&gt;
&lt;li&gt;The fact your monitoring tool can access your database does not mean your database is available.&lt;/li&gt;
&lt;li&gt;The fact your database master is unwell does not mean you should fail over.&lt;/li&gt;
&lt;li&gt;The fact your database master is alive and well does not mean you should not fail over.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Bummer. Let&#39;s review a simplified topology with a few failure scenarios. Some of these scenarios you will find familiar. Some others may be caused by setups you&#39;re not using. I would love to say &lt;em&gt;I&#39;ve seen it all&lt;/em&gt; but the more I see the more I know how strange things can become.&lt;!--more--&gt;&lt;/p&gt;
&lt;p&gt;We will consider the simplified case of a master with three replicas: we have &lt;strong&gt;M&lt;/strong&gt; as master, &lt;strong&gt;A&lt;/strong&gt;, &lt;strong&gt;B&lt;/strong&gt;, &lt;strong&gt;C&lt;/strong&gt; as slaves.&lt;/p&gt;
&lt;blockquote&gt;&lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2015/07/mysql-topologies-failures.png&#34;&gt;&lt;img class=&#34;alignnone size-full wp-image-7280&#34; src=&#34;/blog/blog/assets/mysql-topologies-failures.png&#34; alt=&#34;mysql-topologies-failures&#34; width=&#34;192&#34; height=&#34;108&#34; /&gt;&lt;/a&gt;&lt;/blockquote&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;p&gt;A common monitoring scheme is to monitor each machine&#39;s IP, availability of MySQL port (&lt;strong&gt;3306&lt;/strong&gt;) and responsiveness to some simple query (e.g. &lt;strong&gt;&#34;SELECT 1&#34;&lt;/strong&gt;). Some of these checks may run local to the machine, others remote.&lt;/p&gt;
&lt;p&gt;Now consider your monitoring tool fails to connect to your master.&lt;/p&gt;
&lt;blockquote&gt;&lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2015/07/mysql-topologies-failures-1.png&#34;&gt;&lt;img class=&#34;alignnone size-full wp-image-7281&#34; src=&#34;/blog/blog/assets/mysql-topologies-failures-1.png&#34; alt=&#34;mysql-topologies-failures (1)&#34; width=&#34;192&#34; height=&#34;108&#34; /&gt;&lt;/a&gt;&lt;/blockquote&gt;
&lt;p&gt;I&#39;ve marked the slaves with question marks as the common monitoring schema does not associate the master&#39;s monitoring result to the slaves&#39;.  Can you safely conclude your master is dead? Are your feeling comfortable with initiating a failover process? How about:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Temporary network partitioning; it just so happens that your monitoring tool cannot access the master, though everyone else can.&lt;/li&gt;
&lt;li&gt;DNS/VIP/name cache/name resolving issue. Sometimes similar to the above; does you monitoring tool host think the master&#39;s IP is what it really is? Has something just changed? Some cache expired? Some cache is stale?&lt;/li&gt;
&lt;li&gt;MySQL connection rejection. This could be due to a serious &#34;Too many connections&#34; problem on the master, or due to accidental network noise.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now consider the following case: a first tier slave is failing to connect to the master:&lt;/p&gt;
&lt;blockquote&gt;&lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2015/07/mysql-topologies-failures-2.png&#34;&gt;&lt;img class=&#34;alignnone size-full wp-image-7282&#34; src=&#34;/blog/blog/assets/mysql-topologies-failures-2.png&#34; alt=&#34;mysql-topologies-failures (2)&#34; width=&#34;192&#34; height=&#34;108&#34; /&gt;&lt;/a&gt;&lt;/blockquote&gt;
&lt;p&gt;The slave&#39;s IO thread is broken; do we have a problem here? Is the slave failing to connect because the master is dead, or because the slave itself suffers from a network partitioning glitch?&lt;/p&gt;
&lt;h3&gt;A holistic diagnosis&lt;/h3&gt;
&lt;p&gt;In the holistic approach we couple the master&#39;s monitoring with that of its direct slaves. Before I continue to describe some logic, the previous statement is something we must reflect upon.&lt;/p&gt;
&lt;p&gt;We should associate the master&#39;s state with that of its direct slaves. Hence we must know which are its direct slaves. We might have slaves D, E, F, G replicating from B, C. They are not in our story. But slaves come and go. Get provisioned and de-provisioned. They get repointed elsewhere. Our monitoring needs to be aware of the &lt;em&gt;state&lt;/em&gt; of our replication topology.&lt;/p&gt;
&lt;p&gt;My preferred tool for the job is &lt;a href=&#34;https://github.com/outbrain/orchestrator/&#34;&gt;orchestrator&lt;/a&gt;, since I author it. It is not a standard monitoring tool and does not serve metrics; but it observes your topologies and records them. And notes changes. And acts as a higher level failure detection mechanism which incorporates the logic described below.&lt;/p&gt;
&lt;p&gt;We continue our discussion under the assumption we are able to reliably claim we know our replication topology. Let&#39;s revisit our scenarios from above and then add some.&lt;/p&gt;
&lt;p&gt;We will further only require MySQL client protocol connection to our database servers.&lt;/p&gt;
&lt;h3&gt;Dead master&lt;/h3&gt;
&lt;p&gt;A &#34;real&#34; dead master is perhaps the clearest failure. MySQL has crashed (signal 11); or the kernel panicked; or the disks failed; or power went off. The server is &lt;em&gt;really not serving&lt;/em&gt;. This is observed as:&lt;/p&gt;
&lt;blockquote&gt;&lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2015/07/mysql-topologies-failures-3.png&#34;&gt;&lt;img class=&#34;alignnone size-full wp-image-7284&#34; src=&#34;/blog/blog/assets/mysql-topologies-failures-3.png&#34; alt=&#34;mysql-topologies-failures (3)&#34; width=&#34;192&#34; height=&#34;108&#34; /&gt;&lt;/a&gt;&lt;/blockquote&gt;
&lt;p&gt;In the holistic approach, we observe that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We cannot reach the master (our MySQL client connection fails).&lt;/li&gt;
&lt;li&gt;But we are able to connect to the slaves A, B, C&lt;/li&gt;
&lt;li&gt;And A, B, C &lt;em&gt;are all telling us&lt;/em&gt; they cannot connect to the master&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We have now cross referenced the death of the master with its three slaves. Funny thing is the MySQL server on the master may still be up and running. Perhaps the master is suffering from some weird network partitioning problem (when I say &#34;weird&#34;, I mean we have it; discussed further below). And &lt;em&gt;perhaps&lt;/em&gt; some application is actually still able to talk to the master!&lt;/p&gt;
&lt;p&gt;And yet our entire replication topology is broken. Replication is not there for beauty; it serves our application code. And it&#39;s turning stale. Even if by some chance things are still operating on the master, this still makes for a valid failover scenario.&lt;/p&gt;
&lt;h3&gt;Unreachable master&lt;/h3&gt;
&lt;p&gt;Compare the above with:&lt;/p&gt;
&lt;blockquote&gt;&lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2015/07/mysql-topologies-failures-4.png&#34;&gt;&lt;img class=&#34;alignnone size-full wp-image-7285&#34; src=&#34;/blog/blog/assets/mysql-topologies-failures-4.png&#34; alt=&#34;mysql-topologies-failures (4)&#34; width=&#34;192&#34; height=&#34;108&#34; /&gt;&lt;/a&gt;&lt;/blockquote&gt;
&lt;p&gt;Our monitoring scheme cannot reach our master. But it can reach the slaves, an they&#39;re all saying: &lt;em&gt;&#34;I&#39;m happy!&#34;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This gives us suspicion enough to avoid failing over. We may not actually have a problem: it&#39;s just &lt;em&gt;us&lt;/em&gt; that are unable to connect to the master.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Right?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;There are still interesting use cases. Consider the problem of &lt;strong&gt;&#34;Too many connections&#34;&lt;/strong&gt; on the master. You are unable to connect; the application starts throwing errors; but the slaves are happy. They were there first. They started replicating at the dawn of time, long before there was an issue. Their persistent connections are good to go.&lt;/p&gt;
&lt;p&gt;Or the master may suffer a deadlock. A long, blocking &lt;strong&gt;ALTER TABLE&lt;/strong&gt;. An accidental &lt;strong&gt;FLUSH TABLES WITH READ LOCK&lt;/strong&gt;. Or whatever occasional bug we hit. Slaves are still connected; but new connections are hanging; and your monitoring query is unable to process.&lt;/p&gt;
&lt;p&gt;And still our holistic approach can find that out: as we are able to connect to our slaves, we are also able to ask them: well what have your relay logs have to say about this? Are we progressing in replication position? Do we actually find application content in the slaves&#39; relay logs? We can do all this via MySQL protocol (&lt;strong&gt;&#34;SHOW SLAVE STATUS&#34;&lt;/strong&gt;, &lt;strong&gt;&#34;SHOW RELAYLOG EVENTS&#34;&lt;/strong&gt;).&lt;/p&gt;
&lt;p&gt;Understanding the topology gives you greater insight into your failure case; you have increasing leevels of confidentiality in your analysis. Strike that: in your &lt;em&gt;automated&lt;/em&gt; analysis.&lt;/p&gt;
&lt;h3&gt;Dead master and slaves&lt;/h3&gt;
&lt;p&gt;They&#39;re all &lt;em&gt;gone&lt;/em&gt;!&lt;/p&gt;
&lt;blockquote&gt;&lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2015/07/mysql-topologies-failures-5.png&#34;&gt;&lt;img class=&#34;alignnone size-full wp-image-7287&#34; src=&#34;/blog/blog/assets/mysql-topologies-failures-5.png&#34; alt=&#34;mysql-topologies-failures (5)&#34; width=&#34;192&#34; height=&#34;108&#34; /&gt;&lt;/a&gt;&lt;/blockquote&gt;
&lt;p&gt;You cannot reach the master &lt;em&gt;and&lt;/em&gt; you cannot reach any of its slaves. Once you are able to associate your master and slaves you can conclude you either have a complete DC power failure problem (or is this cross DC?) or you are having a network partitioning problem. Your application may or may not be affected -- but at least you know where to start. Compare with:&lt;/p&gt;
&lt;h3&gt;Failed DC&lt;/h3&gt;
&lt;blockquote&gt;&lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2015/07/mysql-topologies-failures-6.png&#34;&gt;&lt;img class=&#34;alignnone size-full wp-image-7289&#34; src=&#34;/blog/blog/assets/mysql-topologies-failures-6.png&#34; alt=&#34;mysql-topologies-failures (6)&#34; width=&#34;192&#34; height=&#34;108&#34; /&gt;&lt;/a&gt;&lt;/blockquote&gt;
&lt;p&gt;I&#39;m stretching it now, because when a DC fails all the red lights start flashing. Nonetheless, if M, A, B are all in one DC and C is on another, you have yet another diagnosis.&lt;/p&gt;
&lt;h3&gt;Dead master and some slaves&lt;/h3&gt;
&lt;blockquote&gt;&lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2015/07/mysql-topologies-failures-7.png&#34;&gt;&lt;img class=&#34;alignnone size-full wp-image-7290&#34; src=&#34;/blog/blog/assets/mysql-topologies-failures-7.png&#34; alt=&#34;mysql-topologies-failures (7)&#34; width=&#34;192&#34; height=&#34;108&#34; /&gt;&lt;/a&gt;&lt;/blockquote&gt;
&lt;p&gt;Things start getting complicated when you&#39;re unable to get an authorized answer from everyone. What happens if the master is dead as well as one of its slaves? We previously expected all slaves to say &#34;we cannot replicate&#34;. For us, master being unreachable, some slaves being dead and all other complaining on IO thread is good enough indication that the master is dead.&lt;/p&gt;
&lt;h3&gt;All first tier slaves not replicating&lt;/h3&gt;
&lt;blockquote&gt;&lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2015/07/mysql-topologies-failures-9.png&#34;&gt;&lt;img class=&#34;alignnone size-full wp-image-7293&#34; src=&#34;/blog/blog/assets/mysql-topologies-failures-9.png&#34; alt=&#34;mysql-topologies-failures (9)&#34; width=&#34;192&#34; height=&#34;108&#34; /&gt;&lt;/a&gt;&lt;/blockquote&gt;
&lt;p&gt;Not a failover case, but certainly needs to ring the bells. All master&#39;s direct slaves are failing replication on some SQL error or are just stopped. Our topology is turning stale.&lt;/p&gt;
&lt;h3&gt;Intermediate masters&lt;/h3&gt;
&lt;p&gt;With intermediate master the situation is not all that different. In the below:&lt;/p&gt;
&lt;blockquote&gt;&lt;a href=&#34;http://code.openark.org/blog/wp-content/uploads/2015/07/Untitled-presentation.png&#34;&gt;&lt;img class=&#34;alignnone size-full wp-image-7294&#34; src=&#34;/blog/blog/assets/Untitled-presentation.png&#34; alt=&#34;Untitled presentation&#34; width=&#34;480&#34; height=&#34;270&#34; /&gt;&lt;/a&gt;&lt;/blockquote&gt;
&lt;p&gt;The servers &lt;strong&gt;E&lt;/strong&gt;, &lt;strong&gt;F&lt;/strong&gt;, &lt;strong&gt;G&lt;/strong&gt; replicating from &lt;strong&gt;C&lt;/strong&gt; provide us with the holistic view on &lt;strong&gt;C&lt;/strong&gt;. &lt;strong&gt;D&lt;/strong&gt; provides the holistic view on &lt;strong&gt;A&lt;/strong&gt;.&lt;/p&gt;
&lt;h3&gt;Reducing noise&lt;/h3&gt;
&lt;p&gt;Intermediate master failover is a much simpler operation than master failover. Changing masters require name resolve changes (of some sort), whereas moving slaves around the topology affects no one.&lt;/p&gt;
&lt;p&gt;This implies:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We don&#39;t mind over-reacting on failing over intermediate masters&lt;/li&gt;
&lt;li&gt;We pay with more noise&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Sure, we don&#39;t mind failing over &lt;strong&gt;D&lt;/strong&gt; elsewhere, but as &lt;strong&gt;D&lt;/strong&gt; is the only slave of &lt;strong&gt;A&lt;/strong&gt;, it&#39;s enough that &lt;strong&gt;D&lt;/strong&gt; hiccups that we might get an alert (&#34;all&#34; intermediate master&#39;s slaves are not replicating). To that effect &lt;em&gt;orchestrator&lt;/em&gt; treats single slave scenarios differently than multiple slaves scenarios.&lt;/p&gt;
&lt;h3&gt;Not so fun setups and failures&lt;/h3&gt;
&lt;p&gt;At Booking.com we are in transition between setups. We have some legacy configuration, we have a roadmap, two ongoing solutions, some experimental setups, and/or all of the above combined. Sorry.&lt;/p&gt;
&lt;p&gt;Some of our masters are on SAN. We are moving away from this; for those masters on SANs we have cold standbys in an active-passive mode; so master failure -&amp;gt; unmount SAN -&amp;gt; mount SAN on cold standby -&amp;gt; start MySQL on cold standby -&amp;gt; start recovery -&amp;gt; watch some TV -&amp;gt; go shopping -&amp;gt; end recovery.&lt;/p&gt;
&lt;p&gt;Only SANs fail, too. When the master fails, switching over to the cold standby is pointless if the origin of the problem is the SAN. And given that some &lt;em&gt;other&lt;/em&gt; masters share the same SAN... whoa. As I said we&#39;re moving away from this setup for Pseudo GTID and then for Binlog Servers.&lt;/p&gt;
&lt;p&gt;The SAN setup also implied using VIPs for some servers. The slaves reference the SAN master via VIP, and when the cold standby start up it assumes the VIP, and the slaves know nothing about this. Same setup goes for DC masters. What happens when the VIP goes down? MySQL is running happily, but slaves are unable to connect. Does that make for a failover scenario? For intermediate masters we&#39;re pushing it to be so, failing over to a normal local-disk based server; this improves out confidence in non-SAN setups (which we have plenty of, anyhow).&lt;/p&gt;
&lt;h3&gt;Double checking&lt;/h3&gt;
&lt;p&gt;You sample your server once every X seconds. But in a failover scenario you want to make sure your data is up to date. When &lt;em&gt;orchestrator&lt;/em&gt; suspects a dead master (i.e. cannot reach the master) it immediately contacts its direct slaves and checks their status.&lt;/p&gt;
&lt;p&gt;Likewise, when &lt;em&gt;orchestrator&lt;/em&gt; sees a first tier slave with broken IO thread, it immediately contacts the master to check if everything is fine.&lt;/p&gt;
&lt;p&gt;For intermediate masters &lt;em&gt;orchestrator&lt;/em&gt; is not so concerned and does not issue emergency checks.&lt;/p&gt;
&lt;h3&gt;How to fail over&lt;/h3&gt;
&lt;p&gt;Different story. Some other time. But failing over makes for complex decisions, based on who the replicating slaves are; with/out log-slave-updates; with-out GTID; with/out Pseudo-GTID; are binlog servers available; which slaves are available in which data centers. Or you may be using Galera (we&#39;re not) which answers most of the above.&lt;/p&gt;
&lt;p&gt;Anyway we use &lt;em&gt;orchestrator&lt;/em&gt; for that; it knows our topologies, knows how they should look like, understands how to heal them, knows MySQL replication rules, and invokes external processes to do the stuff it doesn&#39;t understand.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>