<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	
	xmlns:georss="http://www.georss.org/georss"
	xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#"
	>

<channel>
	<title>MySQL &#8211; code.openark.org</title>
	<atom:link href="https://shlomi-noach.github.io/blog/category/mysql/feed" rel="self" type="application/rss+xml" />
	<link>http://shlomi-noach.github.io/blog/</link>
	<description>Blog by Shlomi Noach</description>
	<lastBuildDate>Tue, 26 May 2020 17:52:52 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=5.3.3</generator>
<site xmlns="com-wordpress:feed-additions:1">32412571</site>	<item>
		<title>orchestrator on DB AMA: show notes</title>
		<link>https://shlomi-noach.github.io/blog/mysql/orchestrator-on-db-ama-show-notes</link>
				<comments>https://shlomi-noach.github.io/blog/mysql/orchestrator-on-db-ama-show-notes#respond</comments>
				<pubDate>Tue, 26 May 2020 17:52:52 +0000</pubDate>
		<dc:creator><![CDATA[shlomi]]></dc:creator>
				<category><![CDATA[MySQL]]></category>
		<category><![CDATA[Open Source]]></category>
		<category><![CDATA[orchestrator]]></category>

		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=8101</guid>
				<description><![CDATA[Earlier today I presented orchestrator on DB AMA. Thank you to the organizers Morgan Tocker, Liz van Dijk and Frédéric Descamps for hosting me, and thank you to all who participated! This was a no-slides, all command-line walkthrough of some of orchestrator&#8216;s capabilities, highlighting refactoring, topology analysis, takeovers and failovers, and discussing a bit of [&#8230;]]]></description>
								<content:encoded><![CDATA[<p>Earlier today I presented <a href="https://github.com/openark/orchestrator">orchestrator</a> on <a href="https://dbama.now.sh/">DB AMA</a>. Thank you to the organizers Morgan Tocker, Liz van Dijk and Frédéric Descamps for hosting me, and thank you to all who participated!</p>
<p>This was a no-slides, all command-line walkthrough of some of <code>orchestrator</code>&#8216;s capabilities, highlighting refactoring, topology analysis, takeovers and failovers, and discussing a bit of scripting and HTTP API tips.</p>
<p>The recording is available <a href="https://www.youtube.com/watch?v=UngtSlZ1iTQ&amp;feature=emb_logo">on YouTube</a> (also embedded on <a href="https://dbama.now.sh/#history">https://dbama.now.sh/#history</a>).</p>
<p>To present <code>orchestrator</code>, I used the new shiny docker CI environment; it&#8217;s a single docker image running <code>orchestrator</code>, a 4-node MySQL replication topology (courtesy <a href="https://www.dbdeployer.com/">dbdeployer</a>), heartbeat injection, <code>Consul</code>, <code>consul-template</code> and <code>HAProxy</code>. You can run it, too! Just clone the <code>orchestrator</code> repo, then run:</p>
<pre>./script/dock system</pre>
<p>From there, you may follow the same playbook I used in the presentation, available as <a href="https://gist.github.com/shlomi-noach/28986cb30f0e14d51594f0bc741b464c">orchestrator-demo-playbook.sh</a>.</p>
<p>Hope you find the presentation and the playbook to be useful resources.</p>
]]></content:encoded>
							<wfw:commentRss>https://shlomi-noach.github.io/blog/mysql/orchestrator-on-db-ama-show-notes/feed</wfw:commentRss>
		<slash:comments>0</slash:comments>
						<post-id xmlns="com-wordpress:feed-additions:1">8101</post-id>	</item>
		<item>
		<title>orchestrator: what&#8217;s new in CI, testing &#038; development</title>
		<link>https://shlomi-noach.github.io/blog/mysql/orchestrator-whats-new-in-ci-testing-development</link>
				<comments>https://shlomi-noach.github.io/blog/mysql/orchestrator-whats-new-in-ci-testing-development#respond</comments>
				<pubDate>Mon, 11 May 2020 08:01:08 +0000</pubDate>
		<dc:creator><![CDATA[shlomi]]></dc:creator>
				<category><![CDATA[MySQL]]></category>
		<category><![CDATA[dbdeployer]]></category>
		<category><![CDATA[docker]]></category>
		<category><![CDATA[GitHub]]></category>
		<category><![CDATA[Open Source]]></category>
		<category><![CDATA[orchestrator]]></category>
		<category><![CDATA[testing]]></category>

		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=8077</guid>
				<description><![CDATA[Recent focus on development &#38; testing yielded with new orchestrator environments and offerings for developers and with increased reliability and trust. This post illustrates the new changes, and see Developers section on the official documentation for more details. Testing In the past four years orchestrator was developed at GitHub, and using GitHub&#8217;s environments for testing. [&#8230;]]]></description>
								<content:encoded><![CDATA[<p>Recent focus on development &amp; testing yielded with new <a href="https://github.com/openark/orchestrator">orchestrator</a> environments and offerings for developers and with increased reliability and trust. This post illustrates the new changes, and see <a href="https://github.com/openark/orchestrator/tree/master/docs#developers">Developers</a> section on the official documentation for more details.</p>
<h2>Testing</h2>
<p>In the past four years <code>orchestrator</code> was <a href="https://github.blog/2016-12-08-orchestrator-github/">developed at GitHub</a>, and using GitHub&#8217;s environments for <a href="https://github.blog/2017-07-06-mysql-testing-automation-at-github/">testing</a>. This is very useful for testing <code>orchestrator</code>&#8216;s behavior within GitHub, interacting with its internal infrastructure, and validating failover behavior in a production environment. These tests and their results are not visible to the public, though.</p>
<p>Now that <code>orchestrator</code> is developed <a href="https://shlomi-noach.github.io/blog/mysql/the-state-of-orchestrator-2020-spoiler-healthy">outside GitHub</a> (that is, outside GitHub the <em>company</em>, not GitHub the <em>platform</em>) I wanted to improve on the testing framework, making it visible, accessible and contribute-able to the community. Thankfully, the GitHub platform has much to offer on that front and <code>orchestrator</code> now uses <a href="https://github.com/features/actions">GitHub Actions</a> more heavily for testing.</p>
<p>GitHub Actions provide a way to run code in a container in the context of the repository. The most common use case is to run CI tests on receiving a Pull Request. Indeed, when GitHub Actions became available, we switched out of Travis CI and into Actions for <code>orchestrator</code>&#8216;s CI.</p>
<p>Today, <code>orchestrator</code> runs three different tests:</p>
<ul>
<li>Build, unit testing, integration testing, code &amp; doc validation</li>
<li>Upgrade testing</li>
<li>System testing</li>
</ul>
<p>To highlight what each does:<span id="more-8077"></span></p>
<h3>Build, unit testing, integration testing</h3>
<p>Based on the original CI (and possibly will split into distinct tests), this CI Action compiles the code, runs unit tests, runs the suite of <a href="https://github.com/openark/orchestrator/tree/master/tests/integration">integration tests</a> (spins up both <code>MySQL</code> and <code>SQLite</code> databases and runs a series of tests on each backend), this CI job is the &#8220;basic&#8221; test to see that the contributed code even makes sense.</p>
<p>What&#8217;s new in this test is that it now produces an <em>artifact</em>: an <code>orchestrator</code> binary for Linux/amd64. This is again a feature for GitHub Actions; the artifact is kept for a couple months or so per Actions retention policy. <a href="https://github.com/openark/orchestrator/actions/runs/94337568">Here</a>&#8216;s an example; by the time you read this the binary artifact may or may not still be there.</p>
<p>This means you don&#8217;t actually need a development environment on your laptop to be able to build and <code>orchestrator</code> binary. More on this later.</p>
<h3>Upgrade testing</h3>
<p>Until recently not formalized; I&#8217;d test upgrades by deploying them internally at GitHub onto a staging environment. Now upgrades are tested per Pull Request: we spin up a container, deploy <code>orchestrator</code> from <code>master</code> branch using both <code>MySQL</code> and <code>SQLite</code> backends, then checkout the PR branch, and redeploy <code>orchestrator</code> using the existing backends &#8212; this verifies that at least backend-database wise, there&#8217;s not upgrade errors.</p>
<p>At this time the test only validates the database changes are applicable; in the future this may expand onto more elaborate tests.</p>
<h3>System testing</h3>
<p>I&#8217;m most excited about this one. Taking ideas from our approach to <a href="https://shlomi-noach.github.io/blog/mysql/using-dbdeployer-in-ci-tests">testing gh-ost with dbdeployer</a>, I created <a href="https://github.com/openark/orchestrator-ci-env">https://github.com/openark/orchestrator-ci-env</a>, which offers a full blown testing enviroment for <code>orchestrator</code>, including a MySQL replication topology (courtesy <a href="https://www.dbdeployer.com/">dbdeployer</a>), Consul, HAProxy and more.</p>
<p>This CI testing environment can also serve as a playground in your local docker setup, see shortly.</p>
<p>The <a href="https://github.com/openark/orchestrator/tree/master/tests/system">system tests suite</a> offers full blown cluster-wide operations such as graceful takeovers, master failovers, errant GTID transaction analysis and recovery and more. The suite utilizes the CI testing environment, breaks it, rebuilds it, validates it&#8230; Expects specific output, expects specific failure messages, specific analysis, specific outcomes.</p>
<p>As example, with the system tests suite, we can test the behavior of a master failover in a multi-DC, multi-region (obviously simulated) environment, where a server marked as &#8220;candidate&#8221; is lagging behind all others, with strict rules for cross-site/cross-region failovers, and still we wish to see that particular replica get promoted as master. We can test not only the topology aspect of the failover, but also the failover hooks, Consul integration and its effects, etc.</p>
<h2>Development</h2>
<p>There&#8217;s now multiple options for developers/contributors to build or just try out <code>orchestrator</code>.</p>
<h3>Build on GitHub</h3>
<p>As mentioned earlier, you actually don&#8217;t need a development environment. You can use <code>orchestrator</code> CI to build and generate a Linux/amd64 <code>orchestrator</code> binary, which you can download &amp; deploy as you see fit.</p>
<p>I&#8217;ve signed up for the GitHub Codespaces beta program, and hope to make that available for <code>orchestrator</code>, as well.</p>
<h3>Build via Docker</h3>
<p><code>orchestrator</code> offers various Docker build/run environments, accessible via the <code>script/dock</code> script:</p>
<ul>
<li>`script/dock alpine` will build and spawn `orchestrator` on a minimal <code>alpine</code> linux</li>
<li>`script/dock test` will build and run the same CI tests (unit, integration) as mentioned earlier, but on your own docker environemtn</li>
<li>`script/dock pkg` will build and generate `.rpm` and `.deb` packages</li>
</ul>
<h3>CI environment: the &#8220;full orchestrator experience&#8221;</h3>
<p>This is the <code>orchestrator</code> amusement park. Run <code>script/dock system</code> to spawn the aforementioned CI environment used in system tests, and on top of that, an <code>orchestrator</code> setup fully integrated with that system.</p>
<p>So that&#8217;s an <code>orchestrator</code>-MySQL topology-Consul-HAProxy setup, where <code>orchestrator</code> already has the credentials for, and pre-loads the MySQL topology, pre-configured to update Consul upon failover, HAProxy config populated by <code>consul-template</code>, heartbeat injection, and more. It resembles the <a href="https://github.blog/2018-06-20-mysql-high-availability-at-github/">HA setup at GitHub</a>, and in the future I expect to provide alternate setups (on top).</p>
<p>Once in that docker environment, one can try running relocations, failovers, test <code>orchestrator</code>&#8216;s behavior, etc.</p>
<h2>Community</h2>
<p>GitHub recently announced <a href="https://github.blog/2020-05-06-new-from-satellite-2020-github-codespaces-github-discussions-securing-code-in-private-repositories-and-more/#discussions">GitHub Discussions</a> ; think a stackoverflow like place within one&#8217;s repo to ask questions, discuss, vote on answers. It&#8217;s expected to be available this summer. When it does, I&#8217;ll encourage the community to use it instead of today&#8217;s <a href="https://groups.google.com/forum/#!forum/orchestrator-mysql">orchestrator-mysql</a> Google Group and of course the many questions posted as Issues.</p>
<p>There&#8217;s been a bunch of PRs merged recently, with more to come later on. I&#8217;m grateful for all contributions. Please understand if I&#8217;m still slow to respond.</p>
]]></content:encoded>
							<wfw:commentRss>https://shlomi-noach.github.io/blog/mysql/orchestrator-whats-new-in-ci-testing-development/feed</wfw:commentRss>
		<slash:comments>0</slash:comments>
						<post-id xmlns="com-wordpress:feed-additions:1">8077</post-id>	</item>
		<item>
		<title>Pulling this blog out of Planet MySQL aggregator, over community concerns</title>
		<link>https://shlomi-noach.github.io/blog/mysql/pulling-this-blog-out-of-planet-mysql-aggregator-over-community-concerns</link>
				<comments>https://shlomi-noach.github.io/blog/mysql/pulling-this-blog-out-of-planet-mysql-aggregator-over-community-concerns#respond</comments>
				<pubDate>Thu, 23 Apr 2020 15:26:24 +0000</pubDate>
		<dc:creator><![CDATA[shlomi]]></dc:creator>
				<category><![CDATA[MySQL]]></category>
		<category><![CDATA[community]]></category>
		<category><![CDATA[orchestrator]]></category>
		<category><![CDATA[Planet]]></category>

		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=8020</guid>
				<description><![CDATA[I&#8217;ve decided to pull this blog (https://shlomi-noach.github.io/blog/) out of the planet.mysql.com aggregator. planet.mysql.com (formerly planetmysql.com) serves as a blog aggregator, and collects news and blog posts on various MySQL and its ecosystem topics. It collects some vendor and team blogs as well as &#8220;indie&#8221; blogs such as this one. It has traditionally been the go-to [&#8230;]]]></description>
								<content:encoded><![CDATA[<p>I&#8217;ve decided to pull this blog (<a href="https://shlomi-noach.github.io/blog/">https://shlomi-noach.github.io/blog/</a>) out of the <strong>planet.mysql.com</strong> aggregator.</p>
<p><strong>planet.mysql.com</strong> (formerly <strong>planetmysql.com</strong>) serves as a blog aggregator, and collects news and blog posts on various MySQL and its ecosystem topics. It collects some vendor and team blogs as well as &#8220;indie&#8221; blogs such as this one.</p>
<p>It has traditionally been the go-to place to catch up on the latest developments, or to read insightful posts. This blog itself has been aggregated in Planet MySQL for some eleven years.</p>
<p>Planet MySQL used to be owned by the MySQL community team. This recently changed with unwelcoming implications for the community.</p>
<p>I recently noticed how a blog post of mine, <a title="Link to The state of Orchestrator, 2020 (spoiler: healthy)" href="https://shlomi-noach.github.io/blog/mysql/the-state-of-orchestrator-2020-spoiler-healthy" rel="bookmark">The state of Orchestrator, 2020 (spoiler: healthy)</a>, did not get aggregated in Planet MySQL. After a quick discussion and investigation, it was determined (and confirmed) it was filtered out because it contained the word &#8220;MariaDB&#8221;. It has later been confirmed that Planet MySQL now filters out posts indicating its competitors, such as MariaDB, PostgreSQL, MongoDB.</p>
<p>Planet MySQL is owned by Oracle and it is their decision to make. Yes, logic implies they would not want to publish a promotional post for a competitor. However, I wish to explain how this blind filtering negatively affects the community.</p>
<p>But, before that, I&#8217;d like to share that I first attempted to reach out to whoever is in charge of Planet MySQL at this time (my understanding is that this is a marketing team). Sadly, two attempts at reaching out to them individually, and another attempt at reaching out on behalf of a small group of individual contributors, yielded no response. The owners would not have audience with me, and would not hear me out. I find it disappointing and will let others draw morals.</p>
<h3>Why filtering is harmful for the community</h3>
<p>We recognize that <strong>planet.mysql.com</strong> is an important information feed. It is responsible for a massive ratio of the traffic on my blog, and no doubt for many others. Indie blog posts, or small-team blog posts, practically depend on <strong>planet.mysql.com</strong> to get visibility.</p>
<p>And this is particularly important if you&#8217;re an open source developer who is trying to promote an open source project in the MySQL ecosystem. Without this aggregation, you will get significantly less visibility.</p>
<p>But, open source projects in the MySQL ecosystem do not live in MySQL vacuum, and typically target/support MySQL, Percona Server and MariaDB. As examples:</p>
<ul>
<li><a href="https://www.dbdeployer.com/">DBDeployer</a> should understand MariaDB versioning scheme</p>
</li>
<li>
<p><a href="https://www.skeema.io/">skeema</a> needs to recognize MariaDB features not present in MySQL</p>
</li>
<li>
<p><a href="https://proxysql.com/">ProxySQL</a> needs to support MariaDB Galera queries</p>
</li>
<li>
<p><a href="https://github.com/openark/orchestrator">orchestrator</a> needs to support MariaDB&#8217;s GTID flavor</p>
</li>
</ul>
<p>Consider that a blog post of the form &#8220;Project version 1.2.3 now released!&#8221; is likely to mention things like &#8220;fixed MariaDB GTID setup&#8221; or &#8220;MariaDB 10.x now supported&#8221; etc. Consider just pointing out that &#8220;PROJECT X supports MySQL, MariaDB and Percona Server&#8221;.</p>
<p>Consider that merely mentioning &#8220;MariaDB&#8221; gets your blog post filtered out on <strong>planet.mysql.com</strong>. This has an actual impact on open source development in the MySQL ecosystem. We will lose audience and lose adoption.</p>
<p>I believe the MySQL ecosystem as a whole will be negatively affected as result, and this will circle back to MySQL itself. I believe this goes against the very interests of Oracle/MySQL.</p>
<p>I&#8217;ve been around the MySQL community for some 12 years now. From my observation, there is no doubt that MySQL would not thrive as it does today, without the tooling, blogs, presentations and general advice by the community.</p>
<p>This is more than an estimation. I happen to know that, internally at MySQL, they have used or are using open source projects from the community, projects whose blog posts get filtered out today because they mention &#8220;MariaDB&#8221;. I find that disappointing.</p>
<p>I have personally witnessed how open source developments broke existing barriers to enable companies to use MySQL at greater scale, in greater velocity, with greater stability. I was part of such companies and I&#8217;ve personally authored such tools. I&#8217;m disappointed that <strong>planet.mysql.com</strong> filters out my blog posts for those tools and without giving me audience, and extend my disappointment for all open source project maintainers.</p>
<p>At this time I consider <strong>planet.mysql.com</strong> to be a marketing blog, not a community feed, and do not want to participate in its biased aggregation.</p>
]]></content:encoded>
							<wfw:commentRss>https://shlomi-noach.github.io/blog/mysql/pulling-this-blog-out-of-planet-mysql-aggregator-over-community-concerns/feed</wfw:commentRss>
		<slash:comments>0</slash:comments>
						<post-id xmlns="com-wordpress:feed-additions:1">8020</post-id>	</item>
		<item>
		<title>The state of Orchestrator, 2020 (spoiler: healthy)</title>
		<link>https://shlomi-noach.github.io/blog/mysql/the-state-of-orchestrator-2020-spoiler-healthy-2</link>
				<comments>https://shlomi-noach.github.io/blog/mysql/the-state-of-orchestrator-2020-spoiler-healthy-2#respond</comments>
				<pubDate>Tue, 18 Feb 2020 19:14:12 +0000</pubDate>
		<dc:creator><![CDATA[shlomi]]></dc:creator>
				<category><![CDATA[MySQL]]></category>
		<category><![CDATA[GitHub]]></category>
		<category><![CDATA[Open Source]]></category>
		<category><![CDATA[orchestrator]]></category>

		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=8016</guid>
				<description><![CDATA[This post serves as a pointer to my previous announcement about The state of Orchestrator, 2020. Thank you to Tom Krouper who applied his operational engineer expertise to content publishing problems.]]></description>
								<content:encoded><![CDATA[<p>This post serves as a pointer to my previous announcement about <a href="https://shlomi-noach.github.io/blog/mysql/the-state-of-orchestrator-2020-spoiler-healthy">The state of Orchestrator, 2020</a>.</p>
<p>Thank you to <a href="https://github.com/tomkrouper">Tom Krouper</a> who applied his operational engineer expertise to content publishing problems.</p>
]]></content:encoded>
							<wfw:commentRss>https://shlomi-noach.github.io/blog/mysql/the-state-of-orchestrator-2020-spoiler-healthy-2/feed</wfw:commentRss>
		<slash:comments>0</slash:comments>
						<post-id xmlns="com-wordpress:feed-additions:1">8016</post-id>	</item>
		<item>
		<title>The state of Orchestrator, 2020 (spoiler: healthy)</title>
		<link>https://shlomi-noach.github.io/blog/mysql/the-state-of-orchestrator-2020-spoiler-healthy</link>
				<comments>https://shlomi-noach.github.io/blog/mysql/the-state-of-orchestrator-2020-spoiler-healthy#respond</comments>
				<pubDate>Tue, 18 Feb 2020 08:09:05 +0000</pubDate>
		<dc:creator><![CDATA[shlomi]]></dc:creator>
				<category><![CDATA[MySQL]]></category>
		<category><![CDATA[GitHub]]></category>
		<category><![CDATA[Open Source]]></category>
		<category><![CDATA[orchestrator]]></category>

		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=7996</guid>
				<description><![CDATA[Yesterday was my last day at GitHub, and this post explains what this means for orchestrator. First, a quick historical review: 2014: I began work on orchestrator at Outbrain, as https://github.com/outbrain/orchestrator. I authored several open source projects while working for Outbrain, and created orchestrator to solve discovery, visualization and simple refactoring needs. Outbrain was happy [&#8230;]]]></description>
								<content:encoded><![CDATA[<p>Yesterday was my last day at GitHub, and this post explains what this means for <code>orchestrator</code>. First, a quick historical review:</p>
<ul>
<li><strong>2014</strong>: I began work on <code>orchestrator</code> at <a href="https://www.outbrain.com/">Outbrain</a>, as <a href="https://github.com/outbrain/orchestrator">https://github.com/outbrain/orchestrator</a>. I authored several open source projects while working for Outbrain, and created <code>orchestrator</code> to solve discovery, visualization and simple refactoring needs. Outbrain was happy to have the project developed as a public, open source repo from day 1, and it was released under the Apache 2 license. Interestingly, the idea to develop <code>orchestrator</code> came after I attended Percona Live Santa Clara 2014 and watched &#8220;ChatOps: How GitHub Manages MySQL&#8221; by one Sam Lambert.</li>
<li><strong>2015</strong>: Joined <a href="http://booking.com">Booking.com</a> where my main focus was to redesign and solve issues with the existing high availability setup. With Booking.com&#8217;s support, I continued work on <code>orchestrator</code>, pursuing better failure detection and recovery processes. Booking.com was an incredible playground and testbed for <code>orchestrator</code>, a massive deployment of multiple MySQL/MariaDB flavors and configuration.</li>
<li><strong>2016 &#8211; 2020</strong>: Joined <a href="http://github.com">GitHub</a>. GitHub <a href="https://github.blog/2016-12-08-orchestrator-github/">adopted</a> <code>orchestrator</code> and I developed it under GitHub&#8217;s own org, at <a href="https://github.com/github/orchestrator">https://github.com/github/orchestrator</a>. It became <a href="https://github.blog/2018-06-20-mysql-high-availability-at-github/">a core component</a> in github.com&#8217;s high availability design, running failure detection and recoveries across sites and geographical regions, with more to come. These 4+ years have been critical to <code>orchestrator</code>&#8216;s development and saw its widespread use. At this time I&#8217;m aware of multiple large-scale organizations using <code>orchestrator</code> for high availability and failovers. Some of these are GitHub, Booking.com, Shopify, Slack, Wix, Outbrain, and more. <code>orchestrator</code> is the underlying failover mechanism for <a href="https://vitess.io/">vitess</a>, and is also included in Percona&#8217;s <a href="https://www.percona.com/software/database-tools/percona-monitoring-and-management">PMM</a>. These years saw a significant increase in community adoption and contributions, in published content, such as Pythian and Percona technical blog posts, and, not surprisingly, increase in issues and feature requests.</li>
</ul>
<h3><strong><br />
</strong>2020</h3>
<p>GitHub was very kind to support moving the <code>orchestrator</code> repo under my own <a href="https://github.com/openark">https://github.com/openark</a> org. This means all issues, pull requests, releases, forks, stars and watchers have automatically transferred to the new location: <a href="https://github.com/openark/orchestrator">https://github.com/openark/orchestrator</a>. The old links do a &#8220;follow me&#8221; and implicitly direct to the new location. All external links to code and docs still work. I&#8217;m grateful to GitHub for supporting this transfer.</p>
<p>I&#8217;d like to thank all the above companies for their support of <code>orchestrator</code> and of open source in general. Being able to work on the same product throughout three different companies is mind blowing and an incredible opportunity. <code>orchestrator</code> of course remains open source and licensed with Apache 2. Existing Copyrights are unchanged.</p>
<p>As for what&#8217;s next: some personal time off, please understand if there&#8217;s delays to reviews/answers. My intention is to continue developing <code>orchestrator</code>. Naturally, the shape of future development depends on how <code>orchestrator</code> meets my future work. Nothing changes in that respect: my focus on <code>orchestrator</code> has always been first and foremost the pressing business needs, and then community support as possible. There are some interesting ideas by prominent <code>orchestrator</code> users and adopters and I&#8217;ll share more thoughts in due time.</p>
<p>&nbsp;</p>
]]></content:encoded>
							<wfw:commentRss>https://shlomi-noach.github.io/blog/mysql/the-state-of-orchestrator-2020-spoiler-healthy/feed</wfw:commentRss>
		<slash:comments>0</slash:comments>
						<post-id xmlns="com-wordpress:feed-additions:1">7996</post-id>	</item>
		<item>
		<title>Quick hack for GTID_OWN lack</title>
		<link>https://shlomi-noach.github.io/blog/mysql/quick-hack-for-gtid_own-lack</link>
				<comments>https://shlomi-noach.github.io/blog/mysql/quick-hack-for-gtid_own-lack#respond</comments>
				<pubDate>Wed, 11 Dec 2019 08:00:00 +0000</pubDate>
		<dc:creator><![CDATA[shlomi]]></dc:creator>
				<category><![CDATA[MySQL]]></category>
		<category><![CDATA[GTID]]></category>
		<category><![CDATA[Replication]]></category>

		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=7974</guid>
				<description><![CDATA[One of the benefits of MySQL GTIDs is that each server remembers all GTID entries ever executed. Normally these would be ranges, e.g. 0041e600-f1be-11e9-9759-a0369f9435dc:1-3772242 or multi-ranges, e.g. 24a83cd3-e30c-11e9-b43d-121b89fcdde6:1-103775793, 2efbcca6-7ee1-11e8-b2d2-0270c2ed2e5a:1-356487160, 46346470-6561-11e9-9ab7-12aaa4484802:1-26301153, 757fdf0d-740e-11e8-b3f2-0a474bcf1734:1-192371670, d2f5e585-62f5-11e9-82a5-a0369f0ed504:1-10047. One of the common problems in asynchronous replication is the issue of consistent reads. I&#8217;ve just written to the master. Is the data [&#8230;]]]></description>
								<content:encoded><![CDATA[<p>One of the benefits of MySQL GTIDs is that each server remembers <em>all</em> GTID entries ever executed. Normally these would be ranges, e.g. <code>0041e600-f1be-11e9-9759-a0369f9435dc:1-3772242</code> or multi-ranges, e.g. <code>24a83cd3-e30c-11e9-b43d-121b89fcdde6:1-103775793, 2efbcca6-7ee1-11e8-b2d2-0270c2ed2e5a:1-356487160, 46346470-6561-11e9-9ab7-12aaa4484802:1-26301153, 757fdf0d-740e-11e8-b3f2-0a474bcf1734:1-192371670, d2f5e585-62f5-11e9-82a5-a0369f0ed504:1-10047</code>.</p>
<p>One of the common problems in asynchronous replication is the issue of consistent reads. I&#8217;ve just written to the <code>master</code>. Is the data available on a replica yet? We have iterated on this, from reading on <code>master</code>, to heuristically finding up-to-date replicas based on heartbeats (see <a href="https://www.youtube.com/watch?v=ZVBmTgIMOCA">presentation</a> and <a href="https://speakerdeck.com/shlominoach/monitoring-time-in-a-distributed-database-a-play-in-three-acts">slides</a>) via <a href="https://github.com/github/freno">freno</a>, and now settled, on some parts of our apps, to using GTID.</p>
<p>GTIDs are reliable as any replica can give you a definitive answer to the question: <em>have you applied a given transaction or not?</em>. Given a GTID entry, say <code>f7b781a9-cbbd-11e9-affb-008cfa542442:12345</code>, one may query for the following on a replica:</p>
<pre><code>mysql&gt; select gtid_subset('f7b781a9-cbbd-11e9-affb-008cfa542442:12345', @@global.gtid_executed) as transaction_found;
+-------------------+
| transaction_found |
+-------------------+
|                 1 |
+-------------------+

mysql&gt; select gtid_subset('f7b781a9-cbbd-11e9-affb-008cfa542442:123450000', @@global.gtid_executed) as transaction_found;
+-------------------+
| transaction_found |
+-------------------+
|                 0 |
+-------------------+
</code></pre>
<h3>Getting OWN_GTID</h3>
<p>This is all well, but, given some <code>INSERT</code> or <code>UPDATE</code> on the <code>master</code>, how can I tell what&#8217;s the GTID associated with that transaction? There\s good news and bad news.</p>
<ul>
<li>Good news is, you may <code>SET SESSION session_track_gtids = OWN_GTID</code>. This makes the MySQL protocol return the GTID generated by your transaction.</li>
<li>Bad news is, this isn&#8217;t a standard SQL response, and the common MySQL drivers offer you no way to get that information!</li>
</ul>
<p>At GitHub we author our own Ruby driver, and have implemented the functionality to extract <code>OWN_GTID</code>, much like you&#8217;d extract <code>LAST_INSERT_ID</code>. But, how does one solve that without modifying the drivers? Here&#8217;s a poor person&#8217;s solution which gives you an inexact, but good enough, info. Following a write (<code>insert</code>, <code>delete</code>, <code>create</code>, &#8230;), run:</p>
<pre><code>select gtid_subtract(concat(@@server_uuid, ':1-1000000000000000'), gtid_subtract(concat(@@server_uuid, ':1-1000000000000000'), @@global.gtid_executed)) as master_generated_gtid;
</code></pre>
<p>The idea is to &#8220;clean&#8221; the executed GTID set from irrelevant entries, by filtering out all ranges that do not belong to the server you&#8217;ve just written to (the <code>master</code>). The number <code>1000000000000000</code> stands for &#8220;high enough value that will never be reached in practice&#8221; &#8211; set to your own preferred value, but this value should take you beyond <code>300</code> years assuming <code>100,000</code> transactions per second.</p>
<p><span id="more-7974"></span></p>
<p>The value you get is the range on the master itself. e.g.:</p>
<pre><code>mysql&gt; select gtid_subtract(concat(@@server_uuid, ':1-1000000000000000'), gtid_subtract(concat(@@server_uuid, ':1-1000000000000000'), @@global.gtid_executed)) as master_generated_gtid;
+-------------------------------------------------+
| master_generated_gtid                           |
+-------------------------------------------------+
| dc103953-1598-11ea-82a7-008cfa5440e4:1-35807176 |
+-------------------------------------------------+
</code></pre>
<p>You may further parse the above to extract <code>dc103953-1598-11ea-82a7-008cfa5440e4:35807176</code> if you want to hold on to the latest GTID entry. Now, this entry isn&#8217;t necessarily <em>your own</em>. Between the time of your write and the time of your GTID query, other writes will have taken place. But the entry you get is either your own or a later one. If you can find that entry on a replica, that means your write is included on the replica.</p>
<p>One may wonder, why do we need to extract the value at all? Why not just <code>select @@global.gtid_executed</code>? Why filter only the <code>master</code>&#8216;s UUID? Logically, the answer is the same if you do that. But in practice, your query may be unfortunate enough to return some:</p>
<pre><code>select @@global.gtid_executed \G

e71f0cdb-b8ef-11e9-9361-008cfa542442:1-83331,
e742d87f-dea7-11e9-be6d-008cfa542c9e:1-18485,
e7880c0e-ac54-11e9-865a-008cfa544064:1-7331973,
e82043c6-c7d9-11e9-9413-008cfa5440e4:1-61692,
e902678b-b046-11e9-a281-008cfa542c9e:1-83108,
e90d7ff9-e35e-11e9-a9a0-008cfa544064:1-18468,
e929a635-bb40-11e9-9c0d-008cfa5440e4:1-139348,
e9351610-ef1b-11e9-9db4-008cfa5440e4:1-33460918,
e938578d-dc41-11e9-9696-008cfa542442:1-18232,
e947f165-cd53-11e9-b7a1-008cfa5440e4:1-18480,
e9733f37-d537-11e9-8604-008cfa5440e4:1-18396,
e97a0659-e423-11e9-8433-008cfa542442:1-18237,
e98dc1f7-e0f8-11e9-9bbd-008cfa542c9e:1-18482,
ea16027a-d20e-11e9-9845-008cfa542442:1-18098,
ea1e1aa6-e74a-11e9-a7f2-008cfa544064:1-18450,
ea8bc1bd-dd06-11e9-a10c-008cfa542442:1-18203,
eae8c750-aaca-11e9-b17c-008cfa544064:1-85990,
eb1e41e9-af81-11e9-9ceb-008cfa544064:1-86220,
eb3c9b3b-b698-11e9-b67a-008cfa544064:1-18687,
ec6daf7e-b297-11e9-a8a0-008cfa542c9e:1-80652,
eca4af92-c965-11e9-a1f3-008cfa542c9e:1-18333,
ecd110b9-9647-11e9-a48f-008cfa544064:1-24213,
ed26890e-b10b-11e9-a79d-008cfa542c9e:1-83450,
ed92b3bf-c8a0-11e9-8612-008cfa542442:1-18223,
eeb60c82-9a3d-11e9-9ea5-008cfa544064:1-1943152,
eee43e06-c25d-11e9-ba23-008cfa542442:1-105102,
eef4a7fb-b438-11e9-8d4b-008cfa5440e4:1-74717,
eefdbd3b-95b3-11e9-833d-008cfa544064:1-39415,
ef087062-ba7b-11e9-92de-008cfa5440e4:1-9726172,
ef507ff0-98b3-11e9-8b15-008cfa5440e4:1-928030,
ef662471-9a3b-11e9-bd2e-008cfa542c9e:1-954800,
f002e9f7-97ee-11e9-bed0-008cfa542c9e:1-5180743,
f0233228-e9a1-11e9-a142-008cfa542c9e:1-18583,
f04780c4-a864-11e9-9f28-008cfa542c9e:1-83609,
f048acd9-b1d2-11e9-a0b6-008cfa544064:1-70663,
f0573d8c-9978-11e9-9f73-008cfa542c9e:1-85642135,
f0b0a37c-c89c-11e9-804c-008cfa5440e4:1-18488,
f0cfe1ac-e5af-11e9-bc09-008cfa542c9e:1-18552,
f0e4997c-cbc9-11e9-9179-008cfa542442:1-1655552,
f24e481c-b5c4-11e9-aff0-008cfa5440e4:1-83015,
f4578c4b-be6d-11e9-982e-008cfa5440e4:1-132701,
f48bce80-e99f-11e9-94f4-a0369f9432f4:1-18460,
f491adf1-9b04-11e9-bc71-008cfa542c9e:1-962823,
f5d3db74-a929-11e9-90e8-008cfa5440e4:1-75379,
f6696ba7-b750-11e9-b458-008cfa542c9e:1-83096,
f714cb4c-dab7-11e9-adb9-008cfa544064:1-18413,
f7b781a9-cbbd-11e9-affb-008cfa542442:1-18169,
f81f7729-b10d-11e9-b29b-008cfa542442:1-86820,
f88a3298-e903-11e9-88d0-a0369f9432f4:1-18548,
f9467b29-d78c-11e9-b1a2-008cfa5440e4:1-18492,
f9c08f5c-e4ea-11e9-a76c-008cfa544064:1-1667611,
fa633abf-cee3-11e9-9346-008cfa542442:1-18361,
fa8b0e64-bb42-11e9-9913-008cfa542442:1-140089,
fa92234c-cc90-11e9-b337-008cfa544064:1-18324,
fa9755eb-e425-11e9-907d-008cfa542c9e:1-1668270,
fb7843d5-eb38-11e9-a1ff-a0369f9432f4:1-1668957,
fb8ceae5-dd08-11e9-9ed3-008cfa5440e4:1-18526,
fbf9970e-bc07-11e9-9e4f-008cfa5440e4:1-136157,
fc0ffaee-98b1-11e9-8574-008cfa542c9e:1-940999,
fc9bf1e4-ee54-11e9-9ce9-008cfa542c9e:1-18189,
fca4672f-ac56-11e9-8a83-008cfa542442:1-82014,
fcebaa05-dab5-11e9-8356-008cfa542c9e:1-18490,
fd0c88b1-ad1b-11e9-bf3a-008cfa5440e4:1-75167,
fd394feb-e4e4-11e9-bd09-008cfa5440e4:1-18574,
fd687577-b048-11e9-b429-008cfa542442:1-83479,
fdb18995-a79f-11e9-a28d-008cfa542442:1-82351,
fdc72b7f-b696-11e9-ade9-008cfa544064:1-57674,
ff1f3b6b-c967-11e9-ae04-008cfa544064:1-18503,
ff6fe7dc-c186-11e9-9bb4-008cfa5440e4:1-103192,
fff9dd94-ed95-11e9-90b7-008cfa544064:1-911039
</code></pre>
<p>This can happen when you fail over to a new master, multiple times; it happens when you don&#8217;t recycle UUIDs, when you provision new hosts and let MySQL pick their UUID. Returning this amount of data <em>per query</em> is an excessive overhead, hence why we extract the <code>master</code>&#8216;s UUID only, which is guaranteed to be limited in size.</p>
]]></content:encoded>
							<wfw:commentRss>https://shlomi-noach.github.io/blog/mysql/quick-hack-for-gtid_own-lack/feed</wfw:commentRss>
		<slash:comments>0</slash:comments>
						<post-id xmlns="com-wordpress:feed-additions:1">7974</post-id>	</item>
		<item>
		<title>Un-split brain MySQL via gh-mysql-rewind</title>
		<link>https://shlomi-noach.github.io/blog/mysql/un-split-brain-mysql-via-gh-mysql-rewind</link>
				<comments>https://shlomi-noach.github.io/blog/mysql/un-split-brain-mysql-via-gh-mysql-rewind#respond</comments>
				<pubDate>Tue, 05 Mar 2019 13:51:43 +0000</pubDate>
		<dc:creator><![CDATA[shlomi]]></dc:creator>
				<category><![CDATA[MySQL]]></category>
		<category><![CDATA[Open Source]]></category>
		<category><![CDATA[Replication]]></category>

		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=7928</guid>
				<description><![CDATA[We are pleased to release gh-mysql-rewind, a tool that allows us to move MySQL back in time, automatically identify and rewind split brain changes, restoring a split brain server into a healthy replication chain. I recently had the pleasure of presenting gh-mysql-rewind at FOSDEM. Video and slides are available. Consider following along with the video. [&#8230;]]]></description>
								<content:encoded><![CDATA[<p>We are pleased to release <a href="https://github.com/github/gh-mysql-tools/tree/master/rewind">gh-mysql-rewind</a>, a tool that allows us to move MySQL back in time, automatically identify and rewind split brain changes, restoring a split brain server into a healthy replication chain.</p>
<p>I recently had the pleasure of presenting <code>gh-mysql-rewind</code> at <a href="https://fosdem.org/2019/schedule/track/mysql_mariadb_and_friends/">FOSDEM</a>. <a href="https://www.youtube.com/watch?v=UL--ew3n3QI">Video</a> and <a href="https://speakerdeck.com/shlominoach/un-split-brain-mysql">slides</a> are available. Consider following along with the video.</p>
<h2>Motivation</h2>
<p>Consider a split brain scenario: a &#8220;standard&#8221; MySQL replication topology suffered network isolation, and one of the replicas was promoted as new master. Meanwhile, the old master was still receiving writes from co-located apps.</p>
<p>Once the network isolation is over, we have a new master and an old master, and a split-brain situation: some writes only took place on one master; others only took place on the other. What if we wanted to converge the two? What paths do we have to, say, restore the old, demoted master, as a replica of the newly promoted master?</p>
<p>The old master is unlikely to agree to replicate from the new master. Changes have been made. <code>AUTO_INCREMENT</code> values have been taken. <code>UNIQUE</code> constraints will fail.</p>
<p>A few months ago, we at GitHub had <a href="https://github.blog/2018-10-30-oct21-post-incident-analysis/">exactly this scenario</a>. An entire data center went network isolated. Automation failed over to a 2nd DC. Masters in the isolated DC meanwhile kept receiving writes. At the end of the failover we ended up with a split brain scenario &#8211; which we <a href="https://githubengineering.com/mysql-high-availability-at-github/#limitations-and-drawbacks">expected</a>. However, an additional, unexpected constraint forced us to fail back to the original DC.</p>
<p>We had to make a choice: we&#8217;ve already operated for a long time in the 2nd DC and took many writes, that we were unwilling to lose. We were OK to lose (after auditing) the few seconds of writes on the isolated DC. But, how do we converge the data?</p>
<p>Backups are the trivial way out, but they incur long recovery time. Shipping backup data over the network for dozens of servers takes time. Restore time, catching up with changes since backup took place, warming up the servers so that they can handle production traffic, all take time.</p>
<p>Could we have reduces the time for recovery?</p>
<p><span id="more-7928"></span></p>
<p>There are multiple ways to do that: local backups, local delayed replicas, snapshots&#8230; We have embarked on several. In this post I wish to outline <a href="https://github.com/github/gh-mysql-tools/tree/master/rewind">gh-mysql-rewind</a>, which programmatically identifies the rogue (aka &#8220;bad&#8221;) transactions on the network isolated master, rewinds/reverts them, applies some bookkeeping and restores the demoted master as a healthy replica under the newly promoted master, thereby prepared to be promoted if needed.</p>
<h2>General overview</h2>
<p><code>gh-mysql-rewind</code> is a <code>shell</code> script. It utilizes multiple technologies, some of which do not speak to each other, to be able to do the magic. It assumes and utilizes the following:</p>
<ul>
<li>MySQL <a href="https://dev.mysql.com/doc/refman/5.7/en/replication-gtids-concepts.html">GTID replication</a></li>
<li>Row based replication (<code>binlog_format=ROW</code>)</li>
<li><code>binlog_row_image=FULL</code></li>
<li>Use of <a href="https://mariadb.com/kb/en/library/flashback/">MariaDB Flashback</a></li>
<li>Some limitations apply</li>
</ul>
<p>Some breakdown follows.</p>
<h2>GTID</h2>
<p>MySQL GTIDs keep track of all transactions executed on a given server. GTIDs indicate which server (UUID) originated a write, and ranges of transaction sequences. In a clean state, only one writer will generate GTIDs, and on all the replicas we would see the same GTID set, originated with the writer&#8217;s UUID.</p>
<p>In a split brain scenario, we would see divergence. It is possible to use <a href="https://dev.mysql.com/doc/refman/5.7/en/gtid-functions.html#function_gtid-subtract">GTID_SUBTRACT(old_master-GTIDs, new-master-GTIDs)</a> to identify the exact set of transactions executed on the old, demoted master, right after the failover. This is the essence of the split brain.</p>
<p>For example, assume that just before the network partition, GTID on the master was <code>00020192-1111-1111-1111-111111111111:1-5000</code>. Assume after the network partition the new master has UUID of <code>00020193-2222-2222-2222-222222222222</code>. It began to take writes, and after some time its GTID set showed <code>00020192-1111-1111-1111-111111111111:1-5000,00020193-2222-2222-2222-222222222222:1-200</code>.</p>
<p>On the demoted master, other writes took place, leading to the GTID set <code>00020192-1111-1111-1111-111111111111:1-5042</code>.</p>
<p>We will run&#8230;</p>
<pre><code class="sql">SELECT GTID_SUBTRACT(
  '00020192-1111-1111-1111-111111111111:1-5042',
  '00020192-1111-1111-1111-111111111111:1-5000,00020193-2222-2222-2222-222222222222:1-200'
);

&gt; '00020192-1111-1111-1111-111111111111:5001-5042'
</code></pre>
<p>&#8230;to identify the exact set of &#8220;bad transactions&#8221; on the demoted master.</p>
<h2>Row Based Replication</h2>
<p>With row based replication, and with <code>FULL</code> image format, each DML (<code>INSERT</code>, <code>UPDATE</code>, <code>DELETE</code>) writes to the binary log the complete row data before and after the operation. This means the binary log has enough information for us to revert the operation.</p>
<h2>Flashback</h2>
<p>Developed by Alibaba, <code>flashback</code> has been incorporated in <a href="https://mariadb.com/kb/en/library/flashback/">MariaDB</a>. MariaDB&#8217;s <code>mysqlbinlog</code> utility supports a <code>--flashback</code> flag, which interprets the binary log in a special way. Instead of printing out the events in the binary log in order, it prints the inverted operations in reverse order.</p>
<p>To illustrate, let&#8217;s assume this pseudo-code sequence of events in the binary log:</p>
<pre><code class="sql">insert(1, 'a')
insert(2, 'b')
insert(3, 'c')
update(2, 'b')-&gt;(2, 'second')
update(3, 'c')-&gt;(3, 'third')
insert(4, 'd')
delete(1, 'a')
</code></pre>
<p>A <code>--flashback</code> of this binary log would produce:</p>
<pre><code>insert(1, 'a')
delete(4, 'd')
update(3, 'third')-&gt;(3, 'c')
update(2, 'second')-&gt;(2, 'b')
delete(3, 'c')
delete(2, 'b')
delete(1, 'a')
</code></pre>
<p>Alas, MariaDB and <code>flashback</code> do not speak MySQL GTID language. GTIDs are one of the major points where MySQL and MariaDB have diverged beyond compatibility.</p>
<p>The output of MariaDB&#8217;s <code>mysqlbinlog --flashback</code> has neither any mention of GTIDs, nor does the tool take notice of GTIDs in the binary logs in the first place.</p>
<h2>gh-mysql-rewind</h2>
<p>This is where we step in. GTIDs provide the information about <em>what went wrong</em>. <code>flashback</code> has the mechanism to generate the reverse sequence of statements. <code>gh-mysql-rewind</code>:</p>
<ul>
<li>uses GTIDs to detect what went wrong</li>
<li>correlates those GTID entries with binary log files: identifies which binary logs actually contain those GTID events</li>
<li>invokes MariaDB&#8217;s <code>mysqlbinlog --flashback</code> to generate the reverse of those binary logs</li>
<li>injects (dummy) GTID information into the output</li>
<li>computes ETA</li>
</ul>
<p>This last part is worth elaborating. We have created a time machine. We have the mechanics to make it work. But as any Sci-Fi fan knows, one of the most important parts of time travel is knowing ahead where (when) you are going to land. Are you back in the Renaissance? Or are you suddenly to appear on board the French Revolution? Better dress accordingly.</p>
<p>In our scenario it is not enough to move MySQL back in time to <em>some consistent state</em>. We want to know at what time we landed, so that we can instruct the rewinded server to join the replication chain as a healthy replica. In MySQL terms, we need to make MySQL &#8220;forget&#8221; everything that ever happened after the split brain: not only in terms of data (which we already did), but in terms of GTID history.</p>
<p><code>gh-mysql-rewind</code> will do the math to project, ahead of time, at what &#8220;time&#8221; (i.e. GTID set) our time machine arrived. It will issue a `RESET MASTER; SET GLOBAL gtid_purged=&#8217;gtid-of-the-landing-time'&#8221; to make our re-winded MySQL consistent not only with some past dataset, but also with its own perception of the point in time where that dataset existed.</p>
<h2>Limitations</h2>
<p>Some limitations are due to MariaDB&#8217;s incompatibility with MySQL, some are due to MySQL DDL nature, some due to the fact <code>gh-mysql-rewind</code> is a <code>shell</code> script.</p>
<ul>
<li>Cannot rewind DDL. DDLs are silently ignored, and will impose a problem when trying to re-apply them.</li>
<li><code>JSON</code>, <code>POINT</code> data types are not supported.</li>
<li>The logic rewinds the MySQL server farther into the past than strictly required. This simplifies the code considerably, but imposed superfluous time to rewind+reapply, i.e. time to recover.</li>
<li>Currently, this only works one server at a time. If a group of 10 servers were network isolated together, the operation would need to run on each of these 10 servers.</li>
<li>Runs locally on each server. Requires both MySQL&#8217;s <code>mysqlbinlog</code> as well as MariaDB&#8217;s <code>mysqlbinlog</code>.</li>
</ul>
<h2>Testing</h2>
<p>There&#8217;s lot of moving parts to this mechanism. A mixture of technologies that don&#8217;t normally speak to each other, injection of data, prediction of ETA&#8230; How reliable is all this?</p>
<p>We run continuous <code>gh-mysql-rewind</code> testing in production to consistently prove that it works as expected. Our testing uses a non-production, dedicated, functional replica. It contaminates the data on the replica. It lets <code>gh-mysql-rewind</code> automatically move it back in time, it joins the replica back into the healthy chain.</p>
<p>That&#8217;s not enough. We actually create a scenario where we can predict, ahead of testing, what the time-of-arrival will be. We checksum the data on that replica at that time. After contaminating and effectively breaking replication, we expect <code>gh-mysql-rewind</code> to revert the changes back to our predicted point in time. We checksum the data again. We expect 100% match.</p>
<p>See the video or slides for more detail on our testing setup.</p>
<h2>Status</h2>
<p>At this time the tool in one of several solutions we hope to never need to employ. It is stable and tested. We are looking forward to a promising MySQL development that will provide GTID-revert capabilities using standard commands, such as <code>SELECT undo_transaction('00020192-1111-1111-1111-111111111111:5042')</code>.</p>
<p>We have <a href="https://github.com/github/gh-mysql-tools/tree/master/rewind">released</a> <code>gh-mysql-rewind</code> as open source, under the MIT license. The public release is a stripped down version of our own script, which has some GitHub-specific integration. We have general ideas in incorporating this functionality into higher level tools.</p>
<p><code>gh-mysql-rewind</code> is developed by the database-infrastructure team at GitHub.</p>
]]></content:encoded>
							<wfw:commentRss>https://shlomi-noach.github.io/blog/mysql/un-split-brain-mysql-via-gh-mysql-rewind/feed</wfw:commentRss>
		<slash:comments>0</slash:comments>
						<post-id xmlns="com-wordpress:feed-additions:1">7928</post-id>	</item>
		<item>
		<title>MySQL master discovery methods, part 6: other methods</title>
		<link>https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-6-other-methods</link>
				<comments>https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-6-other-methods#comments</comments>
				<pubDate>Tue, 22 May 2018 08:39:46 +0000</pubDate>
		<dc:creator><![CDATA[shlomi]]></dc:creator>
				<category><![CDATA[MySQL]]></category>

		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=7854</guid>
				<description><![CDATA[This is the sixth in a series of posts reviewing methods for MySQL master discovery: the means by which an application connects to the master of a replication tree. Moreover, the means by which, upon master failover, it identifies and connects to the newly promoted master. These posts are not concerned with the manner by [&#8230;]]]></description>
								<content:encoded><![CDATA[<p>This is the sixth in a series of posts reviewing methods for MySQL master discovery: the means by which an application connects to the master of a replication tree. Moreover, the means by which, upon master failover, it identifies and connects to the newly promoted master.</p>
<p>These posts are not concerned with the manner by which the replication failure detection and recovery take place. I will share <code>orchestrator</code> specific configuration/advice, and point out where cross DC <code>orchestrator/raft</code> setup plays part in discovery itself, but for the most part any recovery tool such as <code>MHA</code>, <code>replication-manager</code>, <code>severalnines</code> or other, is applicable.</p>
<h3>Hard coded configuration deployment</h3>
<p>You may use your source/config repo as master service discovery method of sorts.</p>
<p>The master&#8217;s identity would be hard coded into your, say, <code>git</code> repo, to be updated and deployed to production upon failover.</p>
<p>This method is simple and I&#8217;ve seen it being used by companies, in production. Noteworthy:</p>
<p><span id="more-7854"></span></p>
<ul>
<li>This requires a dependency of <code>production</code> on source availability.
<ul>
<li>The failover tool would need to have access to your source environment.</li>
</ul>
</li>
<li>This requires a dependency of <code>production</code> on build/deploy flow.
<ul>
<li>The failover tool would need to kick build, test, deploy process.</li>
</ul>
</li>
<li>Code deployment time can be long.</li>
<li>Deployment must take place on all relevant hosts, and cause for a mass refresh/reload.
<ul>
<li>It should interrupt processes that cannot reload themselves, such as various commonly used scripts.</li>
</ul>
</li>
</ul>
<h3>Synchronous replication</h3>
<p>This series of posts is focused on asynchronous replication, but we will do well to point out a few relevant notes on sychnronous replication (<em>Galera</em>, <em>XtraDB Cluster</em>, <em>InnoDB Cluster</em>).</p>
<ul>
<li>Synchronous replication can act in single-writer mode or in multi-writer mode.</li>
<li>In single writer mode, apps should connect to a particular master.
<ul>
<li>The identity of such master can be achieved by querying the MySQL members of the cluster.</li>
</ul>
</li>
<li>In multi-writer mode, apps can connect to any healthy member of the cluster.
<ul>
<li>This still calls for a check: is the member healthy?</li>
</ul>
</li>
<li>Syncronous replication is not intended to work well cross DC.</li>
</ul>
<p>The last bullet should perhaps be highlighted. In a cross-DC setup, and for cross-DC failovers, we are back to same requirements as with asynchronous replication, and the methods illustrated in this series of posts may apply.</p>
<ul>
<li>VIPs make less sense.</li>
<li>Proxy-based solution make a lot of sense.</li>
</ul>
<h3>All posts in this series</h3>
<ul>
<li><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-1-dns">MySQL master discovery methods, part 1: DNS</a></li>
<li><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-2-vip-dns">MySQL master discovery methods, part 2: VIP &amp; DNS</a></li>
<li><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-3-app-service-discovery">MySQL master discovery methods, part 3: app &amp; service discovery</a></li>
<li><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-4-proxy-heuristics">MySQL master discovery methods, part 4: Proxy heuristics</a></li>
<li><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-5-service-discovery-proxy">MySQL master discovery methods, part 5: Service discovery &amp; Proxy</a></li>
<li><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-6-other-methods">MySQL master discovery methods, part 6: other methods</a></li>
</ul>
]]></content:encoded>
							<wfw:commentRss>https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-6-other-methods/feed</wfw:commentRss>
		<slash:comments>1</slash:comments>
						<post-id xmlns="com-wordpress:feed-additions:1">7854</post-id>	</item>
		<item>
		<title>MySQL master discovery methods, part 5: Service discovery &#038; Proxy</title>
		<link>https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-5-service-discovery-proxy</link>
				<comments>https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-5-service-discovery-proxy#respond</comments>
				<pubDate>Mon, 14 May 2018 08:08:32 +0000</pubDate>
		<dc:creator><![CDATA[shlomi]]></dc:creator>
				<category><![CDATA[MySQL]]></category>
		<category><![CDATA[High availability]]></category>
		<category><![CDATA[orchestrator]]></category>
		<category><![CDATA[Replication]]></category>

		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=7869</guid>
				<description><![CDATA[This is the fifth in a series of posts reviewing methods for MySQL master discovery: the means by which an application connects to the master of a replication tree. Moreover, the means by which, upon master failover, it identifies and connects to the newly promoted master. These posts are not concerned with the manner by [&#8230;]]]></description>
								<content:encoded><![CDATA[<p>This is the fifth in a series of posts reviewing methods for MySQL master discovery: the means by which an application connects to the master of a replication tree. Moreover, the means by which, upon master failover, it identifies and connects to the newly promoted master.</p>
<p>These posts are not concerned with the manner by which the replication failure detection and recovery take place. I will share <code>orchestrator</code> specific configuration/advice, and point out where cross DC <code>orchestrator/raft</code> setup plays part in discovery itself, but for the most part any recovery tool such as <code>MHA</code>, <code>replication-manager</code>, <code>severalnines</code> or other, is applicable.</p>
<p>We discuss asynchronous (or semi-synchronous) replication, a classic single-master-multiple-replicas setup. A later post will briefly discuss synchronous replication (Galera/XtraDB Cluster/InnoDB Cluster).</p>
<h3>Master discovery via Service discovery and Proxy</h3>
<p><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-4-proxy-heuristics">Part 4</a> presented with an anti-pattern setup, where a proxy would infer the identify of the master by drawing conclusions from backend server checks. This led to split brains and undesired scenarios. The problem was the loss of context.</p>
<p>We re-introduce a service discovery component (illustrated in <a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-3-app-service-discovery">part 3</a>), such that:</p>
<ul>
<li>The app does not own the discovery, and</li>
<li>The proxy behaves in an expected and consistent way.</li>
</ul>
<p>In a failover/service discovery/proxy setup, there is clear ownership of duties:</p>
<ul>
<li>The failover tool own the failover itself and the master identity change notification.</li>
<li>The service discovery component is the source of truth as for the identity of the master of a cluster.</li>
<li>The proxy routes traffic but does not make routing decisions.</li>
<li>The app only ever connects to a single target, but should allow for a brief outage while failover takes place.</li>
</ul>
<p>Depending on the technologies used, we can further achieve:</p>
<ul>
<li>Hard cut for connections to old, demoted master <code>M</code>.</li>
<li>Black/hold off for incoming queries for the duration of failover.</li>
</ul>
<p>We explain the setup using the following assumptions and scenarios:</p>
<ul>
<li>All clients connect to master via <code>cluster1-writer.example.net</code>, which resolves to a proxy box.</li>
<li>We fail over from master <code>M</code> to promoted replica <code>R</code>.</li>
</ul>
<p><span id="more-7869"></span></p>
<h3>A non planned failover illustration #1</h3>
<p>Master <code>M</code> has died, the box had a power failure. <code>R</code> gets promoted in its place. Our recovery tool:</p>
<ul>
<li>Updates service discovery component that <code>R</code> is the new master for <code>cluster1</code>.</li>
</ul>
<p>The proxy:</p>
<ul>
<li>Either actively or passively learns that <code>R</code> is the new master, rewires all writes to go to <code>R</code>.</li>
<li>If possible, kills existing connections to <code>M</code>.</li>
</ul>
<p>The app:</p>
<ul>
<li>Needs to know nothing. Its connections to <code>M</code> fail, it reconnects and gets through to <code>R</code>.</li>
</ul>
<h3>A non planned failover illustration #2</h3>
<p>Master <code>M</code> gets network isolated for <code>10</code> seconds, during which time we failover. <code>R</code> gets promoted.</p>
<p>Everything is as before.</p>
<p>If the proxy kills existing connections to <code>M</code>, then the fact <code>M</code> is back alive turns meaningless. No one gets through to <code>M</code>. Clients were never aware of its identity anyhow, just as they are unaware of <code>R</code>&#8216;s identity.</p>
<h3>Planned failover illustration</h3>
<p>We wish to replace the master, for maintenance reasons. We successfully and gracefully promote <code>R</code>.</p>
<ul>
<li>In the process of promotion, <code>M</code> turned read-only.</li>
<li>Immediately following promotion, our failover tool updates service discovery.</li>
<li>Proxy reloads having seen the changes in service discovery.</li>
<li>Our app connects to <code>R</code>.</li>
</ul>
<h3>Discussion</h3>
<p>This is a setup we use at GitHub in production. Our components are:</p>
<ul>
<li><code>orchestrator</code> for failover tool.</li>
<li><em>Consul</em> for service discovery.</li>
<li>GLB (HAProxy) for proxy</li>
<li><em>Consul template</em> running on proxy hosts:
<ul>
<li>listening on changes to Consul&#8217;s KV data</li>
<li>Regenerate <code>haproxy.cfg</code> configuration file</li>
<li><code>reload</code> haproxy</li>
</ul>
</li>
</ul>
<p>As mentioned earlier, the apps need not change anything. They connect to a name that is always resolved to proxy boxes. There is never a DNS change.</p>
<p>At the time of failover, the service discovery component must be up and available, to catch the change. Otherwise we do not strictly require it to be up at all times.</p>
<p>For high availability we will have multiple proxies. Each of whom must listen on changes to K/V. Ideally the name (<code>cluster1-writer.example.net</code> in our example) resolves to any available proxy box.</p>
<ul>
<li>This, in itself, is a high availability issue. Thankfully, managing the HA of a proxy layer is simpler than that of a MySQL layer. Proxy servers tend to be stateless and equal to each other.</li>
<li>See GLB as one example for a highly available proxy layer. Cloud providers, Kubernetes, two level layered proxies, Linux Heartbeat, are all methods to similarly achieve HA.</li>
</ul>
<p>See also:</p>
<ul>
<li><a href="https://blog.pythian.com/mysql-high-availability-with-haproxy-consul-and-orchestrator/">MySQL High Availability With HAProxy, Consul And Orchestrator</a></li>
<li><a href="https://www.percona.com/live/18/sessions/automatic-failovers-with-kubernetes-using-orchestrator-proxysql-and-zookeeper">Automatic Failovers with Kubernetes using Orchestrator, ProxySQL and Zookeeper</a></li>
<li><a href="https://www.percona.com/live/e17/sessions/orchestrating-proxysql-with-orchestrator-and-consul">Orchestrating ProxySQL with Orchestrator and Consul</a></li>
</ul>
<h3>Sample orchestrator configuration</h3>
<p>An <code>orchestrator</code> configuration would look like this:</p>
<pre><code class="json">  "ApplyMySQLPromotionAfterMasterFailover": true,
  "KVClusterMasterPrefix": "mysql/master",
  "ConsulAddress": "127.0.0.1:8500",
  "ZkAddress": "srv-a,srv-b:12181,srv-c",
  "PostMasterFailoverProcesses": [
    “/just/let/me/know about failover on {failureCluster}“,
  ],
</code></pre>
<p>In the above:</p>
<ul>
<li>If <code>ConsulAddress</code> is specified, <code>orchestrator</code> will update given <em>Consul</em> setup with K/V changes.</li>
<li>At <code>3.0.10</code>, <em>ZooKeeper</em>, via <code>ZkAddress</code>, is still not supported by <code>orchestrator</code>.</li>
<li><code>PostMasterFailoverProcesses</code> is here just to point out hooks are not strictly required for the operation to run.</li>
</ul>
<p>See <a href="https://github.com/github/orchestrator/blob/master/docs/configuration.md">orchestrator configuration</a> documentation.</p>
<h3>All posts in this series</h3>
<ul>
<li><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-1-dns">MySQL master discovery methods, part 1: DNS</a></li>
<li><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-2-vip-dns">MySQL master discovery methods, part 2: VIP &amp; DNS</a></li>
<li><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-3-app-service-discovery">MySQL master discovery methods, part 3: app &amp; service discovery</a></li>
<li><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-4-proxy-heuristics">MySQL master discovery methods, part 4: Proxy heuristics</a></li>
<li><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-5-service-discovery-proxy">MySQL master discovery methods, part 5: Service discovery &amp; Proxy</a></li>
<li><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-6-other-methods">MySQL master discovery methods, part 6: other methods</a></li>
</ul>
]]></content:encoded>
							<wfw:commentRss>https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-5-service-discovery-proxy/feed</wfw:commentRss>
		<slash:comments>0</slash:comments>
						<post-id xmlns="com-wordpress:feed-additions:1">7869</post-id>	</item>
		<item>
		<title>MySQL master discovery methods, part 4: Proxy heuristics</title>
		<link>https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-4-proxy-heuristics</link>
				<comments>https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-4-proxy-heuristics#respond</comments>
				<pubDate>Thu, 10 May 2018 06:10:34 +0000</pubDate>
		<dc:creator><![CDATA[shlomi]]></dc:creator>
				<category><![CDATA[MySQL]]></category>
		<category><![CDATA[High availability]]></category>
		<category><![CDATA[orchestrator]]></category>
		<category><![CDATA[Replication]]></category>

		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=7867</guid>
				<description><![CDATA[Note: the method described here is an anti pattern This is the fourth in a series of posts reviewing methods for MySQL master discovery: the means by which an application connects to the master of a replication tree. Moreover, the means by which, upon master failover, it identifies and connects to the newly promoted master. [&#8230;]]]></description>
								<content:encoded><![CDATA[<p><em>Note: the method described here is an anti pattern</em></p>
<p>This is the fourth in a series of posts reviewing methods for MySQL master discovery: the means by which an application connects to the master of a replication tree. Moreover, the means by which, upon master failover, it identifies and connects to the newly promoted master.</p>
<p>These posts are not concerned with the manner by which the replication failure detection and recovery take place. I will share <code>orchestrator</code> specific configuration/advice, and point out where cross DC <code>orchestrator/raft</code> setup plays part in discovery itself, but for the most part any recovery tool such as <code>MHA</code>, <code>replication-manager</code>, <code>severalnines</code> or other, is applicable.</p>
<p>We discuss asynchronous (or semi-synchronous) replication, a classic single-master-multiple-replicas setup. A later post will briefly discuss synchronous replication (Galera/XtraDB Cluster/InnoDB Cluster).</p>
<h3>Master discovery via Proxy Heuristics</h3>
<p>In Proxy Heuristics all clients connect to the master through a proxy. The proxy observes the backend MySQL servers and determines who the master is.</p>
<p><strong>This setup is simple and easy, but is an anti pattern. I recommend against using this method, as explained shortly</strong>.</p>
<p>Clients are all configured to connect to, say, <code>cluster1-writer.proxy.example.net:3306</code>. The proxy will intercept incoming requests either based on hostname or by port. It is aware of all/some MySQL backend servers in that cluster, and will route traffic to the master <code>M</code>.</p>
<p>A simple heuristic that I&#8217;ve seen in use is: pick the server that has <code>read_only=0</code>, a very simple check.</p>
<p>Let&#8217;s take a look at how this works and what can go wrong.</p>
<p><span id="more-7867"></span></p>
<h3>A non planned failover illustration #1</h3>
<p>Master <code>M</code> has died, the box had a power failure. <code>R</code> gets promoted in its place. Our recovery tool:</p>
<ul>
<li>Fails over, but doesn&#8217;t need to run any hooks.</li>
</ul>
<p>The proxy:</p>
<ul>
<li>Knows both about <code>M</code> and <code>R</code>.</li>
<li>Notices <code>M</code> fails health checks (<code>select @@global.read_only</code> returns error since the box is down).</li>
<li>Notices <code>R</code> reports healthy and with <code>read_only=0</code>.</li>
<li>Routes all traffic to <code>R</code>.</li>
</ul>
<p>Success, we&#8217;re happy.</p>
<h3>Configuration tip</h3>
<p>With an automated failover solution, use <code>read_only=1</code> in <code>my.cnf</code> at all times. Only the failover solution will set a server to <code>read_only=0</code>.</p>
<p>With this configuration, when <code>M</code> restarts, MySQL starts up as <code>read_only=1</code>.</p>
<h3>A non planned failover illustration #2</h3>
<p>Master <code>M</code> gets network isolated for <code>10</code> seconds, during which time we failover. <code>R</code> gets promoted. Our tool:</p>
<ul>
<li>Fails over, but doesn&#8217;t need to run any hooks.</li>
</ul>
<p>The proxy:</p>
<ul>
<li>Knows both about <code>M</code> and <code>R</code>.</li>
<li>Notices <code>M</code> fails health checks (<code>select @@global.read_only</code> returns error since the box is down).</li>
<li>Notices <code>R</code> reports healthy and with <code>read_only=0</code>.</li>
<li>Routes all traffic to <code>R</code>.</li>
<li><code>10</code> seconds later <code>M</code> comes back to life, claiming <code>read_only=0</code>.</li>
<li>The proxy now sees two servers reporting as healthy and with <code>read_only=0</code>.</li>
<li>The proxy has no context. It does not know why both are reporting the same. It is unaware of failovers. All it sees is what the backend MySQL servers report.</li>
</ul>
<p>Therein lies the problem: you can not trust multiple servers (MySQL backends) to deterministically pick a leader (the master) without them collaborating on some elaborate consensus communication.</p>
<h3>A non planned failover illustration #3</h3>
<p>Master <code>M</code> box is overloaded, issuing <code>too many connections</code> for incoming connections.</p>
<p>Our tool decides to failover.</p>
<ul>
<li>And doesn&#8217;t need to run any hooks.</li>
</ul>
<p>The proxy:</p>
<ul>
<li>Notices <code>M</code> fails health checks (<code>select @@global.read_only</code> does not respond because of the load).</li>
<li>Notices <code>R</code> reports healthy and with <code>read_only=0</code>.</li>
<li>Routes all traffic to <code>R</code>.</li>
<li>Shortly followed by <code>M</code> recovering (since no more writes are sent its way), claiming <code>read_only=0</code>.</li>
<li>The proxy now sees two servers reporting as healthy and with <code>read_only=0</code>.</li>
</ul>
<p>Again, the proxy has no context, and neither do <code>M</code> and <code>R</code>, for that matter. The context (the fact we failed over from <code>M</code> to <code>R</code>) was known to our failover tool, but was lost along the way.</p>
<h3>Planned failover illustration</h3>
<p>We wish to replace the master, for maintenance reasons. We successfully and gracefully promote <code>R</code>.</p>
<ul>
<li><code>M</code> is available and responsive, we set it to <code>read_only=1</code>.</li>
<li>We set <code>R</code> to <code>read_only=0</code>.</li>
<li>All new connections route to <code>R</code>.</li>
<li>We should also instruct our Proxy to kill all previous connections to <code>M</code>.</li>
</ul>
<p>This works very nicely.</p>
<h3>Discussion</h3>
<p>There is a substantial risk to this method. Correlation between failover and network partitioning/load (illustrations #2 and #3) is reasonable.</p>
<p>The root of the problem is that we expect individual servers to resolve conflicts without speaking to each other: we expect the MySQL servers to correctly claim &#8220;I&#8217;m the master&#8221; without context.</p>
<p>We then add to that problem by using the proxy to &#8220;pick a side&#8221; without giving it any context, either.</p>
<h3>Sample orchestrator configuration</h3>
<p>By way of discouraging use of this method I do not present an <code>orchestrator</code> configuration file.</p>
<h3>All posts in this series</h3>
<ul>
<li><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-1-dns">MySQL master discovery methods, part 1: DNS</a></li>
<li><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-2-vip-dns">MySQL master discovery methods, part 2: VIP &amp; DNS</a></li>
<li><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-3-app-service-discovery">MySQL master discovery methods, part 3: app &amp; service discovery</a></li>
<li><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-4-proxy-heuristics">MySQL master discovery methods, part 4: Proxy heuristics</a></li>
<li><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-5-service-discovery-proxy">MySQL master discovery methods, part 5: Service discovery &amp; Proxy</a></li>
<li><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-6-other-methods">MySQL master discovery methods, part 6: other methods</a></li>
</ul>
]]></content:encoded>
							<wfw:commentRss>https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-4-proxy-heuristics/feed</wfw:commentRss>
		<slash:comments>0</slash:comments>
						<post-id xmlns="com-wordpress:feed-additions:1">7867</post-id>	</item>
	</channel>
</rss>
