<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	
	xmlns:georss="http://www.georss.org/georss"
	xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#"
	>

<channel>
	<title>orchestrator &#8211; code.openark.org</title>
	<atom:link href="https://shlomi-noach.github.io/blog/tag/orchestrator/feed" rel="self" type="application/rss+xml" />
	<link>http://shlomi-noach.github.io/blog/</link>
	<description>Blog by Shlomi Noach</description>
	<lastBuildDate>Tue, 26 May 2020 17:52:52 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=5.3.3</generator>
<site xmlns="com-wordpress:feed-additions:1">32412571</site>	<item>
		<title>orchestrator on DB AMA: show notes</title>
		<link>https://shlomi-noach.github.io/blog/mysql/orchestrator-on-db-ama-show-notes</link>
				<comments>https://shlomi-noach.github.io/blog/mysql/orchestrator-on-db-ama-show-notes#respond</comments>
				<pubDate>Tue, 26 May 2020 17:52:52 +0000</pubDate>
		<dc:creator><![CDATA[shlomi]]></dc:creator>
				<category><![CDATA[MySQL]]></category>
		<category><![CDATA[Open Source]]></category>
		<category><![CDATA[orchestrator]]></category>

		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=8101</guid>
				<description><![CDATA[Earlier today I presented orchestrator on DB AMA. Thank you to the organizers Morgan Tocker, Liz van Dijk and Frédéric Descamps for hosting me, and thank you to all who participated! This was a no-slides, all command-line walkthrough of some of orchestrator&#8216;s capabilities, highlighting refactoring, topology analysis, takeovers and failovers, and discussing a bit of [&#8230;]]]></description>
								<content:encoded><![CDATA[<p>Earlier today I presented <a href="https://github.com/openark/orchestrator">orchestrator</a> on <a href="https://dbama.now.sh/">DB AMA</a>. Thank you to the organizers Morgan Tocker, Liz van Dijk and Frédéric Descamps for hosting me, and thank you to all who participated!</p>
<p>This was a no-slides, all command-line walkthrough of some of <code>orchestrator</code>&#8216;s capabilities, highlighting refactoring, topology analysis, takeovers and failovers, and discussing a bit of scripting and HTTP API tips.</p>
<p>The recording is available <a href="https://www.youtube.com/watch?v=UngtSlZ1iTQ&amp;feature=emb_logo">on YouTube</a> (also embedded on <a href="https://dbama.now.sh/#history">https://dbama.now.sh/#history</a>).</p>
<p>To present <code>orchestrator</code>, I used the new shiny docker CI environment; it&#8217;s a single docker image running <code>orchestrator</code>, a 4-node MySQL replication topology (courtesy <a href="https://www.dbdeployer.com/">dbdeployer</a>), heartbeat injection, <code>Consul</code>, <code>consul-template</code> and <code>HAProxy</code>. You can run it, too! Just clone the <code>orchestrator</code> repo, then run:</p>
<pre>./script/dock system</pre>
<p>From there, you may follow the same playbook I used in the presentation, available as <a href="https://gist.github.com/shlomi-noach/28986cb30f0e14d51594f0bc741b464c">orchestrator-demo-playbook.sh</a>.</p>
<p>Hope you find the presentation and the playbook to be useful resources.</p>
]]></content:encoded>
							<wfw:commentRss>https://shlomi-noach.github.io/blog/mysql/orchestrator-on-db-ama-show-notes/feed</wfw:commentRss>
		<slash:comments>0</slash:comments>
						<post-id xmlns="com-wordpress:feed-additions:1">8101</post-id>	</item>
		<item>
		<title>orchestrator: what&#8217;s new in CI, testing &#038; development</title>
		<link>https://shlomi-noach.github.io/blog/mysql/orchestrator-whats-new-in-ci-testing-development</link>
				<comments>https://shlomi-noach.github.io/blog/mysql/orchestrator-whats-new-in-ci-testing-development#respond</comments>
				<pubDate>Mon, 11 May 2020 08:01:08 +0000</pubDate>
		<dc:creator><![CDATA[shlomi]]></dc:creator>
				<category><![CDATA[MySQL]]></category>
		<category><![CDATA[dbdeployer]]></category>
		<category><![CDATA[docker]]></category>
		<category><![CDATA[GitHub]]></category>
		<category><![CDATA[Open Source]]></category>
		<category><![CDATA[orchestrator]]></category>
		<category><![CDATA[testing]]></category>

		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=8077</guid>
				<description><![CDATA[Recent focus on development &#38; testing yielded with new orchestrator environments and offerings for developers and with increased reliability and trust. This post illustrates the new changes, and see Developers section on the official documentation for more details. Testing In the past four years orchestrator was developed at GitHub, and using GitHub&#8217;s environments for testing. [&#8230;]]]></description>
								<content:encoded><![CDATA[<p>Recent focus on development &amp; testing yielded with new <a href="https://github.com/openark/orchestrator">orchestrator</a> environments and offerings for developers and with increased reliability and trust. This post illustrates the new changes, and see <a href="https://github.com/openark/orchestrator/tree/master/docs#developers">Developers</a> section on the official documentation for more details.</p>
<h2>Testing</h2>
<p>In the past four years <code>orchestrator</code> was <a href="https://github.blog/2016-12-08-orchestrator-github/">developed at GitHub</a>, and using GitHub&#8217;s environments for <a href="https://github.blog/2017-07-06-mysql-testing-automation-at-github/">testing</a>. This is very useful for testing <code>orchestrator</code>&#8216;s behavior within GitHub, interacting with its internal infrastructure, and validating failover behavior in a production environment. These tests and their results are not visible to the public, though.</p>
<p>Now that <code>orchestrator</code> is developed <a href="https://shlomi-noach.github.io/blog/mysql/the-state-of-orchestrator-2020-spoiler-healthy">outside GitHub</a> (that is, outside GitHub the <em>company</em>, not GitHub the <em>platform</em>) I wanted to improve on the testing framework, making it visible, accessible and contribute-able to the community. Thankfully, the GitHub platform has much to offer on that front and <code>orchestrator</code> now uses <a href="https://github.com/features/actions">GitHub Actions</a> more heavily for testing.</p>
<p>GitHub Actions provide a way to run code in a container in the context of the repository. The most common use case is to run CI tests on receiving a Pull Request. Indeed, when GitHub Actions became available, we switched out of Travis CI and into Actions for <code>orchestrator</code>&#8216;s CI.</p>
<p>Today, <code>orchestrator</code> runs three different tests:</p>
<ul>
<li>Build, unit testing, integration testing, code &amp; doc validation</li>
<li>Upgrade testing</li>
<li>System testing</li>
</ul>
<p>To highlight what each does:<span id="more-8077"></span></p>
<h3>Build, unit testing, integration testing</h3>
<p>Based on the original CI (and possibly will split into distinct tests), this CI Action compiles the code, runs unit tests, runs the suite of <a href="https://github.com/openark/orchestrator/tree/master/tests/integration">integration tests</a> (spins up both <code>MySQL</code> and <code>SQLite</code> databases and runs a series of tests on each backend), this CI job is the &#8220;basic&#8221; test to see that the contributed code even makes sense.</p>
<p>What&#8217;s new in this test is that it now produces an <em>artifact</em>: an <code>orchestrator</code> binary for Linux/amd64. This is again a feature for GitHub Actions; the artifact is kept for a couple months or so per Actions retention policy. <a href="https://github.com/openark/orchestrator/actions/runs/94337568">Here</a>&#8216;s an example; by the time you read this the binary artifact may or may not still be there.</p>
<p>This means you don&#8217;t actually need a development environment on your laptop to be able to build and <code>orchestrator</code> binary. More on this later.</p>
<h3>Upgrade testing</h3>
<p>Until recently not formalized; I&#8217;d test upgrades by deploying them internally at GitHub onto a staging environment. Now upgrades are tested per Pull Request: we spin up a container, deploy <code>orchestrator</code> from <code>master</code> branch using both <code>MySQL</code> and <code>SQLite</code> backends, then checkout the PR branch, and redeploy <code>orchestrator</code> using the existing backends &#8212; this verifies that at least backend-database wise, there&#8217;s not upgrade errors.</p>
<p>At this time the test only validates the database changes are applicable; in the future this may expand onto more elaborate tests.</p>
<h3>System testing</h3>
<p>I&#8217;m most excited about this one. Taking ideas from our approach to <a href="https://shlomi-noach.github.io/blog/mysql/using-dbdeployer-in-ci-tests">testing gh-ost with dbdeployer</a>, I created <a href="https://github.com/openark/orchestrator-ci-env">https://github.com/openark/orchestrator-ci-env</a>, which offers a full blown testing enviroment for <code>orchestrator</code>, including a MySQL replication topology (courtesy <a href="https://www.dbdeployer.com/">dbdeployer</a>), Consul, HAProxy and more.</p>
<p>This CI testing environment can also serve as a playground in your local docker setup, see shortly.</p>
<p>The <a href="https://github.com/openark/orchestrator/tree/master/tests/system">system tests suite</a> offers full blown cluster-wide operations such as graceful takeovers, master failovers, errant GTID transaction analysis and recovery and more. The suite utilizes the CI testing environment, breaks it, rebuilds it, validates it&#8230; Expects specific output, expects specific failure messages, specific analysis, specific outcomes.</p>
<p>As example, with the system tests suite, we can test the behavior of a master failover in a multi-DC, multi-region (obviously simulated) environment, where a server marked as &#8220;candidate&#8221; is lagging behind all others, with strict rules for cross-site/cross-region failovers, and still we wish to see that particular replica get promoted as master. We can test not only the topology aspect of the failover, but also the failover hooks, Consul integration and its effects, etc.</p>
<h2>Development</h2>
<p>There&#8217;s now multiple options for developers/contributors to build or just try out <code>orchestrator</code>.</p>
<h3>Build on GitHub</h3>
<p>As mentioned earlier, you actually don&#8217;t need a development environment. You can use <code>orchestrator</code> CI to build and generate a Linux/amd64 <code>orchestrator</code> binary, which you can download &amp; deploy as you see fit.</p>
<p>I&#8217;ve signed up for the GitHub Codespaces beta program, and hope to make that available for <code>orchestrator</code>, as well.</p>
<h3>Build via Docker</h3>
<p><code>orchestrator</code> offers various Docker build/run environments, accessible via the <code>script/dock</code> script:</p>
<ul>
<li>`script/dock alpine` will build and spawn `orchestrator` on a minimal <code>alpine</code> linux</li>
<li>`script/dock test` will build and run the same CI tests (unit, integration) as mentioned earlier, but on your own docker environemtn</li>
<li>`script/dock pkg` will build and generate `.rpm` and `.deb` packages</li>
</ul>
<h3>CI environment: the &#8220;full orchestrator experience&#8221;</h3>
<p>This is the <code>orchestrator</code> amusement park. Run <code>script/dock system</code> to spawn the aforementioned CI environment used in system tests, and on top of that, an <code>orchestrator</code> setup fully integrated with that system.</p>
<p>So that&#8217;s an <code>orchestrator</code>-MySQL topology-Consul-HAProxy setup, where <code>orchestrator</code> already has the credentials for, and pre-loads the MySQL topology, pre-configured to update Consul upon failover, HAProxy config populated by <code>consul-template</code>, heartbeat injection, and more. It resembles the <a href="https://github.blog/2018-06-20-mysql-high-availability-at-github/">HA setup at GitHub</a>, and in the future I expect to provide alternate setups (on top).</p>
<p>Once in that docker environment, one can try running relocations, failovers, test <code>orchestrator</code>&#8216;s behavior, etc.</p>
<h2>Community</h2>
<p>GitHub recently announced <a href="https://github.blog/2020-05-06-new-from-satellite-2020-github-codespaces-github-discussions-securing-code-in-private-repositories-and-more/#discussions">GitHub Discussions</a> ; think a stackoverflow like place within one&#8217;s repo to ask questions, discuss, vote on answers. It&#8217;s expected to be available this summer. When it does, I&#8217;ll encourage the community to use it instead of today&#8217;s <a href="https://groups.google.com/forum/#!forum/orchestrator-mysql">orchestrator-mysql</a> Google Group and of course the many questions posted as Issues.</p>
<p>There&#8217;s been a bunch of PRs merged recently, with more to come later on. I&#8217;m grateful for all contributions. Please understand if I&#8217;m still slow to respond.</p>
]]></content:encoded>
							<wfw:commentRss>https://shlomi-noach.github.io/blog/mysql/orchestrator-whats-new-in-ci-testing-development/feed</wfw:commentRss>
		<slash:comments>0</slash:comments>
						<post-id xmlns="com-wordpress:feed-additions:1">8077</post-id>	</item>
		<item>
		<title>Pulling this blog out of Planet MySQL aggregator, over community concerns</title>
		<link>https://shlomi-noach.github.io/blog/mysql/pulling-this-blog-out-of-planet-mysql-aggregator-over-community-concerns</link>
				<comments>https://shlomi-noach.github.io/blog/mysql/pulling-this-blog-out-of-planet-mysql-aggregator-over-community-concerns#respond</comments>
				<pubDate>Thu, 23 Apr 2020 15:26:24 +0000</pubDate>
		<dc:creator><![CDATA[shlomi]]></dc:creator>
				<category><![CDATA[MySQL]]></category>
		<category><![CDATA[community]]></category>
		<category><![CDATA[orchestrator]]></category>
		<category><![CDATA[Planet]]></category>

		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=8020</guid>
				<description><![CDATA[I&#8217;ve decided to pull this blog (https://shlomi-noach.github.io/blog/) out of the planet.mysql.com aggregator. planet.mysql.com (formerly planetmysql.com) serves as a blog aggregator, and collects news and blog posts on various MySQL and its ecosystem topics. It collects some vendor and team blogs as well as &#8220;indie&#8221; blogs such as this one. It has traditionally been the go-to [&#8230;]]]></description>
								<content:encoded><![CDATA[<p>I&#8217;ve decided to pull this blog (<a href="https://shlomi-noach.github.io/blog/">https://shlomi-noach.github.io/blog/</a>) out of the <strong>planet.mysql.com</strong> aggregator.</p>
<p><strong>planet.mysql.com</strong> (formerly <strong>planetmysql.com</strong>) serves as a blog aggregator, and collects news and blog posts on various MySQL and its ecosystem topics. It collects some vendor and team blogs as well as &#8220;indie&#8221; blogs such as this one.</p>
<p>It has traditionally been the go-to place to catch up on the latest developments, or to read insightful posts. This blog itself has been aggregated in Planet MySQL for some eleven years.</p>
<p>Planet MySQL used to be owned by the MySQL community team. This recently changed with unwelcoming implications for the community.</p>
<p>I recently noticed how a blog post of mine, <a title="Link to The state of Orchestrator, 2020 (spoiler: healthy)" href="https://shlomi-noach.github.io/blog/mysql/the-state-of-orchestrator-2020-spoiler-healthy" rel="bookmark">The state of Orchestrator, 2020 (spoiler: healthy)</a>, did not get aggregated in Planet MySQL. After a quick discussion and investigation, it was determined (and confirmed) it was filtered out because it contained the word &#8220;MariaDB&#8221;. It has later been confirmed that Planet MySQL now filters out posts indicating its competitors, such as MariaDB, PostgreSQL, MongoDB.</p>
<p>Planet MySQL is owned by Oracle and it is their decision to make. Yes, logic implies they would not want to publish a promotional post for a competitor. However, I wish to explain how this blind filtering negatively affects the community.</p>
<p>But, before that, I&#8217;d like to share that I first attempted to reach out to whoever is in charge of Planet MySQL at this time (my understanding is that this is a marketing team). Sadly, two attempts at reaching out to them individually, and another attempt at reaching out on behalf of a small group of individual contributors, yielded no response. The owners would not have audience with me, and would not hear me out. I find it disappointing and will let others draw morals.</p>
<h3>Why filtering is harmful for the community</h3>
<p>We recognize that <strong>planet.mysql.com</strong> is an important information feed. It is responsible for a massive ratio of the traffic on my blog, and no doubt for many others. Indie blog posts, or small-team blog posts, practically depend on <strong>planet.mysql.com</strong> to get visibility.</p>
<p>And this is particularly important if you&#8217;re an open source developer who is trying to promote an open source project in the MySQL ecosystem. Without this aggregation, you will get significantly less visibility.</p>
<p>But, open source projects in the MySQL ecosystem do not live in MySQL vacuum, and typically target/support MySQL, Percona Server and MariaDB. As examples:</p>
<ul>
<li><a href="https://www.dbdeployer.com/">DBDeployer</a> should understand MariaDB versioning scheme</p>
</li>
<li>
<p><a href="https://www.skeema.io/">skeema</a> needs to recognize MariaDB features not present in MySQL</p>
</li>
<li>
<p><a href="https://proxysql.com/">ProxySQL</a> needs to support MariaDB Galera queries</p>
</li>
<li>
<p><a href="https://github.com/openark/orchestrator">orchestrator</a> needs to support MariaDB&#8217;s GTID flavor</p>
</li>
</ul>
<p>Consider that a blog post of the form &#8220;Project version 1.2.3 now released!&#8221; is likely to mention things like &#8220;fixed MariaDB GTID setup&#8221; or &#8220;MariaDB 10.x now supported&#8221; etc. Consider just pointing out that &#8220;PROJECT X supports MySQL, MariaDB and Percona Server&#8221;.</p>
<p>Consider that merely mentioning &#8220;MariaDB&#8221; gets your blog post filtered out on <strong>planet.mysql.com</strong>. This has an actual impact on open source development in the MySQL ecosystem. We will lose audience and lose adoption.</p>
<p>I believe the MySQL ecosystem as a whole will be negatively affected as result, and this will circle back to MySQL itself. I believe this goes against the very interests of Oracle/MySQL.</p>
<p>I&#8217;ve been around the MySQL community for some 12 years now. From my observation, there is no doubt that MySQL would not thrive as it does today, without the tooling, blogs, presentations and general advice by the community.</p>
<p>This is more than an estimation. I happen to know that, internally at MySQL, they have used or are using open source projects from the community, projects whose blog posts get filtered out today because they mention &#8220;MariaDB&#8221;. I find that disappointing.</p>
<p>I have personally witnessed how open source developments broke existing barriers to enable companies to use MySQL at greater scale, in greater velocity, with greater stability. I was part of such companies and I&#8217;ve personally authored such tools. I&#8217;m disappointed that <strong>planet.mysql.com</strong> filters out my blog posts for those tools and without giving me audience, and extend my disappointment for all open source project maintainers.</p>
<p>At this time I consider <strong>planet.mysql.com</strong> to be a marketing blog, not a community feed, and do not want to participate in its biased aggregation.</p>
]]></content:encoded>
							<wfw:commentRss>https://shlomi-noach.github.io/blog/mysql/pulling-this-blog-out-of-planet-mysql-aggregator-over-community-concerns/feed</wfw:commentRss>
		<slash:comments>0</slash:comments>
						<post-id xmlns="com-wordpress:feed-additions:1">8020</post-id>	</item>
		<item>
		<title>The state of Orchestrator, 2020 (spoiler: healthy)</title>
		<link>https://shlomi-noach.github.io/blog/mysql/the-state-of-orchestrator-2020-spoiler-healthy-2</link>
				<comments>https://shlomi-noach.github.io/blog/mysql/the-state-of-orchestrator-2020-spoiler-healthy-2#respond</comments>
				<pubDate>Tue, 18 Feb 2020 19:14:12 +0000</pubDate>
		<dc:creator><![CDATA[shlomi]]></dc:creator>
				<category><![CDATA[MySQL]]></category>
		<category><![CDATA[GitHub]]></category>
		<category><![CDATA[Open Source]]></category>
		<category><![CDATA[orchestrator]]></category>

		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=8016</guid>
				<description><![CDATA[This post serves as a pointer to my previous announcement about The state of Orchestrator, 2020. Thank you to Tom Krouper who applied his operational engineer expertise to content publishing problems.]]></description>
								<content:encoded><![CDATA[<p>This post serves as a pointer to my previous announcement about <a href="https://shlomi-noach.github.io/blog/mysql/the-state-of-orchestrator-2020-spoiler-healthy">The state of Orchestrator, 2020</a>.</p>
<p>Thank you to <a href="https://github.com/tomkrouper">Tom Krouper</a> who applied his operational engineer expertise to content publishing problems.</p>
]]></content:encoded>
							<wfw:commentRss>https://shlomi-noach.github.io/blog/mysql/the-state-of-orchestrator-2020-spoiler-healthy-2/feed</wfw:commentRss>
		<slash:comments>0</slash:comments>
						<post-id xmlns="com-wordpress:feed-additions:1">8016</post-id>	</item>
		<item>
		<title>The state of Orchestrator, 2020 (spoiler: healthy)</title>
		<link>https://shlomi-noach.github.io/blog/mysql/the-state-of-orchestrator-2020-spoiler-healthy</link>
				<comments>https://shlomi-noach.github.io/blog/mysql/the-state-of-orchestrator-2020-spoiler-healthy#respond</comments>
				<pubDate>Tue, 18 Feb 2020 08:09:05 +0000</pubDate>
		<dc:creator><![CDATA[shlomi]]></dc:creator>
				<category><![CDATA[MySQL]]></category>
		<category><![CDATA[GitHub]]></category>
		<category><![CDATA[Open Source]]></category>
		<category><![CDATA[orchestrator]]></category>

		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=7996</guid>
				<description><![CDATA[Yesterday was my last day at GitHub, and this post explains what this means for orchestrator. First, a quick historical review: 2014: I began work on orchestrator at Outbrain, as https://github.com/outbrain/orchestrator. I authored several open source projects while working for Outbrain, and created orchestrator to solve discovery, visualization and simple refactoring needs. Outbrain was happy [&#8230;]]]></description>
								<content:encoded><![CDATA[<p>Yesterday was my last day at GitHub, and this post explains what this means for <code>orchestrator</code>. First, a quick historical review:</p>
<ul>
<li><strong>2014</strong>: I began work on <code>orchestrator</code> at <a href="https://www.outbrain.com/">Outbrain</a>, as <a href="https://github.com/outbrain/orchestrator">https://github.com/outbrain/orchestrator</a>. I authored several open source projects while working for Outbrain, and created <code>orchestrator</code> to solve discovery, visualization and simple refactoring needs. Outbrain was happy to have the project developed as a public, open source repo from day 1, and it was released under the Apache 2 license. Interestingly, the idea to develop <code>orchestrator</code> came after I attended Percona Live Santa Clara 2014 and watched &#8220;ChatOps: How GitHub Manages MySQL&#8221; by one Sam Lambert.</li>
<li><strong>2015</strong>: Joined <a href="http://booking.com">Booking.com</a> where my main focus was to redesign and solve issues with the existing high availability setup. With Booking.com&#8217;s support, I continued work on <code>orchestrator</code>, pursuing better failure detection and recovery processes. Booking.com was an incredible playground and testbed for <code>orchestrator</code>, a massive deployment of multiple MySQL/MariaDB flavors and configuration.</li>
<li><strong>2016 &#8211; 2020</strong>: Joined <a href="http://github.com">GitHub</a>. GitHub <a href="https://github.blog/2016-12-08-orchestrator-github/">adopted</a> <code>orchestrator</code> and I developed it under GitHub&#8217;s own org, at <a href="https://github.com/github/orchestrator">https://github.com/github/orchestrator</a>. It became <a href="https://github.blog/2018-06-20-mysql-high-availability-at-github/">a core component</a> in github.com&#8217;s high availability design, running failure detection and recoveries across sites and geographical regions, with more to come. These 4+ years have been critical to <code>orchestrator</code>&#8216;s development and saw its widespread use. At this time I&#8217;m aware of multiple large-scale organizations using <code>orchestrator</code> for high availability and failovers. Some of these are GitHub, Booking.com, Shopify, Slack, Wix, Outbrain, and more. <code>orchestrator</code> is the underlying failover mechanism for <a href="https://vitess.io/">vitess</a>, and is also included in Percona&#8217;s <a href="https://www.percona.com/software/database-tools/percona-monitoring-and-management">PMM</a>. These years saw a significant increase in community adoption and contributions, in published content, such as Pythian and Percona technical blog posts, and, not surprisingly, increase in issues and feature requests.</li>
</ul>
<h3><strong><br />
</strong>2020</h3>
<p>GitHub was very kind to support moving the <code>orchestrator</code> repo under my own <a href="https://github.com/openark">https://github.com/openark</a> org. This means all issues, pull requests, releases, forks, stars and watchers have automatically transferred to the new location: <a href="https://github.com/openark/orchestrator">https://github.com/openark/orchestrator</a>. The old links do a &#8220;follow me&#8221; and implicitly direct to the new location. All external links to code and docs still work. I&#8217;m grateful to GitHub for supporting this transfer.</p>
<p>I&#8217;d like to thank all the above companies for their support of <code>orchestrator</code> and of open source in general. Being able to work on the same product throughout three different companies is mind blowing and an incredible opportunity. <code>orchestrator</code> of course remains open source and licensed with Apache 2. Existing Copyrights are unchanged.</p>
<p>As for what&#8217;s next: some personal time off, please understand if there&#8217;s delays to reviews/answers. My intention is to continue developing <code>orchestrator</code>. Naturally, the shape of future development depends on how <code>orchestrator</code> meets my future work. Nothing changes in that respect: my focus on <code>orchestrator</code> has always been first and foremost the pressing business needs, and then community support as possible. There are some interesting ideas by prominent <code>orchestrator</code> users and adopters and I&#8217;ll share more thoughts in due time.</p>
<p>&nbsp;</p>
]]></content:encoded>
							<wfw:commentRss>https://shlomi-noach.github.io/blog/mysql/the-state-of-orchestrator-2020-spoiler-healthy/feed</wfw:commentRss>
		<slash:comments>0</slash:comments>
						<post-id xmlns="com-wordpress:feed-additions:1">7996</post-id>	</item>
		<item>
		<title>MySQL master discovery methods, part 5: Service discovery &#038; Proxy</title>
		<link>https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-5-service-discovery-proxy</link>
				<comments>https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-5-service-discovery-proxy#respond</comments>
				<pubDate>Mon, 14 May 2018 08:08:32 +0000</pubDate>
		<dc:creator><![CDATA[shlomi]]></dc:creator>
				<category><![CDATA[MySQL]]></category>
		<category><![CDATA[High availability]]></category>
		<category><![CDATA[orchestrator]]></category>
		<category><![CDATA[Replication]]></category>

		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=7869</guid>
				<description><![CDATA[This is the fifth in a series of posts reviewing methods for MySQL master discovery: the means by which an application connects to the master of a replication tree. Moreover, the means by which, upon master failover, it identifies and connects to the newly promoted master. These posts are not concerned with the manner by [&#8230;]]]></description>
								<content:encoded><![CDATA[<p>This is the fifth in a series of posts reviewing methods for MySQL master discovery: the means by which an application connects to the master of a replication tree. Moreover, the means by which, upon master failover, it identifies and connects to the newly promoted master.</p>
<p>These posts are not concerned with the manner by which the replication failure detection and recovery take place. I will share <code>orchestrator</code> specific configuration/advice, and point out where cross DC <code>orchestrator/raft</code> setup plays part in discovery itself, but for the most part any recovery tool such as <code>MHA</code>, <code>replication-manager</code>, <code>severalnines</code> or other, is applicable.</p>
<p>We discuss asynchronous (or semi-synchronous) replication, a classic single-master-multiple-replicas setup. A later post will briefly discuss synchronous replication (Galera/XtraDB Cluster/InnoDB Cluster).</p>
<h3>Master discovery via Service discovery and Proxy</h3>
<p><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-4-proxy-heuristics">Part 4</a> presented with an anti-pattern setup, where a proxy would infer the identify of the master by drawing conclusions from backend server checks. This led to split brains and undesired scenarios. The problem was the loss of context.</p>
<p>We re-introduce a service discovery component (illustrated in <a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-3-app-service-discovery">part 3</a>), such that:</p>
<ul>
<li>The app does not own the discovery, and</li>
<li>The proxy behaves in an expected and consistent way.</li>
</ul>
<p>In a failover/service discovery/proxy setup, there is clear ownership of duties:</p>
<ul>
<li>The failover tool own the failover itself and the master identity change notification.</li>
<li>The service discovery component is the source of truth as for the identity of the master of a cluster.</li>
<li>The proxy routes traffic but does not make routing decisions.</li>
<li>The app only ever connects to a single target, but should allow for a brief outage while failover takes place.</li>
</ul>
<p>Depending on the technologies used, we can further achieve:</p>
<ul>
<li>Hard cut for connections to old, demoted master <code>M</code>.</li>
<li>Black/hold off for incoming queries for the duration of failover.</li>
</ul>
<p>We explain the setup using the following assumptions and scenarios:</p>
<ul>
<li>All clients connect to master via <code>cluster1-writer.example.net</code>, which resolves to a proxy box.</li>
<li>We fail over from master <code>M</code> to promoted replica <code>R</code>.</li>
</ul>
<p><span id="more-7869"></span></p>
<h3>A non planned failover illustration #1</h3>
<p>Master <code>M</code> has died, the box had a power failure. <code>R</code> gets promoted in its place. Our recovery tool:</p>
<ul>
<li>Updates service discovery component that <code>R</code> is the new master for <code>cluster1</code>.</li>
</ul>
<p>The proxy:</p>
<ul>
<li>Either actively or passively learns that <code>R</code> is the new master, rewires all writes to go to <code>R</code>.</li>
<li>If possible, kills existing connections to <code>M</code>.</li>
</ul>
<p>The app:</p>
<ul>
<li>Needs to know nothing. Its connections to <code>M</code> fail, it reconnects and gets through to <code>R</code>.</li>
</ul>
<h3>A non planned failover illustration #2</h3>
<p>Master <code>M</code> gets network isolated for <code>10</code> seconds, during which time we failover. <code>R</code> gets promoted.</p>
<p>Everything is as before.</p>
<p>If the proxy kills existing connections to <code>M</code>, then the fact <code>M</code> is back alive turns meaningless. No one gets through to <code>M</code>. Clients were never aware of its identity anyhow, just as they are unaware of <code>R</code>&#8216;s identity.</p>
<h3>Planned failover illustration</h3>
<p>We wish to replace the master, for maintenance reasons. We successfully and gracefully promote <code>R</code>.</p>
<ul>
<li>In the process of promotion, <code>M</code> turned read-only.</li>
<li>Immediately following promotion, our failover tool updates service discovery.</li>
<li>Proxy reloads having seen the changes in service discovery.</li>
<li>Our app connects to <code>R</code>.</li>
</ul>
<h3>Discussion</h3>
<p>This is a setup we use at GitHub in production. Our components are:</p>
<ul>
<li><code>orchestrator</code> for failover tool.</li>
<li><em>Consul</em> for service discovery.</li>
<li>GLB (HAProxy) for proxy</li>
<li><em>Consul template</em> running on proxy hosts:
<ul>
<li>listening on changes to Consul&#8217;s KV data</li>
<li>Regenerate <code>haproxy.cfg</code> configuration file</li>
<li><code>reload</code> haproxy</li>
</ul>
</li>
</ul>
<p>As mentioned earlier, the apps need not change anything. They connect to a name that is always resolved to proxy boxes. There is never a DNS change.</p>
<p>At the time of failover, the service discovery component must be up and available, to catch the change. Otherwise we do not strictly require it to be up at all times.</p>
<p>For high availability we will have multiple proxies. Each of whom must listen on changes to K/V. Ideally the name (<code>cluster1-writer.example.net</code> in our example) resolves to any available proxy box.</p>
<ul>
<li>This, in itself, is a high availability issue. Thankfully, managing the HA of a proxy layer is simpler than that of a MySQL layer. Proxy servers tend to be stateless and equal to each other.</li>
<li>See GLB as one example for a highly available proxy layer. Cloud providers, Kubernetes, two level layered proxies, Linux Heartbeat, are all methods to similarly achieve HA.</li>
</ul>
<p>See also:</p>
<ul>
<li><a href="https://blog.pythian.com/mysql-high-availability-with-haproxy-consul-and-orchestrator/">MySQL High Availability With HAProxy, Consul And Orchestrator</a></li>
<li><a href="https://www.percona.com/live/18/sessions/automatic-failovers-with-kubernetes-using-orchestrator-proxysql-and-zookeeper">Automatic Failovers with Kubernetes using Orchestrator, ProxySQL and Zookeeper</a></li>
<li><a href="https://www.percona.com/live/e17/sessions/orchestrating-proxysql-with-orchestrator-and-consul">Orchestrating ProxySQL with Orchestrator and Consul</a></li>
</ul>
<h3>Sample orchestrator configuration</h3>
<p>An <code>orchestrator</code> configuration would look like this:</p>
<pre><code class="json">  "ApplyMySQLPromotionAfterMasterFailover": true,
  "KVClusterMasterPrefix": "mysql/master",
  "ConsulAddress": "127.0.0.1:8500",
  "ZkAddress": "srv-a,srv-b:12181,srv-c",
  "PostMasterFailoverProcesses": [
    “/just/let/me/know about failover on {failureCluster}“,
  ],
</code></pre>
<p>In the above:</p>
<ul>
<li>If <code>ConsulAddress</code> is specified, <code>orchestrator</code> will update given <em>Consul</em> setup with K/V changes.</li>
<li>At <code>3.0.10</code>, <em>ZooKeeper</em>, via <code>ZkAddress</code>, is still not supported by <code>orchestrator</code>.</li>
<li><code>PostMasterFailoverProcesses</code> is here just to point out hooks are not strictly required for the operation to run.</li>
</ul>
<p>See <a href="https://github.com/github/orchestrator/blob/master/docs/configuration.md">orchestrator configuration</a> documentation.</p>
<h3>All posts in this series</h3>
<ul>
<li><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-1-dns">MySQL master discovery methods, part 1: DNS</a></li>
<li><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-2-vip-dns">MySQL master discovery methods, part 2: VIP &amp; DNS</a></li>
<li><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-3-app-service-discovery">MySQL master discovery methods, part 3: app &amp; service discovery</a></li>
<li><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-4-proxy-heuristics">MySQL master discovery methods, part 4: Proxy heuristics</a></li>
<li><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-5-service-discovery-proxy">MySQL master discovery methods, part 5: Service discovery &amp; Proxy</a></li>
<li><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-6-other-methods">MySQL master discovery methods, part 6: other methods</a></li>
</ul>
]]></content:encoded>
							<wfw:commentRss>https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-5-service-discovery-proxy/feed</wfw:commentRss>
		<slash:comments>0</slash:comments>
						<post-id xmlns="com-wordpress:feed-additions:1">7869</post-id>	</item>
		<item>
		<title>MySQL master discovery methods, part 4: Proxy heuristics</title>
		<link>https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-4-proxy-heuristics</link>
				<comments>https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-4-proxy-heuristics#respond</comments>
				<pubDate>Thu, 10 May 2018 06:10:34 +0000</pubDate>
		<dc:creator><![CDATA[shlomi]]></dc:creator>
				<category><![CDATA[MySQL]]></category>
		<category><![CDATA[High availability]]></category>
		<category><![CDATA[orchestrator]]></category>
		<category><![CDATA[Replication]]></category>

		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=7867</guid>
				<description><![CDATA[Note: the method described here is an anti pattern This is the fourth in a series of posts reviewing methods for MySQL master discovery: the means by which an application connects to the master of a replication tree. Moreover, the means by which, upon master failover, it identifies and connects to the newly promoted master. [&#8230;]]]></description>
								<content:encoded><![CDATA[<p><em>Note: the method described here is an anti pattern</em></p>
<p>This is the fourth in a series of posts reviewing methods for MySQL master discovery: the means by which an application connects to the master of a replication tree. Moreover, the means by which, upon master failover, it identifies and connects to the newly promoted master.</p>
<p>These posts are not concerned with the manner by which the replication failure detection and recovery take place. I will share <code>orchestrator</code> specific configuration/advice, and point out where cross DC <code>orchestrator/raft</code> setup plays part in discovery itself, but for the most part any recovery tool such as <code>MHA</code>, <code>replication-manager</code>, <code>severalnines</code> or other, is applicable.</p>
<p>We discuss asynchronous (or semi-synchronous) replication, a classic single-master-multiple-replicas setup. A later post will briefly discuss synchronous replication (Galera/XtraDB Cluster/InnoDB Cluster).</p>
<h3>Master discovery via Proxy Heuristics</h3>
<p>In Proxy Heuristics all clients connect to the master through a proxy. The proxy observes the backend MySQL servers and determines who the master is.</p>
<p><strong>This setup is simple and easy, but is an anti pattern. I recommend against using this method, as explained shortly</strong>.</p>
<p>Clients are all configured to connect to, say, <code>cluster1-writer.proxy.example.net:3306</code>. The proxy will intercept incoming requests either based on hostname or by port. It is aware of all/some MySQL backend servers in that cluster, and will route traffic to the master <code>M</code>.</p>
<p>A simple heuristic that I&#8217;ve seen in use is: pick the server that has <code>read_only=0</code>, a very simple check.</p>
<p>Let&#8217;s take a look at how this works and what can go wrong.</p>
<p><span id="more-7867"></span></p>
<h3>A non planned failover illustration #1</h3>
<p>Master <code>M</code> has died, the box had a power failure. <code>R</code> gets promoted in its place. Our recovery tool:</p>
<ul>
<li>Fails over, but doesn&#8217;t need to run any hooks.</li>
</ul>
<p>The proxy:</p>
<ul>
<li>Knows both about <code>M</code> and <code>R</code>.</li>
<li>Notices <code>M</code> fails health checks (<code>select @@global.read_only</code> returns error since the box is down).</li>
<li>Notices <code>R</code> reports healthy and with <code>read_only=0</code>.</li>
<li>Routes all traffic to <code>R</code>.</li>
</ul>
<p>Success, we&#8217;re happy.</p>
<h3>Configuration tip</h3>
<p>With an automated failover solution, use <code>read_only=1</code> in <code>my.cnf</code> at all times. Only the failover solution will set a server to <code>read_only=0</code>.</p>
<p>With this configuration, when <code>M</code> restarts, MySQL starts up as <code>read_only=1</code>.</p>
<h3>A non planned failover illustration #2</h3>
<p>Master <code>M</code> gets network isolated for <code>10</code> seconds, during which time we failover. <code>R</code> gets promoted. Our tool:</p>
<ul>
<li>Fails over, but doesn&#8217;t need to run any hooks.</li>
</ul>
<p>The proxy:</p>
<ul>
<li>Knows both about <code>M</code> and <code>R</code>.</li>
<li>Notices <code>M</code> fails health checks (<code>select @@global.read_only</code> returns error since the box is down).</li>
<li>Notices <code>R</code> reports healthy and with <code>read_only=0</code>.</li>
<li>Routes all traffic to <code>R</code>.</li>
<li><code>10</code> seconds later <code>M</code> comes back to life, claiming <code>read_only=0</code>.</li>
<li>The proxy now sees two servers reporting as healthy and with <code>read_only=0</code>.</li>
<li>The proxy has no context. It does not know why both are reporting the same. It is unaware of failovers. All it sees is what the backend MySQL servers report.</li>
</ul>
<p>Therein lies the problem: you can not trust multiple servers (MySQL backends) to deterministically pick a leader (the master) without them collaborating on some elaborate consensus communication.</p>
<h3>A non planned failover illustration #3</h3>
<p>Master <code>M</code> box is overloaded, issuing <code>too many connections</code> for incoming connections.</p>
<p>Our tool decides to failover.</p>
<ul>
<li>And doesn&#8217;t need to run any hooks.</li>
</ul>
<p>The proxy:</p>
<ul>
<li>Notices <code>M</code> fails health checks (<code>select @@global.read_only</code> does not respond because of the load).</li>
<li>Notices <code>R</code> reports healthy and with <code>read_only=0</code>.</li>
<li>Routes all traffic to <code>R</code>.</li>
<li>Shortly followed by <code>M</code> recovering (since no more writes are sent its way), claiming <code>read_only=0</code>.</li>
<li>The proxy now sees two servers reporting as healthy and with <code>read_only=0</code>.</li>
</ul>
<p>Again, the proxy has no context, and neither do <code>M</code> and <code>R</code>, for that matter. The context (the fact we failed over from <code>M</code> to <code>R</code>) was known to our failover tool, but was lost along the way.</p>
<h3>Planned failover illustration</h3>
<p>We wish to replace the master, for maintenance reasons. We successfully and gracefully promote <code>R</code>.</p>
<ul>
<li><code>M</code> is available and responsive, we set it to <code>read_only=1</code>.</li>
<li>We set <code>R</code> to <code>read_only=0</code>.</li>
<li>All new connections route to <code>R</code>.</li>
<li>We should also instruct our Proxy to kill all previous connections to <code>M</code>.</li>
</ul>
<p>This works very nicely.</p>
<h3>Discussion</h3>
<p>There is a substantial risk to this method. Correlation between failover and network partitioning/load (illustrations #2 and #3) is reasonable.</p>
<p>The root of the problem is that we expect individual servers to resolve conflicts without speaking to each other: we expect the MySQL servers to correctly claim &#8220;I&#8217;m the master&#8221; without context.</p>
<p>We then add to that problem by using the proxy to &#8220;pick a side&#8221; without giving it any context, either.</p>
<h3>Sample orchestrator configuration</h3>
<p>By way of discouraging use of this method I do not present an <code>orchestrator</code> configuration file.</p>
<h3>All posts in this series</h3>
<ul>
<li><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-1-dns">MySQL master discovery methods, part 1: DNS</a></li>
<li><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-2-vip-dns">MySQL master discovery methods, part 2: VIP &amp; DNS</a></li>
<li><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-3-app-service-discovery">MySQL master discovery methods, part 3: app &amp; service discovery</a></li>
<li><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-4-proxy-heuristics">MySQL master discovery methods, part 4: Proxy heuristics</a></li>
<li><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-5-service-discovery-proxy">MySQL master discovery methods, part 5: Service discovery &amp; Proxy</a></li>
<li><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-6-other-methods">MySQL master discovery methods, part 6: other methods</a></li>
</ul>
]]></content:encoded>
							<wfw:commentRss>https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-4-proxy-heuristics/feed</wfw:commentRss>
		<slash:comments>0</slash:comments>
						<post-id xmlns="com-wordpress:feed-additions:1">7867</post-id>	</item>
		<item>
		<title>MySQL master discovery methods, part 3: app &#038; service discovery</title>
		<link>https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-3-app-service-discovery</link>
				<comments>https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-3-app-service-discovery#respond</comments>
				<pubDate>Tue, 08 May 2018 08:02:19 +0000</pubDate>
		<dc:creator><![CDATA[shlomi]]></dc:creator>
				<category><![CDATA[MySQL]]></category>
		<category><![CDATA[High availability]]></category>
		<category><![CDATA[orchestrator]]></category>
		<category><![CDATA[Replication]]></category>

		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=7865</guid>
				<description><![CDATA[This is the third in a series of posts reviewing methods for MySQL master discovery: the means by which an application connects to the master of a replication tree. Moreover, the means by which, upon master failover, it identifies and connects to the newly promoted master. These posts are not concerned with the manner by [&#8230;]]]></description>
								<content:encoded><![CDATA[<p>This is the third in a series of posts reviewing methods for MySQL master discovery: the means by which an application connects to the master of a replication tree. Moreover, the means by which, upon master failover, it identifies and connects to the newly promoted master.</p>
<p>These posts are not concerned with the manner by which the replication failure detection and recovery take place. I will share <code>orchestrator</code> specific configuration/advice, and point out where cross DC <code>orchestrator/raft</code> setup plays part in discovery itself, but for the most part any recovery tool such as <code>MHA</code>, <code>replication-manager</code>, <code>severalnines</code> or other, is applicable.</p>
<p>We discuss asynchronous (or semi-synchronous) replication, a classic single-master-multiple-replicas setup. A later post will briefly discuss synchronous replication (Galera/XtraDB Cluster/InnoDB Cluster).</p>
<h3>App &amp; service discovery</h3>
<p><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-1-dns">Part 1</a> and <a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-2-vip-dns">part 2</a> presented solutions where the app remained ingorant of master&#8217;s identity. This part takes a complete opposite direction and gives the app ownership on master access.</p>
<p>We introduce a service discovery component. Commonly known are <em>Consul</em>, <em>ZooKeeper</em>, <em>etcd</em>, highly available stores offering key/value (K/V) access, leader election or full blown service discovery &amp; health.</p>
<p>We satisfy ourselves with K/V functionality. A key would be <code>mysql/master/cluster1</code> and a value would be the master&#8217;s hostname/port.</p>
<p>It is the app&#8217;s responsibility at all times to fetch the identity of the master of a given cluster by querying the service discovery component, thereby opening connections to the indicated master.</p>
<p>The service discovery component is expected to be up at all times and to contain the identity of the master for any given cluster.</p>
<p><span id="more-7865"></span></p>
<h3>A non planned failover illustration #1</h3>
<p>Master <code>M</code> has died. <code>R</code> gets promoted in its place. Our recovery tool:</p>
<ul>
<li>Updates the service discovery component, key is <code>mysql/master/cluster1</code>, value is <code>R</code>&#8216;s hostname.</li>
</ul>
<p>Clients:</p>
<ul>
<li>Listen on K/V changes, recognize that master&#8217;s value has changed.</li>
<li>Reconfigure/refresh/reload/do what it takes to speak to new master and to drop connections to old master.</li>
</ul>
<h3>A non planned failover illustration #2</h3>
<p>Master <code>M</code> gets network isolated for <code>10</code> seconds, during which time we failover. <code>R</code> gets promoted. Our tool (as before):</p>
<ul>
<li>Updates the service discovery component, key is <code>mysql/master/cluster1</code>, value is <code>R</code>&#8216;s hostname.</li>
</ul>
<p>Clients (as before):</p>
<ul>
<li>Listen on K/V changes, recognize that master&#8217;s value has changed.</li>
<li>Reconfigure/refresh/reload/do what it takes to speak to new master and to drop connections to old master.</li>
<li>Any changes not taking place in a timely manner imply some connections still use old master <code>M</code>.</li>
</ul>
<h3>Planned failover illustration</h3>
<p>We wish to replace the master, for maintenance reasons. We successfully and gracefully promote <code>R</code>.</p>
<ul>
<li>App should start connecting to <code>R</code>.</li>
</ul>
<h3>Discussion</h3>
<p>The app is the complete owner. This calls for a few concerns:</p>
<ul>
<li>How does a given app refresh and apply the change of master such that no stale connections are kept?
<ul>
<li>Highly concurrent apps may be more difficult to manage.</li>
</ul>
</li>
<li>In a polyglot app setup, you will need all clients to use the same setup. Implement same listen/refresh logic for Ruby, golang, Java, Python, Perl and notably shell scripts.
<ul>
<li>The latter do not play well with such changes.</li>
</ul>
</li>
<li>How can you validate that the change of master has been detected by all app nodes?</li>
</ul>
<p>As for the service discovery:</p>
<ul>
<li>What load will you be placing on your service discovery component?
<ul>
<li>I was familiar with a setup where there were so many apps and app nodes and app instances, such that the amount of connections was too much for the service discovery . In that setup caching layers were created, which introduced their own consistency problems.</li>
</ul>
</li>
<li>How do you handle service discovery outage?
<ul>
<li>A reasonable approach is to keep using last known master idendity should service discovery be down. This, again, plays better wih higher level applications, but less so with scripts.</li>
</ul>
</li>
</ul>
<p>It is worth noting that this setup does not suffer from geographical limitations to the master&#8217;s identity. The master can be anywhere; the service discovery component merely points out where the master is.</p>
<h3>Sample orchestrator configuration</h3>
<p>An <code>orchestrator</code> configuration would look like this:</p>
<pre><code class="json">  "ApplyMySQLPromotionAfterMasterFailover": true,
  "KVClusterMasterPrefix": "mysql/master",
  "ConsulAddress": "127.0.0.1:8500",
  "ZkAddress": "srv-a,srv-b:12181,srv-c",
  "PostMasterFailoverProcesses": [
    “/just/let/me/know about failover on {failureCluster}“,
  ],
</code></pre>
<p>In the above:</p>
<ul>
<li>If <code>ConsulAddress</code> is specified, <code>orchestrator</code> will update given <em>Consul</em> setup with K/V changes.</li>
<li>At <code>3.0.10</code>, <em>ZooKeeper</em>, via <code>ZkAddress</code>, is still not supported by <code>orchestrator</code>.</li>
<li><code>PostMasterFailoverProcesses</code> is here just to point out hooks are not strictly required for the operation to run.</li>
</ul>
<p>See <a href="https://github.com/github/orchestrator/blob/master/docs/configuration.md">orchestrator configuration</a> documentation.</p>
<h3>All posts in this series</h3>
<ul>
<li><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-1-dns">MySQL master discovery methods, part 1: DNS</a></li>
<li><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-2-vip-dns">MySQL master discovery methods, part 2: VIP &amp; DNS</a></li>
<li><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-3-app-service-discovery">MySQL master discovery methods, part 3: app &amp; service discovery</a></li>
<li><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-4-proxy-heuristics">MySQL master discovery methods, part 4: Proxy heuristics</a></li>
<li><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-5-service-discovery-proxy">MySQL master discovery methods, part 5: Service discovery &amp; Proxy</a></li>
<li><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-6-other-methods">MySQL master discovery methods, part 6: other methods</a></li>
</ul>
]]></content:encoded>
							<wfw:commentRss>https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-3-app-service-discovery/feed</wfw:commentRss>
		<slash:comments>0</slash:comments>
						<post-id xmlns="com-wordpress:feed-additions:1">7865</post-id>	</item>
		<item>
		<title>MySQL master discovery methods, part 2: VIP &#038; DNS</title>
		<link>https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-2-vip-dns</link>
				<comments>https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-2-vip-dns#comments</comments>
				<pubDate>Mon, 07 May 2018 06:46:22 +0000</pubDate>
		<dc:creator><![CDATA[shlomi]]></dc:creator>
				<category><![CDATA[MySQL]]></category>
		<category><![CDATA[High availability]]></category>
		<category><![CDATA[orchestrator]]></category>
		<category><![CDATA[Replication]]></category>

		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=7863</guid>
				<description><![CDATA[This is the second in a series of posts reviewing methods for MySQL master discovery: the means by which an application connects to the master of a replication tree. Moreover, the means by which, upon master failover, it identifies and connects to the newly promoted master. These posts are not concerned with the manner by [&#8230;]]]></description>
								<content:encoded><![CDATA[<p>This is the second in a series of posts reviewing methods for MySQL master discovery: the means by which an application connects to the master of a replication tree. Moreover, the means by which, upon master failover, it identifies and connects to the newly promoted master.</p>
<p>These posts are not concerned with the manner by which the replication failure detection and recovery take place. I will share <code>orchestrator</code> specific configuration/advice, and point out where cross DC <code>orchestrator/raft</code> setup plays part in discovery itself, but for the most part any recovery tool such as <code>MHA</code>, <code>replication-manager</code>, <code>severalnines</code> or other, is applicable.</p>
<p>We discuss asynchronous (or semi-synchronous) replication, a classic single-master-multiple-replicas setup. A later post will briefly discuss synchronous replication (Galera/XtraDB Cluster/InnoDB Cluster).</p>
<h3>Master discovery via VIP</h3>
<p>In <a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-1-dns">part 1</a> we saw that one the main drawbacks of DNS discovery is the time it takes for the apps to connect to the promoted master. This is the result of both DNS deployment time as well as client&#8217;s <code>TTL</code>.</p>
<p>A quicker method is offered: use of VIPs (Virtual IPs). As before, apps would connect to <code>cluster1-writer.example.net</code>, <code>cluster2-writer.example.net</code>, etc. However, these would resolve to specific VIPs.</p>
<p>Say <code>cluster1-writer.example.net</code> resolves to <code>10.10.0.1</code>. We let this address float between servers. Each server has its own IP (say <code>10.20.0.XXX</code>) but could also potentially claim the VIP <code>10.10.0.1</code>.</p>
<p>VIPs can be assigned by switches and I will not dwell into the internals, because I&#8217;m not a network expert. However, the following holds:</p>
<ul>
<li>Acquiring a VIP is a very quick operation.</li>
<li>Acquiring a VIP must take place on the acquiring host.</li>
<li>A host may be unable to acquire a VIP should another host holds the same VIP.</li>
<li>A VIP can only be assigned within a bounded space: hosts connected to the same switch; hosts in the same Data Center or availability zone.</li>
</ul>
<p><span id="more-7863"></span></p>
<h3>A non planned failover illustration #1</h3>
<p>Master <code>M</code> has died, the box had a power failure. <code>R</code> gets promoted in its place. Our recovery tool:</p>
<ul>
<li>Attempts to connect to <code>M</code> so that it can give up the VIP. The attempt fails because <code>M</code> is dead.</li>
<li>Connects to <code>R</code> and instructs it to acquire the VIP. Since <code>M</code> is dead there is no objection, and <code>R</code> successfully grabs the VIP.</li>
<li>Any new connections immediately route to the new master <code>R</code>.</li>
<li>Clients with connections to <code>M</code> cannot connect, issue retries, immediately route to <code>R</code>.</li>
</ul>
<h3>A non planned failover illustration #2</h3>
<p>Master <code>M</code> gets network isolated for <code>30</code> seconds, during which time we failover. <code>R</code> gets promoted. Our tool:</p>
<ul>
<li>Attempts to connect to <code>M</code> so that it can give up the VIP. The attempt fails because <code>M</code> is network isolated.</li>
<li>Connects to <code>R</code> and instructs it to acquire the VIP. Since <code>M</code> is network isolated there is no objection, and <code>R</code> successfully grabs the VIP.</li>
<li>Any new connections immediately route to the new master <code>R</code>.</li>
<li>Clients with connections to <code>M</code> cannot connect, issue retries, immediately route to <code>R</code>.</li>
<li><code>30</code> seconds later <code>M</code> reappears, but no one pays any attention.</li>
</ul>
<h3>A non planned failover illustration #3</h3>
<p>Master <code>M</code> box is overloaded. It is not responsive to new connections but may slowly serves existing connections. Our tool decides to failover:</p>
<ul>
<li>Attempts to connect to <code>M</code> so that it can give up the VIP. The attempt fails because <code>M</code> is very loaded.</li>
<li>Connects to <code>R</code> and instructs it to acquire the VIP. Unfortunately, <code>M</code> hasn&#8217;t given up the VIP and still shows up as owning it.</li>
<li>All existing and new connections keep on routing to <code>M</code>, even as <code>R</code> is the new master.</li>
<li>This continues until some time has passed and we are able to manually grab the VIP on <code>R</code>, or until we forcibly network isolate <code>M</code> or forcibly shut it down.</li>
</ul>
<p>We suffer outage.</p>
<h3>Planned failover illustration</h3>
<p>We wish to replace the master, for maintenance reasons. We successfully and gracefully promote <code>R</code>.</p>
<ul>
<li><code>M</code> is available and responsive, we ask it to give up the VIP, which is does.</li>
<li>We ask <code>R</code> to grab the VIP, which it does.</li>
<li>All new connections route to <code>R</code>.</li>
<li>We may still see old connections routing to <code>M</code>. We can forcibly network isolate <code>M</code> to break those connections so as to cause reconnects, or restart apps.</li>
</ul>
<h3>Discussion</h3>
<p>As with DNS discovery, the apps are never told of the change. They may be forcibly restarted though.</p>
<p>Grabbing a VIP is a quick operation. However, consider:</p>
<ul>
<li>It is not guaranteed to succeed. I have seen it fail in various situations.</li>
<li>Since releasing/acquiring of VIP can only take place on the demoted/promoted servers, respectively, our failover tool will need to:
<ul>
<li>Remote SSH onto both boxes, or</li>
<li>Remote exec a command on those boxes</li>
</ul>
</li>
<li>Moreover, the tool will do so sequentially. First we must connect to demoted master to give up the VIP, then to promoted master to acquire it.</li>
<li>This means the time at which the new master grabs the VIP depends on how long it takes to connect to the old master to give up the VIP. Seeing that the old master had <em>trouble</em> causing failover, we can expect correlation to not being able to connect to old master, or seeing slow connect time.</li>
<li>An alternative exists, in the form of <a href="http://clusterlabs.org/pacemaker/">Pacemaker</a>. Consider <a href="https://github.com/Percona-Lab/pacemaker-replication-agents/blob/master/doc/PRM-setup-guide.rst">Percona&#8217;s Replication Manager</a> guide for more insights. Pacemaker provides a single point of access from where the VIP can be moved, and behind the scenes it will communicate to relevant nodes. This makes it simpler on the failover solution configuration.</li>
<li>We are constrained by physical location.</li>
<li>It is still possible for existing connection to keep on communicating to the demoted master, even while the VIP has been moved.</li>
</ul>
<h3>VIP &amp; DNS combined</h3>
<p>Per physical location, we could choose to use VIP. But should we need to failover to a server in another DC, we could choose to combine the DNS discovery, discussed in <a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-1-dns">part 1</a>.</p>
<p>We can expect to see faster failover time on a local physical location, and longer failover time on remote location.</p>
<h3>Sample orchestrator configuration</h3>
<p>What kind of remote exec method will you have? In this sample we will use remote (passwordless) SSH.</p>
<p>An <code>orchestrator</code> configuration would look like this:</p>
<pre><code class="json">  "ApplyMySQLPromotionAfterMasterFailover": true,
  "PostMasterFailoverProcesses": [
    "ssh {failedHost} 'sudo ifconfig the-vip-interface down'",
    "ssh {successorHost} 'sudo ifconfig the-vip-interface up'",
    "/do/what/you/gotta/do to apply dns change for {failureClusterAlias}-writer.example.net to {successorHost}"
  ],  
</code></pre>
<p>In the above:</p>
<ul>
<li>Replace <code>SSH</code> with any remote exec method you may use.
<ul>
<li>But you will need to set up the access/credentials for <code>orchestrator</code> to run those operations.</li>
</ul>
</li>
<li>Replace <code>ifconfig</code> with <code>service quagga stop/start</code> or any method you use to release/grab VIPs.</li>
</ul>
<p>See <a href="https://github.com/github/orchestrator/blob/master/docs/configuration.md">orchestrator configuration</a> documentation.</p>
<h3>All posts in this series</h3>
<ul>
<li><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-1-dns">MySQL master discovery methods, part 1: DNS</a></li>
<li><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-2-vip-dns">MySQL master discovery methods, part 2: VIP &amp; DNS</a></li>
<li><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-3-app-service-discovery">MySQL master discovery methods, part 3: app &amp; service discovery</a></li>
<li><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-4-proxy-heuristics">MySQL master discovery methods, part 4: Proxy heuristics</a></li>
<li><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-5-service-discovery-proxy">MySQL master discovery methods, part 5: Service discovery &amp; Proxy</a></li>
<li><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-6-other-methods">MySQL master discovery methods, part 6: other methods</a></li>
</ul>
]]></content:encoded>
							<wfw:commentRss>https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-2-vip-dns/feed</wfw:commentRss>
		<slash:comments>3</slash:comments>
						<post-id xmlns="com-wordpress:feed-additions:1">7863</post-id>	</item>
		<item>
		<title>MySQL master discovery methods, part 1: DNS</title>
		<link>https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-1-dns</link>
				<comments>https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-1-dns#respond</comments>
				<pubDate>Thu, 03 May 2018 10:56:46 +0000</pubDate>
		<dc:creator><![CDATA[shlomi]]></dc:creator>
				<category><![CDATA[MySQL]]></category>
		<category><![CDATA[High availability]]></category>
		<category><![CDATA[orchestrator]]></category>
		<category><![CDATA[Replication]]></category>

		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=7861</guid>
				<description><![CDATA[This is the first in a series of posts reviewing methods for MySQL master discovery: the means by which an application connects to the master of a replication tree. Moreover, the means by which, upon master failover, it identifies and connects to the newly promoted master. These posts are not concerned with the manner by [&#8230;]]]></description>
								<content:encoded><![CDATA[<p>This is the first in a series of posts reviewing methods for MySQL master discovery: the means by which an application connects to the master of a replication tree. Moreover, the means by which, upon master failover, it identifies and connects to the newly promoted master.</p>
<p>These posts are not concerned with the manner by which the replication failure detection and recovery take place. I will share <code>orchestrator</code> specific configuration/advice, and point out where cross DC <code>orchestrator/raft</code> setup plays part in discovery itself, but for the most part any recovery tool such as <code>MHA</code>, <code>replication-manager</code>, <code>severalnines</code> or other, is applicable.</p>
<p>We discuss asynchronous (or semi-synchronous) replication, a classic single-master-multiple-replicas setup. A later post will briefly discuss synchronous replication (Galera/XtraDB Cluster/InnoDB Cluster).</p>
<h3>Master discovery via DNS</h3>
<p>In DNS master discovery applications connect to the master via a name that gets resolved to the master&#8217;s box. By way of example, apps would target the masters of different clusters by connecting to <code>cluster1-writer.example.net</code>, <code>cluster2-writer.example.net</code>, etc. It is up for the DNS to resolve those names to IPs.</p>
<p><span id="more-7861"></span></p>
<p>Issues for concern are:</p>
<ul>
<li>You will likely have multiple DNS servers. How many? In which data centers / availability zones?</li>
<li>What is your method for distributing/deploying a name change to all your DNS servers?</li>
<li>DNS will indicate a <code>TTL</code> (Time To Live) such that clients can cache the IP associated with a name for a given number of seconds. What is that <code>TTL</code>?</li>
</ul>
<p>As long as things are stable and going well, discovery via DNS makes sense. Trouble begins when the master fails over. Assume <code>M</code> used to be the master, but got demoted. Assume <code>R</code> used to be a replica, that got promoted and is now effectively the master of the topology.</p>
<p>Our failover solution has promoted <code>R</code>, and now needs to somehow apply the change, such that the apps connect to <code>R</code> instead of <code>M</code>. Some notes:</p>
<ul>
<li>The apps need not change configuration. They should still connect to <code>cluster1-writer.example.net</code>, <code>cluster2-writer.example.net</code>, etc.</li>
<li>Our tool instructs DNS servers to make the change.</li>
<li>Clients will still resolve to old IP based on <code>TTL</code>.</li>
</ul>
<h3>A non planned failover illustration #1</h3>
<p>Master <code>M</code> dies. <code>R</code> gets promoted. Our tool instructs all DNS servers on all DCs to update the IP address.</p>
<p>Say <code>TTL</code> is <code>60</code> seconds. Say update to all DNS servers takes <code>10</code> seconds. We will have between <code>10</code> and <code>70</code> seconds until all clients connect to the new master <code>R</code>.</p>
<p>During that time they will continue to attempt connecting to <code>M</code>. Since <code>M</code> is dead, those attempts will fail (thankfully).</p>
<h3>A non planned failover illustration #2</h3>
<p>Master <code>M</code> gets network isolated for <code>30</code> seconds, during which time we failover. <code>R</code> gets promoted. Our tool instructs all DNS servers on all DCs to update the IP address.</p>
<p>Again, assume <code>TTL</code> is <code>60</code> seconds. As before, it will take between <code>10</code> and <code>70</code> seconds for clients to learn of the new IP.</p>
<p>Clients who will require between <code>40</code> and <code>70</code> seconds to learn of the new IP will, however, hit an unfortunate scenario: the old master <code>M</code> reappears on the grid. Those clients will successfully reconnect to <code>M</code> and issue writes, leading to data loss (writes to <code>M</code> no longer replicate anywhere).</p>
<h3>Planned failover illustration</h3>
<p>We wish to replace the master, for maintenance reasons. We successfully and gracefully promote <code>R</code>. We need to change DNS records. Since this is a planned failover, we set the old master to <code>read_only=1</code>, or even better, we network isolated it.</p>
<p>And still our clients take <code>10</code> to <code>70</code> seconds to recognize the new master.</p>
<h3>Discussion</h3>
<p>The above numbers are just illustrative. Perhaps DNS deployment is quicker than <code>10</code> seconds. You should do your own math.</p>
<p><code>TTL</code> is a compromise which you can tune. Setting lower <code>TTL</code> will mitigate the problem, but will cause more hits on the DNS servers.</p>
<p>For planned takeover we can first deploy a change to the <code>TTL</code>, to, say, <code>2sec</code>, wait <code>60sec</code>, then deploy the IP change, then restore <code>TTL</code> to <code>60</code>.</p>
<p>You may choose to restart apps upon DNS deployment. This emulates apps&#8217; awareness of the change.</p>
<h3>Sample orchestrator configuration</h3>
<p><code>orchestrator</code> configuration would look like this:</p>
<pre><code class="json">  "ApplyMySQLPromotionAfterMasterFailover": true,
  "PostMasterFailoverProcesses": [
    "/do/what/you/gotta/do to apply dns change for {failureClusterAlias}-writer.example.net to {successorHost}"
  ],  
</code></pre>
<p>In the above:</p>
<ul>
<li><code>ApplyMySQLPromotionAfterMasterFailover</code> instructs <code>orchestrator</code> to <code>set read_only=0; reset slave all</code> on promoted server.</li>
<li><code>PostMasterFailoverProcesses</code> really depends on your setup. But <code>orchestrator</code> will supply with hints to your scripts: identity of cluster, identity of successor.</li>
</ul>
<p>See <a href="https://github.com/github/orchestrator/blob/master/docs/configuration.md">orchestrator configuration</a> documentation.</p>
<h3>All posts in this series</h3>
<ul>
<li><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-1-dns">MySQL master discovery methods, part 1: DNS</a></li>
<li><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-2-vip-dns">MySQL master discovery methods, part 2: VIP &amp; DNS</a></li>
<li><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-3-app-service-discovery">MySQL master discovery methods, part 3: app &amp; service discovery</a></li>
<li><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-4-proxy-heuristics">MySQL master discovery methods, part 4: Proxy heuristics</a></li>
<li><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-5-service-discovery-proxy">MySQL master discovery methods, part 5: Service discovery &amp; Proxy</a></li>
<li><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-6-other-methods">MySQL master discovery methods, part 6: other methods</a></li>
</ul>
]]></content:encoded>
							<wfw:commentRss>https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-1-dns/feed</wfw:commentRss>
		<slash:comments>0</slash:comments>
						<post-id xmlns="com-wordpress:feed-additions:1">7861</post-id>	</item>
	</channel>
</rss>
