<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	
	xmlns:georss="http://www.georss.org/georss"
	xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#"
	>

<channel>
	<title>TokuDB &#8211; code.openark.org</title>
	<atom:link href="https://shlomi-noach.github.io/blog/tag/tokudb/feed" rel="self" type="application/rss+xml" />
	<link>http://shlomi-noach.github.io/blog/</link>
	<description>Blog by Shlomi Noach</description>
	<lastBuildDate>Sat, 10 Dec 2016 09:26:50 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=5.3.3</generator>
<site xmlns="com-wordpress:feed-additions:1">32412571</site>	<item>
		<title>Percona Live 2015: Reflections</title>
		<link>https://shlomi-noach.github.io/blog/mysql/percona-live-2015-reflections</link>
				<comments>https://shlomi-noach.github.io/blog/mysql/percona-live-2015-reflections#respond</comments>
				<pubDate>Sat, 18 Apr 2015 01:41:07 +0000</pubDate>
		<dc:creator><![CDATA[shlomi]]></dc:creator>
				<category><![CDATA[MySQL]]></category>
		<category><![CDATA[Opinions]]></category>
		<category><![CDATA[PerconaLive]]></category>
		<category><![CDATA[secondary]]></category>
		<category><![CDATA[TokuDB]]></category>

		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=7223</guid>
				<description><![CDATA[Some personal reflections on PerconaLive 2015: Percona acquires Tokutek Well done! Tokutek develops the TokuDB storage engine for MySQL and TokuMX engine for MongoDB. I will discuss the MySQL aspect only. TokuDB was released as open source in 2013. It has attained a lot of traction and I have used it myself for some time. [&#8230;]]]></description>
								<content:encoded><![CDATA[<p>Some personal reflections on <a href="https://www.percona.com/live/mysql-conference-2015/">PerconaLive 2015</a>:</p>
<p><strong>Percona acquires Tokutek</strong></p>
<p>Well done! Tokutek develops the TokuDB storage engine for MySQL and TokuMX engine for MongoDB. I will discuss the MySQL aspect only.</p>
<p>TokuDB was released as open source in 2013. It has attained a lot of traction and I have used it myself for some time. I metÂ issues with locking or otherwise operational difficulties which I reported, and otherwise was fascinated by such features as great compression, online schema changes, and more.</p>
<p>Recently another company, InfiniDB, that also released its MySQL-backed codebase as open source, went out of business. I was afraid the same might happen to Tokutek.</p>
<p>I see Percona&#8217;s purchase as a very good move for the community. I saw a lot of TokuDB interest in Percona for some time now, and it is clearly interested in theÂ technology. I expect they will add their own hands-on experience into the development of more operations-friendly features; put effort in solving locking issues (it&#8217;s been a while since I last checked, of course some of these may have been addressed by now). I am guessing they will work onÂ a Galera/TokuDB integration and offer a &#8220;Toku-XtraDB-Cluster&#8221;.</p>
<p>TokuDB can compete with InnoDB in many places, while in others each will have its distinct advantage.</p>
<p>I see this is as good news for the community.</p>
<p><strong>Community Awards and Lightning Talks</strong></p>
<p>On a completely different subject, I believe it is commonly accepted that this year&#8217;s setup for the community awards &amp;Â lightning talks was unsuccessful. The noise was astounding, human traffic was interrupting and overall this was a poor experience. We (Giuseppe Maxia, Kortney Runyan &amp; myself) madeÂ a quick, informal brainstorming on this and came up with a couple ideas. One of which we hope to try in the upcoming <em>Percona Live Europe &#8211; Amsterdam</em>.</p>
<p>We apologize to the speakers for theÂ difficulties.</p>
<p><strong>Percona Live Europe &#8211; Amsterdam</strong></p>
<p>Haha! Having recently relocated to the Netherlands I&#8217;m of course very happy. But regardless, Percona Live London was fun &#8211; and yetÂ running on low fuel. I think it was a great idea to change location (and more locations expected in the future). This is the path taken by such conferences as OSCon, Velocity, Strata and more. Amsterdam in particular, as I&#8217;ve recently learned, is especially appreciated by many. I think this conf will do great!</p>
<p><strong>Woz</strong></p>
<p>And now for something completely different. Woz&#8217; talk was that. I&#8217;m happy he came; I appreciate that he discussed education; and it was fun.</p>
]]></content:encoded>
							<wfw:commentRss>https://shlomi-noach.github.io/blog/mysql/percona-live-2015-reflections/feed</wfw:commentRss>
		<slash:comments>0</slash:comments>
						<post-id xmlns="com-wordpress:feed-additions:1">7223</post-id>	</item>
		<item>
		<title>TokuDB configuration variables of interest</title>
		<link>https://shlomi-noach.github.io/blog/mysql/tokudb-configuration-variables-of-interest</link>
				<comments>https://shlomi-noach.github.io/blog/mysql/tokudb-configuration-variables-of-interest#comments</comments>
				<pubDate>Wed, 23 Oct 2013 17:42:12 +0000</pubDate>
		<dc:creator><![CDATA[shlomi]]></dc:creator>
				<category><![CDATA[MySQL]]></category>
		<category><![CDATA[Configuration]]></category>
		<category><![CDATA[InnoDB]]></category>
		<category><![CDATA[Performance]]></category>
		<category><![CDATA[TokuDB]]></category>

		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=6613</guid>
				<description><![CDATA[During our experiments I came upon a few TokuDB variables of interest; if you are using TokuDB you might want to look into these: tokudb_analyze_time This is a boundary on the number of seconds an ANALYZE TABLE will operate on each index on each partition on a TokuDB table. That is, if tokudb_analyze_time = 5, [&#8230;]]]></description>
								<content:encoded><![CDATA[<p>During our experiments I came upon a few TokuDB variables of interest; if you are using TokuDB you might want to look into these:</p>
<ul>
<li>
<h4>tokudb_analyze_time</h4>
</li>
</ul>
<p style="padding-left: 30px;">This is a boundary on the number of seconds an <strong>ANALYZE TABLE</strong> will operate on each index on each partition on a TokuDB table.</p>
<p style="padding-left: 30px;">That is, if <strong>tokudb_analyze_time = 5</strong>, and your table has <strong>4</strong> indexes (including <strong>PRIMARY</strong>) and <strong>7</strong> partitions, then the total runtime is limited to <strong>5*4*7 = 140</strong> seconds.</p>
<p style="padding-left: 30px;">Default in <strong>7.1.0</strong>: <strong>5</strong> seconds</p>
<ul>
<li>
<h4>tokudb_cache_size</h4>
</li>
</ul>
<p style="padding-left: 30px;">Similar to <strong>innodb_buffer_pool_size</strong>, this variable sets the amount of memory allocated by TokuDB for caching pages. Like InnoDB the table is clustered within the index, so the cache includes pages for both indexes and data.</p>
<p style="padding-left: 30px;">Default: <strong>50%</strong> of total memory</p>
<ul>
<li>
<h4>tokudb_directio</h4>
</li>
</ul>
<p style="padding-left: 30px;">Boolean, values are <strong>0/1</strong>. Setting <strong>tokudb_directio = 1</strong> is like specifying <strong>innodb_flush_method = O_DIRECT</strong>. Which in turn means the OS should not cache pages requested by TokuDB. Default: <strong>0</strong>.</p>
<p style="padding-left: 30px;">Now here&#8217;s the interesting part: we are used to tell InnoDB to get the most memory we can provide (because we want it to cache as much as it can) and to avoid OS caching (because that would mean a page would appear both in the buffer pool and in OS memory, which is a waste). So the following setup is common:<span id="more-6613"></span></p>
<blockquote style="padding-left: 30px;">
<pre style="padding-left: 30px;"><strong>innodb_buffer_pool_size</strong> = [as much as you can allocate while leaving room for connection memory]G
<strong>innodb_flush_method</strong> = O_DIRECT</pre>
</blockquote>
<p style="padding-left: 30px;">And my first instinct was to do the same for TokuDB. But after speaking to Gerry Narvaja of Tokutek, I realized it was not that simple. The reason TokuDB&#8217;s default memory allocation is <strong>50%</strong> and not, say, <strong>90%</strong>, is that OS cache caches the data in compressed form, while TokuDB cache caches data in uncompressed form. Which means if you limit the TokuDB cache, you allow for more cache to the OS, that is used to cache compressed data, which means <em>more data</em> (hopefully, pending duplicates) in memory.</p>
<p style="padding-left: 30px;">I did try both options and did not see an obvious difference, but did not test this thoroughly. My current setup is:</p>
<blockquote style="padding-left: 30px;">
<pre style="padding-left: 30px;"><strong>#No setup. just keep to the default for both:</strong>
#tokudb_cache_size
#tokudb_directio</pre>
</blockquote>
<ul>
<li>
<h4>tokudb_commit_sync</h4>
</li>
</ul>
<ul>
<li>
<h4>tokudb_fsync_log_period</h4>
</li>
</ul>
<p style="padding-left: 30px;">These two variable are similar in essence to <strong>innodb_flush_log_at_trx_commit</strong>, but allow for finer tuning. With <strong>innodb_flush_log_at_trx_commit</strong> you choose between syncing the transaction log to disk upon each commit and once per second. With <strong>tokudb_commit_sync = 1</strong> (which is default) you get transaction log sync to disk per commit. When <strong>tokudb_commit_sync = 0</strong>, then <strong>tokudb_fsync_log_period</strong> dictates the interval between flushes. So a value of <strong>tokudb_fsync_log_period = 1000</strong> means once per second.</p>
<p style="padding-left: 30px;">Since our original InnoDB installation used <strong>innodb_flush_log_at_trx_commit = 2</strong>, our TokuDB setup is:</p>
<blockquote style="padding-left: 30px;">
<pre style="padding-left: 30px;"><strong>tokudb_commit_sync</strong> = 0
<strong>tokudb_fsync_log_period</strong> = 1000</pre>
</blockquote>
<ul>
<li>
<h4>tokudb_load_save_space</h4>
</li>
</ul>
<p style="padding-left: 30px;">Turned on (value <strong>1</strong>) by default as of TokuDB <strong>7.1.0</strong>, this parameter decides whether temporary file created on bulk load operations (e.g. ALTER TABLE) are compressed or uncompressed. Do yourself a big favour (why? <a href="https://shlomi-noach.github.io/blog/mysql/converting-an-olap-database-to-tokudb-part-2-the-process-of-migration">read here</a>) and keep it on. Our setup is:</p>
<blockquote>
<pre><strong>tokudb_load_save_space</strong> = 1</pre>
</blockquote>
<p>TokuDB&#8217;s general recommendation is: don&#8217;t change the variables; the engine should work well right out of the box. I like the approach (by MySQL <strong>5.5</strong> I already lost count of InnoDB variables that can have noticeable impact; with <strong>5.6</strong> I&#8217;m all but lost). The complete list of configuration variables is found in <a href="http://www.tokutek.com/wp-content/uploads/2013/10/mysql-5.5.30-tokudb-7.1.0-users-guide.pdf">TokuDB&#8217;s Users Guide</a>.</p>
]]></content:encoded>
							<wfw:commentRss>https://shlomi-noach.github.io/blog/mysql/tokudb-configuration-variables-of-interest/feed</wfw:commentRss>
		<slash:comments>14</slash:comments>
						<post-id xmlns="com-wordpress:feed-additions:1">6613</post-id>	</item>
		<item>
		<title>Converting an OLAP database to TokuDB, part 3: operational stuff</title>
		<link>https://shlomi-noach.github.io/blog/mysql/converting-an-olap-database-to-tokudb-part-3-operational-stuff</link>
				<comments>https://shlomi-noach.github.io/blog/mysql/converting-an-olap-database-to-tokudb-part-3-operational-stuff#comments</comments>
				<pubDate>Mon, 14 Oct 2013 10:03:43 +0000</pubDate>
		<dc:creator><![CDATA[shlomi]]></dc:creator>
				<category><![CDATA[MySQL]]></category>
		<category><![CDATA[common_schema]]></category>
		<category><![CDATA[InnoDB]]></category>
		<category><![CDATA[TokuDB]]></category>

		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=6603</guid>
				<description><![CDATA[This is the third post in a series of posts describing our experience in migrating a large DWH server to TokuDB (see 1st and 2nd parts). This post discusses operations; namely ALTER TABLE operations in TokuDB. We ran into quite a few use cases by this time that we can shed light on. Quick recap: [&#8230;]]]></description>
								<content:encoded><![CDATA[<p>This is the third post in a series of posts describing our experience in migrating a large DWH server to TokuDB (see <a href="https://shlomi-noach.github.io/blog/mysql/converting-an-olap-database-to-tokudb-part-1">1st</a> and <a href="https://shlomi-noach.github.io/blog/mysql/converting-an-olap-database-to-tokudb-part-2-the-process-of-migration">2nd</a> parts). This post discusses operations; namely ALTER TABLE operations in TokuDB. We ran into quite a few use cases by this time that we can shed light on.</p>
<p>Quick recap: we&#8217;ve altered one of out DWH slaves to TokuDB, with the goal of migrating most of out servers, including the master, to TokuDB.</p>
<h4>Adding an index</h4>
<p>Shortly after migrating our server to TokuDB we noticed an unreasonably disproportionate slave lag on our TokuDB slave (red line in chart below) as compared to other slaves.</p>
<blockquote><p><a href="https://shlomi-noach.github.io/blog/wp-content/uploads/2013/09/tokudb-slave-lag.png"><img alt="tokudb-slave-lag" src="https://shlomi-noach.github.io/blog/wp-content/uploads/2013/09/tokudb-slave-lag.png" width="700" height="329" /></a></p></blockquote>
<p>Quick investigation led to the fact that, coincidentally, a manual heavy-duty operation was just taking place, which updated some year&#8217;s worth of data retroactively. OK, but why so slow on TokuDB? Another quick investigation led to an apples vs. oranges problem: as depicted in <a href="https://shlomi-noach.github.io/blog/mysql/converting-an-olap-database-to-tokudb-part-1">part 1</a>, our original setup included MONTHly partitioning on our larger tables, whereas we could not do the same in TokuDB, where we settled for YEARly partitioning.</p>
<p>The heavy-duty operation included a query that was relying on the MONTHly partitioning to do reasonable pruning: a <strong>WHERE</strong> condition on a date column did the right partition pruning; but where on InnoDB that would filter <strong>1</strong> month&#8217;s worth of data, on TokuDB it would filter <strong>1</strong> <em>year</em>.</p>
<p>Wasn&#8217;t it suggested that TokuDB has online table operations? I decided to give it a shot, and add a proper index on our date column (I actually created a compound index, but irrelevant).</p>
<p>It took <strong>13</strong> minutes to add an index on a <strong>1GB</strong> TokuDB table (approx. <strong>20GB</strong> InnoDB uncompressed equivalent):</p>
<ul>
<li>The <strong>ALTER</strong> was non blocking: table was unlocked at that duration</li>
<li>The client issuing the <strong>ALTER</strong> <em>was</em> blocked (I thought it would happen completely in the background) &#8212; but who cares?</li>
<li>I would say <strong>13</strong> minutes is fast</li>
</ul>
<p>Not surprisingly adding the index eliminated the problem altogether.</p>
<h4>Modifying a PRIMARY KEY</h4>
<p>It was suggested by our DBA that there was a long time standing need to modify our <strong>PRIMARY KEY</strong>. It was impossible to achieve with our InnoDB setup (not enough disk space for the operation, would take weeks to complete if we did have the disk space). Would it be possible to modify our TokuDB tables? On some of our medium-sized tables we issued an <strong>ALTER</strong> of the form:<span id="more-6603"></span></p>
<blockquote>
<pre>ALTER TABLE my_table DROP PRIMARY KEY, ADD PRIMARY KEY (c1, c2, c3, ...);</pre>
</blockquote>
<p>Time-wise the operation completed in good time. We did note, however, that the <a href="http://common-schema.googlecode.com/svn/trunk/common_schema/doc/html/tokudb_file_map.html">disk space consumed by the new table</a> was <em>doubled</em>. Was it due to the fact we added two columns to our PK? Did that account for the bloated space? I did not believe so, and decided to rebuild the table:</p>
<blockquote>
<pre>OPTIMIZE TABLE my_table</pre>
</blockquote>
<p>Nope. Space not reduced. However we were unconvinced and <a href="https://groups.google.com/forum/#!topic/tokudb-user/ow47QY2pcgU">asked</a>. As usual, we got quick response from the Tokutek team; this was a bug: while our original table used the TOKUDB_SMALL row format (high compression), the table rebuild reset it to TOKUDB_FAST (normal compression), which makes for roughly twice the file size. The bug was filed as: <a href="https://github.com/Tokutek/ft-engine/issues/107">alter table operations that rebuild the table lose the original tokudb compression</a>.</p>
<p>Now, we <em>were</em> altering the <strong>PRIMARY KEY</strong>. We were not expecting an online operation anyhow, and didn&#8217;t mind blocking the table; hence the solution was simple: make sure to spceify the row format:</p>
<blockquote>
<pre>ALTER TABLE my_table DROP PRIMARY KEY, ADD PRIMARY KEY (c1, c2, c3, ...) ENGINE=TokuDB ROW_FORMAT=TOKUDB_SMALL;</pre>
</blockquote>
<p>This worked in terms of disk space &#8212; but we only later realized it would still make us trouble.</p>
<h4>Modifying a PRIMARY KEY on our largest table</h4>
<p>We moved on to our largest table: originally <strong>1TB</strong> InnoDB <strong>COMPRESSED</strong>, worth of <strong>2TB</strong> uncompressed. With TokuDB it went down to <strong>100GB</strong>. Converting this table to TokuDB took about <strong>40</strong> hours, which is just fast. We issued an ALTAR TABLE modifying the PRIMARY KEY as above and waited.</p>
<p>The operation did not complete after <strong>40</strong> hours. Nor after <strong>3</strong> days. By day <strong>4</strong> we thought we might look into this. Fortunately, TokuDB is friendly on <strong>SHOW PROCESSLIST</strong> and provides you with useful information, such as &#8220;<strong>Fetched about 1234567890 rows, loading data still remains</strong>&#8220;. Yikes! We extrapolated the values to realize it would take <strong>2</strong> <em>weeks</em> to complete! Weekend went by and we decided to find a better way. Again, posting on the tokudb-user group, we got a definitive answer: a table rebuild does not utilize the <em>bulk loader</em> (you really want to be friends with the bulk loader, it&#8217;s the process that loads your data quickly).</p>
<p>And so we chose to <strong>KILL</strong> the <strong>ALTER</strong> process and go another way; again, <strong>KILL</strong>s are very easy with TokuDB <strong>ALTER</strong> operations: took <strong>3</strong> minutes to abort this week old operation. The alternative operation was:</p>
<blockquote>
<pre>CREATE TABLE my_table_New LIKE my_table;
ALTER TABLE my_table_New DROP PRIMARY KEY, ADD PRIMARY KEY (c1, c2, c3, ...) ENGINE=TokuDB ROW_FORMAT=TOKUDB_SMALL;
INSERT INTO my_table_New SELECT * FROM my_table;
RENAME TABLE my_table TO my_table_Old, my_table_New TO my_table;
DROP TABLE my_table_Old;</pre>
</blockquote>
<p>The <strong>INSERT INTO &#8230; SELECT</strong> operation does use the bulk loader when you do it on an empty table. It completed within merely <strong>30</strong> hours. Hurrah!</p>
<h4>DROPping a TABLE</h4>
<p>It was an immediate operation to drop our &#8220;Old&#8221; table &#8212; subsecond. Nothing like your InnoDB DROP.</p>
]]></content:encoded>
							<wfw:commentRss>https://shlomi-noach.github.io/blog/mysql/converting-an-olap-database-to-tokudb-part-3-operational-stuff/feed</wfw:commentRss>
		<slash:comments>7</slash:comments>
						<post-id xmlns="com-wordpress:feed-additions:1">6603</post-id>	</item>
		<item>
		<title>Converting an OLAP database to TokuDB, part 2: the process of migration</title>
		<link>https://shlomi-noach.github.io/blog/mysql/converting-an-olap-database-to-tokudb-part-2-the-process-of-migration</link>
				<comments>https://shlomi-noach.github.io/blog/mysql/converting-an-olap-database-to-tokudb-part-2-the-process-of-migration#comments</comments>
				<pubDate>Mon, 09 Sep 2013 03:29:30 +0000</pubDate>
		<dc:creator><![CDATA[shlomi]]></dc:creator>
				<category><![CDATA[MySQL]]></category>
		<category><![CDATA[common_schema]]></category>
		<category><![CDATA[Percona Toolkit]]></category>
		<category><![CDATA[TokuDB]]></category>

		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=6530</guid>
				<description><![CDATA[This is a second in a series of posts describing our experience in migrating a large DWH server to TokuDB. This post discusses the process of migration itself. As a quick recap (read part 1 here), we have a 2TB compressed InnoDB (4TB uncompressed) based DWH server. Space is running low, and we&#8217;re looking at [&#8230;]]]></description>
								<content:encoded><![CDATA[<p>This is a second in a series of posts describing our experience in migrating a large DWH server to TokuDB. This post discusses the process of migration itself.</p>
<p>As a quick recap (<a href="https://shlomi-noach.github.io/blog/mysql/converting-an-olap-database-to-tokudb-part-1">read part 1 here</a>), we have a <strong>2TB</strong> compressed InnoDB (<strong>4TB</strong> uncompressed) based DWH server. Space is running low, and we&#8217;re looking at TokuDB for answers. Early experiments show that TokuDB&#8217;s compression could make a good impact on disk space usage. I&#8217;m still not discussing performance &#8212; keeping this till later post.</p>
<p>Those with weak hearts can skip right to the end, where we finally have a complete conversion. You can also peek at the very end to find out how much <strong>4TB</strong> uncompressed InnoDB data is worth in TokuDB. But you might want to read through. The process was not smooth, and not as expected (it&#8217;s a war story thing). Throughout the migration we got a lot of insight on TokuDB&#8217;s behaviour, limitations, conveniences, inconveniences and more.</p>
<p>Disclosure: I have no personal interests and no company interests; throughout the process we were in touch with Tokutek engineers, getting free, friendly &amp; professional advice and providing with input of our own. Most of this content has already been presented to Tokutek throughout the process. TokuDB is open source and free to use, though commercial license is also available.</p>
<h4>How do you convert 4TB worth of data to TokuDB?</h4>
<p>Obviously one table at a time. But we had another restriction: you may recall I took a live slave for the migration process. And we wanted to end the process with a live slave. So the restriction was: keep it replicating!</p>
<p>How easy would that be? Based on our initial tests, I extrapolated over <strong>20</strong> days of conversion from InnoDB to TokuDB. Even with one table at a time, our largest table was expected to convert in some <strong>12-14</strong> days. Can we retain <strong>14</strong> days of binary logs on a server already running low on disk space? If only I knew then what I know today <img src="https://s.w.org/images/core/emoji/12.0.0-1/72x72/1f642.png" alt="ðŸ™‚" class="wp-smiley" style="height: 1em; max-height: 1em;" /><span id="more-6530"></span></p>
<p>I recently claimed (as I recall it was in one of theÂ <a href="https://twitter.com/DBHangops">@DBHangops</a> meetings) I was <em>done</em> with <strong>ALTER TABLE</strong> statements. I would not touch them again: with <a href="http://openarkkit.googlecode.com/svn/trunk/openarkkit/doc/html/oak-online-alter-table.html">oak-online-alter-table</a> and <a href="http://www.percona.com/doc/percona-toolkit/2.2/pt-online-schema-change.html">pt-online-schema-change</a> I can get better control of my server (and my sleep). Can I use an online migration tool?</p>
<p>Fortunately we are using Statement Based Replication on this MySQL topology. This makes for good news, because triggers are activated on slave as it is replicating its master&#8217;s statements. You can run an online migration tool <em>on the slave</em>, while it <em>keeps replicating</em>. This is so cool. I don&#8217;t need to worry too much about binary logs and relay logs. I can take my time with conversion&#8230;</p>
<p>I chose to useÂ <em>pt-online-schema-change</em>. Why choose Percona&#8217;s tool over my own? Percona&#8217;s tool supports the <strong>&#8211;check-slave-lag</strong> option, which throttles the operation should the server&#8217;s slaves start lagging. Wait, I&#8217;m running the tool <em>on a slave</em>, so what&#8217;s the point? Well, you can cheat and provide <strong>&#8211;check-slave-lag=h=127.0.0.1</strong> so that the tool assumes the localhost is the slave (while it is actually the server being altered); which means it will check <em>on its own slave lag</em> to do the throttling. This works well and is fun to watch.</p>
<h4>Starting the migration</h4>
<p>Some of our tables had the <strong>KEY_BLOCK_SIZE</strong> explicitly declared. As I mentioned in previous post, for TokuDB &lt;= <strong>7.0.4</strong> this causes problems by bloating the indexes instead of compressing them (and Tim Callaghan of Tokutek notes this is fixed in next version). <a href="http://common-schema.googlecode.com/svn/trunk/common_schema/doc/html/sql_alter_table_tokudb.html">common_schema to the rescue</a>: the <strong>ALTER</strong> statement has to include a <strong>DROP</strong> and recreate of all indexes.</p>
<p>This is the place to mention our tables are heavily partitioned. This will play a crucial role in the next events. We use RANGE partitions over dates. We have a monthly partitioning scheme on all partitioned tables. And we have partitions to spare: starting a few years back from today (depending on the table) and made until <strong>Dec. 2025</strong> &#8212; making for some <strong>170 &#8211; 200</strong> partitions per table.</p>
<p>Starting from smallest table (a few rows) and increasing in size, we migrated tables one by one to TokuDB.</p>
<h4>Party crashers</h4>
<p>A few party crashers were made obvious right from the start (they are all being addressed by Tokutek as far as I know):</p>
<ol>
<li><strong>@@datadir</strong>: all TokuDB files reside in @@datadir. You get a pile of files in the same directory where you would find your ib_logfile*, master.info, etc. files.</li>
<li>File names: you do not get file names after table names. Instead, you get cryptic names like <strong>./_myschema_sql_7fda_8e73_*</strong>.You would suspect that there is some uniqueness to the <strong>7fda_8e73</strong> thing; that it relates to a single table &#8212; it doesn&#8217;t. Same table get different file names, different tables get similar names &#8212; there&#8217;s not one regular expression to differentiate tables &#8212; and I do know my regexes.</li>
<li><strong>INFORMATION_SCHEMA.Tokudb_file_map</strong> doesn&#8217;t make it much easier, either. It is meant to tell you about tables to file names mappings. But the tables are not laid out in easy TABLE_SCHEMA, TABLE_NAME columns, but are denormalized themselves, and can be vague and almost ambiguous, to some extent. <em>common_schema</em> to the rescue, its rewritten <a href="http://common-schema.googlecode.com/svn/trunk/common_schema/doc/html/tokudb_file_map.html">tokudb_file_map</a> maps your tables to aggregated list of file names, along with shell commands you would typically want to issue.</li>
</ol>
<p>But let me emphasize these issues, before you think I&#8217;m just spoiled. TokuDB creates multiple files per table: other than the standard <strong>.frm</strong> file, you get one &#8220;main&#8221; file for each table, and then another file for each index. So it&#8217;s quite possible your table will consist of some <strong>7</strong> files.</p>
<p>Well, as you may know, MySQL&#8217;s implementation of partitioning is that each partition is made of its own standalone table, hidden from the user (but easily viewable on the filesystem). Now this means a single table with <strong>170</strong> partitions and a few indexes can make for over <strong>1,000</strong> files. That&#8217;s right &#8211; for a single table. We have a few dozens like this.</p>
<p>Now consider:</p>
<ul>
<li>You may need to accommodate thousands or tens of thousands of files in your <strong>@@datadir</strong></li>
<li>None of which it is easy for you to know who to relate to.</li>
</ul>
<p>So <em>common_schema</em>&#8216;s <strong>tokudb_file_map</strong> gives you this crazy list of <strong>1,000</strong> files which make up your single table. This isn&#8217;t too friendly, either.</p>
<p>I think Tokutek are missing here on one of the greatest advantages they bring to the table. The one thing a DBA want to know when looking at her MySQL&#8217;s filesystem is: how much disk space is used by a schema/table. And this information becomes hard to get. Again, <em>common_schema</em>&#8216;s view will provide you with the script to do it (<strong>du -c &#8230; | tail -1</strong>) &#8212; but you would have to go into MySQL, out to shell&#8230; Bother.</p>
<h4>Crashes and failures</h4>
<p>The above did not stop at inconveniences. Soon enough, and while still altering my smaller tables, I would get failures from <strong>pt-online-schema-change</strong>. Checking manually to eliminate the possibility of a bug in Percona&#8217;s tool, I got:</p>
<blockquote>
<pre>mysql&gt; alter table my_schema.my_table engine=tokudb row_format=tokudb_small;
ERROR 1016 (HY000): Can't open file: './my_schema/#sql-80d_2.frm' (errno: 24)</pre>
</blockquote>
<p>This would happen again and again and again. What&#8217;s the deal here?</p>
<p>Let me cut short on this one: we got over <strong>20,000</strong> files in <strong>@@datadir</strong>. And MySQL was unable to open any more files. Mind you, we had:</p>
<ul>
<li><strong>open_files_limit</strong>=30000</li>
<li><strong>open_table_cache</strong>=16000</li>
<li><strong>table_definition_cache</strong>=3000</li>
</ul>
<p>Quite the generous numbers (also backed up by <strong>ulimit</strong>, to be on the safe side; and also note we&#8217;re using <strong>XFS</strong> file system). And yet, open files were an issue. To prove my point, it was possible to <strong>ALTER</strong> a table with a fewer number of partitions. It was then possible to <strong>ALTER</strong> another smaller table without partitions. It was then impossible to alter any additional partitioned table. Once I dropped partitioning for some very small table some room was made and I was again able to <strong>ALTER</strong> a partitioned table&#8230; And this would happen for completely empty tables &#8212; no data involved. We were on some file limit here.</p>
<p>Another such <strong>ALTER</strong> and the server crashed. This was quite unceremonious. The error log produced nothing; no stack trace. Zit.</p>
<p>I was fervently querying the <strong>Tokudb_file_map</strong> to get a picture of what&#8217;s going on. I would need to do a self join on the table (as <em>common_schema</em>&#8216;s view does) to get a per-table listing of files. This would occasionally crash the server. I guess I had <strong>3</strong> or <strong>4</strong> such crashes.</p>
<h4>Recovery</h4>
<p>On this I feel I&#8217;m quite the veteran here :D. TokuDB recovery works well. As with InnoDB, TokuDB recognize there has been a crash, and before allowing MySQL to open connections it restores to a stable state.</p>
<h4>ALTER TABLE in TokuDB</h4>
<p>Here I found two comforting features (the third and fourth yet to be discovered). As indicated above, I did turn to issue a manual ALTER TABLE. What I found was:</p>
<ol>
<li>A TokuDB <strong>ALTER TABLE</strong> statement lets you know its progress. This is no little thing! Your <strong>SHOW PROCESSLIST</strong> output shows messages like <strong>&#8220;Fetched about 312724000 rows, loading data still remains&#8221;</strong>, or <strong>&#8220;Loading of data about 66.1% done&#8221;</strong>, or <strong>&#8220;Queried about 33113001 rows, Inserted about 33113000 rows&#8221;</strong>. Cool!</li>
<li>Even better, the crash I had during the <strong>ALTER TABLE</strong>? I thought that would be the end of it. If you ever had a crash while <strong>ALTER</strong>ing an InnoDB table, you know how it goes: InnoDB will forever complain about some table existing but not listed (or the other way around). And don&#8217;t get me started with <strong>DISCARD TABLESPACE</strong>; when InnoDB decides it is upset about something &#8211; you cannot convince it otherwise.<br />
Thankfully, TokuDB completely reverted the <strong>ALTER</strong> operation. It removed what temporary files were created (further notes following) and forgot all about it. No complaints, no ghosts. Great! Back to consistency!</li>
</ol>
<h4>What do we do now?</h4>
<p>Throwing my hands up in the air, having worked on this for many days, I thought to myself: OK, I still have this server all to myself. If TokuDB is not going to work out, I have some time to come up with a sharding/commercial solution. Let&#8217;s use up this time and learn something about TokuDB. And I decided to re-create all tables without partitions. My colleague argued that she was not ready to give up on partitioning altogether and we decided to try again with <strong>YEAR</strong>ly partitioing scheme. This would reduce number of files by factor of <strong>12</strong>. Also, <strong>2025</strong> is so far away, we agreed to settle for <strong>2015</strong>. So reducing number of files by factor of <strong>25-30</strong>.</p>
<p>And this made all the difference in the world. Having reduced number of files made the impact we were hoping for. Suddenly all went well. No crashes, no weird complaints, little proliferation of files in <strong>@@datadir</strong>.</p>
<h4>ALTER TABLE</h4>
<p>And I did notice that a manual <strong>ALTER TABLE</strong> went <em>considerably</em> faster than I would expect. And by far faster than the <em>pt-online-schema-change</em> pace. I tried a couple more &#8212; sure thing. <strong>ALTER</strong>ing a table from InnoDB to TokuDB is <em>fast</em>.</p>
<p>How fast?</p>
<ul>
<li>I converted a <strong>47GB</strong> InnoDB COMPRESSED table to TokuDB in <strong>73</strong> minutes. By the way, resulting table size measured <strong>3.4GB</strong>.</li>
<li>A <strong>330GB</strong> InnoDB COMPRESSED table converted to TokuDB took little over <strong>9</strong> hours. I dare you alter 600GB worth of uncompressed data into InnoDB (COMPRESSED) in less than a few days. It went down to <strong>31GB</strong>.</li>
<li>And our largest, <strong>1TB COMPRESSED</strong>Â  table (<strong>2TB</strong> worth of uncompressed data)? There&#8217;s yet another story here.</li>
</ul>
<h4>Altering 1 (2 uncomressed) TB of data</h4>
<p>Here&#8217;s a tip that will save you some exhaustion: <strong>SET tokudb_load_save_space := 1</strong>.</p>
<p>While <strong>ALTER</strong>ing our largest table, I was concerned to find our disk space was running low. Plenty temporary TokuDB files were created. I assumed these would consume only so much disk space, but to my surprise they accumulated and accumulated&#8230; It turns out for <strong>ALTER</strong>ing a table TokuDB creates the equivalent of the table in temporary files, and only then generates the new table. This means you need to have enough room for your own original table, the equivalent in temporary files, and your new table altogether.</p>
<p>With great compression that would be nothing. However you&#8217;ll be surprised to learn that by default those temporary files are <em>not compressed</em>. Thus, the <strong>ALTER</strong> operation consumed more than <strong>1.3TB</strong> of disk space in temporary files, until I had no choice and (<strong>36</strong> hours into the operation) had to <strong>KILL</strong> it before it consumed the entire <strong>3TB</strong> of disk space.</p>
<p>Setting the variable as specified and the next attempt was far more successful: the temporary files were created with same compression algorithm as target table, which left with a lot of free space to work with.</p>
<p>ALTER time took about <strong>40</strong> hours.</p>
<h4>Well, what&#8217;s the resulting size?</h4>
<p>And we were finally done! It took the better part of three weeks to work through all the pitfalls, the <em>pt-online-schems-change</em> attempts, the crashes, the tests, the no-partitions, the <strong>YEAR</strong>ly partitions&#8230; Finally we are with a TokuDB version of our data warehouse.</p>
<p>Suspension is over. We got from <strong>2TB</strong> of InnoDB <strong>COMPRESSED</strong> (<strong>KEY_BLOCK_SIZE=8</strong>) down to <strong>200GB</strong> of <strong>TokuDB_SMALL</strong> (aka agressive, aka lzma) tables.</p>
<p>I mean, this is beyond expectations. It is <em>ridiculously</em> small. From <strong>80%</strong> disk space utilization down to <strong>8%</strong> disk space utilization. <em>Absolutely ridiculous!</em></p>
<h4>Conclusions</h4>
<ul>
<li>TokuDB does not play well with many partitions.</li>
<li>Crashes encountered. Recovery is fine.</li>
<li>Good <strong>ALTER TABLE</strong> experience</li>
<li><strong>SET tokudb_load_save_space := 1</strong></li>
<li>Great compression (<strong>x20</strong> from uncompressed InnoDB; <strong>x10</strong> from KEY_BLOCK_SIZE=8)</li>
</ul>
<h4>Next</h4>
<p>In the following post I&#8217;ll share some observations on how well our newly converted TokuDB slave performs as compared to our equivalent InnoDB slaves; some configuration you might care about; and some things you can do with TokuDB that would be so very painful with InnoDB. Stay tuned!</p>
]]></content:encoded>
							<wfw:commentRss>https://shlomi-noach.github.io/blog/mysql/converting-an-olap-database-to-tokudb-part-2-the-process-of-migration/feed</wfw:commentRss>
		<slash:comments>7</slash:comments>
						<post-id xmlns="com-wordpress:feed-additions:1">6530</post-id>	</item>
		<item>
		<title>Converting an OLAP database to TokuDB, part 1</title>
		<link>https://shlomi-noach.github.io/blog/mysql/converting-an-olap-database-to-tokudb-part-1</link>
				<comments>https://shlomi-noach.github.io/blog/mysql/converting-an-olap-database-to-tokudb-part-1#comments</comments>
				<pubDate>Tue, 03 Sep 2013 07:04:12 +0000</pubDate>
		<dc:creator><![CDATA[shlomi]]></dc:creator>
				<category><![CDATA[MySQL]]></category>
		<category><![CDATA[common_schema]]></category>
		<category><![CDATA[compression]]></category>
		<category><![CDATA[InnoDB]]></category>
		<category><![CDATA[Open Source]]></category>
		<category><![CDATA[Performance]]></category>
		<category><![CDATA[QueryScript]]></category>
		<category><![CDATA[TokuDB]]></category>

		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=6473</guid>
				<description><![CDATA[This is the first in a series of posts describing my impressions of converting a large OLAP server to TokuDB. There&#8217;s a lot to tell, and the experiment is not yet complete, so this is an ongoing blogging. In this post I will describe the case at hand and out initial reasons for looking at [&#8230;]]]></description>
								<content:encoded><![CDATA[<p>This is the first in a series of posts describing my impressions of converting a large OLAP server to TokuDB. There&#8217;s a lot to tell, and the experiment is not yet complete, so this is an ongoing blogging. In this post I will describe the case at hand and out initial reasons for looking at TokuDB.</p>
<p>Disclosure: I have no personal interests and no company interests; we did get friendly, useful and free advice from Tokutek engineers. TokuDB is open source and free to use, though commercial license is also available.</p>
<h4>The case at hand</h4>
<p>We have a large and fast growing DWH MySQL setup. This data warehouse is but one component in a larger data setup, which includes Hadoop, Cassandra and more. For online dashboards and most reports, MySQL is our service. We populate this warehouse mainly via Hive/Hadoop. Thus, we have an hourly load of data from Hive, as well as a larger daily load.</p>
<p>There are some updates on the data, but the majority of writes are just <strong>mysqlimport</strong>s of Hive queries.</p>
<p>Usage of this database is OLAP: no concurrency issues here; we have some should-be-fast-running queries issued by our dashboards, as well as ok-to-run-longer queries issued for reports.</p>
<p>Our initial and most burning trouble is with size. Today we use <strong>COMPRESSED</strong> InnoDB tables (<strong>KEY_BLOCK_SIZE</strong> is default, i.e. <strong>8</strong>). Our data volume sums right now at about <strong>2TB</strong>. I happen to know this translates as <strong>4TB</strong> of uncompressed data.</p>
<p>However growth of data is accelerating. A year ago we would capture a dozen GB per month. Today it is a <strong>100GB</strong> per month, and by the end of this year it may climb to <strong>150GB</strong> per month or more.</p>
<p>Our data is not sharded. We have a simple replication topology of some <strong>6</strong> servers. Machines are quite generous as detailed following. And yet, we will be running out of resources shortly: disk space (total <strong>2.7TB</strong>) is now running low and is expected to run out in about six months. One of my first tasks in Outbrain is to find a solution to our DWH growth problem. The solution could be sharding; it could be a commercial DWH product; anything that works.<span id="more-6473"></span></p>
<h4>The approach we experiment with</h4>
<p>It was at my initial interview that I suggested <a href="http://www.tokutek.com/products/tokudb-for-mysql/">TokuDB</a> might be a good solution, with the primary reason of being so good with compression. And we decided to experiment with this simple (setup-wise) solution of compression. If we could compress the data even by <strong>50%</strong>, that would buy us considerable time. And it&#8217;s the simplest approach as we would need to change nothing at the application side, nor add additional frameworks.</p>
<p>Of course, we were already using InnoDB <strong>COMPRESSED</strong> tables. How about just improving the compression? And here I thought to myself: we can try <strong>KEY_BLOCK_SIZE=4</strong>, which I know would generally compress by <strong>50%</strong> as compared to <strong>KEY_BLOCK_SIZE=8</strong> (not always, but in many use cases). We&#8217;re already using InnoDB so this isn&#8217;t a new beast; it will be &#8220;more of the same&#8221;. It would work.</p>
<p>I got myself a dedicated machine: a slave in our production topology I am free to play with. I installed TokuDB <strong>7.0.1</strong>, later upgraded to <strong>7.0.3</strong>, based on MySQL <strong>5.5.30</strong>.</p>
<p>The machine is a Dell Inc. <strong>PowerEdge R510</strong> machine, with <b>16</b> CPUs @ <b>2.1 GHz</b> and <b>126 GiB</b> RAM, <b>16 GiB</b> Swap. OS is CentOS <strong>5.7</strong>,Â  kernel <strong>2.6.18</strong>. We have RAID <strong>10</strong> over local <strong>10k</strong> RPM SAS disks (10x<strong>600GB</strong> disks)</p>
<h4>How to compare InnoDB &amp; TokuDB?</h4>
<p><strong>2TB</strong> of compressed data (for absolute measurement I consider it to be a <strong>4TB</strong> worth of data) is quite a large setup. How do I do the comparison? I don&#8217;t even have too much disk space here&#8230;</p>
<p>We have tables of various size. Our largest is in itself <strong>1TB</strong> (<strong>2TB</strong> uncompressed) &#8211; half of the entire volume. The rest ranging <strong>330GB</strong>, <strong>140GB</strong>, <strong>120GB</strong>, <strong>90GB</strong>, <strong>50GB</strong> and below. We have <strong>MONTH</strong>ly partitioning schemes on most tables and obviously on our larger tables.</p>
<p>For our smaller tables, we could just <strong>CREATE TABLE test_table LIKE small_table</strong>, populating it and comparing compression. However, the really interesting question (and perhaps the only interesting question compression-wise) is how well would our larger (and specifically largest) tables would compress.</p>
<p>Indeed, for our smaller tables we saw between <strong>20%</strong> to <strong>70%</strong> reduction in size when using stronger InnoDB compression: <strong>KEY_BLOCK_SIZE=4/2/1</strong>. How well would that work on our larger tables? How much slower would it be?</p>
<p>We know MySQL partitions are implemented by actual <em>independent</em> tables. Our testing approach was: let&#8217;s build a test_table from a one month worth of data (== one single partition) of our largest table. We tested:</p>
<ul>
<li>The time it takes to load the entire partition (about <strong>120M</strong> rows, <strong>100GB COMPRESSED</strong> data as seen on <strong>.idb</strong> file)</li>
<li>The time it would take to load a single day&#8217;s worth of data from Hive/Hadoop (loading real data, as does our nightly import)</li>
<li>The time it would take for various important <strong>SELECT</strong> query to execute on this data.</li>
</ul>
<h4>InnoDB vs. TokuDB comparison</h4>
<p>In this post I will only describe our impressions of compression size. I have a lot to say about TokuDB vs InnoDB partitioning and queries; this will wait till later post.</p>
<p>So here goes:</p>
<table border="0" cellspacing="0">
<colgroup width="85"></colgroup>
<colgroup width="155"></colgroup>
<colgroup width="152"></colgroup>
<colgroup width="147"></colgroup>
<colgroup width="141"></colgroup>
<tbody>
<tr>
<td align="LEFT" bgcolor="#E6E6E6" height="31"><b>Engine</b></td>
<td align="LEFT" bgcolor="#E6E6E6"><b>Compression</b></td>
<td align="LEFT" bgcolor="#E6E6E6"><b>Time to Insert 1 month</b></td>
<td align="LEFT" bgcolor="#E6E6E6"><b>Table size (optimized)</b></td>
<td align="LEFT" bgcolor="#E6E6E6"><b>Time to import 1 day</b></td>
</tr>
<tr>
<td align="LEFT" bgcolor="#FFFFCC" height="17">InnoDB</td>
<td align="LEFT" bgcolor="#FFFFCC">8k</td>
<td align="LEFT" bgcolor="#FFFFCC"><strong>10.5h</strong></td>
<td align="LEFT" bgcolor="#FFFFCC">58GB</td>
<td align="LEFT" bgcolor="#FFFFCC"><b>32m</b></td>
</tr>
<tr>
<td align="LEFT" bgcolor="#FFFFCC" height="17">InnoDB</td>
<td align="LEFT" bgcolor="#FFFFCC">4k</td>
<td align="LEFT" bgcolor="#FFFFCC">48h</td>
<td align="LEFT" bgcolor="#FFFFCC">33GB</td>
<td align="LEFT" bgcolor="#FFFFCC">unknown (too long)</td>
</tr>
<tr>
<td align="LEFT" bgcolor="#FFFFCC" height="17">TokuDB</td>
<td align="LEFT" bgcolor="#FFFFCC">quicklz</td>
<td align="LEFT" bgcolor="#FFFFCC">14h</td>
<td align="LEFT" bgcolor="#FFFFCC">17GB</td>
<td align="LEFT" bgcolor="#FFFFCC">40m</td>
</tr>
<tr>
<td align="LEFT" bgcolor="#FFFFCC" height="17">TokuDB</td>
<td align="LEFT" bgcolor="#FFFFCC">lzma (small/aggresive)</td>
<td align="LEFT" bgcolor="#FFFFCC">15h</td>
<td align="LEFT" bgcolor="#FFFFCC"><b>7.5GB</b></td>
<td align="LEFT" bgcolor="#FFFFCC">42m</td>
</tr>
</tbody>
</table>
<p>Some comments and insights:</p>
<ul>
<li>Each test was performed 3-4 times. There were no significant differences on the various cycles.</li>
<li>The <strong>1</strong> month insert was done courtesy <a href="http://common-schema.googlecode.com/svn/trunk/common_schema/doc/html/query_script_split.html">QueryScript split</a>,Â  <strong>5,000</strong> rows at a time, no throttling.</li>
<li>The <strong>1</strong> day import via <em>mysqlimport</em>. There were multiple files imported. Each file is sorted by <strong>PRIMARY KEY ASC</strong>.</li>
<li>Isn&#8217;t it nice to know that your <strong>100GB</strong> InnoDB table actually fits within <strong>58GB</strong> when rebuilt?</li>
<li>For InnoDB <strong>flush_logs_at_trx_commit=2</strong>, <strong>flush_method=O_DIRECT</strong>.</li>
<li>I used default configuration to TokuDB &#8212; touched nothing. More on this in later post.</li>
<li>InnoDB <strong>4k</strong> was <em>prohibitively</em> slow to load data. It was so slow so as to be unacceptable. For the 1 day load it took <strong>1</strong> hour for a mere <strong>20%</strong> of data to load. <strong>1</strong> hour was already marginal for our requirements; waiting for <strong>5</strong> hours was out of the question. I tested several times, never got to wait for completion. Did I say it would just be &#8220;more of the same&#8221;? <strong>4k</strong> turned to be &#8220;not an option&#8221;.</li>
<li>I saw almost no difference in load time between the two TokuDB compression formats. Both somewhat (30%) longer than InnoDB to load, but comparable.</li>
<li>TokuDB compression: nothing short of <em>amazing</em>.</li>
</ul>
<p>With InnoDB <strong>4k</strong> being &#8220;not an option&#8221;, and with both TokuDB compressions being similar in load time yet so different in compression size, we are left with the following conclusion: if we want to compress more than our existing 8k (and we have to) &#8211; TokuDB&#8217;s <em>agressive compression</em> (aka small, aka lzma) is our only option.</p>
<h4>Shameless plug</h4>
<p><a href="http://code.google.com/p/common-schema/">common_schema</a> turned to be quite the &#8220;save the day&#8221; tool here. Not only did we use it to extract 100GB of data from a large dataset and load it onto our tables, it also helped out in the ALTER process for TokuDB: at this time (&lt;=<strong> 7.0.4</strong>) TokuDB still has a bug with <strong>KEY_BLOCK_SIZE</strong>: when this option is found in table definition, it impacts TokuDB&#8217;s indexes by bloating them. This is how <a href="http://common-schema.googlecode.com/svn/trunk/common_schema/doc/html/sql_alter_table_tokudb.html">sql_alter_table_tokudb</a> was born. Hopefully it will be redundant shortly.</p>
<h4>More to come</h4>
<p>Was our test fair? Should we have configure TokuDB differently? Is loading via small <strong>5,000</strong> row chunks the right way?</p>
<p>In the next post I will describe the process of migrating our 4TB worth of data to TokuDB, pitfalls, issues, party crushers, sport spoilers, configuration, recovery, cool behaviour and general advice you should probably want to embrace. At later stage I&#8217;ll describe how our DWH looks after migration. Finally I&#8217;ll share some (ongoing) insights on performance.</p>
<p>You&#8217;ll probably want to know &#8220;How much is (non compressed) <strong>4TB</strong> of data worth in TokuDB?&#8221; Let&#8217;s keep the suspense <img src="https://s.w.org/images/core/emoji/12.0.0-1/72x72/1f642.png" alt="ðŸ™‚" class="wp-smiley" style="height: 1em; max-height: 1em;" /></p>
]]></content:encoded>
							<wfw:commentRss>https://shlomi-noach.github.io/blog/mysql/converting-an-olap-database-to-tokudb-part-1/feed</wfw:commentRss>
		<slash:comments>8</slash:comments>
						<post-id xmlns="com-wordpress:feed-additions:1">6473</post-id>	</item>
		<item>
		<title>Converting compressed InnoDB tables to TokuDB 7.0.1</title>
		<link>https://shlomi-noach.github.io/blog/mysql/converting-compressed-innodb-tables-to-tokudb-7-0-1</link>
				<comments>https://shlomi-noach.github.io/blog/mysql/converting-compressed-innodb-tables-to-tokudb-7-0-1#comments</comments>
				<pubDate>Wed, 05 Jun 2013 07:10:12 +0000</pubDate>
		<dc:creator><![CDATA[shlomi]]></dc:creator>
				<category><![CDATA[MySQL]]></category>
		<category><![CDATA[InnoDB]]></category>
		<category><![CDATA[TokuDB]]></category>

		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=6368</guid>
				<description><![CDATA[Or: how to make it work in TokuDB version 7.0.1. This is a follow up on a discussion on the tokudb-user group. Background I wanted to test TokuDB&#8217;s compression. I took a staging machine of mine, with production data, and migrated it from Percona Server 5.5 To MariaDB 5.5+TokuDB 7.0.1. Migration went well, no problems. [&#8230;]]]></description>
								<content:encoded><![CDATA[<p>Or: how to make it work in TokuDB version <strong>7.0.1</strong>. This is a follow up on a <a href="https://groups.google.com/forum/?fromgroups=#!topic/tokudb-user/hLlHwlp2AL0">discussion on the tokudb-user group</a>.</p>
<h4>Background</h4>
<p>I wanted to test TokuDB&#8217;s compression. I took a staging machine of mine, with production data, and migrated it from <strong>Percona Server 5.5</strong> To <strong>MariaDB 5.5+TokuDB 7.0.1</strong>. Migration went well, no problems.</p>
<p>To my surprise, when I converted tables from InnoDB to TokuDB, I saw an <em>increase</em> in table file size on disk. As explained by Tim Callaghan, this was due to TokuDB interpreting my compressed table&#8217;s <strong>&#8220;KEY_BLOCK_SIZE=4&#8221;</strong> as an instruction for TokuDB&#8217;s page size. TokuDB should be using <strong>4MB</strong> block size, but thinks it&#8217;s being instructed to use <strong>4KB</strong>. Problem is, you <a href="http://bugs.mysql.com/bug.php?id=67727">can&#8217;t get rid of table options</a>. When one converts a table to InnoDB in <strong>ROW_FORMAT=COMPACT</strong>, or even to MyISAM, the <strong>KEY_BLOCK_SIZE</strong> option keeps lurking in the dark.</p>
<p>So until this is hopefully resolved in TokuDB&#8217;s next version, here&#8217;s a way to go around the problem.<span id="more-6368"></span></p>
<h4>The case at hand</h4>
<p>Consider the following table:</p>
<blockquote>
<pre> CREATE TABLE `t` (
  `id` int(10) unsigned NOT NULL AUTO_INCREMENT,
  `c1` int(10) unsigned NOT NULL DEFAULT '0',
  `c2` int(10) unsigned NOT NULL DEFAULT '0',
  `c3` int(10) unsigned NOT NULL DEFAULT '0',
  `c4` timestamp NOT NULL DEFAULT '0000-00-00 00:00:00',
  `c5` timestamp NOT NULL DEFAULT '0000-00-00 00:00:00',
  `c6` smallint(10) unsigned NOT NULL DEFAULT '0',
  `c7` smallint(10) unsigned NOT NULL DEFAULT '0',
  `c8` smallint(10) unsigned NOT NULL DEFAULT '0',
  `c9` smallint(10) unsigned NOT NULL DEFAULT '0',
  `c10` smallint(10) unsigned NOT NULL DEFAULT '0',
  `c11` smallint(10) NOT NULL DEFAULT '0',
  `c12` smallint(10) NOT NULL DEFAULT '0',
  `c13` smallint(10) NOT NULL DEFAULT '0',
  `c14` smallint(10) NOT NULL DEFAULT '0',
  `ct` text NOT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `c1c4` (`c1`,`c4`),
  KEY `c4` (`c4`)
) ENGINE=InnoDB AUTO_INCREMENT=4688271 DEFAULT CHARSET=utf8 ROW_FORMAT=COMPRESSED KEY_BLOCK_SIZE=4</pre>
</blockquote>
<p>Note that it is in <strong>COMPRESSED</strong> format, with <strong>KEY_BLOCK_SIZE=4</strong>. It mostly has <strong>INT</strong> columns, so I don&#8217;t expect it to compress by much.</p>
<p>On disk, the <strong>.ibd</strong> file amounts to <strong>160MB</strong>. Table has<strong> </strong><strong>3,587,488</strong> rows. Same table in InnoDB COMPACT row format amounts to <strong>412MB</strong> on disk.</p>
<p>Converting the table to TokuDB with aggressive compression resulted with:</p>
<blockquote>
<pre>mysql&gt; alter table t engine=tokudb row_format=tokudb_lzma;
Query OK, 3587488 rows affected (29 min 48.79 sec)
Records: 3587488Â  Duplicates: 0Â  Warnings:</pre>
</blockquote>
<p>And over <strong>873MB</strong> of combined files on disk! Also note it took nearly <strong>30</strong> minutes to <strong>ALTER</strong>. Clearly this is not the expected outcome.</p>
<h4>Attempt to make it work</h4>
<p>I tried both the following approaches:</p>
<ul>
<li><strong>alter table t engine=tokudb row_format=tokudb_lzma key_block_size=4096</strong>: thought it would fool TokuDB to think it should create a 4M key block size.</li>
<li><strong>alter table t engine=tokudb row_format=tokudb_lzma key_block_size=0</strong>: try and reset the key block size.</li>
</ul>
<p>Both the above attempts resulted with same bloat in resulting table.</p>
<p>The reason? When ALTERing a table with a nother <strong>KEY_BLOCK_SIZE</strong>, the keys on the table remain with their old <strong>KEY_BLOCK_SIZE</strong>. They are unaffected by the <strong>ALTER</strong>. As suggested by <em>Nail Kashapov</em>, indexes must be rebuilt as well.</p>
<h4>Making it work</h4>
<p>The next <strong>ALTER</strong> modifies the <strong>KEY_BLOCK_SIZE</strong> <em>and</em> rebuilds all the indexes on the table:</p>
<blockquote>
<pre>mysql&gt; alter table t drop primary key, add primary key(id), drop key c1c4, add unique key `c1c4` (c1, c4), drop key c4, add key `c4` (c4), engine=tokudb row_format=tokudb_lzma key_block_size=0;
Query OK, 3587488 rows affected (2 min 7.97 sec)
Records: 3587488Â  Duplicates: 0Â  Warnings: 0</pre>
</blockquote>
<p>Yep! Runtime seems much more agreeable. Total size on disk? Little over <strong>26M</strong>. Did I say I wasn&#8217;t expecting good reduction in terms of compression?</p>
<p>Have done the same for multiple tables; compression is consistently strong (e.g. <strong>16MB</strong> InnoDB compressed -&gt; <strong>3.5MB</strong> TokuDB aggressive, <strong>548MB</strong> InnoDB non-compressed -&gt; <strong>36MB</strong> TokuDB aggressive), on varying table schemata. Very impressive reduction in disk space!</p>
<h4>Conclusion</h4>
<p>Next version of TokuDB is expected to ignore the <strong>KEY_BLOCK_SIZE</strong> table option; until then converting compressed tables to TokuDB is a pain in terms of the syntax &#8212; but worthwhile in terms of disk space.</p>
<p>&nbsp;</p>
]]></content:encoded>
							<wfw:commentRss>https://shlomi-noach.github.io/blog/mysql/converting-compressed-innodb-tables-to-tokudb-7-0-1/feed</wfw:commentRss>
		<slash:comments>4</slash:comments>
						<post-id xmlns="com-wordpress:feed-additions:1">6368</post-id>	</item>
		<item>
		<title>On compiling TokuDB from source</title>
		<link>https://shlomi-noach.github.io/blog/mysql/on-compiling-tokudb-from-source</link>
				<comments>https://shlomi-noach.github.io/blog/mysql/on-compiling-tokudb-from-source#comments</comments>
				<pubDate>Thu, 02 May 2013 05:14:55 +0000</pubDate>
		<dc:creator><![CDATA[shlomi]]></dc:creator>
				<category><![CDATA[MySQL]]></category>
		<category><![CDATA[TokuDB]]></category>

		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=6320</guid>
				<description><![CDATA[Sharing my experience of compiling TokuDB + MariaDB 5.5. Why? Because I must have this patch to Sphinx 2.0.4. Note: I was using what seems to be the &#8220;old&#8221; method of compiling; quoting Leif Walsh: &#8230; We are looking at deprecating that method of building (MariaDB source plus binary fractal tree handlerton). Â It only really [&#8230;]]]></description>
								<content:encoded><![CDATA[<p>Sharing my experience of compiling TokuDB + MariaDB <strong>5.5</strong>. Why? Because I must have <a href="https://shlomi-noach.github.io/blog/mysql/sphinx-sphinx_snippets-mysql-5-5">this patch</a> to Sphinx <strong>2.0.4</strong>.</p>
<p><strong>Note</strong>: I was using what seems to be the &#8220;old&#8221; method of compiling; quoting Leif Walsh:</p>
<blockquote><p>&#8230; We are looking at deprecating that method of building (MariaDB source plus binary fractal tree handlerton). Â It only really needed to be that complex when we were closed source.</p></blockquote>
<p>I also tried the &#8220;new&#8221; method of compiling, which I couldn&#8217;t work out.</p>
<p>Here&#8217;s how it goes: TokuDB is newly <a href="http://www.tokutek.com/2013/04/announcing-tokudb-v7-open-source-and-more/">released as open source</a>. As such, it got a lot of attention, many downloads and I hope it will succeed.</p>
<p>However as stable as the product may be, it&#8217;s new to open source, which means anyone compiling it from source is an early adopter (at least for the compilation process).</p>
<h4>Installation process</h4>
<p>This is an unorthodox, and actually weird process. See <a href="http://www.tokutek.com/wp-content/uploads/2013/04/mariadb-5.5.30-tokudb-7.0.1-users-guide.pdf">section 6 on the Tokutek docs</a>. In order to compile the project you must download:</p>
<ul>
<li>The source code tar.gz</li>
<li><em>And</em> the binary (?!) tar.gz</li>
<li>And the binary checksum</li>
<li>And the Tokutek patches</li>
<li>And the patches checksum<span id="more-6320"></span></li>
</ul>
<p>You extract the source tarball. But instead of doing the standard <strong>&#8220;./configure &amp;&amp; make &amp;&amp; sudo make install&#8221;</strong> you need to copy a shell script calledÂ <strong>tokudb.build.bash</strong> <em>one directory level up</em>, and run it from there.</p>
<p><strong>tokudb.build.bash</strong> lists <strong>gcc47</strong> and <strong>g++47</strong> on rows <strong>3, 4</strong>. Modify <strong>&#8220;gcc47&#8221;</strong> to <strong>&#8220;gcc&#8221;</strong>, modify <strong>&#8220;g++47&#8221;</strong> toÂ  <strong>&#8220;g++&#8221;</strong>. I&#8217;m assuming you don&#8217;t have a binary called <strong>gcc47</strong>. Why would you?</p>
<h4>Dependencies</h4>
<p>You will need <strong>CMake &gt;= 2.8</strong></p>
<p>This means Ubuntu LTS <strong>10.04</strong> users are unable to compile out of the box; will need to manually install later version of CMake.</p>
<p>Also needed is <strong>zlib1g-dev</strong>, <strong>rpmbuild</strong>.</p>
<h4>While compiling</h4>
<p>I ran out of disk space. <em>What?</em> I was using a <strong>10G</strong> partition I use for my compilations. Looking at <strong>&#8220;df -h&#8221;</strong> I get that:</p>
<ul>
<li>The source tarball is extracted (I did it)</li>
<li>The binary tarball is also extracted (someone has to explain this for me)</li>
<li>And inside the source directory we have:</li>
</ul>
<blockquote>
<pre>bash$ df -h
...
1484Â Â Â  build.RelWithDebInfo.rpms
5540Â Â Â  build.RelWithDebInfo</pre>
</blockquote>
<p>At about <strong>7GB</strong> (and counting) of build&#8230; <em>stuff?</em>.</p>
<p><strong>UPDATE</strong>: just ran out on disk space <em>again</em>. Is this an incremental thing? Like every time my compilation fails and I recompile some files are not cleaned up? If so, put them on <strong>/tmp</strong>! OK, moving everything to a <strong>300GB</strong> partition and starting all over.</p>
<h4>More while compiling</h4>
<p><strong></strong>I got errors on missing libraries. Like I was missing <strong>libssl</strong>, <strong>rpmbuild</strong>. This is what the <strong>&#8220;configure&#8221;</strong> script is for &#8212; to test for dependencies. It&#8217;s really a bummer to have to recompile 4-5 times (and it&#8217;s a long compilation), only to find out there&#8217;s another missing package.</p>
<h4>After compiling</h4>
<p>What is the result of the compilation? Not a &#8220;make install&#8221; prepared binary. The result is a MySQL-binary package. Se need to extract and put on <strong>/usr/local/somewhere</strong> etc.</p>
<h4>Conclusions</h4>
<p>The compilation process is unexpected and non-standard. The output is unexpected.</p>
<p>The correct way of doing this is a <strong>&#8220;./configure &amp;&amp; make &amp;&amp; sudo make install&#8221;</strong>. I don&#8217;t understand the need for a binary package while compiling from source. Isn&#8217;t this the chicken and the egg?</p>
<p>A source distribution is no different from a binary distribution. You must have a testing environment to verify the source distribution actually works. This test environment is typically a bare-new-RedHat or a bare-new-Ubuntu etc. The machines at Tokutek are already installed with needed packages. Not so on my compilation machine. I suggest that <strong>apt-get</strong>s and <strong>yum install</strong>s for dependencies are added to the source distribution testing. This is the only reliable way for you guys at Tokutek to know that clients will actually be able to install via source.</p>
]]></content:encoded>
							<wfw:commentRss>https://shlomi-noach.github.io/blog/mysql/on-compiling-tokudb-from-source/feed</wfw:commentRss>
		<slash:comments>14</slash:comments>
						<post-id xmlns="com-wordpress:feed-additions:1">6320</post-id>	</item>
	</channel>
</rss>
