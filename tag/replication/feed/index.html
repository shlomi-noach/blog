<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	
	xmlns:georss="http://www.georss.org/georss"
	xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#"
	>

<channel>
	<title>Replication &#8211; code.openark.org</title>
	<atom:link href="https://shlomi-noach.github.io/blog/tag/replication/feed" rel="self" type="application/rss+xml" />
	<link>http://shlomi-noach.github.io/blog/</link>
	<description>Blog by Shlomi Noach</description>
	<lastBuildDate>Mon, 11 May 2020 06:28:06 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=5.3.3</generator>
<site xmlns="com-wordpress:feed-additions:1">32412571</site>	<item>
		<title>Quick hack for GTID_OWN lack</title>
		<link>https://shlomi-noach.github.io/blog/mysql/quick-hack-for-gtid_own-lack</link>
				<comments>https://shlomi-noach.github.io/blog/mysql/quick-hack-for-gtid_own-lack#respond</comments>
				<pubDate>Wed, 11 Dec 2019 08:00:00 +0000</pubDate>
		<dc:creator><![CDATA[shlomi]]></dc:creator>
				<category><![CDATA[MySQL]]></category>
		<category><![CDATA[GTID]]></category>
		<category><![CDATA[Replication]]></category>

		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=7974</guid>
				<description><![CDATA[One of the benefits of MySQL GTIDs is that each server remembers all GTID entries ever executed. Normally these would be ranges, e.g. 0041e600-f1be-11e9-9759-a0369f9435dc:1-3772242 or multi-ranges, e.g. 24a83cd3-e30c-11e9-b43d-121b89fcdde6:1-103775793, 2efbcca6-7ee1-11e8-b2d2-0270c2ed2e5a:1-356487160, 46346470-6561-11e9-9ab7-12aaa4484802:1-26301153, 757fdf0d-740e-11e8-b3f2-0a474bcf1734:1-192371670, d2f5e585-62f5-11e9-82a5-a0369f0ed504:1-10047. One of the common problems in asynchronous replication is the issue of consistent reads. I&#8217;ve just written to the master. Is the data [&#8230;]]]></description>
								<content:encoded><![CDATA[<p>One of the benefits of MySQL GTIDs is that each server remembers <em>all</em> GTID entries ever executed. Normally these would be ranges, e.g. <code>0041e600-f1be-11e9-9759-a0369f9435dc:1-3772242</code> or multi-ranges, e.g. <code>24a83cd3-e30c-11e9-b43d-121b89fcdde6:1-103775793, 2efbcca6-7ee1-11e8-b2d2-0270c2ed2e5a:1-356487160, 46346470-6561-11e9-9ab7-12aaa4484802:1-26301153, 757fdf0d-740e-11e8-b3f2-0a474bcf1734:1-192371670, d2f5e585-62f5-11e9-82a5-a0369f0ed504:1-10047</code>.</p>
<p>One of the common problems in asynchronous replication is the issue of consistent reads. I&#8217;ve just written to the <code>master</code>. Is the data available on a replica yet? We have iterated on this, from reading on <code>master</code>, to heuristically finding up-to-date replicas based on heartbeats (see <a href="https://www.youtube.com/watch?v=ZVBmTgIMOCA">presentation</a> and <a href="https://speakerdeck.com/shlominoach/monitoring-time-in-a-distributed-database-a-play-in-three-acts">slides</a>) via <a href="https://github.com/github/freno">freno</a>, and now settled, on some parts of our apps, to using GTID.</p>
<p>GTIDs are reliable as any replica can give you a definitive answer to the question: <em>have you applied a given transaction or not?</em>. Given a GTID entry, say <code>f7b781a9-cbbd-11e9-affb-008cfa542442:12345</code>, one may query for the following on a replica:</p>
<pre><code>mysql&gt; select gtid_subset('f7b781a9-cbbd-11e9-affb-008cfa542442:12345', @@global.gtid_executed) as transaction_found;
+-------------------+
| transaction_found |
+-------------------+
|                 1 |
+-------------------+

mysql&gt; select gtid_subset('f7b781a9-cbbd-11e9-affb-008cfa542442:123450000', @@global.gtid_executed) as transaction_found;
+-------------------+
| transaction_found |
+-------------------+
|                 0 |
+-------------------+
</code></pre>
<h3>Getting OWN_GTID</h3>
<p>This is all well, but, given some <code>INSERT</code> or <code>UPDATE</code> on the <code>master</code>, how can I tell what&#8217;s the GTID associated with that transaction? There\s good news and bad news.</p>
<ul>
<li>Good news is, you may <code>SET SESSION session_track_gtids = OWN_GTID</code>. This makes the MySQL protocol return the GTID generated by your transaction.</li>
<li>Bad news is, this isn&#8217;t a standard SQL response, and the common MySQL drivers offer you no way to get that information!</li>
</ul>
<p>At GitHub we author our own Ruby driver, and have implemented the functionality to extract <code>OWN_GTID</code>, much like you&#8217;d extract <code>LAST_INSERT_ID</code>. But, how does one solve that without modifying the drivers? Here&#8217;s a poor person&#8217;s solution which gives you an inexact, but good enough, info. Following a write (<code>insert</code>, <code>delete</code>, <code>create</code>, &#8230;), run:</p>
<pre><code>select gtid_subtract(concat(@@server_uuid, ':1-1000000000000000'), gtid_subtract(concat(@@server_uuid, ':1-1000000000000000'), @@global.gtid_executed)) as master_generated_gtid;
</code></pre>
<p>The idea is to &#8220;clean&#8221; the executed GTID set from irrelevant entries, by filtering out all ranges that do not belong to the server you&#8217;ve just written to (the <code>master</code>). The number <code>1000000000000000</code> stands for &#8220;high enough value that will never be reached in practice&#8221; &#8211; set to your own preferred value, but this value should take you beyond <code>300</code> years assuming <code>100,000</code> transactions per second.</p>
<p><span id="more-7974"></span></p>
<p>The value you get is the range on the master itself. e.g.:</p>
<pre><code>mysql&gt; select gtid_subtract(concat(@@server_uuid, ':1-1000000000000000'), gtid_subtract(concat(@@server_uuid, ':1-1000000000000000'), @@global.gtid_executed)) as master_generated_gtid;
+-------------------------------------------------+
| master_generated_gtid                           |
+-------------------------------------------------+
| dc103953-1598-11ea-82a7-008cfa5440e4:1-35807176 |
+-------------------------------------------------+
</code></pre>
<p>You may further parse the above to extract <code>dc103953-1598-11ea-82a7-008cfa5440e4:35807176</code> if you want to hold on to the latest GTID entry. Now, this entry isn&#8217;t necessarily <em>your own</em>. Between the time of your write and the time of your GTID query, other writes will have taken place. But the entry you get is either your own or a later one. If you can find that entry on a replica, that means your write is included on the replica.</p>
<p>One may wonder, why do we need to extract the value at all? Why not just <code>select @@global.gtid_executed</code>? Why filter only the <code>master</code>&#8216;s UUID? Logically, the answer is the same if you do that. But in practice, your query may be unfortunate enough to return some:</p>
<pre><code>select @@global.gtid_executed \G

e71f0cdb-b8ef-11e9-9361-008cfa542442:1-83331,
e742d87f-dea7-11e9-be6d-008cfa542c9e:1-18485,
e7880c0e-ac54-11e9-865a-008cfa544064:1-7331973,
e82043c6-c7d9-11e9-9413-008cfa5440e4:1-61692,
e902678b-b046-11e9-a281-008cfa542c9e:1-83108,
e90d7ff9-e35e-11e9-a9a0-008cfa544064:1-18468,
e929a635-bb40-11e9-9c0d-008cfa5440e4:1-139348,
e9351610-ef1b-11e9-9db4-008cfa5440e4:1-33460918,
e938578d-dc41-11e9-9696-008cfa542442:1-18232,
e947f165-cd53-11e9-b7a1-008cfa5440e4:1-18480,
e9733f37-d537-11e9-8604-008cfa5440e4:1-18396,
e97a0659-e423-11e9-8433-008cfa542442:1-18237,
e98dc1f7-e0f8-11e9-9bbd-008cfa542c9e:1-18482,
ea16027a-d20e-11e9-9845-008cfa542442:1-18098,
ea1e1aa6-e74a-11e9-a7f2-008cfa544064:1-18450,
ea8bc1bd-dd06-11e9-a10c-008cfa542442:1-18203,
eae8c750-aaca-11e9-b17c-008cfa544064:1-85990,
eb1e41e9-af81-11e9-9ceb-008cfa544064:1-86220,
eb3c9b3b-b698-11e9-b67a-008cfa544064:1-18687,
ec6daf7e-b297-11e9-a8a0-008cfa542c9e:1-80652,
eca4af92-c965-11e9-a1f3-008cfa542c9e:1-18333,
ecd110b9-9647-11e9-a48f-008cfa544064:1-24213,
ed26890e-b10b-11e9-a79d-008cfa542c9e:1-83450,
ed92b3bf-c8a0-11e9-8612-008cfa542442:1-18223,
eeb60c82-9a3d-11e9-9ea5-008cfa544064:1-1943152,
eee43e06-c25d-11e9-ba23-008cfa542442:1-105102,
eef4a7fb-b438-11e9-8d4b-008cfa5440e4:1-74717,
eefdbd3b-95b3-11e9-833d-008cfa544064:1-39415,
ef087062-ba7b-11e9-92de-008cfa5440e4:1-9726172,
ef507ff0-98b3-11e9-8b15-008cfa5440e4:1-928030,
ef662471-9a3b-11e9-bd2e-008cfa542c9e:1-954800,
f002e9f7-97ee-11e9-bed0-008cfa542c9e:1-5180743,
f0233228-e9a1-11e9-a142-008cfa542c9e:1-18583,
f04780c4-a864-11e9-9f28-008cfa542c9e:1-83609,
f048acd9-b1d2-11e9-a0b6-008cfa544064:1-70663,
f0573d8c-9978-11e9-9f73-008cfa542c9e:1-85642135,
f0b0a37c-c89c-11e9-804c-008cfa5440e4:1-18488,
f0cfe1ac-e5af-11e9-bc09-008cfa542c9e:1-18552,
f0e4997c-cbc9-11e9-9179-008cfa542442:1-1655552,
f24e481c-b5c4-11e9-aff0-008cfa5440e4:1-83015,
f4578c4b-be6d-11e9-982e-008cfa5440e4:1-132701,
f48bce80-e99f-11e9-94f4-a0369f9432f4:1-18460,
f491adf1-9b04-11e9-bc71-008cfa542c9e:1-962823,
f5d3db74-a929-11e9-90e8-008cfa5440e4:1-75379,
f6696ba7-b750-11e9-b458-008cfa542c9e:1-83096,
f714cb4c-dab7-11e9-adb9-008cfa544064:1-18413,
f7b781a9-cbbd-11e9-affb-008cfa542442:1-18169,
f81f7729-b10d-11e9-b29b-008cfa542442:1-86820,
f88a3298-e903-11e9-88d0-a0369f9432f4:1-18548,
f9467b29-d78c-11e9-b1a2-008cfa5440e4:1-18492,
f9c08f5c-e4ea-11e9-a76c-008cfa544064:1-1667611,
fa633abf-cee3-11e9-9346-008cfa542442:1-18361,
fa8b0e64-bb42-11e9-9913-008cfa542442:1-140089,
fa92234c-cc90-11e9-b337-008cfa544064:1-18324,
fa9755eb-e425-11e9-907d-008cfa542c9e:1-1668270,
fb7843d5-eb38-11e9-a1ff-a0369f9432f4:1-1668957,
fb8ceae5-dd08-11e9-9ed3-008cfa5440e4:1-18526,
fbf9970e-bc07-11e9-9e4f-008cfa5440e4:1-136157,
fc0ffaee-98b1-11e9-8574-008cfa542c9e:1-940999,
fc9bf1e4-ee54-11e9-9ce9-008cfa542c9e:1-18189,
fca4672f-ac56-11e9-8a83-008cfa542442:1-82014,
fcebaa05-dab5-11e9-8356-008cfa542c9e:1-18490,
fd0c88b1-ad1b-11e9-bf3a-008cfa5440e4:1-75167,
fd394feb-e4e4-11e9-bd09-008cfa5440e4:1-18574,
fd687577-b048-11e9-b429-008cfa542442:1-83479,
fdb18995-a79f-11e9-a28d-008cfa542442:1-82351,
fdc72b7f-b696-11e9-ade9-008cfa544064:1-57674,
ff1f3b6b-c967-11e9-ae04-008cfa544064:1-18503,
ff6fe7dc-c186-11e9-9bb4-008cfa5440e4:1-103192,
fff9dd94-ed95-11e9-90b7-008cfa544064:1-911039
</code></pre>
<p>This can happen when you fail over to a new master, multiple times; it happens when you don&#8217;t recycle UUIDs, when you provision new hosts and let MySQL pick their UUID. Returning this amount of data <em>per query</em> is an excessive overhead, hence why we extract the <code>master</code>&#8216;s UUID only, which is guaranteed to be limited in size.</p>
]]></content:encoded>
							<wfw:commentRss>https://shlomi-noach.github.io/blog/mysql/quick-hack-for-gtid_own-lack/feed</wfw:commentRss>
		<slash:comments>0</slash:comments>
						<post-id xmlns="com-wordpress:feed-additions:1">7974</post-id>	</item>
		<item>
		<title>Un-split brain MySQL via gh-mysql-rewind</title>
		<link>https://shlomi-noach.github.io/blog/mysql/un-split-brain-mysql-via-gh-mysql-rewind</link>
				<comments>https://shlomi-noach.github.io/blog/mysql/un-split-brain-mysql-via-gh-mysql-rewind#respond</comments>
				<pubDate>Tue, 05 Mar 2019 13:51:43 +0000</pubDate>
		<dc:creator><![CDATA[shlomi]]></dc:creator>
				<category><![CDATA[MySQL]]></category>
		<category><![CDATA[Open Source]]></category>
		<category><![CDATA[Replication]]></category>

		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=7928</guid>
				<description><![CDATA[We are pleased to release gh-mysql-rewind, a tool that allows us to move MySQL back in time, automatically identify and rewind split brain changes, restoring a split brain server into a healthy replication chain. I recently had the pleasure of presenting gh-mysql-rewind at FOSDEM. Video and slides are available. Consider following along with the video. [&#8230;]]]></description>
								<content:encoded><![CDATA[<p>We are pleased to release <a href="https://github.com/github/gh-mysql-tools/tree/master/rewind">gh-mysql-rewind</a>, a tool that allows us to move MySQL back in time, automatically identify and rewind split brain changes, restoring a split brain server into a healthy replication chain.</p>
<p>I recently had the pleasure of presenting <code>gh-mysql-rewind</code> at <a href="https://fosdem.org/2019/schedule/track/mysql_mariadb_and_friends/">FOSDEM</a>. <a href="https://www.youtube.com/watch?v=UL--ew3n3QI">Video</a> and <a href="https://speakerdeck.com/shlominoach/un-split-brain-mysql">slides</a> are available. Consider following along with the video.</p>
<h2>Motivation</h2>
<p>Consider a split brain scenario: a &#8220;standard&#8221; MySQL replication topology suffered network isolation, and one of the replicas was promoted as new master. Meanwhile, the old master was still receiving writes from co-located apps.</p>
<p>Once the network isolation is over, we have a new master and an old master, and a split-brain situation: some writes only took place on one master; others only took place on the other. What if we wanted to converge the two? What paths do we have to, say, restore the old, demoted master, as a replica of the newly promoted master?</p>
<p>The old master is unlikely to agree to replicate from the new master. Changes have been made. <code>AUTO_INCREMENT</code> values have been taken. <code>UNIQUE</code> constraints will fail.</p>
<p>A few months ago, we at GitHub had <a href="https://github.blog/2018-10-30-oct21-post-incident-analysis/">exactly this scenario</a>. An entire data center went network isolated. Automation failed over to a 2nd DC. Masters in the isolated DC meanwhile kept receiving writes. At the end of the failover we ended up with a split brain scenario &#8211; which we <a href="https://githubengineering.com/mysql-high-availability-at-github/#limitations-and-drawbacks">expected</a>. However, an additional, unexpected constraint forced us to fail back to the original DC.</p>
<p>We had to make a choice: we&#8217;ve already operated for a long time in the 2nd DC and took many writes, that we were unwilling to lose. We were OK to lose (after auditing) the few seconds of writes on the isolated DC. But, how do we converge the data?</p>
<p>Backups are the trivial way out, but they incur long recovery time. Shipping backup data over the network for dozens of servers takes time. Restore time, catching up with changes since backup took place, warming up the servers so that they can handle production traffic, all take time.</p>
<p>Could we have reduces the time for recovery?</p>
<p><span id="more-7928"></span></p>
<p>There are multiple ways to do that: local backups, local delayed replicas, snapshots&#8230; We have embarked on several. In this post I wish to outline <a href="https://github.com/github/gh-mysql-tools/tree/master/rewind">gh-mysql-rewind</a>, which programmatically identifies the rogue (aka &#8220;bad&#8221;) transactions on the network isolated master, rewinds/reverts them, applies some bookkeeping and restores the demoted master as a healthy replica under the newly promoted master, thereby prepared to be promoted if needed.</p>
<h2>General overview</h2>
<p><code>gh-mysql-rewind</code> is a <code>shell</code> script. It utilizes multiple technologies, some of which do not speak to each other, to be able to do the magic. It assumes and utilizes the following:</p>
<ul>
<li>MySQL <a href="https://dev.mysql.com/doc/refman/5.7/en/replication-gtids-concepts.html">GTID replication</a></li>
<li>Row based replication (<code>binlog_format=ROW</code>)</li>
<li><code>binlog_row_image=FULL</code></li>
<li>Use of <a href="https://mariadb.com/kb/en/library/flashback/">MariaDB Flashback</a></li>
<li>Some limitations apply</li>
</ul>
<p>Some breakdown follows.</p>
<h2>GTID</h2>
<p>MySQL GTIDs keep track of all transactions executed on a given server. GTIDs indicate which server (UUID) originated a write, and ranges of transaction sequences. In a clean state, only one writer will generate GTIDs, and on all the replicas we would see the same GTID set, originated with the writer&#8217;s UUID.</p>
<p>In a split brain scenario, we would see divergence. It is possible to use <a href="https://dev.mysql.com/doc/refman/5.7/en/gtid-functions.html#function_gtid-subtract">GTID_SUBTRACT(old_master-GTIDs, new-master-GTIDs)</a> to identify the exact set of transactions executed on the old, demoted master, right after the failover. This is the essence of the split brain.</p>
<p>For example, assume that just before the network partition, GTID on the master was <code>00020192-1111-1111-1111-111111111111:1-5000</code>. Assume after the network partition the new master has UUID of <code>00020193-2222-2222-2222-222222222222</code>. It began to take writes, and after some time its GTID set showed <code>00020192-1111-1111-1111-111111111111:1-5000,00020193-2222-2222-2222-222222222222:1-200</code>.</p>
<p>On the demoted master, other writes took place, leading to the GTID set <code>00020192-1111-1111-1111-111111111111:1-5042</code>.</p>
<p>We will run&#8230;</p>
<pre><code class="sql">SELECT GTID_SUBTRACT(
  '00020192-1111-1111-1111-111111111111:1-5042',
  '00020192-1111-1111-1111-111111111111:1-5000,00020193-2222-2222-2222-222222222222:1-200'
);

&gt; '00020192-1111-1111-1111-111111111111:5001-5042'
</code></pre>
<p>&#8230;to identify the exact set of &#8220;bad transactions&#8221; on the demoted master.</p>
<h2>Row Based Replication</h2>
<p>With row based replication, and with <code>FULL</code> image format, each DML (<code>INSERT</code>, <code>UPDATE</code>, <code>DELETE</code>) writes to the binary log the complete row data before and after the operation. This means the binary log has enough information for us to revert the operation.</p>
<h2>Flashback</h2>
<p>Developed by Alibaba, <code>flashback</code> has been incorporated in <a href="https://mariadb.com/kb/en/library/flashback/">MariaDB</a>. MariaDB&#8217;s <code>mysqlbinlog</code> utility supports a <code>--flashback</code> flag, which interprets the binary log in a special way. Instead of printing out the events in the binary log in order, it prints the inverted operations in reverse order.</p>
<p>To illustrate, let&#8217;s assume this pseudo-code sequence of events in the binary log:</p>
<pre><code class="sql">insert(1, 'a')
insert(2, 'b')
insert(3, 'c')
update(2, 'b')-&gt;(2, 'second')
update(3, 'c')-&gt;(3, 'third')
insert(4, 'd')
delete(1, 'a')
</code></pre>
<p>A <code>--flashback</code> of this binary log would produce:</p>
<pre><code>insert(1, 'a')
delete(4, 'd')
update(3, 'third')-&gt;(3, 'c')
update(2, 'second')-&gt;(2, 'b')
delete(3, 'c')
delete(2, 'b')
delete(1, 'a')
</code></pre>
<p>Alas, MariaDB and <code>flashback</code> do not speak MySQL GTID language. GTIDs are one of the major points where MySQL and MariaDB have diverged beyond compatibility.</p>
<p>The output of MariaDB&#8217;s <code>mysqlbinlog --flashback</code> has neither any mention of GTIDs, nor does the tool take notice of GTIDs in the binary logs in the first place.</p>
<h2>gh-mysql-rewind</h2>
<p>This is where we step in. GTIDs provide the information about <em>what went wrong</em>. <code>flashback</code> has the mechanism to generate the reverse sequence of statements. <code>gh-mysql-rewind</code>:</p>
<ul>
<li>uses GTIDs to detect what went wrong</li>
<li>correlates those GTID entries with binary log files: identifies which binary logs actually contain those GTID events</li>
<li>invokes MariaDB&#8217;s <code>mysqlbinlog --flashback</code> to generate the reverse of those binary logs</li>
<li>injects (dummy) GTID information into the output</li>
<li>computes ETA</li>
</ul>
<p>This last part is worth elaborating. We have created a time machine. We have the mechanics to make it work. But as any Sci-Fi fan knows, one of the most important parts of time travel is knowing ahead where (when) you are going to land. Are you back in the Renaissance? Or are you suddenly to appear on board the French Revolution? Better dress accordingly.</p>
<p>In our scenario it is not enough to move MySQL back in time to <em>some consistent state</em>. We want to know at what time we landed, so that we can instruct the rewinded server to join the replication chain as a healthy replica. In MySQL terms, we need to make MySQL &#8220;forget&#8221; everything that ever happened after the split brain: not only in terms of data (which we already did), but in terms of GTID history.</p>
<p><code>gh-mysql-rewind</code> will do the math to project, ahead of time, at what &#8220;time&#8221; (i.e. GTID set) our time machine arrived. It will issue a `RESET MASTER; SET GLOBAL gtid_purged=&#8217;gtid-of-the-landing-time'&#8221; to make our re-winded MySQL consistent not only with some past dataset, but also with its own perception of the point in time where that dataset existed.</p>
<h2>Limitations</h2>
<p>Some limitations are due to MariaDB&#8217;s incompatibility with MySQL, some are due to MySQL DDL nature, some due to the fact <code>gh-mysql-rewind</code> is a <code>shell</code> script.</p>
<ul>
<li>Cannot rewind DDL. DDLs are silently ignored, and will impose a problem when trying to re-apply them.</li>
<li><code>JSON</code>, <code>POINT</code> data types are not supported.</li>
<li>The logic rewinds the MySQL server farther into the past than strictly required. This simplifies the code considerably, but imposed superfluous time to rewind+reapply, i.e. time to recover.</li>
<li>Currently, this only works one server at a time. If a group of 10 servers were network isolated together, the operation would need to run on each of these 10 servers.</li>
<li>Runs locally on each server. Requires both MySQL&#8217;s <code>mysqlbinlog</code> as well as MariaDB&#8217;s <code>mysqlbinlog</code>.</li>
</ul>
<h2>Testing</h2>
<p>There&#8217;s lot of moving parts to this mechanism. A mixture of technologies that don&#8217;t normally speak to each other, injection of data, prediction of ETA&#8230; How reliable is all this?</p>
<p>We run continuous <code>gh-mysql-rewind</code> testing in production to consistently prove that it works as expected. Our testing uses a non-production, dedicated, functional replica. It contaminates the data on the replica. It lets <code>gh-mysql-rewind</code> automatically move it back in time, it joins the replica back into the healthy chain.</p>
<p>That&#8217;s not enough. We actually create a scenario where we can predict, ahead of testing, what the time-of-arrival will be. We checksum the data on that replica at that time. After contaminating and effectively breaking replication, we expect <code>gh-mysql-rewind</code> to revert the changes back to our predicted point in time. We checksum the data again. We expect 100% match.</p>
<p>See the video or slides for more detail on our testing setup.</p>
<h2>Status</h2>
<p>At this time the tool in one of several solutions we hope to never need to employ. It is stable and tested. We are looking forward to a promising MySQL development that will provide GTID-revert capabilities using standard commands, such as <code>SELECT undo_transaction('00020192-1111-1111-1111-111111111111:5042')</code>.</p>
<p>We have <a href="https://github.com/github/gh-mysql-tools/tree/master/rewind">released</a> <code>gh-mysql-rewind</code> as open source, under the MIT license. The public release is a stripped down version of our own script, which has some GitHub-specific integration. We have general ideas in incorporating this functionality into higher level tools.</p>
<p><code>gh-mysql-rewind</code> is developed by the database-infrastructure team at GitHub.</p>
]]></content:encoded>
							<wfw:commentRss>https://shlomi-noach.github.io/blog/mysql/un-split-brain-mysql-via-gh-mysql-rewind/feed</wfw:commentRss>
		<slash:comments>0</slash:comments>
						<post-id xmlns="com-wordpress:feed-additions:1">7928</post-id>	</item>
		<item>
		<title>MySQL master discovery methods, part 5: Service discovery &#038; Proxy</title>
		<link>https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-5-service-discovery-proxy</link>
				<comments>https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-5-service-discovery-proxy#respond</comments>
				<pubDate>Mon, 14 May 2018 08:08:32 +0000</pubDate>
		<dc:creator><![CDATA[shlomi]]></dc:creator>
				<category><![CDATA[MySQL]]></category>
		<category><![CDATA[High availability]]></category>
		<category><![CDATA[orchestrator]]></category>
		<category><![CDATA[Replication]]></category>

		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=7869</guid>
				<description><![CDATA[This is the fifth in a series of posts reviewing methods for MySQL master discovery: the means by which an application connects to the master of a replication tree. Moreover, the means by which, upon master failover, it identifies and connects to the newly promoted master. These posts are not concerned with the manner by [&#8230;]]]></description>
								<content:encoded><![CDATA[<p>This is the fifth in a series of posts reviewing methods for MySQL master discovery: the means by which an application connects to the master of a replication tree. Moreover, the means by which, upon master failover, it identifies and connects to the newly promoted master.</p>
<p>These posts are not concerned with the manner by which the replication failure detection and recovery take place. I will share <code>orchestrator</code> specific configuration/advice, and point out where cross DC <code>orchestrator/raft</code> setup plays part in discovery itself, but for the most part any recovery tool such as <code>MHA</code>, <code>replication-manager</code>, <code>severalnines</code> or other, is applicable.</p>
<p>We discuss asynchronous (or semi-synchronous) replication, a classic single-master-multiple-replicas setup. A later post will briefly discuss synchronous replication (Galera/XtraDB Cluster/InnoDB Cluster).</p>
<h3>Master discovery via Service discovery and Proxy</h3>
<p><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-4-proxy-heuristics">Part 4</a> presented with an anti-pattern setup, where a proxy would infer the identify of the master by drawing conclusions from backend server checks. This led to split brains and undesired scenarios. The problem was the loss of context.</p>
<p>We re-introduce a service discovery component (illustrated in <a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-3-app-service-discovery">part 3</a>), such that:</p>
<ul>
<li>The app does not own the discovery, and</li>
<li>The proxy behaves in an expected and consistent way.</li>
</ul>
<p>In a failover/service discovery/proxy setup, there is clear ownership of duties:</p>
<ul>
<li>The failover tool own the failover itself and the master identity change notification.</li>
<li>The service discovery component is the source of truth as for the identity of the master of a cluster.</li>
<li>The proxy routes traffic but does not make routing decisions.</li>
<li>The app only ever connects to a single target, but should allow for a brief outage while failover takes place.</li>
</ul>
<p>Depending on the technologies used, we can further achieve:</p>
<ul>
<li>Hard cut for connections to old, demoted master <code>M</code>.</li>
<li>Black/hold off for incoming queries for the duration of failover.</li>
</ul>
<p>We explain the setup using the following assumptions and scenarios:</p>
<ul>
<li>All clients connect to master via <code>cluster1-writer.example.net</code>, which resolves to a proxy box.</li>
<li>We fail over from master <code>M</code> to promoted replica <code>R</code>.</li>
</ul>
<p><span id="more-7869"></span></p>
<h3>A non planned failover illustration #1</h3>
<p>Master <code>M</code> has died, the box had a power failure. <code>R</code> gets promoted in its place. Our recovery tool:</p>
<ul>
<li>Updates service discovery component that <code>R</code> is the new master for <code>cluster1</code>.</li>
</ul>
<p>The proxy:</p>
<ul>
<li>Either actively or passively learns that <code>R</code> is the new master, rewires all writes to go to <code>R</code>.</li>
<li>If possible, kills existing connections to <code>M</code>.</li>
</ul>
<p>The app:</p>
<ul>
<li>Needs to know nothing. Its connections to <code>M</code> fail, it reconnects and gets through to <code>R</code>.</li>
</ul>
<h3>A non planned failover illustration #2</h3>
<p>Master <code>M</code> gets network isolated for <code>10</code> seconds, during which time we failover. <code>R</code> gets promoted.</p>
<p>Everything is as before.</p>
<p>If the proxy kills existing connections to <code>M</code>, then the fact <code>M</code> is back alive turns meaningless. No one gets through to <code>M</code>. Clients were never aware of its identity anyhow, just as they are unaware of <code>R</code>&#8216;s identity.</p>
<h3>Planned failover illustration</h3>
<p>We wish to replace the master, for maintenance reasons. We successfully and gracefully promote <code>R</code>.</p>
<ul>
<li>In the process of promotion, <code>M</code> turned read-only.</li>
<li>Immediately following promotion, our failover tool updates service discovery.</li>
<li>Proxy reloads having seen the changes in service discovery.</li>
<li>Our app connects to <code>R</code>.</li>
</ul>
<h3>Discussion</h3>
<p>This is a setup we use at GitHub in production. Our components are:</p>
<ul>
<li><code>orchestrator</code> for failover tool.</li>
<li><em>Consul</em> for service discovery.</li>
<li>GLB (HAProxy) for proxy</li>
<li><em>Consul template</em> running on proxy hosts:
<ul>
<li>listening on changes to Consul&#8217;s KV data</li>
<li>Regenerate <code>haproxy.cfg</code> configuration file</li>
<li><code>reload</code> haproxy</li>
</ul>
</li>
</ul>
<p>As mentioned earlier, the apps need not change anything. They connect to a name that is always resolved to proxy boxes. There is never a DNS change.</p>
<p>At the time of failover, the service discovery component must be up and available, to catch the change. Otherwise we do not strictly require it to be up at all times.</p>
<p>For high availability we will have multiple proxies. Each of whom must listen on changes to K/V. Ideally the name (<code>cluster1-writer.example.net</code> in our example) resolves to any available proxy box.</p>
<ul>
<li>This, in itself, is a high availability issue. Thankfully, managing the HA of a proxy layer is simpler than that of a MySQL layer. Proxy servers tend to be stateless and equal to each other.</li>
<li>See GLB as one example for a highly available proxy layer. Cloud providers, Kubernetes, two level layered proxies, Linux Heartbeat, are all methods to similarly achieve HA.</li>
</ul>
<p>See also:</p>
<ul>
<li><a href="https://blog.pythian.com/mysql-high-availability-with-haproxy-consul-and-orchestrator/">MySQL High Availability With HAProxy, Consul And Orchestrator</a></li>
<li><a href="https://www.percona.com/live/18/sessions/automatic-failovers-with-kubernetes-using-orchestrator-proxysql-and-zookeeper">Automatic Failovers with Kubernetes using Orchestrator, ProxySQL and Zookeeper</a></li>
<li><a href="https://www.percona.com/live/e17/sessions/orchestrating-proxysql-with-orchestrator-and-consul">Orchestrating ProxySQL with Orchestrator and Consul</a></li>
</ul>
<h3>Sample orchestrator configuration</h3>
<p>An <code>orchestrator</code> configuration would look like this:</p>
<pre><code class="json">  "ApplyMySQLPromotionAfterMasterFailover": true,
  "KVClusterMasterPrefix": "mysql/master",
  "ConsulAddress": "127.0.0.1:8500",
  "ZkAddress": "srv-a,srv-b:12181,srv-c",
  "PostMasterFailoverProcesses": [
    “/just/let/me/know about failover on {failureCluster}“,
  ],
</code></pre>
<p>In the above:</p>
<ul>
<li>If <code>ConsulAddress</code> is specified, <code>orchestrator</code> will update given <em>Consul</em> setup with K/V changes.</li>
<li>At <code>3.0.10</code>, <em>ZooKeeper</em>, via <code>ZkAddress</code>, is still not supported by <code>orchestrator</code>.</li>
<li><code>PostMasterFailoverProcesses</code> is here just to point out hooks are not strictly required for the operation to run.</li>
</ul>
<p>See <a href="https://github.com/github/orchestrator/blob/master/docs/configuration.md">orchestrator configuration</a> documentation.</p>
<h3>All posts in this series</h3>
<ul>
<li><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-1-dns">MySQL master discovery methods, part 1: DNS</a></li>
<li><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-2-vip-dns">MySQL master discovery methods, part 2: VIP &amp; DNS</a></li>
<li><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-3-app-service-discovery">MySQL master discovery methods, part 3: app &amp; service discovery</a></li>
<li><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-4-proxy-heuristics">MySQL master discovery methods, part 4: Proxy heuristics</a></li>
<li><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-5-service-discovery-proxy">MySQL master discovery methods, part 5: Service discovery &amp; Proxy</a></li>
<li><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-6-other-methods">MySQL master discovery methods, part 6: other methods</a></li>
</ul>
]]></content:encoded>
							<wfw:commentRss>https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-5-service-discovery-proxy/feed</wfw:commentRss>
		<slash:comments>0</slash:comments>
						<post-id xmlns="com-wordpress:feed-additions:1">7869</post-id>	</item>
		<item>
		<title>MySQL master discovery methods, part 4: Proxy heuristics</title>
		<link>https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-4-proxy-heuristics</link>
				<comments>https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-4-proxy-heuristics#respond</comments>
				<pubDate>Thu, 10 May 2018 06:10:34 +0000</pubDate>
		<dc:creator><![CDATA[shlomi]]></dc:creator>
				<category><![CDATA[MySQL]]></category>
		<category><![CDATA[High availability]]></category>
		<category><![CDATA[orchestrator]]></category>
		<category><![CDATA[Replication]]></category>

		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=7867</guid>
				<description><![CDATA[Note: the method described here is an anti pattern This is the fourth in a series of posts reviewing methods for MySQL master discovery: the means by which an application connects to the master of a replication tree. Moreover, the means by which, upon master failover, it identifies and connects to the newly promoted master. [&#8230;]]]></description>
								<content:encoded><![CDATA[<p><em>Note: the method described here is an anti pattern</em></p>
<p>This is the fourth in a series of posts reviewing methods for MySQL master discovery: the means by which an application connects to the master of a replication tree. Moreover, the means by which, upon master failover, it identifies and connects to the newly promoted master.</p>
<p>These posts are not concerned with the manner by which the replication failure detection and recovery take place. I will share <code>orchestrator</code> specific configuration/advice, and point out where cross DC <code>orchestrator/raft</code> setup plays part in discovery itself, but for the most part any recovery tool such as <code>MHA</code>, <code>replication-manager</code>, <code>severalnines</code> or other, is applicable.</p>
<p>We discuss asynchronous (or semi-synchronous) replication, a classic single-master-multiple-replicas setup. A later post will briefly discuss synchronous replication (Galera/XtraDB Cluster/InnoDB Cluster).</p>
<h3>Master discovery via Proxy Heuristics</h3>
<p>In Proxy Heuristics all clients connect to the master through a proxy. The proxy observes the backend MySQL servers and determines who the master is.</p>
<p><strong>This setup is simple and easy, but is an anti pattern. I recommend against using this method, as explained shortly</strong>.</p>
<p>Clients are all configured to connect to, say, <code>cluster1-writer.proxy.example.net:3306</code>. The proxy will intercept incoming requests either based on hostname or by port. It is aware of all/some MySQL backend servers in that cluster, and will route traffic to the master <code>M</code>.</p>
<p>A simple heuristic that I&#8217;ve seen in use is: pick the server that has <code>read_only=0</code>, a very simple check.</p>
<p>Let&#8217;s take a look at how this works and what can go wrong.</p>
<p><span id="more-7867"></span></p>
<h3>A non planned failover illustration #1</h3>
<p>Master <code>M</code> has died, the box had a power failure. <code>R</code> gets promoted in its place. Our recovery tool:</p>
<ul>
<li>Fails over, but doesn&#8217;t need to run any hooks.</li>
</ul>
<p>The proxy:</p>
<ul>
<li>Knows both about <code>M</code> and <code>R</code>.</li>
<li>Notices <code>M</code> fails health checks (<code>select @@global.read_only</code> returns error since the box is down).</li>
<li>Notices <code>R</code> reports healthy and with <code>read_only=0</code>.</li>
<li>Routes all traffic to <code>R</code>.</li>
</ul>
<p>Success, we&#8217;re happy.</p>
<h3>Configuration tip</h3>
<p>With an automated failover solution, use <code>read_only=1</code> in <code>my.cnf</code> at all times. Only the failover solution will set a server to <code>read_only=0</code>.</p>
<p>With this configuration, when <code>M</code> restarts, MySQL starts up as <code>read_only=1</code>.</p>
<h3>A non planned failover illustration #2</h3>
<p>Master <code>M</code> gets network isolated for <code>10</code> seconds, during which time we failover. <code>R</code> gets promoted. Our tool:</p>
<ul>
<li>Fails over, but doesn&#8217;t need to run any hooks.</li>
</ul>
<p>The proxy:</p>
<ul>
<li>Knows both about <code>M</code> and <code>R</code>.</li>
<li>Notices <code>M</code> fails health checks (<code>select @@global.read_only</code> returns error since the box is down).</li>
<li>Notices <code>R</code> reports healthy and with <code>read_only=0</code>.</li>
<li>Routes all traffic to <code>R</code>.</li>
<li><code>10</code> seconds later <code>M</code> comes back to life, claiming <code>read_only=0</code>.</li>
<li>The proxy now sees two servers reporting as healthy and with <code>read_only=0</code>.</li>
<li>The proxy has no context. It does not know why both are reporting the same. It is unaware of failovers. All it sees is what the backend MySQL servers report.</li>
</ul>
<p>Therein lies the problem: you can not trust multiple servers (MySQL backends) to deterministically pick a leader (the master) without them collaborating on some elaborate consensus communication.</p>
<h3>A non planned failover illustration #3</h3>
<p>Master <code>M</code> box is overloaded, issuing <code>too many connections</code> for incoming connections.</p>
<p>Our tool decides to failover.</p>
<ul>
<li>And doesn&#8217;t need to run any hooks.</li>
</ul>
<p>The proxy:</p>
<ul>
<li>Notices <code>M</code> fails health checks (<code>select @@global.read_only</code> does not respond because of the load).</li>
<li>Notices <code>R</code> reports healthy and with <code>read_only=0</code>.</li>
<li>Routes all traffic to <code>R</code>.</li>
<li>Shortly followed by <code>M</code> recovering (since no more writes are sent its way), claiming <code>read_only=0</code>.</li>
<li>The proxy now sees two servers reporting as healthy and with <code>read_only=0</code>.</li>
</ul>
<p>Again, the proxy has no context, and neither do <code>M</code> and <code>R</code>, for that matter. The context (the fact we failed over from <code>M</code> to <code>R</code>) was known to our failover tool, but was lost along the way.</p>
<h3>Planned failover illustration</h3>
<p>We wish to replace the master, for maintenance reasons. We successfully and gracefully promote <code>R</code>.</p>
<ul>
<li><code>M</code> is available and responsive, we set it to <code>read_only=1</code>.</li>
<li>We set <code>R</code> to <code>read_only=0</code>.</li>
<li>All new connections route to <code>R</code>.</li>
<li>We should also instruct our Proxy to kill all previous connections to <code>M</code>.</li>
</ul>
<p>This works very nicely.</p>
<h3>Discussion</h3>
<p>There is a substantial risk to this method. Correlation between failover and network partitioning/load (illustrations #2 and #3) is reasonable.</p>
<p>The root of the problem is that we expect individual servers to resolve conflicts without speaking to each other: we expect the MySQL servers to correctly claim &#8220;I&#8217;m the master&#8221; without context.</p>
<p>We then add to that problem by using the proxy to &#8220;pick a side&#8221; without giving it any context, either.</p>
<h3>Sample orchestrator configuration</h3>
<p>By way of discouraging use of this method I do not present an <code>orchestrator</code> configuration file.</p>
<h3>All posts in this series</h3>
<ul>
<li><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-1-dns">MySQL master discovery methods, part 1: DNS</a></li>
<li><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-2-vip-dns">MySQL master discovery methods, part 2: VIP &amp; DNS</a></li>
<li><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-3-app-service-discovery">MySQL master discovery methods, part 3: app &amp; service discovery</a></li>
<li><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-4-proxy-heuristics">MySQL master discovery methods, part 4: Proxy heuristics</a></li>
<li><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-5-service-discovery-proxy">MySQL master discovery methods, part 5: Service discovery &amp; Proxy</a></li>
<li><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-6-other-methods">MySQL master discovery methods, part 6: other methods</a></li>
</ul>
]]></content:encoded>
							<wfw:commentRss>https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-4-proxy-heuristics/feed</wfw:commentRss>
		<slash:comments>0</slash:comments>
						<post-id xmlns="com-wordpress:feed-additions:1">7867</post-id>	</item>
		<item>
		<title>MySQL master discovery methods, part 3: app &#038; service discovery</title>
		<link>https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-3-app-service-discovery</link>
				<comments>https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-3-app-service-discovery#respond</comments>
				<pubDate>Tue, 08 May 2018 08:02:19 +0000</pubDate>
		<dc:creator><![CDATA[shlomi]]></dc:creator>
				<category><![CDATA[MySQL]]></category>
		<category><![CDATA[High availability]]></category>
		<category><![CDATA[orchestrator]]></category>
		<category><![CDATA[Replication]]></category>

		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=7865</guid>
				<description><![CDATA[This is the third in a series of posts reviewing methods for MySQL master discovery: the means by which an application connects to the master of a replication tree. Moreover, the means by which, upon master failover, it identifies and connects to the newly promoted master. These posts are not concerned with the manner by [&#8230;]]]></description>
								<content:encoded><![CDATA[<p>This is the third in a series of posts reviewing methods for MySQL master discovery: the means by which an application connects to the master of a replication tree. Moreover, the means by which, upon master failover, it identifies and connects to the newly promoted master.</p>
<p>These posts are not concerned with the manner by which the replication failure detection and recovery take place. I will share <code>orchestrator</code> specific configuration/advice, and point out where cross DC <code>orchestrator/raft</code> setup plays part in discovery itself, but for the most part any recovery tool such as <code>MHA</code>, <code>replication-manager</code>, <code>severalnines</code> or other, is applicable.</p>
<p>We discuss asynchronous (or semi-synchronous) replication, a classic single-master-multiple-replicas setup. A later post will briefly discuss synchronous replication (Galera/XtraDB Cluster/InnoDB Cluster).</p>
<h3>App &amp; service discovery</h3>
<p><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-1-dns">Part 1</a> and <a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-2-vip-dns">part 2</a> presented solutions where the app remained ingorant of master&#8217;s identity. This part takes a complete opposite direction and gives the app ownership on master access.</p>
<p>We introduce a service discovery component. Commonly known are <em>Consul</em>, <em>ZooKeeper</em>, <em>etcd</em>, highly available stores offering key/value (K/V) access, leader election or full blown service discovery &amp; health.</p>
<p>We satisfy ourselves with K/V functionality. A key would be <code>mysql/master/cluster1</code> and a value would be the master&#8217;s hostname/port.</p>
<p>It is the app&#8217;s responsibility at all times to fetch the identity of the master of a given cluster by querying the service discovery component, thereby opening connections to the indicated master.</p>
<p>The service discovery component is expected to be up at all times and to contain the identity of the master for any given cluster.</p>
<p><span id="more-7865"></span></p>
<h3>A non planned failover illustration #1</h3>
<p>Master <code>M</code> has died. <code>R</code> gets promoted in its place. Our recovery tool:</p>
<ul>
<li>Updates the service discovery component, key is <code>mysql/master/cluster1</code>, value is <code>R</code>&#8216;s hostname.</li>
</ul>
<p>Clients:</p>
<ul>
<li>Listen on K/V changes, recognize that master&#8217;s value has changed.</li>
<li>Reconfigure/refresh/reload/do what it takes to speak to new master and to drop connections to old master.</li>
</ul>
<h3>A non planned failover illustration #2</h3>
<p>Master <code>M</code> gets network isolated for <code>10</code> seconds, during which time we failover. <code>R</code> gets promoted. Our tool (as before):</p>
<ul>
<li>Updates the service discovery component, key is <code>mysql/master/cluster1</code>, value is <code>R</code>&#8216;s hostname.</li>
</ul>
<p>Clients (as before):</p>
<ul>
<li>Listen on K/V changes, recognize that master&#8217;s value has changed.</li>
<li>Reconfigure/refresh/reload/do what it takes to speak to new master and to drop connections to old master.</li>
<li>Any changes not taking place in a timely manner imply some connections still use old master <code>M</code>.</li>
</ul>
<h3>Planned failover illustration</h3>
<p>We wish to replace the master, for maintenance reasons. We successfully and gracefully promote <code>R</code>.</p>
<ul>
<li>App should start connecting to <code>R</code>.</li>
</ul>
<h3>Discussion</h3>
<p>The app is the complete owner. This calls for a few concerns:</p>
<ul>
<li>How does a given app refresh and apply the change of master such that no stale connections are kept?
<ul>
<li>Highly concurrent apps may be more difficult to manage.</li>
</ul>
</li>
<li>In a polyglot app setup, you will need all clients to use the same setup. Implement same listen/refresh logic for Ruby, golang, Java, Python, Perl and notably shell scripts.
<ul>
<li>The latter do not play well with such changes.</li>
</ul>
</li>
<li>How can you validate that the change of master has been detected by all app nodes?</li>
</ul>
<p>As for the service discovery:</p>
<ul>
<li>What load will you be placing on your service discovery component?
<ul>
<li>I was familiar with a setup where there were so many apps and app nodes and app instances, such that the amount of connections was too much for the service discovery . In that setup caching layers were created, which introduced their own consistency problems.</li>
</ul>
</li>
<li>How do you handle service discovery outage?
<ul>
<li>A reasonable approach is to keep using last known master idendity should service discovery be down. This, again, plays better wih higher level applications, but less so with scripts.</li>
</ul>
</li>
</ul>
<p>It is worth noting that this setup does not suffer from geographical limitations to the master&#8217;s identity. The master can be anywhere; the service discovery component merely points out where the master is.</p>
<h3>Sample orchestrator configuration</h3>
<p>An <code>orchestrator</code> configuration would look like this:</p>
<pre><code class="json">  "ApplyMySQLPromotionAfterMasterFailover": true,
  "KVClusterMasterPrefix": "mysql/master",
  "ConsulAddress": "127.0.0.1:8500",
  "ZkAddress": "srv-a,srv-b:12181,srv-c",
  "PostMasterFailoverProcesses": [
    “/just/let/me/know about failover on {failureCluster}“,
  ],
</code></pre>
<p>In the above:</p>
<ul>
<li>If <code>ConsulAddress</code> is specified, <code>orchestrator</code> will update given <em>Consul</em> setup with K/V changes.</li>
<li>At <code>3.0.10</code>, <em>ZooKeeper</em>, via <code>ZkAddress</code>, is still not supported by <code>orchestrator</code>.</li>
<li><code>PostMasterFailoverProcesses</code> is here just to point out hooks are not strictly required for the operation to run.</li>
</ul>
<p>See <a href="https://github.com/github/orchestrator/blob/master/docs/configuration.md">orchestrator configuration</a> documentation.</p>
<h3>All posts in this series</h3>
<ul>
<li><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-1-dns">MySQL master discovery methods, part 1: DNS</a></li>
<li><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-2-vip-dns">MySQL master discovery methods, part 2: VIP &amp; DNS</a></li>
<li><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-3-app-service-discovery">MySQL master discovery methods, part 3: app &amp; service discovery</a></li>
<li><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-4-proxy-heuristics">MySQL master discovery methods, part 4: Proxy heuristics</a></li>
<li><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-5-service-discovery-proxy">MySQL master discovery methods, part 5: Service discovery &amp; Proxy</a></li>
<li><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-6-other-methods">MySQL master discovery methods, part 6: other methods</a></li>
</ul>
]]></content:encoded>
							<wfw:commentRss>https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-3-app-service-discovery/feed</wfw:commentRss>
		<slash:comments>0</slash:comments>
						<post-id xmlns="com-wordpress:feed-additions:1">7865</post-id>	</item>
		<item>
		<title>MySQL master discovery methods, part 2: VIP &#038; DNS</title>
		<link>https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-2-vip-dns</link>
				<comments>https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-2-vip-dns#comments</comments>
				<pubDate>Mon, 07 May 2018 06:46:22 +0000</pubDate>
		<dc:creator><![CDATA[shlomi]]></dc:creator>
				<category><![CDATA[MySQL]]></category>
		<category><![CDATA[High availability]]></category>
		<category><![CDATA[orchestrator]]></category>
		<category><![CDATA[Replication]]></category>

		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=7863</guid>
				<description><![CDATA[This is the second in a series of posts reviewing methods for MySQL master discovery: the means by which an application connects to the master of a replication tree. Moreover, the means by which, upon master failover, it identifies and connects to the newly promoted master. These posts are not concerned with the manner by [&#8230;]]]></description>
								<content:encoded><![CDATA[<p>This is the second in a series of posts reviewing methods for MySQL master discovery: the means by which an application connects to the master of a replication tree. Moreover, the means by which, upon master failover, it identifies and connects to the newly promoted master.</p>
<p>These posts are not concerned with the manner by which the replication failure detection and recovery take place. I will share <code>orchestrator</code> specific configuration/advice, and point out where cross DC <code>orchestrator/raft</code> setup plays part in discovery itself, but for the most part any recovery tool such as <code>MHA</code>, <code>replication-manager</code>, <code>severalnines</code> or other, is applicable.</p>
<p>We discuss asynchronous (or semi-synchronous) replication, a classic single-master-multiple-replicas setup. A later post will briefly discuss synchronous replication (Galera/XtraDB Cluster/InnoDB Cluster).</p>
<h3>Master discovery via VIP</h3>
<p>In <a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-1-dns">part 1</a> we saw that one the main drawbacks of DNS discovery is the time it takes for the apps to connect to the promoted master. This is the result of both DNS deployment time as well as client&#8217;s <code>TTL</code>.</p>
<p>A quicker method is offered: use of VIPs (Virtual IPs). As before, apps would connect to <code>cluster1-writer.example.net</code>, <code>cluster2-writer.example.net</code>, etc. However, these would resolve to specific VIPs.</p>
<p>Say <code>cluster1-writer.example.net</code> resolves to <code>10.10.0.1</code>. We let this address float between servers. Each server has its own IP (say <code>10.20.0.XXX</code>) but could also potentially claim the VIP <code>10.10.0.1</code>.</p>
<p>VIPs can be assigned by switches and I will not dwell into the internals, because I&#8217;m not a network expert. However, the following holds:</p>
<ul>
<li>Acquiring a VIP is a very quick operation.</li>
<li>Acquiring a VIP must take place on the acquiring host.</li>
<li>A host may be unable to acquire a VIP should another host holds the same VIP.</li>
<li>A VIP can only be assigned within a bounded space: hosts connected to the same switch; hosts in the same Data Center or availability zone.</li>
</ul>
<p><span id="more-7863"></span></p>
<h3>A non planned failover illustration #1</h3>
<p>Master <code>M</code> has died, the box had a power failure. <code>R</code> gets promoted in its place. Our recovery tool:</p>
<ul>
<li>Attempts to connect to <code>M</code> so that it can give up the VIP. The attempt fails because <code>M</code> is dead.</li>
<li>Connects to <code>R</code> and instructs it to acquire the VIP. Since <code>M</code> is dead there is no objection, and <code>R</code> successfully grabs the VIP.</li>
<li>Any new connections immediately route to the new master <code>R</code>.</li>
<li>Clients with connections to <code>M</code> cannot connect, issue retries, immediately route to <code>R</code>.</li>
</ul>
<h3>A non planned failover illustration #2</h3>
<p>Master <code>M</code> gets network isolated for <code>30</code> seconds, during which time we failover. <code>R</code> gets promoted. Our tool:</p>
<ul>
<li>Attempts to connect to <code>M</code> so that it can give up the VIP. The attempt fails because <code>M</code> is network isolated.</li>
<li>Connects to <code>R</code> and instructs it to acquire the VIP. Since <code>M</code> is network isolated there is no objection, and <code>R</code> successfully grabs the VIP.</li>
<li>Any new connections immediately route to the new master <code>R</code>.</li>
<li>Clients with connections to <code>M</code> cannot connect, issue retries, immediately route to <code>R</code>.</li>
<li><code>30</code> seconds later <code>M</code> reappears, but no one pays any attention.</li>
</ul>
<h3>A non planned failover illustration #3</h3>
<p>Master <code>M</code> box is overloaded. It is not responsive to new connections but may slowly serves existing connections. Our tool decides to failover:</p>
<ul>
<li>Attempts to connect to <code>M</code> so that it can give up the VIP. The attempt fails because <code>M</code> is very loaded.</li>
<li>Connects to <code>R</code> and instructs it to acquire the VIP. Unfortunately, <code>M</code> hasn&#8217;t given up the VIP and still shows up as owning it.</li>
<li>All existing and new connections keep on routing to <code>M</code>, even as <code>R</code> is the new master.</li>
<li>This continues until some time has passed and we are able to manually grab the VIP on <code>R</code>, or until we forcibly network isolate <code>M</code> or forcibly shut it down.</li>
</ul>
<p>We suffer outage.</p>
<h3>Planned failover illustration</h3>
<p>We wish to replace the master, for maintenance reasons. We successfully and gracefully promote <code>R</code>.</p>
<ul>
<li><code>M</code> is available and responsive, we ask it to give up the VIP, which is does.</li>
<li>We ask <code>R</code> to grab the VIP, which it does.</li>
<li>All new connections route to <code>R</code>.</li>
<li>We may still see old connections routing to <code>M</code>. We can forcibly network isolate <code>M</code> to break those connections so as to cause reconnects, or restart apps.</li>
</ul>
<h3>Discussion</h3>
<p>As with DNS discovery, the apps are never told of the change. They may be forcibly restarted though.</p>
<p>Grabbing a VIP is a quick operation. However, consider:</p>
<ul>
<li>It is not guaranteed to succeed. I have seen it fail in various situations.</li>
<li>Since releasing/acquiring of VIP can only take place on the demoted/promoted servers, respectively, our failover tool will need to:
<ul>
<li>Remote SSH onto both boxes, or</li>
<li>Remote exec a command on those boxes</li>
</ul>
</li>
<li>Moreover, the tool will do so sequentially. First we must connect to demoted master to give up the VIP, then to promoted master to acquire it.</li>
<li>This means the time at which the new master grabs the VIP depends on how long it takes to connect to the old master to give up the VIP. Seeing that the old master had <em>trouble</em> causing failover, we can expect correlation to not being able to connect to old master, or seeing slow connect time.</li>
<li>An alternative exists, in the form of <a href="http://clusterlabs.org/pacemaker/">Pacemaker</a>. Consider <a href="https://github.com/Percona-Lab/pacemaker-replication-agents/blob/master/doc/PRM-setup-guide.rst">Percona&#8217;s Replication Manager</a> guide for more insights. Pacemaker provides a single point of access from where the VIP can be moved, and behind the scenes it will communicate to relevant nodes. This makes it simpler on the failover solution configuration.</li>
<li>We are constrained by physical location.</li>
<li>It is still possible for existing connection to keep on communicating to the demoted master, even while the VIP has been moved.</li>
</ul>
<h3>VIP &amp; DNS combined</h3>
<p>Per physical location, we could choose to use VIP. But should we need to failover to a server in another DC, we could choose to combine the DNS discovery, discussed in <a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-1-dns">part 1</a>.</p>
<p>We can expect to see faster failover time on a local physical location, and longer failover time on remote location.</p>
<h3>Sample orchestrator configuration</h3>
<p>What kind of remote exec method will you have? In this sample we will use remote (passwordless) SSH.</p>
<p>An <code>orchestrator</code> configuration would look like this:</p>
<pre><code class="json">  "ApplyMySQLPromotionAfterMasterFailover": true,
  "PostMasterFailoverProcesses": [
    "ssh {failedHost} 'sudo ifconfig the-vip-interface down'",
    "ssh {successorHost} 'sudo ifconfig the-vip-interface up'",
    "/do/what/you/gotta/do to apply dns change for {failureClusterAlias}-writer.example.net to {successorHost}"
  ],  
</code></pre>
<p>In the above:</p>
<ul>
<li>Replace <code>SSH</code> with any remote exec method you may use.
<ul>
<li>But you will need to set up the access/credentials for <code>orchestrator</code> to run those operations.</li>
</ul>
</li>
<li>Replace <code>ifconfig</code> with <code>service quagga stop/start</code> or any method you use to release/grab VIPs.</li>
</ul>
<p>See <a href="https://github.com/github/orchestrator/blob/master/docs/configuration.md">orchestrator configuration</a> documentation.</p>
<h3>All posts in this series</h3>
<ul>
<li><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-1-dns">MySQL master discovery methods, part 1: DNS</a></li>
<li><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-2-vip-dns">MySQL master discovery methods, part 2: VIP &amp; DNS</a></li>
<li><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-3-app-service-discovery">MySQL master discovery methods, part 3: app &amp; service discovery</a></li>
<li><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-4-proxy-heuristics">MySQL master discovery methods, part 4: Proxy heuristics</a></li>
<li><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-5-service-discovery-proxy">MySQL master discovery methods, part 5: Service discovery &amp; Proxy</a></li>
<li><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-6-other-methods">MySQL master discovery methods, part 6: other methods</a></li>
</ul>
]]></content:encoded>
							<wfw:commentRss>https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-2-vip-dns/feed</wfw:commentRss>
		<slash:comments>3</slash:comments>
						<post-id xmlns="com-wordpress:feed-additions:1">7863</post-id>	</item>
		<item>
		<title>MySQL master discovery methods, part 1: DNS</title>
		<link>https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-1-dns</link>
				<comments>https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-1-dns#respond</comments>
				<pubDate>Thu, 03 May 2018 10:56:46 +0000</pubDate>
		<dc:creator><![CDATA[shlomi]]></dc:creator>
				<category><![CDATA[MySQL]]></category>
		<category><![CDATA[High availability]]></category>
		<category><![CDATA[orchestrator]]></category>
		<category><![CDATA[Replication]]></category>

		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=7861</guid>
				<description><![CDATA[This is the first in a series of posts reviewing methods for MySQL master discovery: the means by which an application connects to the master of a replication tree. Moreover, the means by which, upon master failover, it identifies and connects to the newly promoted master. These posts are not concerned with the manner by [&#8230;]]]></description>
								<content:encoded><![CDATA[<p>This is the first in a series of posts reviewing methods for MySQL master discovery: the means by which an application connects to the master of a replication tree. Moreover, the means by which, upon master failover, it identifies and connects to the newly promoted master.</p>
<p>These posts are not concerned with the manner by which the replication failure detection and recovery take place. I will share <code>orchestrator</code> specific configuration/advice, and point out where cross DC <code>orchestrator/raft</code> setup plays part in discovery itself, but for the most part any recovery tool such as <code>MHA</code>, <code>replication-manager</code>, <code>severalnines</code> or other, is applicable.</p>
<p>We discuss asynchronous (or semi-synchronous) replication, a classic single-master-multiple-replicas setup. A later post will briefly discuss synchronous replication (Galera/XtraDB Cluster/InnoDB Cluster).</p>
<h3>Master discovery via DNS</h3>
<p>In DNS master discovery applications connect to the master via a name that gets resolved to the master&#8217;s box. By way of example, apps would target the masters of different clusters by connecting to <code>cluster1-writer.example.net</code>, <code>cluster2-writer.example.net</code>, etc. It is up for the DNS to resolve those names to IPs.</p>
<p><span id="more-7861"></span></p>
<p>Issues for concern are:</p>
<ul>
<li>You will likely have multiple DNS servers. How many? In which data centers / availability zones?</li>
<li>What is your method for distributing/deploying a name change to all your DNS servers?</li>
<li>DNS will indicate a <code>TTL</code> (Time To Live) such that clients can cache the IP associated with a name for a given number of seconds. What is that <code>TTL</code>?</li>
</ul>
<p>As long as things are stable and going well, discovery via DNS makes sense. Trouble begins when the master fails over. Assume <code>M</code> used to be the master, but got demoted. Assume <code>R</code> used to be a replica, that got promoted and is now effectively the master of the topology.</p>
<p>Our failover solution has promoted <code>R</code>, and now needs to somehow apply the change, such that the apps connect to <code>R</code> instead of <code>M</code>. Some notes:</p>
<ul>
<li>The apps need not change configuration. They should still connect to <code>cluster1-writer.example.net</code>, <code>cluster2-writer.example.net</code>, etc.</li>
<li>Our tool instructs DNS servers to make the change.</li>
<li>Clients will still resolve to old IP based on <code>TTL</code>.</li>
</ul>
<h3>A non planned failover illustration #1</h3>
<p>Master <code>M</code> dies. <code>R</code> gets promoted. Our tool instructs all DNS servers on all DCs to update the IP address.</p>
<p>Say <code>TTL</code> is <code>60</code> seconds. Say update to all DNS servers takes <code>10</code> seconds. We will have between <code>10</code> and <code>70</code> seconds until all clients connect to the new master <code>R</code>.</p>
<p>During that time they will continue to attempt connecting to <code>M</code>. Since <code>M</code> is dead, those attempts will fail (thankfully).</p>
<h3>A non planned failover illustration #2</h3>
<p>Master <code>M</code> gets network isolated for <code>30</code> seconds, during which time we failover. <code>R</code> gets promoted. Our tool instructs all DNS servers on all DCs to update the IP address.</p>
<p>Again, assume <code>TTL</code> is <code>60</code> seconds. As before, it will take between <code>10</code> and <code>70</code> seconds for clients to learn of the new IP.</p>
<p>Clients who will require between <code>40</code> and <code>70</code> seconds to learn of the new IP will, however, hit an unfortunate scenario: the old master <code>M</code> reappears on the grid. Those clients will successfully reconnect to <code>M</code> and issue writes, leading to data loss (writes to <code>M</code> no longer replicate anywhere).</p>
<h3>Planned failover illustration</h3>
<p>We wish to replace the master, for maintenance reasons. We successfully and gracefully promote <code>R</code>. We need to change DNS records. Since this is a planned failover, we set the old master to <code>read_only=1</code>, or even better, we network isolated it.</p>
<p>And still our clients take <code>10</code> to <code>70</code> seconds to recognize the new master.</p>
<h3>Discussion</h3>
<p>The above numbers are just illustrative. Perhaps DNS deployment is quicker than <code>10</code> seconds. You should do your own math.</p>
<p><code>TTL</code> is a compromise which you can tune. Setting lower <code>TTL</code> will mitigate the problem, but will cause more hits on the DNS servers.</p>
<p>For planned takeover we can first deploy a change to the <code>TTL</code>, to, say, <code>2sec</code>, wait <code>60sec</code>, then deploy the IP change, then restore <code>TTL</code> to <code>60</code>.</p>
<p>You may choose to restart apps upon DNS deployment. This emulates apps&#8217; awareness of the change.</p>
<h3>Sample orchestrator configuration</h3>
<p><code>orchestrator</code> configuration would look like this:</p>
<pre><code class="json">  "ApplyMySQLPromotionAfterMasterFailover": true,
  "PostMasterFailoverProcesses": [
    "/do/what/you/gotta/do to apply dns change for {failureClusterAlias}-writer.example.net to {successorHost}"
  ],  
</code></pre>
<p>In the above:</p>
<ul>
<li><code>ApplyMySQLPromotionAfterMasterFailover</code> instructs <code>orchestrator</code> to <code>set read_only=0; reset slave all</code> on promoted server.</li>
<li><code>PostMasterFailoverProcesses</code> really depends on your setup. But <code>orchestrator</code> will supply with hints to your scripts: identity of cluster, identity of successor.</li>
</ul>
<p>See <a href="https://github.com/github/orchestrator/blob/master/docs/configuration.md">orchestrator configuration</a> documentation.</p>
<h3>All posts in this series</h3>
<ul>
<li><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-1-dns">MySQL master discovery methods, part 1: DNS</a></li>
<li><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-2-vip-dns">MySQL master discovery methods, part 2: VIP &amp; DNS</a></li>
<li><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-3-app-service-discovery">MySQL master discovery methods, part 3: app &amp; service discovery</a></li>
<li><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-4-proxy-heuristics">MySQL master discovery methods, part 4: Proxy heuristics</a></li>
<li><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-5-service-discovery-proxy">MySQL master discovery methods, part 5: Service discovery &amp; Proxy</a></li>
<li><a href="https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-6-other-methods">MySQL master discovery methods, part 6: other methods</a></li>
</ul>
]]></content:encoded>
							<wfw:commentRss>https://shlomi-noach.github.io/blog/mysql/mysql-master-discovery-methods-part-1-dns/feed</wfw:commentRss>
		<slash:comments>0</slash:comments>
						<post-id xmlns="com-wordpress:feed-additions:1">7861</post-id>	</item>
		<item>
		<title>Using dbdeployer in CI tests</title>
		<link>https://shlomi-noach.github.io/blog/mysql/using-dbdeployer-in-ci-tests</link>
				<comments>https://shlomi-noach.github.io/blog/mysql/using-dbdeployer-in-ci-tests#respond</comments>
				<pubDate>Tue, 20 Feb 2018 07:29:58 +0000</pubDate>
		<dc:creator><![CDATA[shlomi]]></dc:creator>
				<category><![CDATA[MySQL]]></category>
		<category><![CDATA[dbdeployer]]></category>
		<category><![CDATA[gh-ost]]></category>
		<category><![CDATA[Open Source]]></category>
		<category><![CDATA[Replication]]></category>
		<category><![CDATA[testing]]></category>

		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=7848</guid>
				<description><![CDATA[I was very pleased when Giuseppe Maxia (aka datacharmer) unveiled dbdeployer in his talk at pre-FOSDEM MySQL day. The announcement came just at the right time. I wish to briefly describe how we use dbdeployer (work in progress). The case for gh-ost A user opened an issue on gh-ost, and the user was using MySQL [&#8230;]]]></description>
								<content:encoded><![CDATA[<p>I was very pleased when Giuseppe Maxia (aka <a href="http://datacharmer.blogspot.co.il/">datacharmer</a>) unveiled <a href="https://github.com/datacharmer/dbdeployer">dbdeployer</a> in his talk at <a href="http://lefred.be/content/pre-fosdem-mysql-day-2018-the-schedule/">pre-FOSDEM MySQL day</a>. The announcement came just at the right time. I wish to briefly describe how we use <code>dbdeployer</code> (work in progress).</p>
<h3>The case for gh-ost</h3>
<p>A user opened <a href="https://github.com/github/gh-ost/issues/538">an issue</a> on <a href="https://github.com/github/gh-ost"><code>gh-ost</code></a>, and the user was using MySQL <code>5.5</code>. <code>gh-ost</code> is being tested on <code>5.7</code> where the problem does not reproduce. A discussion with Gillian Gunson raised the concern of not testing on all versions. Can we run <code>gh-ost</code> tests for all MySQL/Percona/MariaDB versions? Should we? How easy would it be?</p>
<h3>gh-ost tests</h3>
<p><code>gh-ost</code> has three different test types:</p>
<ul>
<li>Unit tests: these are plain <code>golang</code> logic tests which are very easy and quick to run.</li>
<li>Integration tests: the topic of this post, see following. Today these do not run as part of an automated CI testing.</li>
<li>System tests: putting our production tables to the test, continuously migrating our production data on dedicated replicas, verifying checksums are identical and data is intact, <a href="https://githubengineering.com/mysql-testing-automation-at-github/#schema-migrations">read more</a>.</li>
</ul>
<p>Unit tests are already running as part of automated CI (every PR is subjected to those tests). Systems tests are clearly tied to our production servers. What&#8217;s the deal with the integration tests?<span id="more-7848"></span></p>
<h3>gh-ost integration tests</h3>
<p>The <code>gh-ost</code> <a href="https://github.com/github/gh-ost/tree/master/localtests">integration tests </a>are a suite of scenarios which verify <code>gh-ost</code>&#8216;s operation is sound. These scenarios are mostly concerned with data types, special <code>alter</code> statements etc. Is converting <code>DATETIME</code> to <code>TIMESTAMP</code> working properly? Are <code>latin1</code> columns being updated correctly? How about renaming a column? Changing a <code>PRIMARY KEY</code>? Column reorder? <code>5.7</code> JSON values? And so on. Each test will recreate the table, run migration, stop replication, check the result, resume replication&#8230;</p>
<p>The environment for these tests is a master-replica setup, where <code>gh-ost</code> modifies on the table on the replica and can then checksum or compare both the original and the altered <em>ghost</em> table.</p>
<p>We develop <code>gh-ost</code> internally at GitHub, but it&#8217;s also an open source project. We have our own internal CI environment, but then we also wish the public to have visibility into test failures (so that a user can submit a PR and get a reliable automated feedback). We use <a href="https://travis-ci.org/">Travis CI</a> for the public facing tests.</p>
<p>To run <code>gh-ost</code>&#8216;s integration tests as described above as part of our CI tests we should be able to:</p>
<ul>
<li>Create a master/replica setup in CI.</li>
<li>Actually, create a master/replica setup in <em>any</em> CI, and namely in Travis CI.</li>
<li>Actually, create multiple master/replica setups, of varying versions and vendors, in any ci, including both our internal CI and Travis CI.</li>
</ul>
<p>I was about to embark on a MySQL Sandbox setup, which I was not keen on. But FOSDEM was around the corner and I had other things to complete beforehand. Lucky me, <code>dbdeplyer</code> stepped in.</p>
<h3>dbdeployer</h3>
<p><code>dbdeployer</code> is a rewrite, a replacement to <a href="https://mysqlsandbox.net/">MySQL Sandbox</a>. I&#8217;ve been using MySQL Sandbox for many years, and my laptop is running two sandboxes at this very moment. But MySQL Sandbox has a few limitations or complications:</p>
<ul>
<li>Perl. Versions of Perl. Dependencies of packages of Perl. I mean, it&#8217;s fine, we can automate that.</li>
<li>Command line flag complexity: I always get lost in the complexity of the flags.</li>
<li>Get it right or prepare for battle: if you deployed something, but not the way you wanted, there&#8217;s sometimes limbo situations where you cannot re-deploy the same sandbox again, or you should start deleting files everywhere.</li>
<li>Deploy, not remove. Adding a sandbox is one thing. How about removing it?</li>
</ul>
<p><code>dbdeployer</code> is a <code>golang</code> rewrite, which solves the dependency problem. It ships as a single binary and nothing more is needed. It is simple to use. While it generates the equivalence of a that of a MySQL Sandbox, it does so with less command line flags and less confusion. There&#8217;s first class handling of the MySQL binaries: you unpack MySQL tarballs, you can list what&#8217;s available. You can then create sandbox environments: replication, standalone, etc. You can then delete those.</p>
<p>It&#8217;s pretty simple and I have not much more to add &#8212; which is the best thing about it.</p>
<p>So, with <code>dbdeployer</code> it is easy to create a master/replica. Something like:</p>
<blockquote>
<pre>dbdeployer unpack path/to/5.7.21.tar.gz --unpack-version=5.7.21 --sandbox-binary <span class="pl-smi">${PWD}</span>/sandbox/binary
dbdeployer replication 5.7.21 --nodes 2 --sandbox-binary <span class="pl-smi">${PWD}</span>/sandbox/binary --sandbox-home <span class="pl-smi">${PWD}</span>/sandboxes --gtid --my-cnf-options log_slave_updates --my-cnf-options log_bin --my-cnf-options binlog_format=ROW</pre>
</blockquote>
<h3>Where does it all fit in, and what about the MySQL binaries though?</h3>
<p>So, should <code>dbdeployer</code> be part of the <code>gh-ost</code> repo? And where does one get those MySQL binaries from? Are they to be part of the <code>gh-ost</code> repo? Aren&#8217;t they a few GB to extract?</p>
<p>Neither <code>dbdeployer</code> nor MySQL binaries should be added to the <code>gh-ost</code> repo. And fortunately, Giuseppe also <a href="https://github.com/datacharmer/mysql-docker-minimal">solved</a> the MySQL binaries problem.</p>
<p>The scheme I&#8217;m looking at right now is as follows:</p>
<ul>
<li>A new public repo, <a href="https://github.com/github/gh-ost-ci-env">gh-ost-ci-env</a> is created. This repo includes:
<ul>
<li><code>dbdeployer</code> compiled binaries</li>
<li>Minimal MySQL tarballs for selected versions. Those tarballs are reasonably small: between `14MB` and `44MB` at this time.</li>
</ul>
</li>
<li><code>gh-ost</code>&#8216;s CI to <code>git clone https://github.com/github/gh-ost-ci-env.git</code> (<a href="https://github.com/github/gh-ost/pull/546/files#diff-d78e7acd07ce8a2aad92026ae10cbec2R7">code</a>)</li>
<li><code>gh-ost</code>&#8216;s CI to setup a master/replica sandbox (<a href="https://github.com/github/gh-ost/pull/546/files#diff-d78e7acd07ce8a2aad92026ae10cbec2R13">one</a>, <a href="https://github.com/github/gh-ost/pull/546/files#diff-d78e7acd07ce8a2aad92026ae10cbec2R15">two</a>).</li>
<li><a href="https://github.com/github/gh-ost/pull/546/files#diff-d78e7acd07ce8a2aad92026ae10cbec2R30">Kick the tests</a>.</li>
</ul>
<p>The above is a work in progress:</p>
<ul>
<li>At this time only runs a single MySQL version.</li>
<li>There is a known issue where after a test, replication may take time to resume. Currently on slower boxes (such as the Travis CI containers) this leads to failures.</li>
</ul>
<p>Another concern I have at this time is build time. For a single MySQL version, it takes some <code>5-7</code> minutes on my local laptop to run all integration tests. It will be faster on our internal CI. It will be considerably <em>slower</em> on Travis CI, I can expect between <code>10m - 15m</code>. Add multiple versions and we&#8217;re looking at a <code>1hr</code> build. Such long build times will affect our development and delivery times, and so we will split them off the main build. I need to consider what the best approach is.</p>
<p>That&#8217;s all for now. I&#8217;m pretty excited for the potential of <code>dbdeployer</code> and will be looking into incorporating the same for <code>orchestrator</code> CI tests.</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
]]></content:encoded>
							<wfw:commentRss>https://shlomi-noach.github.io/blog/mysql/using-dbdeployer-in-ci-tests/feed</wfw:commentRss>
		<slash:comments>0</slash:comments>
						<post-id xmlns="com-wordpress:feed-additions:1">7848</post-id>	</item>
		<item>
		<title>orchestrator 3.0.6: faster crash detection &#038; recoveries, auto Pseudo-GTID, semi-sync and more</title>
		<link>https://shlomi-noach.github.io/blog/mysql/orchestrator-3-0-6-faster-crash-detection-recoveries-auto-pseudo-gtid-semi-sync-and-more</link>
				<comments>https://shlomi-noach.github.io/blog/mysql/orchestrator-3-0-6-faster-crash-detection-recoveries-auto-pseudo-gtid-semi-sync-and-more#respond</comments>
				<pubDate>Mon, 29 Jan 2018 09:40:05 +0000</pubDate>
		<dc:creator><![CDATA[shlomi]]></dc:creator>
				<category><![CDATA[MySQL]]></category>
		<category><![CDATA[High availability]]></category>
		<category><![CDATA[Open Source]]></category>
		<category><![CDATA[orchestrator]]></category>
		<category><![CDATA[Replication]]></category>

		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=7833</guid>
				<description><![CDATA[orchestrator 3.0.6 is released and includes some exciting improvements and features. It quickly follows up on 3.0.5 released recently, and this post gives a breakdown of some notable changes: Faster failure detection Recall that orchestrator uses a holistic approach for failure detection: it reads state not only from the failed server (e.g. master) but also [&#8230;]]]></description>
								<content:encoded><![CDATA[<p><code>orchestrator</code> <a href="https://github.com/github/orchestrator/releases/tag/v3.0.6"><strong>3.0.6</strong> is released</a> and includes some exciting improvements and features. It quickly follows up on <a href="https://github.com/github/orchestrator/releases/tag/v3.0.5"><strong>3.0.5</strong></a> released recently, and this post gives a breakdown of some notable changes:</p>
<h3>Faster failure detection</h3>
<p>Recall that <code>orchestrator</code> uses a holistic approach for <a href="https://github.com/github/orchestrator/blob/master/docs/failure-detection.md#failure-detection">failure detection</a>: it reads state not only from the failed server (e.g. master) but also from its replicas. <code>orchestrator</code> now detects failure faster than before:</p>
<ul>
<li>A detection cycle has been eliminated, leading to quicker resolution of a failure. On our setup, where we poll servers every <code>5sec</code>, failure detection time dropped from <code>7-10sec</code> to <code>3-5sec</code>, <em>keeping reliability</em>. The reduction in time does not lead to increased false positives.<br />
Side note: you may see increased not-quite-failure analysis such as &#8220;I can&#8217;t see the master&#8221; (<code>UnreachableMaster</code>).</li>
<li>Better handling of network scenarios where packets are dropped. Instead of hanging till TCP timeout, <code>orchestrator</code> now observes server discovery asynchronously. We have <a href="https://githubengineering.com/mysql-testing-automation-at-github/#failovers">specialized failover tests</a> that simulate dropped packets. The change reduces detection time by some <code>5sec</code>.</li>
</ul>
<h3>Faster master recoveries</h3>
<p>Promoting a new master is a complex task which attempts to promote the best replica out of the pool of replicas. It&#8217;s not always the most up-to-date replica. The choice varies depending on replica configuration, version, and state.</p>
<p>With recent changes, <code>orchestrator</code> is able to to recognize, early on, that the replica it would like to promote as master is <em>ideal</em>. Assuming that is the case, <code>orchestrator</code> is able to immediate promote it (i.e. run hooks, set <code>read_only=0</code> etc.), and run the rest of the failover logic, i.e. the rewiring of replicas under the newly promoted master, asynchronously.</p>
<p>This allows the promoted server to take writes sooner, even while its replicas are not yet connected. It also means external hooks are executed sooner.</p>
<p>Between faster detection and faster recoveries, we&#8217;re looking at some <code>10sec</code> reduction in overall recovery time: from moment of crash to moment where a new master accepts writes. We stand now at <code>&lt; 20sec</code> in almost all cases, and <code>&lt; 15s</code> in optimal cases. Those times are measured on our failover tests.</p>
<p>We are working on reducing failover time unrelated to <code>orchestrator</code> and hope to update soon.</p>
<h3>Automated Pseudo-GTID</h3>
<p>As reminder, Pseudo-GTID is an alternative to GTID, without the kind of commitment you make with GTID. It provides similar &#8220;point your replica under any other server&#8221; behavior GTID allows.<span id="more-7833"></span></p>
<p>There&#8217;s still <em>many</em> setups out there where GTID is not (yet?) deployed and enabled. However, Pseudo-GTID is often misunderstood, and though I&#8217;ve blogged and presented Pseudo-GTID many times in the past, I still find myself explaining to people the setup is simple and does not involve change to one&#8217;s topologies.</p>
<p>Well, it just got simpler. <code>orchestrator</code> is now able to automatically inject Pseudo-GTID for you.</p>
<p>Say the word: <code>"AutoPseudoGTID": true</code>, grant <a href="https://github.com/github/orchestrator/blob/master/docs/configuration-discovery-pseudo-gtid.md#automated-pseudo-gtid-injection">the necessary privilege</a>, and your non-GTID topology is suddenly supercharged with magical Pseudo-GTID tokens that provide you with:</p>
<ul>
<li>Arbitrary relocation of replicas</li>
<li>Automated or manual failovers (masters <em>and</em> intermediate masters)</li>
<li>Vendor freedom: runs on Oracle MySQL, Percona Server, MariaDB, or all of the above at the very same time.</li>
<li>Version freedom (still on <code>5.5</code>? No problem. Oh, this gets you crash-safe replication as extra bonus, too)</li>
</ul>
<p>Auto-Pseudo-GTID further simplifies the infrastructure in that you no longer need to take care of injecting Pseudo-GTID onto the master as well as handle master identity changes. No more <code>event_scheduler</code> to enable/disable nor services to <code>start/stop</code>.</p>
<p>More and more setups are moving to GTID. We may, too! But I find it peculiar that Pseudo-GTID was suggested <code>4</code> years ago, when <code>5.6</code> GTID was already released, and still many setups are not yet running GTID. If you&#8217;re not using GTID, please try Pseudo-GTID! <a href="https://github.com/github/orchestrator/blob/master/docs/pseudo-gtid.md">Read more</a>.</p>
<h3>Semi-sync support</h3>
<p>Semi-sync has been internally supported via a specialized patch contributed by Vitess, to flag a server as semi-sync-able and handle enablement of semi-sync upon master failover.</p>
<p><code>orchestrator</code> now supports semi-sync more generically. You may use <code>orchestrator</code> to enable/disable semi-sync master/replica side, via <code>orchestrator -c enable-semi-sync-master</code>, <code>orchestrator -c enable-semi-sync-replica</code>, <code>orchestrator -c disable-semi-sync-master</code>, <code>orchestrator -c disable-semi-sync-replica</code> commands (or API equivalent).</p>
<p>The API will also tell you whether semi-sync is enabled on instances. Noteworthy that configured != enabled. A server can be configured with <code>rpl_semi_sync_master_enabled=ON</code>, but if no semi-sync replicas are found, the <code>Rpl_semi_sync_master_status</code> state is <code>OFF</code>.</p>
<h3>More</h3>
<p>UI changes, removal of prepared statements, documentation updates, raft updates&#8230;</p>
<p><a href="https://github.com/github/orchestrator"><code>orchestrator</code></a> is free and open source and released under the Apache 2 license. It is authored at and used by GitHub.</p>
<p>I&#8217;ll be presenting <code>orchestrator/raft</code> in <a href="https://fosdem.org/2018/schedule/event/orchestrator_raft/">FOSDEM next week</a>, at the MySQL and Friends Room.</p>
]]></content:encoded>
							<wfw:commentRss>https://shlomi-noach.github.io/blog/mysql/orchestrator-3-0-6-faster-crash-detection-recoveries-auto-pseudo-gtid-semi-sync-and-more/feed</wfw:commentRss>
		<slash:comments>0</slash:comments>
						<post-id xmlns="com-wordpress:feed-additions:1">7833</post-id>	</item>
		<item>
		<title>orchestrator/raft: Pre-Release 3.0</title>
		<link>https://shlomi-noach.github.io/blog/mysql/orchestratorraft-pre-release-3-0</link>
				<comments>https://shlomi-noach.github.io/blog/mysql/orchestratorraft-pre-release-3-0#comments</comments>
				<pubDate>Thu, 03 Aug 2017 08:41:11 +0000</pubDate>
		<dc:creator><![CDATA[shlomi]]></dc:creator>
				<category><![CDATA[MySQL]]></category>
		<category><![CDATA[Failover]]></category>
		<category><![CDATA[High availability]]></category>
		<category><![CDATA[Open Source]]></category>
		<category><![CDATA[orchestrator]]></category>
		<category><![CDATA[raft]]></category>
		<category><![CDATA[Replication]]></category>

		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=7740</guid>
				<description><![CDATA[orchestrator 3.0 Pre-Release is now available. Most notable are Raft consensus, SQLite backend support, orchestrator-client no-binary-required client script. TL;DR You may now set up high availability for orchestrator via raft consensus, without need to set up high availability for orchestrator&#8216;s backend MySQL servers (such as Galera/InnoDB Cluster). In fact, you can run a orchestrator/raft setup [&#8230;]]]></description>
								<content:encoded><![CDATA[<p><code>orchestrator</code> <strong>3.0 Pre-Release</strong> is <a href="https://github.com/github/orchestrator/releases/tag/v3.0.pre-release">now available</a>. Most notable are <strong>Raft</strong> consensus, <strong>SQLite</strong> backend support, <strong>orchestrator-client</strong> no-binary-required client script.</p>
<h3>TL;DR</h3>
<p>You may now set up high availability for <code>orchestrator</code> via <code>raft</code> consensus, without need to set up high availability for <code>orchestrator</code>&#8216;s backend MySQL servers (such as Galera/InnoDB Cluster). In fact, you can run a <code>orchestrator/raft</code> setup using embedded <code>SQLite</code> backend DB. Read on.</p>
<p><code>orchestrator</code> still supports the existing shared backend DB paradigm; nothing dramatic changes if you upgrade to <strong>3.0</strong> and do not configure <code>raft</code>.</p>
<h3>orchestrator/raft</h3>
<p><a href="https://raft.github.io/">Raft</a> is a consensus protocol, supporting leader election and consensus across a distributed system.  In an <code>orchestrator/raft</code> setup <code>orchestrator</code> nodes talk to each other via raft protocol, form consensus and elect a leader. Each <code>orchestrator</code> node has its own <em>dedicated</em> backend database. The backend databases do not speak to each other; only the <code>orchestrator</code> nodes speak to each other.</p>
<p>No MySQL replication setup needed; the backend DBs act as standalone servers. In fact, the backend server doesn&#8217;t have to be MySQL, and <code>SQLite</code> is supported. <code>orchestrator</code> now ships with <code>SQLite</code> embedded, no external dependency needed.<span id="more-7740"></span></p>
<blockquote><p><a href="https://shlomi-noach.github.io/blog/wp-content/uploads/2017/08/orchestrator-ha-raft.png"><img class="alignnone size-full wp-image-7743" src="https://shlomi-noach.github.io/blog/wp-content/uploads/2017/08/orchestrator-ha-raft.png" alt="" width="824" height="326" srcset="https://shlomi-noach.github.io/blog/wp-content/uploads/2017/08/orchestrator-ha-raft.png 824w, https://shlomi-noach.github.io/blog/wp-content/uploads/2017/08/orchestrator-ha-raft-300x119.png 300w, https://shlomi-noach.github.io/blog/wp-content/uploads/2017/08/orchestrator-ha-raft-768x304.png 768w" sizes="(max-width: 824px) 100vw, 824px" /></a></p></blockquote>
<p>In a <code>orchestrator/raft</code> setup, all <code>orchestrator</code> nodes talk to each other. One and only one is elected as <em>leader</em>. To become a leader a node must be part of a <em>quorum</em>. On a <code>3</code> node setup, it takes <code>2</code> nodes to form a quorum. On a <code>5</code> node setup, it takes <code>3</code> nodes to form a quorum.</p>
<p>Only the leader will run failovers. This much is similar to the existing shared-backend DB setup. However in a <code>orchestrator/raft</code> setup each node is independent, and each <code>orchestrator</code> node <em>runs discoveries</em>. This means a MySQL server in your topology will be routinely visited and probed by not one <code>orchestrator</code> node, but by all <code>3</code> (or <code>5</code>, or what have you) nodes in your raft cluster.</p>
<p>Any communication to <code>orchestrator</code> must take place through the leader. One may not tamper directly with the backend DBs anymore, since the <code>leader</code> is the one authoritative entity to replicate and announce changes to its peer nodes. See <strong>orchestrator-client</strong> section following.</p>
<p>For details, please refer to the documentation:</p>
<ul>
<li><a href="https://github.com/github/orchestrator/blob/master/docs/raft.md">orchestrator/raft: overview</a></li>
<li><a href="https://github.com/github/orchestrator/blob/master/docs/raft-vs-sync-repl.md">orchestrator/raft vs. shared backend DB setup, comparison</a></li>
</ul>
<p>The <code>orchetrator/raft</code> setup comes to solve several issues, the most obvious is high availability for the <code>orchestrator</code> service: in a <code>3</code> node setup any single <code>orchestrator</code> node can go down and <code>orchestrator</code> will reliably continue probing, detecting failures and recovering from failures.</p>
<ul>
<li>See all <a href="https://github.com/github/orchestrator/blob/master/docs/high-availability.md">orchestrator high availability solutions</a></li>
</ul>
<p>Another issue solve by <code>orchestrator/raft</code> is network isolation, in particularly cross-DC, also refered to as <em>fencing</em>. Some visualization will help describe the issue.</p>
<p>Consider this 3 data-center replication setup. The master, along with a few replicas, resides on <strong>DC1</strong>. Two additional DCs have intermediate masters, aka local-masters, that relay replication to local replicas.</p>
<blockquote><p><a href="https://shlomi-noach.github.io/blog/wp-content/uploads/2017/08/raft-3dc-1.png"><img class="alignnone wp-image-7752 size-medium" src="https://shlomi-noach.github.io/blog/wp-content/uploads/2017/08/raft-3dc-1-300x252.png" alt="" width="300" height="252" srcset="https://shlomi-noach.github.io/blog/wp-content/uploads/2017/08/raft-3dc-1-300x252.png 300w, https://shlomi-noach.github.io/blog/wp-content/uploads/2017/08/raft-3dc-1.png 698w" sizes="(max-width: 300px) 100vw, 300px" /></a></p></blockquote>
<p>We place <code>3 orchestrator</code> nodes in a <code>raft</code> setup, each in a different DC. Note that traffic between <code>orchestrator</code> nodes is very low, and cross DC latencies still conveniently support the <code>raft</code> communication. Also note that backend DB writes have nothing to do with cross-DC traffic and are unaffected by latencies.</p>
<p>Consider what happens if DC1 gets network isolated: no traffic in or out DC1</p>
<blockquote><p><a href="https://shlomi-noach.github.io/blog/wp-content/uploads/2017/08/raft-3dc-dc1-isolated.png"><img class="alignnone size-medium wp-image-7755" src="https://shlomi-noach.github.io/blog/wp-content/uploads/2017/08/raft-3dc-dc1-isolated-300x249.png" alt="" width="300" height="249" srcset="https://shlomi-noach.github.io/blog/wp-content/uploads/2017/08/raft-3dc-dc1-isolated-300x249.png 300w, https://shlomi-noach.github.io/blog/wp-content/uploads/2017/08/raft-3dc-dc1-isolated.png 698w" sizes="(max-width: 300px) 100vw, 300px" /></a></p></blockquote>
<p>Each <code>orchestrator</code> nodes operates independently, and each will see a different state. DC1&#8217;s <code>orchestrator</code> will see all servers in DC2, DC3 as dead, but figure the master itself is fine, along with its local DC1 replicas:</p>
<blockquote><p><a href="https://shlomi-noach.github.io/blog/wp-content/uploads/2017/08/raft-3dc-dc1-isolated-dc1-view.png"><img class="alignnone size-medium wp-image-7756" src="https://shlomi-noach.github.io/blog/wp-content/uploads/2017/08/raft-3dc-dc1-isolated-dc1-view-300x248.png" alt="" width="300" height="248" srcset="https://shlomi-noach.github.io/blog/wp-content/uploads/2017/08/raft-3dc-dc1-isolated-dc1-view-300x248.png 300w, https://shlomi-noach.github.io/blog/wp-content/uploads/2017/08/raft-3dc-dc1-isolated-dc1-view.png 698w" sizes="(max-width: 300px) 100vw, 300px" /></a></p></blockquote>
<p>However both <code>orchestrator</code> nodes in DC2 and DC3 will see a different picture: they will see all DC1&#8217;s servers as dead, with local masters in DC2 and DC3 having broken replication:</p>
<blockquote><p><a href="https://shlomi-noach.github.io/blog/wp-content/uploads/2017/08/raft-3dc-dc1-isolated-dc2-dc3-view.png"><img class="alignnone size-medium wp-image-7757" src="https://shlomi-noach.github.io/blog/wp-content/uploads/2017/08/raft-3dc-dc1-isolated-dc2-dc3-view-300x255.png" alt="" width="300" height="255" srcset="https://shlomi-noach.github.io/blog/wp-content/uploads/2017/08/raft-3dc-dc1-isolated-dc2-dc3-view-300x255.png 300w, https://shlomi-noach.github.io/blog/wp-content/uploads/2017/08/raft-3dc-dc1-isolated-dc2-dc3-view.png 698w" sizes="(max-width: 300px) 100vw, 300px" /></a></p></blockquote>
<p>Who gets to choose?</p>
<p>In the <code>orchestrator/raft</code> setup, only the leader runs failovers. The leader must be part of a quorum. Hence the leader will be an <code>orchestrator</code> node in either DC2 or DC3. DC1&#8217;s <code>orchestrator</code> will <em>know</em> it is isolated, that it isn&#8217;t part of the quorum, hence will step down from leadership (that&#8217;s the premise of the <code>raft</code> consensus protocol), hence will not run recoveries.</p>
<p>There will be no split brain in this scenario. The <code>orchestrator</code> leader, be it in DC2 or DC3, will act to recover and promote a master from within DC2 or DC3. A possible outcome would be:</p>
<blockquote><p><a href="https://shlomi-noach.github.io/blog/wp-content/uploads/2017/08/raft-3dc-dc1-isolated-recovered.png"><img class="alignnone size-medium wp-image-7758" src="https://shlomi-noach.github.io/blog/wp-content/uploads/2017/08/raft-3dc-dc1-isolated-recovered-300x259.png" alt="" width="300" height="259" srcset="https://shlomi-noach.github.io/blog/wp-content/uploads/2017/08/raft-3dc-dc1-isolated-recovered-300x259.png 300w, https://shlomi-noach.github.io/blog/wp-content/uploads/2017/08/raft-3dc-dc1-isolated-recovered.png 680w" sizes="(max-width: 300px) 100vw, 300px" /></a></p></blockquote>
<p>What if you only have 2 data centers?</p>
<p>In such case it is advisable to put two <code>orchestrator</code> nodes, one in each of your DCs, and a <em>third</em> <code>orchestrator</code> node as a mediator, in a 3rd DC, or in a different availability zone. A cloud offering should do well:</p>
<blockquote><p><a href="https://shlomi-noach.github.io/blog/wp-content/uploads/2017/08/raft-2dc-mediator.png"><img class="alignnone size-medium wp-image-7759" src="https://shlomi-noach.github.io/blog/wp-content/uploads/2017/08/raft-2dc-mediator-300x254.png" alt="" width="300" height="254" srcset="https://shlomi-noach.github.io/blog/wp-content/uploads/2017/08/raft-2dc-mediator-300x254.png 300w, https://shlomi-noach.github.io/blog/wp-content/uploads/2017/08/raft-2dc-mediator.png 688w" sizes="(max-width: 300px) 100vw, 300px" /></a></p></blockquote>
<p>The <code>orchestrator/raft</code> setup plays nice and allows one to <a href="https://github.com/openark/raft/pull/1">nominate</a> a preferred leader.</p>
<h3>SQLite</h3>
<p>Suggested and requested by many, is to remove <code>orchestrator</code>&#8216;s own dependency on a MySQL backend. <code>orchestrator</code> now supports a SQLite backend.</p>
<p><code>SQLite</code> is a transactional, relational, embedded database, and as of <code>3.0</code> it is embedded within <code>orchestrator</code>, no external dependency required.</p>
<p><code>SQLite</code> doesn&#8217;t replicate, doesn&#8217;t support client/server protocol. As such, it cannot work as a shared database backend. <code>SQLite</code> is only available on:</p>
<ul>
<li>A single node setup: good for local dev installations, testing server, CI servers (indeed, <code>SQLite</code> now runs in <code>orchestrator</code>&#8216;s CI)</li>
<li><code>orchestrator/raft</code> setup, where, as noted above, backend DBs do not communicate with each other in the first place and are each dedicated to their own <code>orchestrator</code> node.</li>
</ul>
<p>It should be pointed out that <code>SQLite</code> is a great transactional database, however <code>MySQL</code> is more performant. Load on backend DB is directly (and mostly linearly) affected by the number of probed servers. If you have <code>50</code> servers in your topologies or <code>500</code> servers, that matters. The probing frequency of course also matters for the write frequency on your backend DB. I would suggest if you have thousands of backend servers, to stick with <code>MySQL</code>. If dozens, <code>SQLite</code> should be good to go. In between is a gray zone, and at any case run your own experiments.</p>
<p>At this time <code>SQLite</code> is configured to commit to file; there is a different setup where <code>SQLite</code> places data in-memory, which makes it faster to execute. Occasional dumps required for durability. <code>orchestrator</code> may support this mode in the future.</p>
<h3>orchestrator-client</h3>
<p>You install <code>orchestrator</code> as a service on a few boxes; but then how do you access it from other hosts?</p>
<ul>
<li>Either <code>curl</code> the <code>orchestrator</code> API</li>
<li>Or, as most do, install <code>orchestrator-cli</code> package, which includes the <code>orchestrator</code> binary, everywhere.</li>
</ul>
<p>The latter implies:</p>
<ul>
<li>Having the <code>orchestrator</code> binary installed everywhere, hence updated everywhere.</li>
<li>Having the <code>/etc/orchestrator.conf.json</code>deployed everywhere, along with credentials.</li>
</ul>
<p>The <code>orchestrator/raft</code> setup does not support running <code>orchestrator</code> in command-line mode. Reason: in this mode <code>orchestrator</code> talks directly to the shared backend DB. There is no shared backend DB in the <code>orchestrator/raft</code> setup, and all communication must go through the leader service. This is a change of paradigm.</p>
<p>So, back to <code>curl</code>ing the HTTP API. Enter <a href="https://github.com/github/orchestrator/blob/master/docs/orchestrator-client.md"><strong>orchestrator-client</strong></a> which mimics the command line interface, while running <code>curl | jq</code> requests against the HTTP API. <code>orchestrator-client</code>, however, is just a shell script.</p>
<p><code>orchestrator-client</code> will work well on either <code>orchestrator/raft</code> or on your existing non-raft setups. If you like, you may replace your remote <code>orchestrator</code> installations and your <code>/etc/orchestrator.conf.json</code> deployments with this script. You will need to provide the script with a hint: the <code>$ORCHESTRATOR_API</code> environment variable should be set to point to the <code>orchestrator</code> HTTP API.</p>
<p>Here&#8217;s the fun part:</p>
<ul>
<li>You will either have a proxy on top of your <code>orchestrator</code> service cluster, and you would <code>export ORCHESTRATOR_API=http://my.orchestrator.service/api</code></li>
<li>Or you will provide <code>orchestrator-client</code> with all <code>orchestrator</code> node identities, as in <code>export ORCHESTRATOR_API="https://orchestrator.host1:3000/api https://orchestrator.host2:3000/api https://orchestrator.host3:3000/api"</code> .<br />
<code>orchestrator-client</code> will <strong>figure the identity of the leader</strong> and will forward requests to the leader. At least scripting-wise, you will not require a proxy.</li>
</ul>
<h3>Status</h3>
<p><code>orchestrator 3.0</code> is a <strong>Pre-Release</strong>. We are running a mostly-passive <code>orchestrator/raft</code> setup in production. It is mostly-passive in that it is not in charge of failovers yet. Otherwise it probes and analyzes our topologies, as well as runs failure detection. We will continue to improve operational aspects of the <code>orchestrator/raft</code> setup (see <a href="https://github.com/github/orchestrator/issues/246">this issue</a>).</p>
<p>&nbsp;</p>
]]></content:encoded>
							<wfw:commentRss>https://shlomi-noach.github.io/blog/mysql/orchestratorraft-pre-release-3-0/feed</wfw:commentRss>
		<slash:comments>4</slash:comments>
						<post-id xmlns="com-wordpress:feed-additions:1">7740</post-id>	</item>
	</channel>
</rss>
