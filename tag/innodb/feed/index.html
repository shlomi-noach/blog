<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	
	xmlns:georss="http://www.georss.org/georss"
	xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#"
	>

<channel>
	<title>InnoDB &#8211; code.openark.org</title>
	<atom:link href="https://shlomi-noach.github.io/blog/tag/innodb/feed" rel="self" type="application/rss+xml" />
	<link>http://shlomi-noach.github.io/blog/</link>
	<description>Blog by Shlomi Noach</description>
	<lastBuildDate>Sun, 20 Apr 2014 06:14:26 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=5.3.3</generator>
<site xmlns="com-wordpress:feed-additions:1">32412571</site>	<item>
		<title>The mystery of MySQL 5.6 excessive buffer pool flushing</title>
		<link>https://shlomi-noach.github.io/blog/mysql/the-mystery-of-mysql-5-6-excessive-buffer-pool-flushing</link>
				<comments>https://shlomi-noach.github.io/blog/mysql/the-mystery-of-mysql-5-6-excessive-buffer-pool-flushing#comments</comments>
				<pubDate>Sun, 20 Apr 2014 05:16:13 +0000</pubDate>
		<dc:creator><![CDATA[shlomi]]></dc:creator>
				<category><![CDATA[MySQL]]></category>
		<category><![CDATA[InnoDB]]></category>
		<category><![CDATA[Performance]]></category>
		<category><![CDATA[Replication]]></category>

		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=6843</guid>
				<description><![CDATA[I&#8217;m experimenting with upgrading to MySQL 5.6 and am experiencing an unexplained increase in disk I/O utilization. After discussing this with several people I&#8217;m publishing in the hope that someone has an enlightenment on this. We have a few dozens servers in a normal replication topology. On this particular replication topology we&#8217;ve already evaluated that [&#8230;]]]></description>
								<content:encoded><![CDATA[<p>I&#8217;m experimenting with upgrading to MySQL <strong>5.6</strong> and am experiencing an unexplained increase in disk I/O utilization. After discussing this with several people I&#8217;m publishing in the hope that someone has an enlightenment on this.</p>
<p>We have a few dozens servers in a normal replication topology. On this particular replication topology we&#8217;ve already evaluated that <strong>STATEMENT</strong> based replication is faster than <strong>ROW</strong> based replication, and so we use <strong>SBR</strong>. We have two different workloads on our slaves, applied by two different HAProxy groups, on three different data centres. Hardware-wise, servers of two groups use either Virident SSD cards or normal SAS spindle disks.</p>
<p>Our servers are I/O bound. A common query used by both workloads looks up data that does not necessarily have a hotspot, and is very large in volume. DML is low, and we only have a few hundred statements per second executed on master (and propagated through replication).</p>
<p>We have upgraded <strong>6</strong> servers from all datacenters to <strong>5.6</strong>, both on SSD and spindle disks, and are experiencing the following phenomena:<span id="more-6843"></span></p>
<ul>
<li>A substantial increase in disk I/O utilization. See a <strong>10</strong> day breakdown (upgrade is visible on <strong>04/14</strong>) this goes on like this many days later:<br />
<blockquote><p><a href="https://shlomi-noach.github.io/blog/wp-content/uploads/2014/04/5.5-to-5.6-disk-utilization-10-days.png"><img class="alignnone wp-image-6845 size-full" src="https://shlomi-noach.github.io/blog/wp-content/uploads/2014/04/5.5-to-5.6-disk-utilization-10-days.png" alt="5.5-to-5.6-disk-utilization-10-days" width="700" height="400" srcset="https://shlomi-noach.github.io/blog/wp-content/uploads/2014/04/5.5-to-5.6-disk-utilization-10-days.png 700w, https://shlomi-noach.github.io/blog/wp-content/uploads/2014/04/5.5-to-5.6-disk-utilization-10-days-300x171.png 300w" sizes="(max-width: 700px) 100vw, 700px" /></a></p></blockquote>
</li>
</ul>
<ul>
<li>A substantial increase in InnoDB buffer pool pages flush: Mr. Blue is our newly upgraded server; it joins Mr. Green upgraded a couple weeks ago. Mr. Red is still <strong>5.5</strong>. This is the only MySQL graph that I could directly relate to the increase in I/O:<br />
<blockquote><p><a href="https://shlomi-noach.github.io/blog/wp-content/uploads/2014/04/5.5-to-5.6-rise-in-innodb-buffer-pool-pages-flushed.png"><img class="alignnone size-full wp-image-6848" src="https://shlomi-noach.github.io/blog/wp-content/uploads/2014/04/5.5-to-5.6-rise-in-innodb-buffer-pool-pages-flushed.png" alt="5.5-to-5.6-rise-in-innodb-buffer-pool-pages-flushed" width="700" height="350" srcset="https://shlomi-noach.github.io/blog/wp-content/uploads/2014/04/5.5-to-5.6-rise-in-innodb-buffer-pool-pages-flushed.png 700w, https://shlomi-noach.github.io/blog/wp-content/uploads/2014/04/5.5-to-5.6-rise-in-innodb-buffer-pool-pages-flushed-300x150.png 300w" sizes="(max-width: 700px) 100vw, 700px" /></a></p></blockquote>
</li>
<li>No change in workload (it takes some 60 minutes for caches to warm up, so measuring after that time). Same equal share of serving as dictated by HAProxy. Same amount of queries. Same amount of everything.</li>
<li>Faster replication speed, on single thread &#8211; that&#8217;s the good part! We see <strong>30%</strong> and more improvement in replication speed. Tested by stopping <strong>SLAVE SQL_THREAD</strong> for a number of pre-defined minutes, then measuring time it took for slave to catch up, up to 10 seconds lag. The results vary depending on the time of day and serving workload on slaves, but it is <em>consistently far faster</em> with <strong>5.6</strong>.</li>
</ul>
<p>The faster replication speed motivates us to continue with the experiment, and is of a significant factor in our decision. However we are concerned about the I/O utilization and excessive flushing.</p>
<p>The above graphs depict the <strong>5.6</strong> status without any configuration changes as compared to <strong>5.5</strong>. I took some days to reconfigure the following variables, with no change to the rate of flushed pages (though some changes visible in double-wite buffer writes):</p>
<ul>
<li>innodb_log_file_size=128M/2G</li>
<li>innodb_adaptive_flushing:=0/1</li>
<li>innodb_adaptive_flushing_lwm:=0/70</li>
<li>innodb_max_dirty_pages_pct := 75/90</li>
<li>innodb_flush_neighbors:=0/1</li>
<li>innodb_max_dirty_pages_pct_lwm:=75/90</li>
<li>innodb_old_blocks_time:=0/1000</li>
<li>innodb_io_capacity:=50/100/200</li>
<li>innodb_io_capacity_max:=50/100/1000</li>
<li>relay_log_info_repository:=&#8217;table&#8217;/&#8217;file&#8217;</li>
<li>master_info_repository:=&#8217;table&#8217;/&#8217;file&#8217;</li>
<li>default_tmp_storage_engine:=&#8217;myisam&#8217;/&#8217;innodb&#8217;</li>
<li>eq_range_index_dive_limit:=0/10</li>
</ul>
<p>And more&#8230; Have done patient one-by-one or combinations of the above where it made sense. As you see I began with the usual suspects and moved on to more esoteric stuff. I concentrated on new variables introduced in <strong>5.6</strong>, or ones where the defaults have changed, or ones we have explicitly changed the defaults from.</p>
<p>The above is consistent on all upgraded servers. On SSD the disk utilization is lower, but still concerning.</p>
<p>Our use case is very different from the one <a href="http://yoshinorimatsunobu.blogspot.co.il/2013/12/single-thread-performance-regression-in.html">presented by Yoshinori Matsunobu</a>. and apparently not too many have experienced upgrading to <strong>5.6</strong>. I&#8217;m hoping someone might shed some light.</p>
]]></content:encoded>
							<wfw:commentRss>https://shlomi-noach.github.io/blog/mysql/the-mystery-of-mysql-5-6-excessive-buffer-pool-flushing/feed</wfw:commentRss>
		<slash:comments>15</slash:comments>
						<post-id xmlns="com-wordpress:feed-additions:1">6843</post-id>	</item>
		<item>
		<title>TokuDB configuration variables of interest</title>
		<link>https://shlomi-noach.github.io/blog/mysql/tokudb-configuration-variables-of-interest</link>
				<comments>https://shlomi-noach.github.io/blog/mysql/tokudb-configuration-variables-of-interest#comments</comments>
				<pubDate>Wed, 23 Oct 2013 17:42:12 +0000</pubDate>
		<dc:creator><![CDATA[shlomi]]></dc:creator>
				<category><![CDATA[MySQL]]></category>
		<category><![CDATA[Configuration]]></category>
		<category><![CDATA[InnoDB]]></category>
		<category><![CDATA[Performance]]></category>
		<category><![CDATA[TokuDB]]></category>

		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=6613</guid>
				<description><![CDATA[During our experiments I came upon a few TokuDB variables of interest; if you are using TokuDB you might want to look into these: tokudb_analyze_time This is a boundary on the number of seconds an ANALYZE TABLE will operate on each index on each partition on a TokuDB table. That is, if tokudb_analyze_time = 5, [&#8230;]]]></description>
								<content:encoded><![CDATA[<p>During our experiments I came upon a few TokuDB variables of interest; if you are using TokuDB you might want to look into these:</p>
<ul>
<li>
<h4>tokudb_analyze_time</h4>
</li>
</ul>
<p style="padding-left: 30px;">This is a boundary on the number of seconds an <strong>ANALYZE TABLE</strong> will operate on each index on each partition on a TokuDB table.</p>
<p style="padding-left: 30px;">That is, if <strong>tokudb_analyze_time = 5</strong>, and your table has <strong>4</strong> indexes (including <strong>PRIMARY</strong>) and <strong>7</strong> partitions, then the total runtime is limited to <strong>5*4*7 = 140</strong> seconds.</p>
<p style="padding-left: 30px;">Default in <strong>7.1.0</strong>: <strong>5</strong> seconds</p>
<ul>
<li>
<h4>tokudb_cache_size</h4>
</li>
</ul>
<p style="padding-left: 30px;">Similar to <strong>innodb_buffer_pool_size</strong>, this variable sets the amount of memory allocated by TokuDB for caching pages. Like InnoDB the table is clustered within the index, so the cache includes pages for both indexes and data.</p>
<p style="padding-left: 30px;">Default: <strong>50%</strong> of total memory</p>
<ul>
<li>
<h4>tokudb_directio</h4>
</li>
</ul>
<p style="padding-left: 30px;">Boolean, values are <strong>0/1</strong>. Setting <strong>tokudb_directio = 1</strong> is like specifying <strong>innodb_flush_method = O_DIRECT</strong>. Which in turn means the OS should not cache pages requested by TokuDB. Default: <strong>0</strong>.</p>
<p style="padding-left: 30px;">Now here&#8217;s the interesting part: we are used to tell InnoDB to get the most memory we can provide (because we want it to cache as much as it can) and to avoid OS caching (because that would mean a page would appear both in the buffer pool and in OS memory, which is a waste). So the following setup is common:<span id="more-6613"></span></p>
<blockquote style="padding-left: 30px;">
<pre style="padding-left: 30px;"><strong>innodb_buffer_pool_size</strong> = [as much as you can allocate while leaving room for connection memory]G
<strong>innodb_flush_method</strong> = O_DIRECT</pre>
</blockquote>
<p style="padding-left: 30px;">And my first instinct was to do the same for TokuDB. But after speaking to Gerry Narvaja of Tokutek, I realized it was not that simple. The reason TokuDB&#8217;s default memory allocation is <strong>50%</strong> and not, say, <strong>90%</strong>, is that OS cache caches the data in compressed form, while TokuDB cache caches data in uncompressed form. Which means if you limit the TokuDB cache, you allow for more cache to the OS, that is used to cache compressed data, which means <em>more data</em> (hopefully, pending duplicates) in memory.</p>
<p style="padding-left: 30px;">I did try both options and did not see an obvious difference, but did not test this thoroughly. My current setup is:</p>
<blockquote style="padding-left: 30px;">
<pre style="padding-left: 30px;"><strong>#No setup. just keep to the default for both:</strong>
#tokudb_cache_size
#tokudb_directio</pre>
</blockquote>
<ul>
<li>
<h4>tokudb_commit_sync</h4>
</li>
</ul>
<ul>
<li>
<h4>tokudb_fsync_log_period</h4>
</li>
</ul>
<p style="padding-left: 30px;">These two variable are similar in essence to <strong>innodb_flush_log_at_trx_commit</strong>, but allow for finer tuning. With <strong>innodb_flush_log_at_trx_commit</strong> you choose between syncing the transaction log to disk upon each commit and once per second. With <strong>tokudb_commit_sync = 1</strong> (which is default) you get transaction log sync to disk per commit. When <strong>tokudb_commit_sync = 0</strong>, then <strong>tokudb_fsync_log_period</strong> dictates the interval between flushes. So a value of <strong>tokudb_fsync_log_period = 1000</strong> means once per second.</p>
<p style="padding-left: 30px;">Since our original InnoDB installation used <strong>innodb_flush_log_at_trx_commit = 2</strong>, our TokuDB setup is:</p>
<blockquote style="padding-left: 30px;">
<pre style="padding-left: 30px;"><strong>tokudb_commit_sync</strong> = 0
<strong>tokudb_fsync_log_period</strong> = 1000</pre>
</blockquote>
<ul>
<li>
<h4>tokudb_load_save_space</h4>
</li>
</ul>
<p style="padding-left: 30px;">Turned on (value <strong>1</strong>) by default as of TokuDB <strong>7.1.0</strong>, this parameter decides whether temporary file created on bulk load operations (e.g. ALTER TABLE) are compressed or uncompressed. Do yourself a big favour (why? <a href="https://shlomi-noach.github.io/blog/mysql/converting-an-olap-database-to-tokudb-part-2-the-process-of-migration">read here</a>) and keep it on. Our setup is:</p>
<blockquote>
<pre><strong>tokudb_load_save_space</strong> = 1</pre>
</blockquote>
<p>TokuDB&#8217;s general recommendation is: don&#8217;t change the variables; the engine should work well right out of the box. I like the approach (by MySQL <strong>5.5</strong> I already lost count of InnoDB variables that can have noticeable impact; with <strong>5.6</strong> I&#8217;m all but lost). The complete list of configuration variables is found in <a href="http://www.tokutek.com/wp-content/uploads/2013/10/mysql-5.5.30-tokudb-7.1.0-users-guide.pdf">TokuDB&#8217;s Users Guide</a>.</p>
]]></content:encoded>
							<wfw:commentRss>https://shlomi-noach.github.io/blog/mysql/tokudb-configuration-variables-of-interest/feed</wfw:commentRss>
		<slash:comments>14</slash:comments>
						<post-id xmlns="com-wordpress:feed-additions:1">6613</post-id>	</item>
		<item>
		<title>Converting an OLAP database to TokuDB, part 3: operational stuff</title>
		<link>https://shlomi-noach.github.io/blog/mysql/converting-an-olap-database-to-tokudb-part-3-operational-stuff</link>
				<comments>https://shlomi-noach.github.io/blog/mysql/converting-an-olap-database-to-tokudb-part-3-operational-stuff#comments</comments>
				<pubDate>Mon, 14 Oct 2013 10:03:43 +0000</pubDate>
		<dc:creator><![CDATA[shlomi]]></dc:creator>
				<category><![CDATA[MySQL]]></category>
		<category><![CDATA[common_schema]]></category>
		<category><![CDATA[InnoDB]]></category>
		<category><![CDATA[TokuDB]]></category>

		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=6603</guid>
				<description><![CDATA[This is the third post in a series of posts describing our experience in migrating a large DWH server to TokuDB (see 1st and 2nd parts). This post discusses operations; namely ALTER TABLE operations in TokuDB. We ran into quite a few use cases by this time that we can shed light on. Quick recap: [&#8230;]]]></description>
								<content:encoded><![CDATA[<p>This is the third post in a series of posts describing our experience in migrating a large DWH server to TokuDB (see <a href="https://shlomi-noach.github.io/blog/mysql/converting-an-olap-database-to-tokudb-part-1">1st</a> and <a href="https://shlomi-noach.github.io/blog/mysql/converting-an-olap-database-to-tokudb-part-2-the-process-of-migration">2nd</a> parts). This post discusses operations; namely ALTER TABLE operations in TokuDB. We ran into quite a few use cases by this time that we can shed light on.</p>
<p>Quick recap: we&#8217;ve altered one of out DWH slaves to TokuDB, with the goal of migrating most of out servers, including the master, to TokuDB.</p>
<h4>Adding an index</h4>
<p>Shortly after migrating our server to TokuDB we noticed an unreasonably disproportionate slave lag on our TokuDB slave (red line in chart below) as compared to other slaves.</p>
<blockquote><p><a href="https://shlomi-noach.github.io/blog/wp-content/uploads/2013/09/tokudb-slave-lag.png"><img alt="tokudb-slave-lag" src="https://shlomi-noach.github.io/blog/wp-content/uploads/2013/09/tokudb-slave-lag.png" width="700" height="329" /></a></p></blockquote>
<p>Quick investigation led to the fact that, coincidentally, a manual heavy-duty operation was just taking place, which updated some year&#8217;s worth of data retroactively. OK, but why so slow on TokuDB? Another quick investigation led to an apples vs. oranges problem: as depicted in <a href="https://shlomi-noach.github.io/blog/mysql/converting-an-olap-database-to-tokudb-part-1">part 1</a>, our original setup included MONTHly partitioning on our larger tables, whereas we could not do the same in TokuDB, where we settled for YEARly partitioning.</p>
<p>The heavy-duty operation included a query that was relying on the MONTHly partitioning to do reasonable pruning: a <strong>WHERE</strong> condition on a date column did the right partition pruning; but where on InnoDB that would filter <strong>1</strong> month&#8217;s worth of data, on TokuDB it would filter <strong>1</strong> <em>year</em>.</p>
<p>Wasn&#8217;t it suggested that TokuDB has online table operations? I decided to give it a shot, and add a proper index on our date column (I actually created a compound index, but irrelevant).</p>
<p>It took <strong>13</strong> minutes to add an index on a <strong>1GB</strong> TokuDB table (approx. <strong>20GB</strong> InnoDB uncompressed equivalent):</p>
<ul>
<li>The <strong>ALTER</strong> was non blocking: table was unlocked at that duration</li>
<li>The client issuing the <strong>ALTER</strong> <em>was</em> blocked (I thought it would happen completely in the background) &#8212; but who cares?</li>
<li>I would say <strong>13</strong> minutes is fast</li>
</ul>
<p>Not surprisingly adding the index eliminated the problem altogether.</p>
<h4>Modifying a PRIMARY KEY</h4>
<p>It was suggested by our DBA that there was a long time standing need to modify our <strong>PRIMARY KEY</strong>. It was impossible to achieve with our InnoDB setup (not enough disk space for the operation, would take weeks to complete if we did have the disk space). Would it be possible to modify our TokuDB tables? On some of our medium-sized tables we issued an <strong>ALTER</strong> of the form:<span id="more-6603"></span></p>
<blockquote>
<pre>ALTER TABLE my_table DROP PRIMARY KEY, ADD PRIMARY KEY (c1, c2, c3, ...);</pre>
</blockquote>
<p>Time-wise the operation completed in good time. We did note, however, that the <a href="http://common-schema.googlecode.com/svn/trunk/common_schema/doc/html/tokudb_file_map.html">disk space consumed by the new table</a> was <em>doubled</em>. Was it due to the fact we added two columns to our PK? Did that account for the bloated space? I did not believe so, and decided to rebuild the table:</p>
<blockquote>
<pre>OPTIMIZE TABLE my_table</pre>
</blockquote>
<p>Nope. Space not reduced. However we were unconvinced and <a href="https://groups.google.com/forum/#!topic/tokudb-user/ow47QY2pcgU">asked</a>. As usual, we got quick response from the Tokutek team; this was a bug: while our original table used the TOKUDB_SMALL row format (high compression), the table rebuild reset it to TOKUDB_FAST (normal compression), which makes for roughly twice the file size. The bug was filed as: <a href="https://github.com/Tokutek/ft-engine/issues/107">alter table operations that rebuild the table lose the original tokudb compression</a>.</p>
<p>Now, we <em>were</em> altering the <strong>PRIMARY KEY</strong>. We were not expecting an online operation anyhow, and didn&#8217;t mind blocking the table; hence the solution was simple: make sure to spceify the row format:</p>
<blockquote>
<pre>ALTER TABLE my_table DROP PRIMARY KEY, ADD PRIMARY KEY (c1, c2, c3, ...) ENGINE=TokuDB ROW_FORMAT=TOKUDB_SMALL;</pre>
</blockquote>
<p>This worked in terms of disk space &#8212; but we only later realized it would still make us trouble.</p>
<h4>Modifying a PRIMARY KEY on our largest table</h4>
<p>We moved on to our largest table: originally <strong>1TB</strong> InnoDB <strong>COMPRESSED</strong>, worth of <strong>2TB</strong> uncompressed. With TokuDB it went down to <strong>100GB</strong>. Converting this table to TokuDB took about <strong>40</strong> hours, which is just fast. We issued an ALTAR TABLE modifying the PRIMARY KEY as above and waited.</p>
<p>The operation did not complete after <strong>40</strong> hours. Nor after <strong>3</strong> days. By day <strong>4</strong> we thought we might look into this. Fortunately, TokuDB is friendly on <strong>SHOW PROCESSLIST</strong> and provides you with useful information, such as &#8220;<strong>Fetched about 1234567890 rows, loading data still remains</strong>&#8220;. Yikes! We extrapolated the values to realize it would take <strong>2</strong> <em>weeks</em> to complete! Weekend went by and we decided to find a better way. Again, posting on the tokudb-user group, we got a definitive answer: a table rebuild does not utilize the <em>bulk loader</em> (you really want to be friends with the bulk loader, it&#8217;s the process that loads your data quickly).</p>
<p>And so we chose to <strong>KILL</strong> the <strong>ALTER</strong> process and go another way; again, <strong>KILL</strong>s are very easy with TokuDB <strong>ALTER</strong> operations: took <strong>3</strong> minutes to abort this week old operation. The alternative operation was:</p>
<blockquote>
<pre>CREATE TABLE my_table_New LIKE my_table;
ALTER TABLE my_table_New DROP PRIMARY KEY, ADD PRIMARY KEY (c1, c2, c3, ...) ENGINE=TokuDB ROW_FORMAT=TOKUDB_SMALL;
INSERT INTO my_table_New SELECT * FROM my_table;
RENAME TABLE my_table TO my_table_Old, my_table_New TO my_table;
DROP TABLE my_table_Old;</pre>
</blockquote>
<p>The <strong>INSERT INTO &#8230; SELECT</strong> operation does use the bulk loader when you do it on an empty table. It completed within merely <strong>30</strong> hours. Hurrah!</p>
<h4>DROPping a TABLE</h4>
<p>It was an immediate operation to drop our &#8220;Old&#8221; table &#8212; subsecond. Nothing like your InnoDB DROP.</p>
]]></content:encoded>
							<wfw:commentRss>https://shlomi-noach.github.io/blog/mysql/converting-an-olap-database-to-tokudb-part-3-operational-stuff/feed</wfw:commentRss>
		<slash:comments>7</slash:comments>
						<post-id xmlns="com-wordpress:feed-additions:1">6603</post-id>	</item>
		<item>
		<title>Converting an OLAP database to TokuDB, part 1</title>
		<link>https://shlomi-noach.github.io/blog/mysql/converting-an-olap-database-to-tokudb-part-1</link>
				<comments>https://shlomi-noach.github.io/blog/mysql/converting-an-olap-database-to-tokudb-part-1#comments</comments>
				<pubDate>Tue, 03 Sep 2013 07:04:12 +0000</pubDate>
		<dc:creator><![CDATA[shlomi]]></dc:creator>
				<category><![CDATA[MySQL]]></category>
		<category><![CDATA[common_schema]]></category>
		<category><![CDATA[compression]]></category>
		<category><![CDATA[InnoDB]]></category>
		<category><![CDATA[Open Source]]></category>
		<category><![CDATA[Performance]]></category>
		<category><![CDATA[QueryScript]]></category>
		<category><![CDATA[TokuDB]]></category>

		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=6473</guid>
				<description><![CDATA[This is the first in a series of posts describing my impressions of converting a large OLAP server to TokuDB. There&#8217;s a lot to tell, and the experiment is not yet complete, so this is an ongoing blogging. In this post I will describe the case at hand and out initial reasons for looking at [&#8230;]]]></description>
								<content:encoded><![CDATA[<p>This is the first in a series of posts describing my impressions of converting a large OLAP server to TokuDB. There&#8217;s a lot to tell, and the experiment is not yet complete, so this is an ongoing blogging. In this post I will describe the case at hand and out initial reasons for looking at TokuDB.</p>
<p>Disclosure: I have no personal interests and no company interests; we did get friendly, useful and free advice from Tokutek engineers. TokuDB is open source and free to use, though commercial license is also available.</p>
<h4>The case at hand</h4>
<p>We have a large and fast growing DWH MySQL setup. This data warehouse is but one component in a larger data setup, which includes Hadoop, Cassandra and more. For online dashboards and most reports, MySQL is our service. We populate this warehouse mainly via Hive/Hadoop. Thus, we have an hourly load of data from Hive, as well as a larger daily load.</p>
<p>There are some updates on the data, but the majority of writes are just <strong>mysqlimport</strong>s of Hive queries.</p>
<p>Usage of this database is OLAP: no concurrency issues here; we have some should-be-fast-running queries issued by our dashboards, as well as ok-to-run-longer queries issued for reports.</p>
<p>Our initial and most burning trouble is with size. Today we use <strong>COMPRESSED</strong> InnoDB tables (<strong>KEY_BLOCK_SIZE</strong> is default, i.e. <strong>8</strong>). Our data volume sums right now at about <strong>2TB</strong>. I happen to know this translates as <strong>4TB</strong> of uncompressed data.</p>
<p>However growth of data is accelerating. A year ago we would capture a dozen GB per month. Today it is a <strong>100GB</strong> per month, and by the end of this year it may climb to <strong>150GB</strong> per month or more.</p>
<p>Our data is not sharded. We have a simple replication topology of some <strong>6</strong> servers. Machines are quite generous as detailed following. And yet, we will be running out of resources shortly: disk space (total <strong>2.7TB</strong>) is now running low and is expected to run out in about six months. One of my first tasks in Outbrain is to find a solution to our DWH growth problem. The solution could be sharding; it could be a commercial DWH product; anything that works.<span id="more-6473"></span></p>
<h4>The approach we experiment with</h4>
<p>It was at my initial interview that I suggested <a href="http://www.tokutek.com/products/tokudb-for-mysql/">TokuDB</a> might be a good solution, with the primary reason of being so good with compression. And we decided to experiment with this simple (setup-wise) solution of compression. If we could compress the data even by <strong>50%</strong>, that would buy us considerable time. And it&#8217;s the simplest approach as we would need to change nothing at the application side, nor add additional frameworks.</p>
<p>Of course, we were already using InnoDB <strong>COMPRESSED</strong> tables. How about just improving the compression? And here I thought to myself: we can try <strong>KEY_BLOCK_SIZE=4</strong>, which I know would generally compress by <strong>50%</strong> as compared to <strong>KEY_BLOCK_SIZE=8</strong> (not always, but in many use cases). We&#8217;re already using InnoDB so this isn&#8217;t a new beast; it will be &#8220;more of the same&#8221;. It would work.</p>
<p>I got myself a dedicated machine: a slave in our production topology I am free to play with. I installed TokuDB <strong>7.0.1</strong>, later upgraded to <strong>7.0.3</strong>, based on MySQL <strong>5.5.30</strong>.</p>
<p>The machine is a Dell Inc. <strong>PowerEdge R510</strong> machine, with <b>16</b> CPUs @ <b>2.1 GHz</b> and <b>126 GiB</b> RAM, <b>16 GiB</b> Swap. OS is CentOS <strong>5.7</strong>,Â  kernel <strong>2.6.18</strong>. We have RAID <strong>10</strong> over local <strong>10k</strong> RPM SAS disks (10x<strong>600GB</strong> disks)</p>
<h4>How to compare InnoDB &amp; TokuDB?</h4>
<p><strong>2TB</strong> of compressed data (for absolute measurement I consider it to be a <strong>4TB</strong> worth of data) is quite a large setup. How do I do the comparison? I don&#8217;t even have too much disk space here&#8230;</p>
<p>We have tables of various size. Our largest is in itself <strong>1TB</strong> (<strong>2TB</strong> uncompressed) &#8211; half of the entire volume. The rest ranging <strong>330GB</strong>, <strong>140GB</strong>, <strong>120GB</strong>, <strong>90GB</strong>, <strong>50GB</strong> and below. We have <strong>MONTH</strong>ly partitioning schemes on most tables and obviously on our larger tables.</p>
<p>For our smaller tables, we could just <strong>CREATE TABLE test_table LIKE small_table</strong>, populating it and comparing compression. However, the really interesting question (and perhaps the only interesting question compression-wise) is how well would our larger (and specifically largest) tables would compress.</p>
<p>Indeed, for our smaller tables we saw between <strong>20%</strong> to <strong>70%</strong> reduction in size when using stronger InnoDB compression: <strong>KEY_BLOCK_SIZE=4/2/1</strong>. How well would that work on our larger tables? How much slower would it be?</p>
<p>We know MySQL partitions are implemented by actual <em>independent</em> tables. Our testing approach was: let&#8217;s build a test_table from a one month worth of data (== one single partition) of our largest table. We tested:</p>
<ul>
<li>The time it takes to load the entire partition (about <strong>120M</strong> rows, <strong>100GB COMPRESSED</strong> data as seen on <strong>.idb</strong> file)</li>
<li>The time it would take to load a single day&#8217;s worth of data from Hive/Hadoop (loading real data, as does our nightly import)</li>
<li>The time it would take for various important <strong>SELECT</strong> query to execute on this data.</li>
</ul>
<h4>InnoDB vs. TokuDB comparison</h4>
<p>In this post I will only describe our impressions of compression size. I have a lot to say about TokuDB vs InnoDB partitioning and queries; this will wait till later post.</p>
<p>So here goes:</p>
<table border="0" cellspacing="0">
<colgroup width="85"></colgroup>
<colgroup width="155"></colgroup>
<colgroup width="152"></colgroup>
<colgroup width="147"></colgroup>
<colgroup width="141"></colgroup>
<tbody>
<tr>
<td align="LEFT" bgcolor="#E6E6E6" height="31"><b>Engine</b></td>
<td align="LEFT" bgcolor="#E6E6E6"><b>Compression</b></td>
<td align="LEFT" bgcolor="#E6E6E6"><b>Time to Insert 1 month</b></td>
<td align="LEFT" bgcolor="#E6E6E6"><b>Table size (optimized)</b></td>
<td align="LEFT" bgcolor="#E6E6E6"><b>Time to import 1 day</b></td>
</tr>
<tr>
<td align="LEFT" bgcolor="#FFFFCC" height="17">InnoDB</td>
<td align="LEFT" bgcolor="#FFFFCC">8k</td>
<td align="LEFT" bgcolor="#FFFFCC"><strong>10.5h</strong></td>
<td align="LEFT" bgcolor="#FFFFCC">58GB</td>
<td align="LEFT" bgcolor="#FFFFCC"><b>32m</b></td>
</tr>
<tr>
<td align="LEFT" bgcolor="#FFFFCC" height="17">InnoDB</td>
<td align="LEFT" bgcolor="#FFFFCC">4k</td>
<td align="LEFT" bgcolor="#FFFFCC">48h</td>
<td align="LEFT" bgcolor="#FFFFCC">33GB</td>
<td align="LEFT" bgcolor="#FFFFCC">unknown (too long)</td>
</tr>
<tr>
<td align="LEFT" bgcolor="#FFFFCC" height="17">TokuDB</td>
<td align="LEFT" bgcolor="#FFFFCC">quicklz</td>
<td align="LEFT" bgcolor="#FFFFCC">14h</td>
<td align="LEFT" bgcolor="#FFFFCC">17GB</td>
<td align="LEFT" bgcolor="#FFFFCC">40m</td>
</tr>
<tr>
<td align="LEFT" bgcolor="#FFFFCC" height="17">TokuDB</td>
<td align="LEFT" bgcolor="#FFFFCC">lzma (small/aggresive)</td>
<td align="LEFT" bgcolor="#FFFFCC">15h</td>
<td align="LEFT" bgcolor="#FFFFCC"><b>7.5GB</b></td>
<td align="LEFT" bgcolor="#FFFFCC">42m</td>
</tr>
</tbody>
</table>
<p>Some comments and insights:</p>
<ul>
<li>Each test was performed 3-4 times. There were no significant differences on the various cycles.</li>
<li>The <strong>1</strong> month insert was done courtesy <a href="http://common-schema.googlecode.com/svn/trunk/common_schema/doc/html/query_script_split.html">QueryScript split</a>,Â  <strong>5,000</strong> rows at a time, no throttling.</li>
<li>The <strong>1</strong> day import via <em>mysqlimport</em>. There were multiple files imported. Each file is sorted by <strong>PRIMARY KEY ASC</strong>.</li>
<li>Isn&#8217;t it nice to know that your <strong>100GB</strong> InnoDB table actually fits within <strong>58GB</strong> when rebuilt?</li>
<li>For InnoDB <strong>flush_logs_at_trx_commit=2</strong>, <strong>flush_method=O_DIRECT</strong>.</li>
<li>I used default configuration to TokuDB &#8212; touched nothing. More on this in later post.</li>
<li>InnoDB <strong>4k</strong> was <em>prohibitively</em> slow to load data. It was so slow so as to be unacceptable. For the 1 day load it took <strong>1</strong> hour for a mere <strong>20%</strong> of data to load. <strong>1</strong> hour was already marginal for our requirements; waiting for <strong>5</strong> hours was out of the question. I tested several times, never got to wait for completion. Did I say it would just be &#8220;more of the same&#8221;? <strong>4k</strong> turned to be &#8220;not an option&#8221;.</li>
<li>I saw almost no difference in load time between the two TokuDB compression formats. Both somewhat (30%) longer than InnoDB to load, but comparable.</li>
<li>TokuDB compression: nothing short of <em>amazing</em>.</li>
</ul>
<p>With InnoDB <strong>4k</strong> being &#8220;not an option&#8221;, and with both TokuDB compressions being similar in load time yet so different in compression size, we are left with the following conclusion: if we want to compress more than our existing 8k (and we have to) &#8211; TokuDB&#8217;s <em>agressive compression</em> (aka small, aka lzma) is our only option.</p>
<h4>Shameless plug</h4>
<p><a href="http://code.google.com/p/common-schema/">common_schema</a> turned to be quite the &#8220;save the day&#8221; tool here. Not only did we use it to extract 100GB of data from a large dataset and load it onto our tables, it also helped out in the ALTER process for TokuDB: at this time (&lt;=<strong> 7.0.4</strong>) TokuDB still has a bug with <strong>KEY_BLOCK_SIZE</strong>: when this option is found in table definition, it impacts TokuDB&#8217;s indexes by bloating them. This is how <a href="http://common-schema.googlecode.com/svn/trunk/common_schema/doc/html/sql_alter_table_tokudb.html">sql_alter_table_tokudb</a> was born. Hopefully it will be redundant shortly.</p>
<h4>More to come</h4>
<p>Was our test fair? Should we have configure TokuDB differently? Is loading via small <strong>5,000</strong> row chunks the right way?</p>
<p>In the next post I will describe the process of migrating our 4TB worth of data to TokuDB, pitfalls, issues, party crushers, sport spoilers, configuration, recovery, cool behaviour and general advice you should probably want to embrace. At later stage I&#8217;ll describe how our DWH looks after migration. Finally I&#8217;ll share some (ongoing) insights on performance.</p>
<p>You&#8217;ll probably want to know &#8220;How much is (non compressed) <strong>4TB</strong> of data worth in TokuDB?&#8221; Let&#8217;s keep the suspense <img src="https://s.w.org/images/core/emoji/12.0.0-1/72x72/1f642.png" alt="ðŸ™‚" class="wp-smiley" style="height: 1em; max-height: 1em;" /></p>
]]></content:encoded>
							<wfw:commentRss>https://shlomi-noach.github.io/blog/mysql/converting-an-olap-database-to-tokudb-part-1/feed</wfw:commentRss>
		<slash:comments>8</slash:comments>
						<post-id xmlns="com-wordpress:feed-additions:1">6473</post-id>	</item>
		<item>
		<title>Easy SELECT COUNT(*) with split()</title>
		<link>https://shlomi-noach.github.io/blog/mysql/easy-select-count-with-split</link>
				<comments>https://shlomi-noach.github.io/blog/mysql/easy-select-count-with-split#comments</comments>
				<pubDate>Sat, 08 Jun 2013 04:41:13 +0000</pubDate>
		<dc:creator><![CDATA[shlomi]]></dc:creator>
				<category><![CDATA[MySQL]]></category>
		<category><![CDATA[common_schema]]></category>
		<category><![CDATA[InnoDB]]></category>

		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=6379</guid>
				<description><![CDATA[The two conservative ways of getting the number of rows in an InnoDB table are: SELECT COUNT(*) FROM my_table: provides with an accurate number, but makes for a long running transaction which take ages on large tables. Long transactions make for locks SELECT TABLE_ROWS FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA=&#8217;my_schema&#8217; AND TABLE_NAME=&#8217;my_table&#8217;, or get same info via [&#8230;]]]></description>
								<content:encoded><![CDATA[<p>The two conservative ways of getting the number of rows in an InnoDB table are:</p>
<ul>
<li><strong>SELECT COUNT(*) FROM my_table</strong>:<br />
provides with an accurate number, but makes for a long running transaction which take ages on large tables. Long transactions make for locks</li>
<li><strong>SELECT TABLE_ROWS FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA=&#8217;my_schema&#8217; AND TABLE_NAME=&#8217;my_table&#8217;</strong>, or get same info via <strong>SHOW TABLE STATUS</strong>.<br />
Gives immediate response, but the value can be <em>way off</em>; it can be two times as large as real value, or half the value. For query execution plans this may be a &#8220;good enough&#8221; estimation, but typically you just can&#8217;t trust it for your own purposes.</li>
</ul>
<h4>Get a good estimate using chunks</h4>
<p>You can get a good estimate by calculating the total number of rows in steps. Walk the table 1,000 rows at a time, and keep a counter. Each chunk is its own transaction, so, if the table is modified while counting, the final value does not make for an accurate account at any point in time. Typically this should be a far better estimate than <strong>TABLE_ROWS</strong>.</p>
<p><a href="http://common-schema.googlecode.com/svn/trunk/common_schema/doc/html/query_script_split.html">QueryScript&#8217;s split()</a> construct provides you with the means to work this out. Consider this script:<span id="more-6379"></span></p>
<blockquote>
<pre>set @total := 0;

split(SELECT COUNT(*) FROM world.City INTO @chunk) {
Â  set @total = @total + @chunk;
}

select @total;</pre>
</blockquote>
<p><strong>split()</strong> breaks the above <strong>SELECT COUNT(*)</strong> into distinct chunks, like:</p>
<blockquote>
<pre>SELECT COUNT(*) FROM world.City WHERE ((((`City`.`ID` &gt; '3000'))) AND (((`City`.`ID` &lt; '4000')) OR ((`City`.`ID` = '4000')))) INTO @chunk</pre>
</blockquote>
<p>You can make this a one liner like this:</p>
<blockquote><p>call common_schema.run(&#8220;set @total := 0;split(SELECT COUNT(*) FROM world.City INTO @chunk) set @total = @total + @chunk; select @total;&#8221;);</p></blockquote>
<p>If you like to watch the progress, add some verbose:</p>
<blockquote>
<pre>call common_schema.run("set @total := 0;split(SELECT COUNT(*) FROM world.City INTO @chunk) {set @total = @total + @chunk; select $split_step, @total} select @total;");</pre>
</blockquote>
<p><em>QueryScript</em> is available via <a href="https://code.google.com/p/common-schema/">common_schema</a>.</p>
]]></content:encoded>
							<wfw:commentRss>https://shlomi-noach.github.io/blog/mysql/easy-select-count-with-split/feed</wfw:commentRss>
		<slash:comments>4</slash:comments>
						<post-id xmlns="com-wordpress:feed-additions:1">6379</post-id>	</item>
		<item>
		<title>Converting compressed InnoDB tables to TokuDB 7.0.1</title>
		<link>https://shlomi-noach.github.io/blog/mysql/converting-compressed-innodb-tables-to-tokudb-7-0-1</link>
				<comments>https://shlomi-noach.github.io/blog/mysql/converting-compressed-innodb-tables-to-tokudb-7-0-1#comments</comments>
				<pubDate>Wed, 05 Jun 2013 07:10:12 +0000</pubDate>
		<dc:creator><![CDATA[shlomi]]></dc:creator>
				<category><![CDATA[MySQL]]></category>
		<category><![CDATA[InnoDB]]></category>
		<category><![CDATA[TokuDB]]></category>

		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=6368</guid>
				<description><![CDATA[Or: how to make it work in TokuDB version 7.0.1. This is a follow up on a discussion on the tokudb-user group. Background I wanted to test TokuDB&#8217;s compression. I took a staging machine of mine, with production data, and migrated it from Percona Server 5.5 To MariaDB 5.5+TokuDB 7.0.1. Migration went well, no problems. [&#8230;]]]></description>
								<content:encoded><![CDATA[<p>Or: how to make it work in TokuDB version <strong>7.0.1</strong>. This is a follow up on a <a href="https://groups.google.com/forum/?fromgroups=#!topic/tokudb-user/hLlHwlp2AL0">discussion on the tokudb-user group</a>.</p>
<h4>Background</h4>
<p>I wanted to test TokuDB&#8217;s compression. I took a staging machine of mine, with production data, and migrated it from <strong>Percona Server 5.5</strong> To <strong>MariaDB 5.5+TokuDB 7.0.1</strong>. Migration went well, no problems.</p>
<p>To my surprise, when I converted tables from InnoDB to TokuDB, I saw an <em>increase</em> in table file size on disk. As explained by Tim Callaghan, this was due to TokuDB interpreting my compressed table&#8217;s <strong>&#8220;KEY_BLOCK_SIZE=4&#8221;</strong> as an instruction for TokuDB&#8217;s page size. TokuDB should be using <strong>4MB</strong> block size, but thinks it&#8217;s being instructed to use <strong>4KB</strong>. Problem is, you <a href="http://bugs.mysql.com/bug.php?id=67727">can&#8217;t get rid of table options</a>. When one converts a table to InnoDB in <strong>ROW_FORMAT=COMPACT</strong>, or even to MyISAM, the <strong>KEY_BLOCK_SIZE</strong> option keeps lurking in the dark.</p>
<p>So until this is hopefully resolved in TokuDB&#8217;s next version, here&#8217;s a way to go around the problem.<span id="more-6368"></span></p>
<h4>The case at hand</h4>
<p>Consider the following table:</p>
<blockquote>
<pre> CREATE TABLE `t` (
  `id` int(10) unsigned NOT NULL AUTO_INCREMENT,
  `c1` int(10) unsigned NOT NULL DEFAULT '0',
  `c2` int(10) unsigned NOT NULL DEFAULT '0',
  `c3` int(10) unsigned NOT NULL DEFAULT '0',
  `c4` timestamp NOT NULL DEFAULT '0000-00-00 00:00:00',
  `c5` timestamp NOT NULL DEFAULT '0000-00-00 00:00:00',
  `c6` smallint(10) unsigned NOT NULL DEFAULT '0',
  `c7` smallint(10) unsigned NOT NULL DEFAULT '0',
  `c8` smallint(10) unsigned NOT NULL DEFAULT '0',
  `c9` smallint(10) unsigned NOT NULL DEFAULT '0',
  `c10` smallint(10) unsigned NOT NULL DEFAULT '0',
  `c11` smallint(10) NOT NULL DEFAULT '0',
  `c12` smallint(10) NOT NULL DEFAULT '0',
  `c13` smallint(10) NOT NULL DEFAULT '0',
  `c14` smallint(10) NOT NULL DEFAULT '0',
  `ct` text NOT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `c1c4` (`c1`,`c4`),
  KEY `c4` (`c4`)
) ENGINE=InnoDB AUTO_INCREMENT=4688271 DEFAULT CHARSET=utf8 ROW_FORMAT=COMPRESSED KEY_BLOCK_SIZE=4</pre>
</blockquote>
<p>Note that it is in <strong>COMPRESSED</strong> format, with <strong>KEY_BLOCK_SIZE=4</strong>. It mostly has <strong>INT</strong> columns, so I don&#8217;t expect it to compress by much.</p>
<p>On disk, the <strong>.ibd</strong> file amounts to <strong>160MB</strong>. Table has<strong> </strong><strong>3,587,488</strong> rows. Same table in InnoDB COMPACT row format amounts to <strong>412MB</strong> on disk.</p>
<p>Converting the table to TokuDB with aggressive compression resulted with:</p>
<blockquote>
<pre>mysql&gt; alter table t engine=tokudb row_format=tokudb_lzma;
Query OK, 3587488 rows affected (29 min 48.79 sec)
Records: 3587488Â  Duplicates: 0Â  Warnings:</pre>
</blockquote>
<p>And over <strong>873MB</strong> of combined files on disk! Also note it took nearly <strong>30</strong> minutes to <strong>ALTER</strong>. Clearly this is not the expected outcome.</p>
<h4>Attempt to make it work</h4>
<p>I tried both the following approaches:</p>
<ul>
<li><strong>alter table t engine=tokudb row_format=tokudb_lzma key_block_size=4096</strong>: thought it would fool TokuDB to think it should create a 4M key block size.</li>
<li><strong>alter table t engine=tokudb row_format=tokudb_lzma key_block_size=0</strong>: try and reset the key block size.</li>
</ul>
<p>Both the above attempts resulted with same bloat in resulting table.</p>
<p>The reason? When ALTERing a table with a nother <strong>KEY_BLOCK_SIZE</strong>, the keys on the table remain with their old <strong>KEY_BLOCK_SIZE</strong>. They are unaffected by the <strong>ALTER</strong>. As suggested by <em>Nail Kashapov</em>, indexes must be rebuilt as well.</p>
<h4>Making it work</h4>
<p>The next <strong>ALTER</strong> modifies the <strong>KEY_BLOCK_SIZE</strong> <em>and</em> rebuilds all the indexes on the table:</p>
<blockquote>
<pre>mysql&gt; alter table t drop primary key, add primary key(id), drop key c1c4, add unique key `c1c4` (c1, c4), drop key c4, add key `c4` (c4), engine=tokudb row_format=tokudb_lzma key_block_size=0;
Query OK, 3587488 rows affected (2 min 7.97 sec)
Records: 3587488Â  Duplicates: 0Â  Warnings: 0</pre>
</blockquote>
<p>Yep! Runtime seems much more agreeable. Total size on disk? Little over <strong>26M</strong>. Did I say I wasn&#8217;t expecting good reduction in terms of compression?</p>
<p>Have done the same for multiple tables; compression is consistently strong (e.g. <strong>16MB</strong> InnoDB compressed -&gt; <strong>3.5MB</strong> TokuDB aggressive, <strong>548MB</strong> InnoDB non-compressed -&gt; <strong>36MB</strong> TokuDB aggressive), on varying table schemata. Very impressive reduction in disk space!</p>
<h4>Conclusion</h4>
<p>Next version of TokuDB is expected to ignore the <strong>KEY_BLOCK_SIZE</strong> table option; until then converting compressed tables to TokuDB is a pain in terms of the syntax &#8212; but worthwhile in terms of disk space.</p>
<p>&nbsp;</p>
]]></content:encoded>
							<wfw:commentRss>https://shlomi-noach.github.io/blog/mysql/converting-compressed-innodb-tables-to-tokudb-7-0-1/feed</wfw:commentRss>
		<slash:comments>4</slash:comments>
						<post-id xmlns="com-wordpress:feed-additions:1">6368</post-id>	</item>
		<item>
		<title>State of InnDB Online DDL in MySQL 5.6.9-RC (good news included)</title>
		<link>https://shlomi-noach.github.io/blog/mysql/state-of-inndb-online-ddl-in-mysql-5-6-9-rc-good-news-included</link>
				<comments>https://shlomi-noach.github.io/blog/mysql/state-of-inndb-online-ddl-in-mysql-5-6-9-rc-good-news-included#comments</comments>
				<pubDate>Tue, 18 Dec 2012 11:21:12 +0000</pubDate>
		<dc:creator><![CDATA[shlomi]]></dc:creator>
				<category><![CDATA[MySQL]]></category>
		<category><![CDATA[InnoDB]]></category>
		<category><![CDATA[New Features]]></category>
		<category><![CDATA[Review]]></category>

		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=5882</guid>
				<description><![CDATA[5.6.9-RC is out, and I was curious to see how the online DDL has improved since my 5.6.8 review. I also owe James Day this review, since he came up with results inconsistent with my own. We both agreed the dataset I was using was too small, but I got similar results even on larger [&#8230;]]]></description>
								<content:encoded><![CDATA[<p><strong>5.6.9-RC</strong> is <a href="https://blogs.oracle.com/MySQL/entry/mysql_5_6_9_release">out</a>, and I was curious to see how the <a href="http://dev.mysql.com/doc/refman/5.6/en/innodb-online-ddl.html">online DDL</a> has improved since <a href="https://shlomi-noach.github.io/blog/mysql/state-of-inndb-online-ddl-in-mysql-5-6-8-rc">my 5.6.8 review</a>. I also owe James Day this review, since he came up with results inconsistent with my own.</p>
<p>We both agreed the dataset I was using was too small, but I got similar results even on larger scale. Then some time passed, and <strong>5.6.9</strong> was announced.</p>
<p>So for the <strong>5.6.9</strong> test I took one of my real tables on production. It is not extremely large: it&#8217;s a ~ <strong>300MB</strong> <strong>.ibd</strong> file, in the following format:</p>
<blockquote>
<pre>mysql&gt; show create table tbl \G

CREATE TABLE `tbl` (
Â  `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT,
Â  `a` varchar(255) CHARACTER SET utf8 NOT NULL DEFAULT '',
Â  `w` smallint(11) NOT NULL DEFAULT '0',
Â  `d` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
Â  `icount` smallint(5) unsigned NOT NULL DEFAULT '0',
Â  PRIMARY KEY (`id`) KEY_BLOCK_SIZE=8,
Â  UNIQUE KEY `u_idx` (`a`,`w`,`d`) KEY_BLOCK_SIZE=8,
Â  KEY `d` (`d`) KEY_BLOCK_SIZE=8
) ENGINE=InnoDB AUTO_INCREMENT=16960441 DEFAULT CHARSET=latin1 ROW_FORMAT=COMPRESSED KEY_BLOCK_SIZE=16</pre>
</blockquote>
<p>Got some <strong>2.5M</strong> rows in the table; desktop machine, <strong>64</strong> bit Linux, mysqlsandbox.</p>
<p>I have crossed several DDL statements with several DML statements. The DDL statements in this test are (<strong>ALTER TABLE&#8230;</strong>):<span id="more-5882"></span></p>
<ul>
<li><strong>ROW_FORMAT=COMPACT</strong></li>
<li><strong>AUTO_INCREMENT=16960441</strong></li>
<li><strong>ADD INDEX (w)</strong></li>
<li><strong>DROP INDEX w</strong></li>
<li><strong>ADD COLUMN c CHAR(1) NOT NULL</strong></li>
<li><strong>DROP COLUMN c</strong></li>
</ul>
<p>The DML statements are:</p>
<ol>
<li><strong>select max(id) from test.tbl;</strong> &#8212; this queries the AUTO_INCREMENT value, which is of course a PRIMARY KEY</li>
<li><strong>select min(d) from test.tbl;</strong> &#8212; there is an index on d, and normal execution plan is to optimize table away and just use the index</li>
<li><strong>select min(icount) from test.tbl;</strong> &#8212; there is no index on icount, and full table scan is required</li>
<li><strong>update test.tbl set d = d + interval 1 second where id = 8057370;</strong> &#8212; the UPDATE uses the PRIMARY KEY</li>
<li><strong>update test.tbl set d = d + interval 1 second where icount = 200;</strong> &#8212; will affect <strong>4</strong> rows, but requires full scan.</li>
</ol>
<p>The results?</p>
<table border="0" cellspacing="0">
<colgroup width="243"></colgroup>
<colgroup width="92"></colgroup>
<colgroup width="131"></colgroup>
<colgroup span="5" width="85"></colgroup>
<tbody>
<tr>
<td align="LEFT" bgcolor="#E6E6FF" height="47"><strong>ALTER TABLE&#8230;</strong></td>
<td align="LEFT" bgcolor="#E6E6FF"><strong>Time (sec)</strong></td>
<td align="LEFT" bgcolor="#E6E6FF"><strong>General comments</strong></td>
<td align="LEFT" bgcolor="#E6E6FF"><strong>select max PK</strong></td>
<td align="LEFT" bgcolor="#E6E6FF"><strong>select min by index</strong></td>
<td align="LEFT" bgcolor="#E6E6FF"><strong>select min by full scan</strong></td>
<td align="LEFT" bgcolor="#E6E6FF"><strong>update by PK</strong></td>
<td align="LEFT" bgcolor="#E6E6FF"><strong>update by full scan</strong></td>
</tr>
<tr>
<td align="LEFT" bgcolor="#FFFFCC" height="17">ROW_FORMAT=COMPACT</td>
<td align="RIGHT" bgcolor="#FFFFCC">183</td>
<td align="LEFT" bgcolor="#FFFFCC"></td>
<td align="LEFT" bgcolor="#FFFFCC"><span style="color: #00ae00;">online</span></td>
<td align="LEFT" bgcolor="#FFFFCC"><span style="color: #00ae00;">online</span></td>
<td align="LEFT" bgcolor="#FFFFCC"><span style="color: #00ae00;">online</span></td>
<td align="LEFT" bgcolor="#FFFFCC"><span style="color: #00ae00;">online</span></td>
<td align="LEFT" bgcolor="#FFFFCC"><span style="color: #00ae00;">online</span></td>
</tr>
<tr>
<td align="LEFT" bgcolor="#FFFFCC" height="16">AUTO_INCREMENT=16960441</td>
<td align="RIGHT" bgcolor="#FFFFCC">0.24</td>
<td align="LEFT" bgcolor="#FFFFCC">[Instant operation]</td>
<td align="LEFT" bgcolor="#FFFFCC">n/a</td>
<td align="LEFT" bgcolor="#FFFFCC">n/a</td>
<td align="LEFT" bgcolor="#FFFFCC">n/a</td>
<td align="LEFT" bgcolor="#FFFFCC">n/a</td>
<td align="LEFT" bgcolor="#FFFFCC">n/a</td>
</tr>
<tr>
<td align="LEFT" bgcolor="#FFFFCC" height="17">ADD INDEX (w)</td>
<td align="RIGHT" bgcolor="#FFFFCC">21</td>
<td align="LEFT" bgcolor="#FFFFCC"></td>
<td align="LEFT" bgcolor="#FFFFCC"><span style="color: #00ae00;">online</span></td>
<td align="LEFT" bgcolor="#FFFFCC"><span style="color: #00ae00;">online</span></td>
<td align="LEFT" bgcolor="#FFFFCC"><span style="color: #00ae00;">online</span></td>
<td align="LEFT" bgcolor="#FFFFCC"><span style="color: #00ae00;">online</span></td>
<td align="LEFT" bgcolor="#FFFFCC"><span style="color: #00ae00;">online</span></td>
</tr>
<tr>
<td align="LEFT" bgcolor="#FFFFCC" height="16">DROP INDEX w</td>
<td align="RIGHT" bgcolor="#FFFFCC">0.1</td>
<td align="LEFT" bgcolor="#FFFFCC">[Instant operation]</td>
<td align="LEFT" bgcolor="#FFFFCC">n/a</td>
<td align="LEFT" bgcolor="#FFFFCC">n/a</td>
<td align="LEFT" bgcolor="#FFFFCC">n/a</td>
<td align="LEFT" bgcolor="#FFFFCC">n/a</td>
<td align="LEFT" bgcolor="#FFFFCC">n/a</td>
</tr>
<tr>
<td align="LEFT" bgcolor="#FFFFCC" height="17">ADD COLUMN c CHAR(1) NOT NULL</td>
<td align="RIGHT" bgcolor="#FFFFCC">103</td>
<td align="LEFT" bgcolor="#FFFFCC"></td>
<td align="LEFT" bgcolor="#FFFFCC"><span style="color: #00ae00;">online</span></td>
<td align="LEFT" bgcolor="#FFFFCC"><span style="color: #00ae00;">online</span></td>
<td align="LEFT" bgcolor="#FFFFCC"><span style="color: #00ae00;">online</span></td>
<td align="LEFT" bgcolor="#FFFFCC"><span style="color: #00ae00;">online</span></td>
<td align="LEFT" bgcolor="#FFFFCC"><span style="color: #00ae00;">online</span></td>
</tr>
<tr>
<td align="LEFT" bgcolor="#FFFFCC" height="17">DROP COLUMN c</td>
<td align="RIGHT" bgcolor="#FFFFCC">110</td>
<td align="LEFT" bgcolor="#FFFFCC"></td>
<td align="LEFT" bgcolor="#FFFFCC"><span style="color: #00ae00;">online</span></td>
<td align="LEFT" bgcolor="#FFFFCC"><span style="color: #00ae00;">online</span></td>
<td align="LEFT" bgcolor="#FFFFCC"><span style="color: #00ae00;">online</span></td>
<td align="LEFT" bgcolor="#FFFFCC"><span style="color: #00ae00;">online</span></td>
<td align="LEFT" bgcolor="#FFFFCC"><span style="color: #00ae00;">online</span></td>
</tr>
</tbody>
</table>
<h4>Notes</h4>
<ul>
<li>All operations were online: operations did not wait for <strong>ALTER</strong> to complete.</li>
<li>I executed all operations multiple times during each <strong>ALTER</strong>.</li>
<li>In addition, I executed operations from another client.</li>
<li>Some operations were fast, others sometimes took as long as <strong>7.34</strong> seconds to complete. This is no small matter: the time it took for each DML was indeterministic, and longer than what it would usually take it. That&#8217;s perfectly understandable. Just note that some operations took exceedingly long time to complete. My understanding is that the <strong>ALTER</strong> operations happens in chunks. DML statements are allowed in between these chunks. This is the reason why on smaller tables there didn&#8217;t seem to be any &#8220;online&#8221; statement: the chunks were just too large in relation to table size. And so, and this is still my own understanding, your query may get lucky or unlucky depending on the exact moment it has been issued.</li>
<li>I did not try it with <strong>FOREIGN KEY</strong>s. I previously concluded that foreign keys were a no-go for online DDL. I&#8217;m not sure if this is still the case. Another time for this test &#8211; but it must take place.</li>
</ul>
<h4>Conclusions</h4>
<p>Still RC &#8211; but for the first time the online DDL seem to deliver what&#8217;s promised. I&#8217;m very happy to see this.</p>
<p>I am yet to understand how the <strong>ALTER</strong> works via replication. With single threaded replication I would assume it&#8217;s back to &#8220;wait till I&#8217;m done&#8221; on the slave, in which case the <em>&#8220;online&#8221;</em> term is not there yet. Even on multi-threaded replication DML on same schema would hang. I&#8217;m happy to be corrected on this by an authority.</p>
<p>My predicament is that <a href="http://openarkkit.googlecode.com/svn/trunk/openarkkit/doc/html/oak-online-alter-table.html">oak-online-alter-table</a> or <a href="http://www.percona.com/doc/percona-toolkit/2.1/pt-online-schema-change.html">pt-online-schema-change</a> are here to stay for the next couple of years at least. Some operations, like partitioning, are not supported by current online InnoDB DDL. Also, these scripts allow you some control over the speed at which the <strong>ALTER</strong> process works, allowing for pre-defined sleep time in between chunks, so as to let the server &#8211; and its slaves &#8211; recover their breath.</p>
<p>Nonetheless, big kudos for the InnoDB team at Oracle for pulling this one out!</p>
]]></content:encoded>
							<wfw:commentRss>https://shlomi-noach.github.io/blog/mysql/state-of-inndb-online-ddl-in-mysql-5-6-9-rc-good-news-included/feed</wfw:commentRss>
		<slash:comments>4</slash:comments>
						<post-id xmlns="com-wordpress:feed-additions:1">5882</post-id>	</item>
		<item>
		<title>Killing InnoDB idle transactions</title>
		<link>https://shlomi-noach.github.io/blog/mysql/killing-innodb-idle-transactions</link>
				<comments>https://shlomi-noach.github.io/blog/mysql/killing-innodb-idle-transactions#comments</comments>
				<pubDate>Tue, 04 Dec 2012 12:23:12 +0000</pubDate>
		<dc:creator><![CDATA[shlomi]]></dc:creator>
				<category><![CDATA[MySQL]]></category>
		<category><![CDATA[common_schema]]></category>
		<category><![CDATA[INFORMATION_SCHEMA]]></category>
		<category><![CDATA[InnoDB]]></category>
		<category><![CDATA[scripts]]></category>

		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=5422</guid>
				<description><![CDATA[The issue of terminating long-time idle open InnoDB transaction has been discussed recently by many. I wish to add my share, by proposing a quick and clean solution via common_schema. common_schema 1.2 provides with the innodb_transactions view, which relies on INNODB_TRX &#8211; one of the InnoDB Plugin views in INFORMATION_SCHEMA &#8211; as well as on [&#8230;]]]></description>
								<content:encoded><![CDATA[<p>The issue of terminating long-time idle open InnoDB transaction has been discussed recently by many. I wish to add my share, by proposing a quick and clean solution via <a href="http://code.google.com/p/common-schema/">common_schema</a>.</p>
<p><em>common_schema <strong>1.2</strong></em> provides with the <a href="http://common-schema.googlecode.com/svn/trunk/common_schema/doc/html/innodb_transactions.html"><strong>innodb_transactions</strong></a> view, which relies on <strong>INNODB_TRX</strong> &#8211; one of the InnoDB Plugin views in <strong>INFORMATION_SCHEMA</strong> &#8211; as well as on <strong>PROCESSLIST</strong>, and so is able to determine with certainty that a transaction has been idle for a long time.</p>
<p><strong>innodb_transactions</strong> offers us with a <strong>sql_kill_query</strong> column, which produces a <strong>&#8216;KILL QUERY 12345&#8217;</strong> type of value. So we can:</p>
<blockquote>
<pre>SELECT <strong>sql_kill_query</strong> FROM <strong>innodb_transactions</strong> WHERE <strong>trx_idle_seconds &gt;= 10; 
</strong>+-------------------+
| sql_kill_queryÂ Â Â  |
+-------------------+
| KILL QUERY 292509 |
| KILL QUERY 292475 |
+-------------------+<strong> </strong></pre>
</blockquote>
<p><em>common_schema</em>&#8216;s useful <a href="http://common-schema.googlecode.com/svn/trunk/common_schema/doc/html/eval.html"><strong>eval()</strong></a> routine allows us to actually invoke those <strong>KILL</strong> statements, all in a one-liner:</p>
<blockquote>
<pre>call <strong>eval</strong>(<span style="color: #003366;">'SELECT <strong>sql_kill_query</strong> FROM innodb_transactions WHERE <strong>trx_idle_seconds &gt;= 10</strong>'</span>);</pre>
</blockquote>
<h4>Technical details<span id="more-5422"></span></h4>
<ul>
<li><strong>trx_idle_seconds</strong> notes the time, in seconds, the transaction has been idle, or 0 if the transaction is not idle at all.</li>
<li><strong>sql_kill_query</strong> is a self-generated SQL query which kills the running query, e.g. <strong>&#8216;KILL QUERY 12345&#8217;</strong>.</li>
<li><strong>eval()</strong> takes a query as text, retrieves the SQL resulting column, and executes it live.</li>
</ul>
<h4>Background details</h4>
<p>The connection between <strong>INNODB_TRX</strong> and <strong>PROCESSLIST</strong> is not synchronous. It is possible that by the time one is querying <strong>INNODB_TRX</strong>, <strong>PROCESSLIST</strong> data may change (e.g. next query is already replacing the one you were considering in <strong>INNODB_TRX</strong>). But in our case it is of little consequence: we are interested in transactions that have been idle for quite some time. Say, <strong>10</strong> seconds. So we are not troubled by having <strong>200</strong> queries per second changing under our hands.</p>
<p>If the transaction has been asleep for <strong>10</strong> seconds, and we decide to kill it, well, it is possible that just as we kill it it will turn active again. It&#8217;s a risk we take no matter what kind of solution we apply, since there&#8217;s no atomic &#8220;get-status-and-kill&#8221; operation on InnoDB transactions.</p>
<p>The above solution is manual: one must invoke the query which kills the idle transactions. This is as opposed to a built-in server feature which does the same. Events can used to semi-automate this: one can call upon this query once every <strong>10</strong> seconds, for example.</p>
<p>See the many related and inspiring solutions below:</p>
<ul>
<li><a href="http://mysqlblog.fivefarmers.com/2012/08/28/identifying-and-killing-blocking-transactions-in-innodb/">Identifying and killing blocking transactions in InnoDB</a></li>
<li><a href="http://www.markleith.co.uk/2011/05/31/finding-and-killing-long-running-innodb-transactions-with-events/">Finding and killing long running InnoDB transactions with Events</a></li>
<li><a href="http://datacharmer.blogspot.co.il/2008/10/using-event-scheduler-to-purge-process.html">Using the event scheduler to purge the process list</a></li>
<li><a href="http://www.mysqlperformanceblog.com/2011/03/08/how-to-debug-long-running-transactions-in-mysql/">How to debug long-running transactions in MySQL</a></li>
<li><a href="http://yoshinorimatsunobu.blogspot.co.il/2011/04/tracking-long-running-transactions-in.html">Tracking long running transactions in MySQL</a></li>
</ul>
]]></content:encoded>
							<wfw:commentRss>https://shlomi-noach.github.io/blog/mysql/killing-innodb-idle-transactions/feed</wfw:commentRss>
		<slash:comments>4</slash:comments>
						<post-id xmlns="com-wordpress:feed-additions:1">5422</post-id>	</item>
		<item>
		<title>State of InnDB Online DDL in MySQL 5.6.8-RC</title>
		<link>https://shlomi-noach.github.io/blog/mysql/state-of-inndb-online-ddl-in-mysql-5-6-8-rc</link>
				<comments>https://shlomi-noach.github.io/blog/mysql/state-of-inndb-online-ddl-in-mysql-5-6-8-rc#comments</comments>
				<pubDate>Tue, 20 Nov 2012 09:49:14 +0000</pubDate>
		<dc:creator><![CDATA[shlomi]]></dc:creator>
				<category><![CDATA[MySQL]]></category>
		<category><![CDATA[InnoDB]]></category>
		<category><![CDATA[New Features]]></category>
		<category><![CDATA[openark kit]]></category>
		<category><![CDATA[Review]]></category>

		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=5823</guid>
				<description><![CDATA[5.6.8-rc is out, and so I&#8217;m following up on InnoDB&#8217;s online DDL new feature: the ability to SELECT, INSERT, DELETE, UPDATE a table even while an ALTER TABLE is executing on same table. The brief summary Not as advertised; many things can&#8217;t be done. The longer review I&#8217;m using 5.6.8-rc 64bit binary distribution for Linux, [&#8230;]]]></description>
								<content:encoded><![CDATA[<p><strong>5.6.8-rc</strong> is out, and so I&#8217;m following up on InnoDB&#8217;s online DDL new feature: the ability to SELECT, INSERT, DELETE, UPDATE a table even while an ALTER TABLE is executing on same table.</p>
<h4>The brief summary</h4>
<p>Not as advertised; many things can&#8217;t be done.</p>
<h4>The longer review</h4>
<p>I&#8217;m using <strong>5.6.8-rc 64bit</strong> binary distribution for Linux, installed via <a href="http://mysqlsandbox.net/">mysqlsandbox</a>. My hardware is irrelevant, but the fact I&#8217;m testing on my laptop assists me in that <strong>ALTER TABLE</strong> operations take a while, so that I&#8217;m able to easily type commands in two terminals and have the time to watch them being executed. Query cache is disabled.<span id="more-5823"></span></p>
<p>I&#8217;m using the sakila sample database, and in particular I&#8217;m working with the rental table. Here&#8217;s the table definition:</p>
<blockquote>
<pre>CREATE TABLE `rental` (
Â  `rental_id` int(11) NOT NULL AUTO_INCREMENT,
Â  `rental_date` datetime NOT NULL,
Â  `inventory_id` mediumint(8) unsigned NOT NULL,
Â  `customer_id` smallint(5) unsigned NOT NULL,
Â  `return_date` datetime DEFAULT NULL,
Â  `staff_id` tinyint(3) unsigned NOT NULL,
Â  `last_update` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
Â  PRIMARY KEY (`rental_id`),
Â  UNIQUE KEY `rental_date` (`rental_date`,`inventory_id`,`customer_id`),
Â  KEY `idx_fk_inventory_id` (`inventory_id`),
Â  KEY `idx_fk_customer_id` (`customer_id`),
Â  KEY `idx_fk_staff_id` (`staff_id`),
Â  CONSTRAINT `fk_rental_staff` FOREIGN KEY (`staff_id`) REFERENCES `staff` (`staff_id`) ON UPDATE CASCADE,
Â  CONSTRAINT `fk_rental_inventory` FOREIGN KEY (`inventory_id`) REFERENCES `inventory` (`inventory_id`) ON UPDATE CASCADE,
Â  CONSTRAINT `fk_rental_customer` FOREIGN KEY (`customer_id`) REFERENCES `customer` (`customer_id`) ON UPDATE CASCADE
) ENGINE=InnoDB AUTO_INCREMENT=16050 DEFAULT CHARSET=utf8</pre>
</blockquote>
<p>Highlights for the table: <strong>AUTO_INCREMENT PRIMARY KEY</strong>, some columns indexed, some not, and Foreign Keys in place. Pretty much a standard table. It contains <strong>16,044</strong> rows. Row format is <strong>COMPACT</strong>.</p>
<p>What I want to know is: which DDL commands allow for which online DML commands?</p>
<p>So, on terminal #1 I will issue queries like:</p>
<blockquote>
<pre>node1 5.6.8-rc-log sakila&gt; alter table <strong>sakila.rental</strong> ROW_FORMAT=COMPACT <strong>/* or whatever */</strong>;
Query OK, 0 rows affected (10.57 sec)
Records: 0Â  Duplicates: 0Â  Warnings: 0</pre>
</blockquote>
<p>And during the above operation, I will execute the following on terminal #2:</p>
<ol>
<li><strong>select max(rental_id) from sakila.rental;</strong> this queries the AUTO_INCREMENT value, which is of course a PRIMARY KEY</li>
<li><strong>select min(rental_date) from sakila.rental</strong>; there is an index on rental_date, and normal execution plan is to optimize table away and just use the index</li>
<li><strong>select min(return_date) from sakila.rental</strong>; there is no index on return_date, and full table scan is required</li>
<li><strong>update rental set return_date = return_date + interval 1 second where rental_id=3</strong>; the UPDATE uses the PRIMARY KEY</li>
<li><strong>update rental set return_date = return_date + interval 1 second where return_date = NOW()</strong>; won&#8217;t actually affect anything, but requires full scan.</li>
</ol>
<p>So here are the results:</p>
<blockquote>
<pre>+-------------------------------------------------------------+-------+---------------------------+---------------+---------------------+-------------------------+--------------+---------------------+
| ALTER statementÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  | TimeÂ  | General commentsÂ Â Â Â Â Â Â Â Â  | select max PK | select min by index | select min by full scan | update by PK | update by full scan |
+-------------------------------------------------------------+-------+---------------------------+---------------+---------------------+-------------------------+--------------+---------------------+
| ROW_FORMAT=COMPACTÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  | 10.92 |Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  | <span style="color: #008000;">Instant</span>Â Â Â Â Â Â  | <span style="color: #008000;">Instant</span>Â Â Â Â Â Â Â Â Â Â Â Â  | <span style="color: #008000;">Instant</span>Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  | <span style="color: #800000;">blocked</span>Â Â Â Â Â  | <span style="color: #800000;">blocked</span>Â Â Â Â Â Â Â Â Â Â Â Â  |
| AUTO_INCREMENT=16051Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  |Â  0.06 | Instant, no table rebuild | N/AÂ Â Â Â Â Â Â Â Â Â  | N/AÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  | N/AÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  | N/AÂ Â Â Â Â Â Â Â Â  | N/AÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  |
| ADD INDEX(last_update)Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  |Â  2.37 |Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  | <span style="color: #800000;">blocked</span>Â Â Â Â Â Â  | <span style="color: #800000;">blocked</span>Â Â Â Â Â Â Â Â Â Â Â Â  | <span style="color: #800000;">blocked</span>Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  | <span style="color: #800000;">blocked</span>Â Â Â Â Â  | <span style="color: #800000;">blocked</span>Â Â Â Â Â Â Â Â Â Â Â Â  |
| ADD INDEX(last_update), ALGORITHM=INPLACEÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  |Â  1.83 |Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  | <span style="color: #800000;">blocked</span>Â Â Â Â Â Â  | <span style="color: #800000;">blocked</span>Â Â Â Â Â Â Â Â Â Â Â Â  | <span style="color: #800000;">blocked</span>Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  | <span style="color: #800000;">blocked</span>Â Â Â Â Â  | <span style="color: #800000;">blocked</span>Â Â Â Â Â Â Â Â Â Â Â Â  |
| ADD INDEX(last_update), ALGORITHM=INPLACE, LOCK=NONEÂ Â Â Â Â Â Â  |Â  0.00 | ERROR 1235 (42000): ... Â  | N/AÂ Â Â Â Â Â Â Â Â Â  | N/AÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  | N/AÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  | N/AÂ Â Â Â Â Â Â Â Â  | N/AÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  |
| ADD COLUMN c CHAR(1) NOT NULLÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  | 11.20 |Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  | <span style="color: #008000;">Instant</span>Â Â Â Â Â Â  | <span style="color: #008000;">Instant</span>Â Â Â Â Â Â Â Â Â Â Â Â  | <span style="color: #008000;">Instant</span>Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  | <span style="color: #800000;">blocked</span>Â Â Â Â Â  | <span style="color: #800000;">blocked</span>Â Â Â Â Â Â Â Â Â Â Â Â  |
| ADD COLUMN c CHAR(1) NOT NULL, ALGORITHM=INPLACE, LOCK=NONE |Â  0.00 | ERROR 1235 (42000): .   Â  | N/AÂ Â Â Â Â Â Â Â Â Â  | N/AÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  | N/AÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  | N/AÂ Â Â Â Â Â Â Â Â  | N/AÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  |
+-------------------------------------------------------------+-------+---------------------------+---------------+---------------------+-------------------------+--------------+---------------------+</pre>
</blockquote>
<p>Rather surprising, I would say.</p>
<ul>
<li><em>None</em> of my tests resolved with online write (<strong>UPDATE</strong>). At best I could get online read (<strong>SEELCT</strong>).<br />
<strong></strong></li>
<li><strong>AUTO_INCREMENT</strong> is instantaneous. High time for that! It&#8217;s just some number in the <strong>.frm</strong> file, never understood the need for table rebuild.</li>
<li>Apparently <strong>ADD COLUMN</strong> is <em>more online</em> than <strong>ADD INDEX</strong>, and I&#8217;ve tested this again and again and again to make sure I was doing it right. This is quite weird, even according to the <a href="http://dev.mysql.com/doc/refman/5.6/en/innodb-online-ddl.html">docs</a>.</li>
<li>In none of the above tests (and others, non listed), have I been able to specify <strong>LOCK=NONE</strong>. It&#8217;s always <strong>ERROR 1235 (42000): This version of MySQL doesn&#8217;t yet support &#8216;alter table sakila.rental &lt;whatever&gt;, algorithm=inplace, lock=none&#8217;</strong>.</li>
</ul>
<p>So what&#8217;s so online about this? Online reads are nice, but most everyone cannot accept blocking writes (for same reason no one would use <em>mysqlhotcopy</em>, also so wrongly named). This leaves us again with <a href="http://openarkkit.googlecode.com/svn/trunk/openarkkit/doc/html/oak-online-alter-table.html">oak-online-alter-table</a> and <a href="http://www.percona.com/doc/percona-toolkit/2.1/pt-online-schema-change.html">pt-online-schema-change</a>.</p>
<h4>The butler did it</h4>
<p>Apologies to the butler, the <strong>FOREIGN KEY</strong>s did it. Let&#8217;s try the same again without foreign keys:</p>
<blockquote>
<pre>node1 5.6.8-rc-log sakila&gt; create table rental2 like rental;
node1 5.6.8-rc-log sakila&gt; insert into rental2 select * from rental;
node1 5.6.8-rc-log sakila&gt; rename table rental to rental_old, rental2 to rental;
Query OK, 0 rows affected (0.31 sec)</pre>
</blockquote>
<p>Here are the results:</p>
<blockquote>
<pre>+-------------------------------------------------------------+-------+---------------------------+----------------+---------------------+-------------------------+----------------+---------------------+
| ALTER statementÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  | Time  | General commentsÂ Â Â Â Â Â Â Â Â  | select max PKÂ  | select min by index | select min by full scan | update by PKÂ Â  | update by full scan |
+-------------------------------------------------------------+-------+---------------------------+----------------+---------------------+-------------------------+----------------+---------------------+
| ROW_FORMAT=COMPACTÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  | 11.03 |Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  | <span style="color: #008000;">Instant</span>Â Â Â Â Â Â Â  | <span style="color: #008000;">Instant</span>Â Â Â Â Â Â Â Â Â Â Â Â  | <span style="color: #008000;">Instant</span>Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  | <span style="color: #008000;">Instant</span>Â Â Â Â Â Â Â  | <span style="color: #008000;">Instant</span>Â Â Â Â Â Â Â Â Â Â Â Â  |
| AUTO_INCREMENT=16051Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  |Â  0.05 | Instant, no table rebuild | N/AÂ Â Â Â Â Â Â Â Â Â Â  | N/AÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  | N/AÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  | N/AÂ Â Â Â Â Â Â Â Â Â Â  | N/AÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  |
| ADD INDEX(last_update)Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  |Â  2.04 |Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  | * <span style="color: #ff6600;">Inconsistent</span> | * <span style="color: #ff6600;">Inconsistent</span>Â Â Â Â Â  | * <span style="color: #ff6600;">Inconsistent</span>Â Â Â Â Â Â Â Â Â  | <span style="color: #800000;">blocked</span>Â Â Â Â Â Â Â  | <span style="color: #800000;">blocked</span>Â Â Â Â Â Â Â Â Â Â Â Â  |
| ADD INDEX(last_update), ALGORITHM=INPLACE, LOCK=NONEÂ Â Â Â Â Â Â  |Â  3.14 |Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  | * <span style="color: #ff6600;">Inconsistent</span> | * <span style="color: #ff6600;">Inconsistent</span>Â Â Â Â Â  | * <span style="color: #ff6600;">Inconsistent</span>Â Â Â Â Â Â Â Â Â  | <span style="color: #800000;">blocked</span>Â Â Â Â Â Â Â  | <span style="color: #800000;">blocked</span>Â Â Â Â Â Â Â Â Â Â Â Â  |
| ADD COLUMN c CHAR(1) NOT NULLÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  |Â    ** |Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  | * <span style="color: #ff6600;">Inconsistent</span> | * <span style="color: #ff6600;">Inconsistent</span>Â Â Â Â Â  | * <span style="color: #ff6600;">Inconsistent</span>Â Â Â Â Â Â Â Â Â  | * <span style="color: #ff6600;">Inconsistent</span> | * <span style="color: #ff6600;">Inconsistent</span>Â Â Â Â Â  |
| ADD COLUMN c CHAR(1) NOT NULL, ALGORITHM=INPLACE, LOCK=NONE |Â Â Â  ** |Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  | * <span style="color: #ff6600;">Inconsistent</span> | * <span style="color: #ff6600;">Inconsistent</span>Â Â Â Â Â  | * <span style="color: #ff6600;">Inconsistent</span>Â Â Â Â Â Â Â Â Â  | * <span style="color: #ff6600;">Inconsistent</span> | * <span style="color: #ff6600;">Inconsistent</span>Â Â Â Â Â  |
+-------------------------------------------------------------+-------+---------------------------+----------------+---------------------+-------------------------+----------------+---------------------+</pre>
</blockquote>
<p>What&#8217;s going on here?</p>
<ul>
<li><strong>ALGORITHM=INPLACE, LOCK=NONE</strong> is accepted! Bad, bad foreign keys!<br />
<strong></strong></li>
<li><strong>* ADD INDEX</strong> usually allows for concurrent reads, but after repeated tests <strong>SELECT</strong>s start to block. Then they don&#8217;t work concurrently anymore until table is recreated. But even that not always, so I&#8217;m not sure what the inconsistency is.</li>
<li><strong>* ADD COLUMN</strong> is still more concurrent than <strong>ADD INDEX</strong>, and actually allows for concurrent writes! Though, inconsistently. Sometimes it does not allow for concurrent writes.</li>
<li><strong>** ADD COLUMN</strong> runtime highly affected by concurrent queries. It wents as high as <strong>45</strong> seconds on my laptop. Now, to make things clear, I&#8217;m not running an automated benchmark here: I&#8217;m copying+pasting the statements from my editor to the mysql CLI. So, maybe <strong>10</strong> or <strong>15</strong><strong>SELECT</strong> and <strong>UPDATE</strong> queries executes. How does that justify <strong>35</strong> seconds delay in table rebuild?</li>
</ul>
<h4>Some conclusions:</h4>
<ul>
<li>The documentation does not specify anything about <strong>FOREIGN KEY</strong>s crashing the party. It should.</li>
<li>The documentation specifically mentions the <strong>ADD/DROP INDEX</strong> statements to be online. <strong>ADD INDEX</strong> is less online than <strong>ADD COLUMN</strong>.</li>
<li>Everything is still shaky. Sometimes things work, sometimes they don&#8217;t.</li>
<li>Runtimes are unproportionally affected by concurrent queries.</li>
<li>For the meantime, I keep to my online alter table scripts. Been using them for <strong>3.5</strong> years now.</li>
</ul>
]]></content:encoded>
							<wfw:commentRss>https://shlomi-noach.github.io/blog/mysql/state-of-inndb-online-ddl-in-mysql-5-6-8-rc/feed</wfw:commentRss>
		<slash:comments>1</slash:comments>
						<post-id xmlns="com-wordpress:feed-additions:1">5823</post-id>	</item>
		<item>
		<title>InnoDB DDL: kudos to quick responders on bugs.mysql.com</title>
		<link>https://shlomi-noach.github.io/blog/mysql/innodb-ddl-kudos-to-quick-responders-on-bugs-mysql-com</link>
				<comments>https://shlomi-noach.github.io/blog/mysql/innodb-ddl-kudos-to-quick-responders-on-bugs-mysql-com#comments</comments>
				<pubDate>Thu, 18 Oct 2012 16:55:29 +0000</pubDate>
		<dc:creator><![CDATA[shlomi]]></dc:creator>
				<category><![CDATA[MySQL]]></category>
		<category><![CDATA[InnoDB]]></category>
		<category><![CDATA[New Features]]></category>

		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=5696</guid>
				<description><![CDATA[Continuing my experiments with 5.6 InnoDB online DDL, a bug which I&#8217;ve opened, and another which I commented on were quickly answered and explained by the Oracle/MySQL team. On both accounts I&#8217;m happy to acknowledge the issue is resolved; in both cases I failed to produce a real bug scenario. Good lesson. Kudos for quick [&#8230;]]]></description>
								<content:encoded><![CDATA[<p>Continuing my <a href="https://shlomi-noach.github.io/blog/mysql/experimenting-with-5-6-innodb-online-ddl-bugs-included">experiments with 5.6 InnoDB online DDL</a>, a bug which I&#8217;ve opened, and another which I commented on were quickly answered and explained by the Oracle/MySQL team.</p>
<p>On both accounts I&#8217;m happy to acknowledge the issue is resolved; in both cases I failed to produce a real bug scenario. Good lesson. <em>Kudos for quick and informative responses!</em></p>
<p>What&#8217;s left of my experiment, then? Still a lot to check.</p>
<p>I am mainly still confused with which operations exactly can use <strong>LOCK=NONE</strong> (allowing for updated to table while <strong>ALTER</strong>ing). So far I am only able to produce <strong>ALTER</strong>s with <strong>LOCK=SHARED</strong>, meaning table is readable, but cannot be updated.</p>
<p>I will want to test speeds. I&#8217;ve so far been content with slow response times for queries over altered tables. How well will that endure under heavy load?</p>
]]></content:encoded>
							<wfw:commentRss>https://shlomi-noach.github.io/blog/mysql/innodb-ddl-kudos-to-quick-responders-on-bugs-mysql-com/feed</wfw:commentRss>
		<slash:comments>5</slash:comments>
						<post-id xmlns="com-wordpress:feed-additions:1">5696</post-id>	</item>
	</channel>
</rss>
