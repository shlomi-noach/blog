<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	
	xmlns:georss="http://www.georss.org/georss"
	xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#"
	>

<channel>
	<title>Backup &#8211; code.openark.org</title>
	<atom:link href="https://shlomi-noach.github.io/blog/tag/backup/feed" rel="self" type="application/rss+xml" />
	<link>http://shlomi-noach.github.io/blog/</link>
	<description>Blog by Shlomi Noach</description>
	<lastBuildDate>Mon, 13 Oct 2014 12:59:24 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=5.3.3</generator>
<site xmlns="com-wordpress:feed-additions:1">32412571</site>	<item>
		<title>Announcing orchestrator-agent</title>
		<link>https://shlomi-noach.github.io/blog/mysql/announcing-orchestrator-agent</link>
				<comments>https://shlomi-noach.github.io/blog/mysql/announcing-orchestrator-agent#comments</comments>
				<pubDate>Mon, 13 Oct 2014 12:59:24 +0000</pubDate>
		<dc:creator><![CDATA[shlomi]]></dc:creator>
				<category><![CDATA[MySQL]]></category>
		<category><![CDATA[Backup]]></category>
		<category><![CDATA[Open Source]]></category>
		<category><![CDATA[orchestrator]]></category>
		<category><![CDATA[Replication]]></category>

		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=7019</guid>
				<description><![CDATA[orchestrator-agent is a side-kick, complementary project of orchestrator, implementing a daemon service on one&#8217;s MySQL hosts which communicates with and accepts commands from orchestrator, built with the original purpose of providing an automated solution for provisioning new or corrupted slaves. It was built by Outbrain, with Outbrain&#8217;s specific use case in mind. While we release [&#8230;]]]></description>
								<content:encoded><![CDATA[<p><strong><a href="https://github.com/outbrain/orchestrator-agent">orchestrator-agent</a></strong> is a side-kick, complementary project of <strong><a href="https://github.com/outbrain/orchestrator">orchestrator</a></strong>, implementing a daemon service on one&#8217;s MySQL hosts which communicates with and accepts commands from <em>orchestrator</em>, built with the original purpose of providing an automated solution for provisioning new or corrupted slaves.</p>
<p>It was built by <a href="https://github.com/outbrain">Outbrain</a>, with Outbrain&#8217;s specific use case in mind. While we release it as open source, only a small part of its functionality will appeal to the public (this is why it&#8217;s not strictly part of the <em>orchestrator</em> project, which is a general purpose, wide-audience solution). Nevertheless, it is a simple implementation of a daemon, such that can be easily extended by the community. The project is open for pull-requests!</p>
<p>A quick breakdown of <em>orchestrator-agent</em> is as follows:</p>
<ul>
<li>Executes as a daemon on linux hosts</li>
<li>Interacts and invokes OS commands (via <em>bash</em>)</li>
<li>Does not directly interact with a MySQL server running on that host (does not connect via mysql credentials)</li>
<li>Expects a single MySQL service on host</li>
<li>Can control the MySQL service (e.g. stop, start)</li>
<li>Is familiar with LVM layer on host</li>
<li>Can take LVM snapshots, mount snapshots, remove snapshots</li>
<li>Is familiar with the MySQL data directory, disk usage, file system</li>
<li>Can send snapshot data from a mounted snapshot on a running MySQL host</li>
<li>Can prepare data directory and receive snapshot data from another host</li>
<li>Recognizes local/remote datacenters</li>
<li>Controlled by <em>orchestrator,</em> two <em>orchestrator-agents</em> implement an automated and audited solution for seeding a new/corrupted MySQL host based on a running server.</li>
</ul>
<p><span id="more-7019"></span>Offline, hard-copy backups aside, Outbrain implements quick backups via LVM snapshots. Some slaves (depending on strength, datacenter etc.) take the role of <em>snapshot servers</em>. These slaves take an LVM snapshot once per day, and <em>keep it open</em> unmounted. At any given time we might have multiple open snapshots on a snapshot server. At a few minutes notice, we are able to restore MySQL to yesterday&#8217;s, the day before, two days before, &#8230; state. This only takes mounting of the desired snapshot and starting MySQL over the mounted volume.</p>
<p>We&#8217;ve in fact put this method to practice in at least one major occasion last year, that proved it to be of significant worth: we were able to fix a compromise to some dataset within minutes of action.</p>
<p>But we also use these snapshots on a daily basis, putting them constantly to practice (hence validating this backup method routinely): we provision new MySQL servers based on those snapshots. A new or corrupted server can be seeded with a snapshot data (preferably from a host in same datacenter). We don&#8217;t need to perform a backup <em>now</em>. We can take the one from <em>tonight</em>.</p>
<p>There is some fuss around this: need to verify MySQL not running on target host, clean up the target host, choose the best source host, choose the best snapshot on source host, mount it, deliver the data, accept the data, do post-copy cleanup, start the MySQL service on target host, &#8230; Or we might choose to actually do take a fresh, immediate snapshot from a server not configured as <em>snapshot server</em>&#8230; We used to do all these by shell scripts, but realized this will not sustain us.</p>
<p><em>Orchestrator-agent</em> was invented with this particular use case in mind, and <em>orchestrator</em> was extended to support these operations. <em>Orchestrator</em> can act as the controller of a seed/provisioning operation, getting info from/to <em>orchestrator-agent</em> and coordinating two agents to send/receive the data.</p>
<p>The agent still relies on some external commands (via <em>bash</em>) to implement some functionality; to some extent, we have broken down complex shell scripts then wrapped again, now controlled and audited by the agent; but this will slowly change as we move more code into <em>orchestrator-agent</em> itself.</p>
<h4>Should you use orchestrator-agent?</h4>
<p>If, like us, you use LVM snapshots, at least as a partial form of backup &#8211; bingo!</p>
<p>Otherwise, <em>orchestrator-agent</em> can assist in stopping/starting the MySQL service, tailing the error log; simple stuff like that &#8212; probably too much fuss for such basic operations.</p>
<p>But it can be easily extended to support other forms of backup (though do note that <em>orchestrator-agent</em> does not and should not perform scheduled backups, merely manipulate them); so if you like to contribute code into <em>orchestrator-agent</em>, please let us know! It would be nice to have a solution that works for multiple use cases.</p>
<h4>Get it</h4>
<p><em>orchestrator-agent</em> is released under the <a href="https://github.com/outbrain/orchestrator-agent/blob/master/LICENSE">Apache 2.0 license</a>, and is free to use.</p>
<p>Pre-compiled, <strong>RPM</strong> and <strong>deb</strong> packages <a href="https://github.com/outbrain/orchestrator-agent/releases">can be found on the releases page</a>.</p>
<p>&nbsp;</p>
]]></content:encoded>
							<wfw:commentRss>https://shlomi-noach.github.io/blog/mysql/announcing-orchestrator-agent/feed</wfw:commentRss>
		<slash:comments>1</slash:comments>
						<post-id xmlns="com-wordpress:feed-additions:1">7019</post-id>	</item>
		<item>
		<title>A new MySQL backups temperature scale, with showers</title>
		<link>https://shlomi-noach.github.io/blog/mysql/a-new-mysql-backups-temperature-scale-with-showers</link>
				<comments>https://shlomi-noach.github.io/blog/mysql/a-new-mysql-backups-temperature-scale-with-showers#comments</comments>
				<pubDate>Tue, 26 Jun 2012 06:08:15 +0000</pubDate>
		<dc:creator><![CDATA[shlomi]]></dc:creator>
				<category><![CDATA[MySQL]]></category>
		<category><![CDATA[Backup]]></category>

		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=4602</guid>
				<description><![CDATA[We&#8217;re used to Cold, Warm and Hot backups. This scale of three temperatures does not quite reflect the impact of backups on your MySQL database. In this post I offer a new backup temperature scale, and (somewhat seriously) compare it with showers. Call it the backup shower scale. A database backup is like a shower: [&#8230;]]]></description>
								<content:encoded><![CDATA[<p>We&#8217;re used to Cold, Warm and Hot backups. This scale of three temperatures does not quite reflect the impact of backups on your MySQL database.</p>
<p>In this post I offer a new backup temperature scale, and (somewhat seriously) compare it with showers. Call it the <em>backup shower scale</em>.</p>
<blockquote><p>A database backup is like a shower: the colder it is, the less time you want to spend doing it.</p></blockquote>
<h4>Cold</h4>
<p>A cold backup requires taking your database down (i.e. stop the service).</p>
<p>Example: file system copy</p>
<p>This can work well for replicating slaves, which may not be required for normal operation. You take the slave down, turn off the service, make your backup, turn everything on again, and let the backup catch up. Just make sure its master has all the necessary binary logs.</p>
<p>A friend was staying at my place and was lecturing me on the benefits of cold showers; how it was good for your health. A couple hours later, preparing for bed, I hear him exclaiming from the bathroom: <em>&#8220;Whaaaaa! There&#8217;s no hot water!&#8221;<span id="more-4602"></span></em></p>
<h4>Warm</h4>
<p>A backup which requires locking down your database as read-only.</p>
<p>Examples: MyDumper, mysqlhotcopy (MyISAM only), mysqldump (non-transactional)</p>
<p>Again, this may do well on slaves. As for working out a warm backup on the master: I don&#8217;t know. Is a mildly-warm shower really any better than a cold one?</p>
<h4>Not quite hot</h4>
<p>A backup which is supposedly non-interruptive to normal operation. You are allowed to keep writes to the database. But, things are not quite as they seem.</p>
<p>Example: mysqldump &#8211;single-transaction</p>
<p><em>mysqldump</em> can make a backup in an open transaction. With InnoDB this means a consistent snapshot of the data. But open transactions lead to locks. These accumulate. MVCC makes for changes unable to fall back into the baseline, waiting for the backup&#8217;s transaction to complete. Eventually, there are so many locks that your database is as good as dead. <em>mysqldump</em> doesn&#8217;t work well for very large databases under heavy load.</p>
<p>[<em>UPDATE</em>: As per Domas&#8217; clarification, MyDumper falls under this category, as well]</p>
<p>In terms of shower, you have reasonably hot water for some time, but they eventually run cold.</p>
<h4>Hot</h4>
<p>A hot backup is such that does not impose any constraints on the database itself. But it may impose load on other components, such as the disks, memory, CPU. This leads to an implicit impact on MySQL&#8217;s performance.</p>
<p>An LVM snapshot works this way. With LVM snapshots, writes to the database cause for blocks copy-on-write. The disk is busier. MySQL has to compete for disk use. An LVM backup works just well, but has a noticeable impact on the database&#8217;s ability to keep up as before.</p>
<p>You may have enough hot water for your shower, but there&#8217;s not enough pressure. The minute the dishwasher starts working, your shower turns to a sad dripping.</p>
<h4>Searing hot</h4>
<p>A searing hot backup is one that does not interfere with database operation, explicitly or implicitly, to a reasonable extent.</p>
<p>Xtrabackup with throttling is such a backup. The impact can be made low such that it is unnoticeable. You are willing to have the backup take longer time to complete.</p>
]]></content:encoded>
							<wfw:commentRss>https://shlomi-noach.github.io/blog/mysql/a-new-mysql-backups-temperature-scale-with-showers/feed</wfw:commentRss>
		<slash:comments>5</slash:comments>
						<post-id xmlns="com-wordpress:feed-additions:1">4602</post-id>	</item>
		<item>
		<title>Upgrading to Barracuda &#038; getting rid of huge ibdata1 file</title>
		<link>https://shlomi-noach.github.io/blog/mysql/upgrading-to-barracuda-getting-rid-of-huge-ibdata1-file</link>
				<comments>https://shlomi-noach.github.io/blog/mysql/upgrading-to-barracuda-getting-rid-of-huge-ibdata1-file#comments</comments>
				<pubDate>Tue, 15 Feb 2011 08:01:15 +0000</pubDate>
		<dc:creator><![CDATA[shlomi]]></dc:creator>
				<category><![CDATA[MySQL]]></category>
		<category><![CDATA[Backup]]></category>
		<category><![CDATA[Configuration]]></category>
		<category><![CDATA[InnoDB]]></category>
		<category><![CDATA[mysqldump]]></category>
		<category><![CDATA[Replication]]></category>

		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=3304</guid>
				<description><![CDATA[Some of this is old stuff, but more people are now converting to InnoDB plugin, so as to enjoy table compression, performance boosts. Same holds for people converting to Percona&#8217;s XtraDB. InnoDB plugin requires innodb_file_per_table. No more shared tablespace file. So your ibdata1 file is some 150GB, and it won&#8217;t reduce. Really, it won&#8217;t reduce. [&#8230;]]]></description>
								<content:encoded><![CDATA[<p>Some of this is old stuff, but more people are now converting to InnoDB plugin, so as to enjoy table compression, performance boosts. Same holds for people converting to Percona&#8217;s XtraDB. InnoDB plugin requires <strong>innodb_file_per_table</strong>. No more shared tablespace file.</p>
<p>So your <strong>ibdata1</strong> file is some <strong>150GB</strong>, and it won&#8217;t reduce. Really, it won&#8217;t reduce. You set <strong>innodb_file_per_table=1</strong>, do <strong>ALTER TABLE t ENGINE=InnoDB</strong> (optionally <strong>ROW_FORMAT=COMPRESSED KEY_BLOCK_SIZE=8</strong>), and you get all your tables in file-per-table <strong>.ibd</strong> files.</p>
<p>But the original <strong>ibdata1</strong> file is still there. It has to be there, don&#8217;t delete it! It contains more than your old data.</p>
<p>InnoDB tablespace files never reduce in size, it&#8217;s an old-time annoyance. The only way to go round it, if you need the space, is to completely drop them and start afresh. That&#8217;s one of the things so nice about file-per-table: an <strong>ALTER TABLE</strong> actually creates a new tablespace file and drops the original one.</p>
<h4>The procedure</h4>
<p>The procedure is somewhat painful:</p>
<ul>
<li>Dump everything logically (either use <em>mysqldump</em>, <a href="http://www.maatkit.org/doc/mk-parallel-dump.html">mk-parallel-dump</a>, or do it your own way)</li>
<li>Erase your data (literally, delete everything under your <strong>datadir</strong>)</li>
<li>Generate a new empty database</li>
<li>Load your dumped data.<span id="more-3304"></span></li>
</ul>
<h4>Using replication</h4>
<p>Replication makes this less painful. Set up a slave, have it follow up on the master.</p>
<ul>
<li>Stop your slave.</li>
<li>Make sure to backup the replication position (e.g. write <strong>SHOW SLAVE STATUS</strong> on a safe location, or copy <strong>master.info</strong> file).</li>
<li>Work out the dump-erase-generate-load steps on the slave.</li>
<li>Reattach the slave to the master using saved data.</li>
</ul>
<p>For this to succeed you must keep enough binary logs on the master for the entire dump-load period, which could be lengthy.</p>
<h4>Upgrading to barracuda</h4>
<p>If you wish to upgrade your InnoDB tables to <em>Barracuda</em> format, my advice is this:</p>
<ol>
<li>Follow the steps above to generate a file-per-table working slave</li>
<li>Stop the slave</li>
<li>Configure <strong>skip_slave_start</strong></li>
<li>Restart MySQL</li>
<li>One by one do the <strong>ALTER TABLE</strong> into <em>Barracuda</em> format (<strong>ROW_FORMAT=COMPACT</strong> or <strong>ROW_FORMAT=COMPRESSED</strong>)</li>
</ol>
<p>Note that if you&#8217;re about to do table compression, the <strong>ALTER</strong> statements become <em>considerably</em> slower the better the compression is.</p>
<p>If your dataset is very large, and you can&#8217;t keep so many binary logs, you may wish to break step <strong>5</strong> above into:</p>
<ul>
<li>ALTER a large table</li>
<li>Restart MySQL</li>
<li>Start slave, wait for it to catch up</li>
<li>Restart MySQL again</li>
</ul>
<p>and do the same for all large tables.</p>
<h4>Why all these restarts?</h4>
<p>I&#8217;ve been upgrading to Barracuda for a long time now. I have clearly noticed that <strong>ALTER</strong> into a <strong>COMPRESSED</strong> format works considerably slower after the slave has done some &#8220;real work&#8221;. This in particular relates to the last &#8220;renaming table&#8221; stage. There was a bug with earlier InnoDB plugin versions which made this stage hang. It was solved. But it still takes some time for this last, weird stage, where the new replacement table is complete, and it&#8217;s actually been renamed in place of the old table, and the old table renamed into something like &#8220;#sql-12345.ibd&#8221;, and all that needs to be done is have it dropped, and&#8230; Well, it takes time.</p>
<p>My observation is it works faster on a freshly started server. Which is why I take the bother to restart MySQL before each large table conversion.</p>
]]></content:encoded>
							<wfw:commentRss>https://shlomi-noach.github.io/blog/mysql/upgrading-to-barracuda-getting-rid-of-huge-ibdata1-file/feed</wfw:commentRss>
		<slash:comments>16</slash:comments>
						<post-id xmlns="com-wordpress:feed-additions:1">3304</post-id>	</item>
		<item>
		<title>An argument for using mysqldump</title>
		<link>https://shlomi-noach.github.io/blog/mysql/an-argument-for-using-mysqldump</link>
				<comments>https://shlomi-noach.github.io/blog/mysql/an-argument-for-using-mysqldump#comments</comments>
				<pubDate>Tue, 09 Nov 2010 04:29:54 +0000</pubDate>
		<dc:creator><![CDATA[shlomi]]></dc:creator>
				<category><![CDATA[MySQL]]></category>
		<category><![CDATA[Backup]]></category>
		<category><![CDATA[mysqldump]]></category>

		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=3080</guid>
				<description><![CDATA[I fully agree with Morgan&#8217;s An argument for not using mysqldump. This post does not come to contradict it, but rather shed a positive light on mysqldump. I usually prefer an LVM snapshot based backup, or using XtraBackup. And, with databases as large as dozens of GB and above, I consider mysqldump to be a [&#8230;]]]></description>
								<content:encoded><![CDATA[<p>I fully agree with Morgan&#8217;s <a href="http://www.mysqlperformanceblog.com/2010/11/08/an-argument-for-not-using-mysqldump-in-production/">An argument for not using mysqldump</a>. This post does not come to contradict it, but rather shed a positive light on <em>mysqldump</em>.</p>
<p>I usually prefer an LVM snapshot based backup, or using XtraBackup. And, with databases as large as dozens of GB and above, I consider <em>mysqldump</em> to be a poor alternative. Poor in runtime, poor in overhead while taking the backup.</p>
<p>However once in a while I get to be reminded that <em>mysqldump just works</em>.</p>
<p>As a recent example, I had a server which was killed after an ALTER TABLE statement hanged forever (table already ALTERed, but old scheme never dropped). The old table data still hanged around the file system, but was not recognized by InnoDB. Trying out DISCARD TABLESPACE did not do the job, and eventually file was dropped.</p>
<p>So far, reasonable. InnoDB would complain about some table it never recognized in the first place, but all would work. That is, until backup was concerned. With <em>innobackup</em> or XtraBackup the restore would fail on some internal problem. LVM would work, but would only copy+paste the problem: <em>innobackup</em> would never again be able to be used on this database.<span id="more-3080"></span></p>
<p>It turned out a <strong>120GB</strong> InnoDB compressed data (roughly <strong>250GB</strong> uncompressed) would dump in <strong>&#8211;single-transaction</strong> in a matter of <strong>4</strong> hours and would restore in a matter of some <strong>20</strong> hours. A whole lot more than the <strong>3</strong> hours total it would take for an LVM backup for that database. But the data would load well; no missing tablespaces.</p>
<p>I&#8217;ve had similar incidents in the past. Not to mention the issue of compressing shared tablespace file.</p>
<p>There&#8217;s something about being able to say &#8220;<em>I&#8217;m not sure how long this is going to take; maybe a day or two. But in the end, we will have problems P1, P2 &amp; P3 resolved</em>&#8220;.</p>
<p>I like the <em>clean state</em> you get from a <em>mysqldump</em> restore.</p>
]]></content:encoded>
							<wfw:commentRss>https://shlomi-noach.github.io/blog/mysql/an-argument-for-using-mysqldump/feed</wfw:commentRss>
		<slash:comments>9</slash:comments>
						<post-id xmlns="com-wordpress:feed-additions:1">3080</post-id>	</item>
		<item>
		<title>mylvmbackup HOWTO: minimal privileges &#038; filesystem copy</title>
		<link>https://shlomi-noach.github.io/blog/mysql/mylvmbackup-howto-minimal-privileges-filesystem-copy</link>
				<comments>https://shlomi-noach.github.io/blog/mysql/mylvmbackup-howto-minimal-privileges-filesystem-copy#comments</comments>
				<pubDate>Tue, 17 Aug 2010 17:42:40 +0000</pubDate>
		<dc:creator><![CDATA[shlomi]]></dc:creator>
				<category><![CDATA[MySQL]]></category>
		<category><![CDATA[Backup]]></category>
		<category><![CDATA[Linux]]></category>
		<category><![CDATA[Open Source]]></category>
		<category><![CDATA[scripts]]></category>
		<category><![CDATA[Security]]></category>

		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=2839</guid>
				<description><![CDATA[This HOWTO discusses two (unrelated) issues with mylvmbackup: The minimal privileges required to take MySQL backups with mylvmbackup. Making (non compressed) file system copy of one&#8217;s data files. Minimal privileges Some just give mylvmbackup the root account, which is far too permissive. We now consider what the minimal requirements of mylvmbackup are. The queries mylvmbackup [&#8230;]]]></description>
								<content:encoded><![CDATA[<p>This HOWTO discusses two (unrelated) issues with <a href="http://www.lenzg.net/mylvmbackup/"><em>mylvmbackup</em></a>:</p>
<ul>
<li>The minimal privileges required to take MySQL backups with <em>mylvmbackup.</em></li>
<li>Making (non compressed) file system copy of one&#8217;s data files.</li>
</ul>
<h4>Minimal privileges</h4>
<p>Some just give <em>mylvmbackup</em> the root account, which is far too permissive. We now consider what the minimal requirements of <em>mylvmbackup</em> are.</p>
<p>The queries <em>mylvmbackup</em> issues are:</p>
<ul>
<li><strong>FLUSH TABLES</strong></li>
<li><strong>FLUSH TABLES WITH READ LOCK</strong></li>
<li><strong>SHOW MASTER STATUS</strong></li>
<li><strong>SHOW SLAVE STATUS</strong></li>
<li><strong>UNLOCK TABLES</strong></li>
</ul>
<p>Both <strong>SHOW MASTER STATUS</strong> &amp; <strong>SHOW SLAVE STATUS</strong> require either the <strong>SUPER</strong> or <strong>REPLICATION CLIENT</strong> privilege. Since <strong>SUPER</strong> is more powerful, we choose <strong>REPLICATION CLIENT</strong>.</p>
<p>The <strong>FLUSH TABLES</strong> * and <strong>UNLOCK TABLES</strong> require the <strong>RELOAD</strong> privilege.</p>
<p>However, we are not done yet. <em>mylvmbackup</em> connects to the <strong>mysql</strong> database, which means we must also have some privilege there, too. We choose the <strong>SELECT</strong> privilege.</p>
<p><span id="more-2839"></span>Finally, here are the commands to create a <em>mylvmbackup</em> user with minimal privileges:</p>
<blockquote>
<pre>CREATE USER 'mylvmbackup'@'localhost' IDENTIFIED BY '12345';
GRANT RELOAD, REPLICATION CLIENT ON *.* TO 'mylvmbackup'@'localhost';
GRANT SELECT ON mysql.* TO 'mylvmbackup'@'localhost';
</pre>
</blockquote>
<p>In the <strong>mylvmbackup.conf</strong> file, the correlating rows are:</p>
<blockquote>
<pre>[mysql]
user=mylvmbackup
password=12345
host=localhost
</pre>
</blockquote>
<h4>Filesystem copy</h4>
<p>By default, <em>mylvmbackup</em> creates a <strong>.tar.gz</strong> compressed backup file of your data. This is good if the reason you&#8217;re running <em>mylvmbackup</em> is to, well, make a backup. However, as with all backups, one may be making the backup so as to create a replication server. But in this case you don&#8217;t really want compressed data: you want the data extracted on the replication server, just as it is on the original host.</p>
<p><em>mylvmbackup</em> supports backing up the files using <em>rsync</em>.</p>
<p>To copy MySQL data to a remote host, configure the following in the mylvmbackup.conf file:</p>
<blockquote>
<pre>[fs]
backupdir=shlomi@backuphost:/data/backup/mysql
[misc]
backuptype=rsync
</pre>
</blockquote>
<p>You may be prompted to enter password, unless you have the user&#8217;s public key stored on the remote host.</p>
<p>Normally, <em>rsync</em> is considered as <strong>r</strong>emote-<strong>sync</strong>, but it also works on local file systems. If you have a remote directory mounted on your file system (e.g. with <em>nfs</em>), you can use the fact that <em>rsync</em> works just as well with local file systems:</p>
<blockquote>
<pre>[fs]
backupdir=/mnt/backup/mysql
[misc]
backuptype=rsync
</pre>
</blockquote>
<p>Voila! Your backup is complete.</p>
]]></content:encoded>
							<wfw:commentRss>https://shlomi-noach.github.io/blog/mysql/mylvmbackup-howto-minimal-privileges-filesystem-copy/feed</wfw:commentRss>
		<slash:comments>7</slash:comments>
						<post-id xmlns="com-wordpress:feed-additions:1">2839</post-id>	</item>
		<item>
		<title>Tips for taking MySQL backups using LVM</title>
		<link>https://shlomi-noach.github.io/blog/mysql/tips-for-taking-mysql-backups-using-lvm</link>
				<comments>https://shlomi-noach.github.io/blog/mysql/tips-for-taking-mysql-backups-using-lvm#comments</comments>
				<pubDate>Tue, 03 Aug 2010 06:45:29 +0000</pubDate>
		<dc:creator><![CDATA[shlomi]]></dc:creator>
				<category><![CDATA[MySQL]]></category>
		<category><![CDATA[Backup]]></category>
		<category><![CDATA[Linux]]></category>
		<category><![CDATA[scripts]]></category>

		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=2717</guid>
				<description><![CDATA[LVM uses copy-on-write to implement snapshots. Whenever you&#8217;re writing data to some page, LVM copies the original page (the way it looked like when the snapshot was taken) to the snapshot volume. The snapshot volume must be large enough to accommodate all pages written to for the duration of the snapshot&#8217;s lifetime. In other words, [&#8230;]]]></description>
								<content:encoded><![CDATA[<p>LVM uses copy-on-write to implement snapshots. Whenever you&#8217;re writing data to some page, LVM copies the original page (the way it looked like when the snapshot was taken) to the snapshot volume. The snapshot volume must be large enough to accommodate all pages written to for the duration of the snapshot&#8217;s lifetime. In other words, you must be able to copy the data somewhere outside (tape, NFS, rsync, etc.) in less time than it would take for the snapshot to fill up.</p>
<p>While LVM allows for hot backups of MySQL, it still poses an impact on the disks. An LVM snapshot backup may not go unnoticed by the MySQL users.</p>
<p>Some general guidelines for making life easier with LVM backups follow.</p>
<h4>Lighter, longer snapshots</h4>
<p>If you&#8217;re confident that you have enough space on your snapshot volume, you may take the opportunity to make for a <em>longer</em> backup time. Why? Because you would then be able to reduce the stress from the file system. Use <strong>ionice</strong> when copying your data from the snapshot volume:</p>
<blockquote>
<pre>ionice -c 2 cp -R /mnt/mysql_snapshot /mnt/backup/daily/20100719/
</pre>
</blockquote>
<p><em>[Update: this is only on the cfq I/O scheduler; thanks, Vojtech]</em></p>
<h4>Are you running out of space?</h4>
<p>Monitor snapshot&#8217;s allocated size: if there&#8217;s just one snapshot, do it like this:<span id="more-2717"></span></p>
<blockquote>
<pre>lvdisplay | grep Allocated                                                                                                                  Mon Jul 19 09:51:29 2010

 Allocated to snapshot  3.63%
</pre>
</blockquote>
<p>Don&#8217;t let it reach 100%.</p>
<h4>Avoid running out of space</h4>
<p>To make sure you don&#8217;t run out of snapshot allocated size, stop all administrative scripts.</p>
<ul>
<li>Are you running your weekly purging of old data? You will be writing a lot of pages, and all will have to fit in the snapshot.</li>
<li>Building your reports? You may be creating large temporary tables; make sure these are not on the snapshot volume.</li>
<li>Rebuilding your Sphinx fulltext index? Make sure it is not on the snapshot volume, or postpone till after backup.</li>
</ul>
<p>You will gain not only snapshot space, but also faster backups.</p>
<h4>Someone did the job before you</h4>
<p>Use <a href="http://www.lenzg.net/mylvmbackup/">mylvmbackup</a>: the MySQL LVM backup script by Lenz Grimmer. Or do it manually: follow this old-yet-relevant <a href="http://www.mysqlperformanceblog.com/2006/08/21/using-lvm-for-mysql-backup-and-replication-setup/">post</a> by Peter Zaitsev.</p>
]]></content:encoded>
							<wfw:commentRss>https://shlomi-noach.github.io/blog/mysql/tips-for-taking-mysql-backups-using-lvm/feed</wfw:commentRss>
		<slash:comments>5</slash:comments>
						<post-id xmlns="com-wordpress:feed-additions:1">2717</post-id>	</item>
		<item>
		<title>A MyISAM backup is blocking as read-only, including mysqldump backup</title>
		<link>https://shlomi-noach.github.io/blog/mysql/a-myisam-backup-is-blocking-as-read-only-including-mysqldump-backup</link>
				<comments>https://shlomi-noach.github.io/blog/mysql/a-myisam-backup-is-blocking-as-read-only-including-mysqldump-backup#comments</comments>
				<pubDate>Tue, 18 May 2010 17:29:05 +0000</pubDate>
		<dc:creator><![CDATA[shlomi]]></dc:creator>
				<category><![CDATA[MySQL]]></category>
		<category><![CDATA[Backup]]></category>
		<category><![CDATA[InnoDB]]></category>
		<category><![CDATA[MyISAM]]></category>

		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=2441</guid>
				<description><![CDATA[Actually this is (almost) all I wanted to say. This is intentionally posted with all related keywords in title, in the hope that a related search on Google will result with this post on first page. I&#8217;m just still encountering companies who use MyISAM as their storage engine and are unaware that their nightly backup [&#8230;]]]></description>
								<content:encoded><![CDATA[<p>Actually this is (almost) all I wanted to say. This is intentionally posted with all related keywords in title, in the hope that a related search on Google will result with this post on first page.</p>
<p>I&#8217;m just still encountering companies who use MyISAM as their storage engine and are <em>unaware</em> that their nightly backup actually blocks their application, basically rendering their product unavailable for long minutes to hours on a nightly basis.</p>
<p>So this is posted as a warning for those who were not aware of this fact.</p>
<p>There is no hot (non blocking) backup for MyISAM. Closest would be file system snapshot, but even this requires flushing of tables, which may take a while to complete. If you must have a hot backup, then either use replication &#8211; and take the risk of the slave not being in complete sync with the master &#8211; or use another storage engine, i.e. InnoDB.</p>
]]></content:encoded>
							<wfw:commentRss>https://shlomi-noach.github.io/blog/mysql/a-myisam-backup-is-blocking-as-read-only-including-mysqldump-backup/feed</wfw:commentRss>
		<slash:comments>7</slash:comments>
						<post-id xmlns="com-wordpress:feed-additions:1">2441</post-id>	</item>
		<item>
		<title>Defined your MySQL backup &#038; recovery plan recently?</title>
		<link>https://shlomi-noach.github.io/blog/mysql/defined-your-mysql-backup-recovery-plan-recently</link>
				<comments>https://shlomi-noach.github.io/blog/mysql/defined-your-mysql-backup-recovery-plan-recently#respond</comments>
				<pubDate>Wed, 17 Feb 2010 09:52:00 +0000</pubDate>
		<dc:creator><![CDATA[shlomi]]></dc:creator>
				<category><![CDATA[MySQL]]></category>
		<category><![CDATA[Backup]]></category>
		<category><![CDATA[Replication]]></category>

		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=1976</guid>
				<description><![CDATA[Following up on Ronald Bradford&#8216;s Checked your MySQL recovery process recently? post, I wish to add a prequel. To see whether you have a clear definition of your backup requirements, ask yourself these questions: Is there a backup/restore plan? Is there a written backup/restore plan? How fast do you need to recover a backup? What&#8217;s [&#8230;]]]></description>
								<content:encoded><![CDATA[<p>Following up on <a href="http://ronaldbradford.com/">Ronald Bradford</a>&#8216;s <a href="http://ronaldbradford.com/blog/checked-your-mysql-recovery-process-recently-2010-02-15/">Checked your MySQL recovery process recently?</a> post, I wish to add a <em>prequel</em>.</p>
<p>To see whether you have a clear definition of your backup requirements, ask yourself these questions:</p>
<ul>
<li>Is there a backup/restore plan?</li>
<li>Is there a written backup/restore plan?</li>
<li>How fast do you need to recover a backup? What&#8217;s the longest downtime you will allow from the point of failure to the point of restored, functional database?</li>
<li>How much data are you willing to lose in case of crash? A second&#8217;s worth of data? An hour&#8217;s worth? A day&#8217;s worth? None?</li>
<li>Are you willing to allow that the database becomes read-only when taking the backup? Or completely down?</li>
<li>Are you willing to take the risk that the backup will not be 100% compatible with the data? (Backing up your slave holds this risk)</li>
<li>Is your manager willing to all that you are willing?</li>
<li>Is the backup plan known to the management team, or do they just know that &#8220;<em>the database has backups</em>&#8220;?</li>
</ul>
<p>The above checklist is by no means complete.</p>
<p>I have a vivid memory of a very good sys admin who failed on the last two points. He had some very sour days when recovering a lost file from tape took much longer than was affordable to some contract.</p>
<p>I found that technical people rarely share the same views as marketing/management. Make sure to consult with the management team; they will have a clearer view on what the company can afford and what it cannot afford.</p>
]]></content:encoded>
							<wfw:commentRss>https://shlomi-noach.github.io/blog/mysql/defined-your-mysql-backup-recovery-plan-recently/feed</wfw:commentRss>
		<slash:comments>0</slash:comments>
						<post-id xmlns="com-wordpress:feed-additions:1">1976</post-id>	</item>
		<item>
		<title>On restoring a single table from mysqldump</title>
		<link>https://shlomi-noach.github.io/blog/mysql/on-restoring-a-single-table-from-mysqldump</link>
				<comments>https://shlomi-noach.github.io/blog/mysql/on-restoring-a-single-table-from-mysqldump#comments</comments>
				<pubDate>Tue, 01 Dec 2009 08:25:00 +0000</pubDate>
		<dc:creator><![CDATA[shlomi]]></dc:creator>
				<category><![CDATA[MySQL]]></category>
		<category><![CDATA[Backup]]></category>
		<category><![CDATA[Books]]></category>
		<category><![CDATA[mysqldump]]></category>
		<category><![CDATA[Performance]]></category>
		<category><![CDATA[scripts]]></category>

		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=1630</guid>
				<description><![CDATA[Following Restore one table from an ALL database dump and Restore a Single Table From mysqldump, I would like to add my own thoughts and comments on the subject. I also wish to note performance issues with the two suggested solutions, and offer improvements. Problem relevance While the problem is interesting, I just want to [&#8230;]]]></description>
								<content:encoded><![CDATA[<p>Following <a href="http://everythingmysql.ning.com/profiles/blogs/restore-one-table-from-an-all">Restore one table from an ALL database dump</a> and <a href="http://gtowey.blogspot.com/2009/11/restore-single-table-from-mysqldump.html">Restore a Single Table From mysqldump</a>, I would like to add my own thoughts and comments on the subject.</p>
<p>I also wish to note performance issues with the two suggested solutions, and offer improvements.</p>
<h4>Problem relevance</h4>
<p>While the problem is interesting, I just want to note that it is relevant in very specific database dimensions. Too small &#8211; and it doesn&#8217;t matter how you solve it (e.g. just open vi/emacs and copy+paste). Too big &#8211; and it would not be worthwhile to restore from <em>mysqldump</em> anyway. I would suggest that the problem is interesting in the whereabouts of a few dozen GB worth of data.</p>
<h4>Problem recap</h4>
<p>Given a dump file (generated by mysqldump), how do you restore a single table, without making any changes to other tables?</p>
<p>Let&#8217;s review the two referenced solutions. I&#8217;ll be using the <a href="http://dev.mysql.com/doc/employee/en/employee.html">employees db</a> on <a href="https://launchpad.net/mysql-sandbox">mysql-sandbox</a> for testing. I&#8217;ll choose a very small table to restore: <strong>departments</strong> (only a few rows in this table).</p>
<h4>Security based solution</h4>
<p><a href="http://everythingmysql.ning.com/profiles/blogs/restore-one-table-from-an-all"><strong>Chris</strong></a> offers to create a special purpose account, which will only have write (CREATE, INSERT, etc.) privileges on the particular table to restore. Cool hack! But, I&#8217;m afraid, not too efficient, for two reasons:<span id="more-1630"></span></p>
<ol>
<li>MySQL needs to process all irrelevant queries (ALTER, INSERT, &#8230;) only to disallow them due to access violation errors.</li>
<li>Assuming restore is from remote host, we overload the network with all said irrelevant queries.</li>
</ol>
<p>Just how inefficient? Let&#8217;s time it:</p>
<blockquote>
<pre>mysql&gt; grant usage on *.* to 'restoreuser'@'localhost';
mysql&gt; grant select on *.* to 'restoreuser'@'localhost';
mysql&gt; grant all on employees.departments to 'restoreuser'@'localhost';

$ time mysql --user=restoreuser --socket=/tmp/mysql_sandbox21701.sock --force employees &lt; /tmp/employees.sql
...
ERROR 1142 (42000) at line 343: INSERT command denied to user 'restoreuser'@'localhost' for table 'titles'
ERROR 1142 (42000) at line 344: ALTER command denied to user 'restoreuser'@'localhost' for table 'titles'
...
(lot's of these messages)
...

real    <strong>0m31.945s</strong>
user    0m6.328s
sys     0m0.508s</pre>
</blockquote>
<p>So, at about <strong>30</strong> seconds to restore a 9 rows table.</p>
<h4>Text filtering based solution.</h4>
<p><a href="http://gtowey.blogspot.com/2009/11/restore-single-table-from-mysqldump.html"><strong>gtowey</strong></a> offers parsing the dump file beforehand:</p>
<ul>
<li>First, parse with <em>grep</em>, to detect rows where tables are referenced within dump file</li>
<li>Second, parse with <em>sed</em>, extracting relevant rows.</li>
</ul>
<p>Let&#8217;s time this one:</p>
<blockquote>
<pre>$ time grep -n 'Table structure' /tmp/employees.sql
23:-- Table structure for table `departments`
48:-- Table structure for table `dept_emp`
89:-- Table structure for table `dept_manager`
117:-- Table structure for table `employees`
161:-- Table structure for table `salaries`
301:-- Table structure for table `titles`

real    <strong>0m0.397s</strong>
user    0m0.232s
sys     0m0.164s

$ time sed -n 23,48p /tmp/employees.sql | ./use employees

real    <strong>0m0.562s</strong>
user    0m0.380s
sys     0m0.176s</pre>
</blockquote>
<p>Much faster: about <strong>1</strong> second, compared to <strong>30</strong> seconds from above.</p>
<p>Nevertheless, I find two issues here:</p>
<ol>
<li>A correctness problem: this solution somewhat assumes that there&#8217;s only a single table with desired name. I say &#8220;somewhat&#8221; since it leaves this for the user.</li>
<li>An efficiency problem: it reads the dump file <em>twice</em>. First parsing it with <em>grep</em>, then with <em>sed</em>.</li>
</ol>
<h4>A third solution</h4>
<p><em>sed</em> is much stronger than presented. In fact, the inquiry made by <em>grep</em> in gtowey&#8217;s solution can be easily handled by <em>sed</em>:</p>
<blockquote>
<pre>$ time sed -n "/^-- Table structure for table \`departments\`/,/^-- Table structure for table/p" /tmp/employees.sql | ./use employees

real    <strong>0m0.573s</strong>
user    0m0.416s
sys     0m0.152s</pre>
</blockquote>
<p>So, the <strong>&#8220;/^&#8211; Table structure for table \`departments\`/,/^&#8211; Table structure for table/p&#8221;</strong> part tells <em>sed</em> to only print those rows starting from the <strong>departments</strong> table structure, and ending in the next table structure (this is for clarity: had department been the last table, there would not be a next table, but we could nevertheless solve this using other anchors).</p>
<p>And, we only do it in <strong>0.57</strong> seconds: about half the time of previous attempt.</p>
<p>Now, just to be more correct, we only wish to consider the <strong>employees.department</strong> table. So, <em>assuming</em> there&#8217;s more than one database dumped (and, by consequence, <strong>USE</strong> statements in the dump-file), we use:</p>
<blockquote>
<pre>cat /tmp/employees.sql | sed -n "/^USE \`employees\`/,/^USE \`/p" | sed -n "/^-- Table structure for table \`departments\`/,/^-- Table structure for table/p" | ./use employees</pre>
</blockquote>
<h4>Further notes</h4>
<ul>
<li>All tests used warmed-up caches.</li>
<li>The sharp eyed readers would notice that <strong>departments</strong> is the first table in the dump file. Would that give an unfair advantage to the parsing-based restore methods? The answer is no. I&#8217;ve created an <strong>xdepartments</strong> table, to be located at the end of the dump. The difference in time is neglectful and inconclusive; we&#8217;re still at ~0.58-0.59 seconds. The effect will be more visible on really large dumps; but then, so would the security-based effects.</li>
</ul>
<p>[<strong>UPDATE</strong>: see also following similar post: <a href="http://blog.tsheets.com/2008/tips-tricks/extract-a-single-table-from-a-mysqldump-file.html">Extract a Single Table from a mysqldump File</a>]</p>
<h4>Conclusion</h4>
<p><a href="http://www.amazon.com/Classic-Shell-Scripting-Arnold-Robbins/dp/0596005954/ref=sr_1_1"><img class="alignright" title="classic-shell-scripting" src="https://shlomi-noach.github.io/blog/wp-content/uploads/2009/12/classic-shell-scripting.png" alt="classic-shell-scripting" width="144" height="189" /></a>Its is always best to test on large datasets, to get a feel on performance.</p>
<p>It&#8217;s best to save MySQL the trouble of parsing &amp; ignoring statements. Scripting utilities like <em>sed</em>, <em>awk</em> &amp; <em>grep</em> have been around for ages, and are well optimized. They excel at text processing.</p>
<p>I&#8217;ve used <em>sed</em> many times in transforming dump outputs; for example, in converting MyISAM to InnoDB tables; to convert Antelope InnoDB tables to Barracuda format, etc. grep &amp; awk are also very useful.</p>
<p>May I recommend, at this point, reading <a href="http://www.amazon.com/Classic-Shell-Scripting-Arnold-Robbins/dp/0596005954/ref=sr_1_1">Classic Shell Scripting</a>, a very easy to follow book, which lists the most popular command line utilities like <em>grep</em>, <em>sed</em>, <em>awk</em>, <em>sort</em>, (countless more) and shell scripting in general. While most of these utilities are well known, the book excels in providing suprisingly practical, simple solution to common tasks.</p>
]]></content:encoded>
							<wfw:commentRss>https://shlomi-noach.github.io/blog/mysql/on-restoring-a-single-table-from-mysqldump/feed</wfw:commentRss>
		<slash:comments>14</slash:comments>
						<post-id xmlns="com-wordpress:feed-additions:1">1630</post-id>	</item>
		<item>
		<title>Parameters to use on mysqldump</title>
		<link>https://shlomi-noach.github.io/blog/mysql/parameters-to-use-on-mysqldump</link>
				<comments>https://shlomi-noach.github.io/blog/mysql/parameters-to-use-on-mysqldump#comments</comments>
				<pubDate>Mon, 13 Oct 2008 07:03:50 +0000</pubDate>
		<dc:creator><![CDATA[shlomi]]></dc:creator>
				<category><![CDATA[MySQL]]></category>
		<category><![CDATA[Backup]]></category>
		<category><![CDATA[mysqldump]]></category>
		<category><![CDATA[Replication]]></category>

		<guid isPermaLink="false">https://shlomi-noach.github.io/blog/?p=4</guid>
				<description><![CDATA[mysqldump is commonly used for making a MySQL database backup or for setting up a replication. As in all mysql binaries, there are quite a few parameters to mysqldump. Some are just niceties but some flags are a must. Of course, choosing the parameters to use greatly depends on your requirements, database setup, network capacity [&#8230;]]]></description>
								<content:encoded><![CDATA[<p>mysqldump is commonly used for making a MySQL database backup or for setting up a replication.</p>
<p>As in all mysql binaries, there are quite a few parameters to mysqldump. Some are just niceties but some flags are a must. Of course, choosing the parameters to use greatly depends on your requirements, database setup, network capacity etc.</p>
<p>Here is my usual setup for mysqldump. The parameters below apply for an InnoDB based schema (no MyISAM, Memory tables). Parameters can be specified on the command line, or under the <code>[mysqld]</code> scope in the MySQL configuration file.</p>
<blockquote>
<p style="text-align: left;"><code>mysqldump -u dump_user -p -h db_host --routines --master-data --single-transaction  --skip-add-locks --skip-lock-tables --default-character-set=utf8 --compress my_db</code></p>
</blockquote>
<p>Let&#8217;s review these parameters and see their effect:<span id="more-4"></span></p>
<ul>
<li><code>-u</code> or <code>--user</code>: This is the user which initiates the dump. Depending on other parameters, the user may need to have quite a few privileges, such as <code>SELECT</code>, <code>RELOAD</code>, <code>FILE</code>, <code>REPLICATION CLIENT</code> etc. Since I do not usually allow for remote root access into mysql, I create a temporary user solely for the purpose of the dump (many times it&#8217;s a one-time action), for the specific machine from which the dump is run, and provide this user with all necessary permissions.</li>
<li><code>-h</code> or <code>--host</code>: I try not to dump from the same machine on which MySQL is running. If I do, I prefer to dump into a different disk from that on which the data and log files reside. The dump itself may create a heavy load on the machine (setting locks, performing lots of non cached IO operations). Since the target of the dump is mostly to create a backup on another machine, or set up replication on another machine, the dump has better not run from the MySQL machine.</li>
<li><code>--routines</code>: It is really an annoyance to have to remember this flag. In contrast to &#8211;triggers, which is by default TRUE, the <code>--routines</code> parameter is by default FALSE, which means if you forget it &#8211; you don&#8217;t get the stored functions and procedures in your schema.</li>
<li><code>--master-data</code>: I always enable binary logs on the MySQL nodes I work on. While binary logs may lead to more IO operations (writing binary logs make for more disk writes, obviously, but also disable some InnoDB optimizations), may consume more disk space (once I&#8217;ve worked with a company which had such a burst of traffic, that the binary logs to completely filled their disk in less than one day). If binary logs are enabled, the <code>--master-data</code> parameter allows for easy replication setup: the dump includes the <code>CHANGE MASTER TO MASTER_LOG_FILE='...', MASTER_LOG_POS=...</code> statement, so no need to do stuff like <code>SHOW MASTER STATUS</code> on the dumped node. Optionally, you can set <code>--master-data=2</code> to have the statement commented.</li>
<li><code>--single-transaction</code> <code>--skip-add-locks</code> <code>--skip-lock-tables</code>: When working with transactional-only storage engines (InnoDB is the most popular choice, but new engines are coming: Falcon, PBXT, Transactional-Maria, SolidDB and more), these parameters allow for a non-interruptive backup, which does not place read locks on all tables. It is possible to keep on reading and writing to the database while mysqldump is running with single transaction. Running in this mode does have its penalty: more IO operations (due to MVCC&#8217;s duplication of data while many transactions access the same data for Read/Write). The server is likely to perform more slowly during the dump time.</li>
<li><code>--default-character-set=utf8</code>: I&#8217;ve seen so many MySQL installations in which world-wide textual data was stored in the Latin1 charset than I can remember. Many developers, who are testing using standard English data, are not even aware of the issues arrising from changing the data later on to utf8. But even those who are, are usually unaware of the necessity to configure the character set on a per connection basis, or for their specific clients (JDBC or PHP connectors, etc). mysqldump is no different, and if you have non-latin text in your tables, always remember to set this option.</li>
<li><code>--compress</code>: when dumping to another machine, especially a remote one, using this option to GZIP the data between the MySQL server and the mysqldump client. This will make for more CPU operations, but CPU is usually cheap nowdays, and the compression may well save you hours of network transfer time.</li>
</ul>
]]></content:encoded>
							<wfw:commentRss>https://shlomi-noach.github.io/blog/mysql/parameters-to-use-on-mysqldump/feed</wfw:commentRss>
		<slash:comments>10</slash:comments>
						<post-id xmlns="com-wordpress:feed-additions:1">4</post-id>	</item>
	</channel>
</rss>
